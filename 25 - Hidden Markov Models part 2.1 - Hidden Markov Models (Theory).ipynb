{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extend the basic idea of Markov Models to Hidden Markov Models\n",
    "\n",
    "we encountered the concept of latent/hidden variables before (Kmeans,GMMs) , the name \"Hidden Markov Models\" implies that this concept is central to HMMs\n",
    "\n",
    "The basic idea is that something is going on besided what we can see/observe/measure\n",
    "\n",
    "what we observe is usually stochastic/random , since if it was determinisitc we could have predicted it without doing any machine learning at all\n",
    "\n",
    "The assumption that we make when there are hidden/latent variables is that there is some cause behind the scenes thats leading to the observations that we see\n",
    "\n",
    "In HMMs the hidden cause is in itself stochastic , a random process , a markov chain !\n",
    "\n",
    "---\n",
    "\n",
    "One example is geneteics , our physical features are the manifestation of some biological code , now that it is readable its not hidden in the sense that we can measure it , but of course there was a time that we could not , even so people still use HMMs to model how genes map to observable attributes\n",
    "\n",
    "Another example is speech-to-text , a computer cant read the words we are trying to say , but it can use an internal language model , a model of likely sequences of hidden states , to try to match those to the sounds that it hears , in this example , what is observed is the sound signal , and the latent variables are the sentence/phrase that we are saying\n",
    "\n",
    "---\n",
    "\n",
    "so how do we go from MM to HMMs , lets take an example\n",
    "\n",
    "Suppose we are in a carnival and a magician has 2 coins hidden coins\n",
    "\n",
    "He will chose one coin to flip at random , all what we get to see is the result of a coin flip H/T\n",
    "\n",
    "since we can see the result of the coin toss H/T are our observable variables , the hidden states are which coung the magicain chose to flip , we cant see this so its hidden\n",
    "\n",
    "This is called a stochastic/random process since it is a sequence of random variables\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Define HMM</h4>\n",
    "    \n",
    "An HMM has three parts $\\pi$,A,B:\n",
    "\n",
    "<ul>\n",
    "    <li>$\\pi$ : initial distribution\n",
    "        <ul>\n",
    "            <li>$\\pi_i$ = probability of starting at state i</li>\n",
    "            <li>suppose magician really likes coin 1 , $\\pi_1 = 0.9$ </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "            <br>\n",
    "    <li>$A$ : state transitions\n",
    "        <ul>\n",
    "            <li>$A_{i,j}$ = probability of going from state i to state j</li>\n",
    "            <li>In HMMs , the states are hidden , so A corresponds to transitionining from one hidden state to another hidden state</li>\n",
    "            <li>Suppose magician gets bored quickly , $A_{1,2} = 0.9$ , $A_{2,1} = 0.9$ so the probability of going from one coin to another is 0.9 , and the probability of staying at the same coin is 0.1</li> \n",
    "        </ul>\n",
    "    </li>\n",
    "            <br>\n",
    "        <li>$B$ : Emission/Observation Probabilities\n",
    "        <ul>\n",
    "            <li>$B_{j,k}$ = probability of observing symbol k in state j</li> \n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Independance Assumptions</h4>\n",
    "\n",
    "In HMMs , we make more independance assumptions that hte markov assumptions\n",
    "\n",
    "Remember that the markov assumption is that the current state depends only on the previous state but is independant on any state before previous state\n",
    "\n",
    "Now that we have both observed and hidden variables in our model , we have another independance assumption\n",
    "\n",
    "observed k depends only on state j , so the observation at time t , depends only on the state at time t , but not on any other time or any other state or any other observation\n",
    "\n",
    "---\n",
    "\n",
    "With Markov models there were two main things we can do , Get probability of a sequence and Train the model\n",
    "\n",
    "With HMMs , training would be harder since we would run up agains the numerical accuaracy of a computer (we will see later)\n",
    "\n",
    "There is also one more task with HMMs that we will go over , finding the most likely sequence of hidden states\n",
    "\n",
    "In speech-to-text for example , the sequence of hidden states would be the words that are being said "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take some examples on how HMMs might work for several real-life applications , these are POS tagging and Stock Prices\n",
    "\n",
    " ---\n",
    " \n",
    "<h4>Parts-of-Speech (POS) Tagging</h4>\n",
    "\n",
    "Observation sequence is a sentence\n",
    "\n",
    "Goal : find corresponding POS tags for each word\n",
    "\n",
    "Now we might ask why is HMM appropriate ?\n",
    "\n",
    "consider that language has structure which goes deeper than just the words themselves , in other words it follows a hidden state sequence model - we might call it grammar\n",
    "\n",
    "consider the following sentence :\n",
    "\n",
    "<table width = \"500\" >\n",
    "    <tr>\n",
    "        <td style = 'text-align:center'>I</td>\n",
    "        <td style = 'text-align:center'>walked</td>\n",
    "        <td style = 'text-align:center'>to</td>\n",
    "        <td style = 'text-align:center'>the</td>\n",
    "        <td style = 'text-align:center'>store</td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td style = 'text-align:center'>Pronoun</td>\n",
    "        <td style = 'text-align:center'>verb</td>\n",
    "        <td style = 'text-align:center'>adposition</td>\n",
    "        <td style = 'text-align:center'>determiner</td>\n",
    "        <td style = 'text-align:center'>noun</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Now consider another sentence :\n",
    "\n",
    "<table width = \"500\" >\n",
    "    <tr>\n",
    "        <td style = 'text-align:center'>Torki</td>\n",
    "        <td style = 'text-align:center'>drove</td>\n",
    "        <td style = 'text-align:center'>to</td>\n",
    "        <td style = 'text-align:center'>the</td>\n",
    "        <td style = 'text-align:center'>park</td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td style = 'text-align:center'>Pronoun</td>\n",
    "        <td style = 'text-align:center'>verb</td>\n",
    "        <td style = 'text-align:center'>adposition</td>\n",
    "        <td style = 'text-align:center'>determiner</td>\n",
    "        <td style = 'text-align:center'>noun</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "we can see that these two sentences have very similar structure , the POS tags are nearly identical\n",
    "\n",
    "our intuition tells us that the english language follows a set of grammatical rules\n",
    "\n",
    "for example :\n",
    "\n",
    "<table width = \"500\" >\n",
    "    <tr>\n",
    "        <td style = 'text-align:center'>drove</td>\n",
    "        <td style = 'text-align:center'>Torki</td>\n",
    "        <td style = 'text-align:center'>the</td>\n",
    "        <td style = 'text-align:center'>park</td>\n",
    "        <td style = 'text-align:center'>to</td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td style = 'text-align:center'>verb</td>\n",
    "        <td style = 'text-align:center'>noun</td>\n",
    "        <td style = 'text-align:center'>determiner</td>\n",
    "        <td style = 'text-align:center'>noun</td>\n",
    "        <td style = 'text-align:center'>adposition</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "This sentence is not comprehensible because it does not follow the rules of grammar , this sequence of POS tags is unallowable, we can also say that this sentence is highly impropable\n",
    "\n",
    "so we know that there is a hidden structure to language , not strictly hidden since we can wrtie down the rules of grammar , but hidden is a sense that there are billions of web pages online full of text without POS tags\n",
    "\n",
    "we could hire an expert in English grammar to tag them or use a latent variable model such that HMM to do the job for us\n",
    "\n",
    "<img src='extras/25.1.PNG' width = '700'></img>\n",
    "\n",
    "consider the word mil which can be a nount or verb\n",
    "\n",
    "\"I drank a glass of milk\" $\\leftarrow$ noun\n",
    "\n",
    "\"Theif intended to milk victims of their life savings\" $\\leftarrow$ verb\n",
    "\n",
    "The POS tag for milk depends on POS tags that came before it ,  one more reason why a markov model could be useful to model this sequence\n",
    "\n",
    "---\n",
    "\n",
    "If we would use an HMM for a POS tagger , this is what it would look like :\n",
    "\n",
    "<img src='extras/25.2.PNG' width = '700'></img>\n",
    "\n",
    "$z_t$ represents the POS tag at time t\n",
    "\n",
    "$x_t$ represents the word at time t\n",
    "\n",
    "In this example , the markov assumption implies that each POS tag depends only on the previous POS tag\n",
    "\n",
    "The Emision probability governs the probability of generating each word from our vocabulary given the current POS tag\n",
    "\n",
    "<ul>\n",
    "    <li>if tag is verb $\\rightarrow$ probability of run may be 0.01</li>\n",
    "    <li>if tag is verb $\\rightarrow$ probability of 'basket' should be 0 , since its not a verb</li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Stock Prices</h4>\n",
    "\n",
    "The previous example was an example of a discrete observation HMM , since words are discrete categorical objects\n",
    "\n",
    "Stock Prices are continuous values , so this will be an example of continuous observation HMM \n",
    "\n",
    "We know that Stock prices are extremely complex and very hard to predict , but we may have an idea of the factors that can affect a stock price\n",
    "\n",
    "for example if we receive news that a company is releasing a state-of-the-art high-demand product , we might expect its stock to go up\n",
    "\n",
    "another example is facebook , which has got in trouble with the governement over its misshandling of data , that caused its stock price to go down\n",
    "\n",
    "In  general , we might not be able to enumerate all possible causes of a stock price going up/down\n",
    "\n",
    "Therefore an HMM with latent variables could be a godd model for this , since it can be used to find hidden causes\n",
    "\n",
    "---\n",
    "\n",
    "lets draw what this would look like\n",
    "\n",
    "<img src='extras/25.3.PNG' width = '700'></img>\n",
    "\n",
    "Again we have a sequence of hidden states and again we make the markov assumption that each hidden state depends only on the previous hidden state\n",
    "\n",
    "The Emission probabilities are now Gaussian (or mixture of Gaussian) , of course we can choose other continuous distributions , but the Gaussian is the msot common\n",
    "\n",
    "One important distinction between this example and the POS example is that in POS example hidden states are observed (in the training and validation set)\n",
    "\n",
    "In this example the hidden states are truly hidden , All we observe is a time series (like a neural network , nodes are not guaranteed to have a meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to look at the parameters of an HMM more in depth , $\\pi$ , A , B , lets take an example :\n",
    "\n",
    "Hidden states is whether\n",
    "<ul>\n",
    "    <li>hot</li>\n",
    "    <li>cold</li>\n",
    "</ul>\n",
    "\n",
    "Now for some reason we lost all data about whether BUT what we do have are receipts from ice cream shop :)\n",
    "\n",
    "so we know which day we bough ice cream and which day we didnt\n",
    "\n",
    "So our sequence of observations would be a sequence of whether we bought ice crean that day or not over the course of many days\n",
    "\n",
    "from this we would like to infer which days were hot days and which days were cold days\n",
    "\n",
    "<img src='extras/25.4.PNG' width='400'><img>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>State Transition Porbabilities</h4>\n",
    "\n",
    "we know that \n",
    "\n",
    "$A(i,j) = p(z_t = j | z_{t-1} = i) \\forall t$\n",
    "\n",
    "here z denotes hidden states\n",
    "\n",
    "there are two important things to remember about the state transition probabilities \n",
    "\n",
    "<ul>\n",
    "    <li>This distribution is stationary , same probabilities for all time</li>\n",
    "    <li>The hidden states in an HMM are Markov (follow a markov model hence the name) , so the state I am in now depends only on the previous state and not on any state before that</li>\n",
    "</ul>\n",
    "\n",
    "lets take an example for what A might look like :\n",
    "\n",
    "\n",
    "If today is hot , tomorrow will probably be hot , same if cold , so high probability of going from hot to hot , or from cold to cold\n",
    "\n",
    "<img src = 'extras/25.5.PNG' width='400'><img>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Initial State Distribution</h4>\n",
    "\n",
    "we know that \n",
    "\n",
    "$\\pi_i = p(z_1 = i)$\n",
    "\n",
    "In our example , it makes sense that the probabilites be 0.5 since it just depends on when we started recording our sequence\n",
    "\n",
    "<img src = 'extras/25.6.PNG' width='400'><img>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Emission/Observation Probabilities</h4>\n",
    "\n",
    "Here we can see another important assumption that we make in an HMM , the observation that we make depends only on the current state , in other words , if I buy ice cream today , that depends only on whether or not today is hot or called , it does not depend on the whether in any other previous/future day\n",
    "\n",
    "for example :\n",
    "\n",
    "<ul>\n",
    "    <li>B(ice cream | hot day) = 0.9</li>\n",
    "    <li>B(no ice cream | hot day) = 0.1</li>\n",
    "    <li>B(ice cream | cold day) = 0.2</li>\n",
    "    <li>B(no ice cream | hot day) = 0.8</li>\n",
    "</ul>\n",
    "\n",
    "<img src = 'extras/25.7.PNG' width='400'><img>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Conventions</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>M = total number of hidden states</li>\n",
    "    <li>i,j = 1...M to index the state</li>\n",
    "    <li>K = number of possible discrete observations</li>\n",
    "    <li>we might also use V , since we sometimes call the set of different observation we can make a Vocabulary , rather than being a vocabulary of words , we use a more generic term : symbols</li>\n",
    "    <li>z or s = hidden state (we like z for latent variables in general)</li>\n",
    "    <li>$z_t$ / z(t) or $s_t$ / s(t) = hidden state at time t</li>\n",
    "    <li>T = total number of steps in a sequence</li>\n",
    "    <li>$\\pi$ a vector of size M (initial state distribution)</li>\n",
    "    <li>A = a matrix of size MxM (state transition matrix) , indexed like $A_{i,j} / A(i,j)$</li>\n",
    "    <li>B = a matrix of size MxK (emission probability matrix) , indexed like $B_{j,k} / B(j,k)$ , where k = 1...K</li>\n",
    "    <li>$x_t$ / x(t) = observation at time t (can take any value from k=1...K)</li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Continuous Observations</h4>\n",
    "\n",
    "For our ice cream example observations were binary (bought ice cream or didnt)\n",
    "\n",
    "If observations are continuous (e.g. stock price) , then B would no longer be a matrix , but rather a continuous probability distribution (each $B_j$ would be a Gaussian or GMM or other continuous distribution)\n",
    "\n",
    "if we model each observation as a gaussian , then our B function would be a gaussian\n",
    "\n",
    "A gaussian has two parameters , mean $\\mu$ and variance $\\sigma^2$\n",
    "\n",
    "since each state j will have its own gaussian distribution that means we will also index $\\mu$ and sigma using j\n",
    "\n",
    "$$B(j,x_t) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2_j}} \\exp \\left( - \\frac{\\left(x_t - \\mu_j \\right)^2}{2 \\sigma^2_j}\\right)$$\n",
    "\n",
    "This also means that we have M different $\\mu$'s and $\\sigma$'s\n",
    "\n",
    "when this is discussed alter in detail , we will see how we can use GMMs instead of simple gaussians\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will discuss the 3 problems of an HMM , then we will discuss the solution for each of the problems\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Problem #1 : finding the probability of a sequence</h4>\n",
    "\n",
    "suppose we are using a gaussian distribution to measure the grades of everyone who took an exam\n",
    "\n",
    "we can then ask what is the probability (density) of some student getting some grade ?\n",
    "\n",
    "we normally call the probability of a datapoint p(data|params) the likelihood\n",
    "\n",
    "the likelihood can be used in supervised l earning learning (ex: Bayes classifier)\n",
    "\n",
    "The likelihood is also out objective function , we will see that what we are doing is just the usual \"Maximum likelihood estimation\"\n",
    "\n",
    "<h4>Problem #2 : Finding the most likely sequence of hidden states , given an observation sequence</h4>\n",
    "\n",
    "Take for example a parts-of-speech (verb,Noun,..) tagger , we can create a model where the parts-of-speech is the hidden state and the actual word in a setnece is the observation\n",
    "\n",
    "therefore , using the solution to this problem , we can take a setnece as input , for ex \"I love dogs and cats\" ,  and output the corresponding POS tags , \"< pronoun >< verb >< noun >< conjunction >< noun >\"\n",
    "\n",
    "Another example is speech recognition , in this example the words that are spoken are the hidden states , and the audio signal make up the observation\n",
    "\n",
    "In order to map words to sounds , we would have to be able to split the input audio sequence into a sequence of syllables , assume that this can be done , and that would make up our observation\n",
    "\n",
    "Then given this observation we can use an HMM to recover which words are being spoken\n",
    "\n",
    "One more , consider the phrase :\n",
    "\n",
    "\"Recognise speech\"\n",
    "\n",
    "And another very similar sounding phrase :\n",
    "\n",
    "\"Wreck a nice beach\"\n",
    "\n",
    "In some scenarios , the obserevation sequences (the actual sound signals) might be very similar or even exactly the same \n",
    "\n",
    "The HMMs job is to return the most likely sequence of hidden states , so while both sequences of words (hidden states) could have led to the observation we would still return \"Recognise speech\" since thats a much more likely phrase , the solution to problem #2 solves this issue\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Problem #3 : Training the model</h4>\n",
    "\n",
    "As with any model we need to find its parameters using the training data\n",
    "\n",
    "The typical approach is \"Maximum Likelihood Estimation\"\n",
    "\n",
    "<ul>\n",
    "    <li>Set up likelihood function (Problem #1)</li>\n",
    "    <li>Maximise likelihood wrt params</li>\n",
    "</ul>\n",
    "\n",
    "Lets take an example , again we are modelling the grades of our students using a gaussian , so we need to find the mean and variance which are parameters of the gaussian\n",
    "\n",
    "we do this by setting up the likelihood function L\n",
    "\n",
    "$$L = \\prod^N_{i=1} p(x_i;\\mu,\\sigma)$$\n",
    "\n",
    "then we maximise L wrt $\\mu$ and $\\sigma$\n",
    "\n",
    "$$\\hat \\mu , \\hat \\sigma = \\arg \\max_{\\mu,\\sigma} L$$\n",
    "\n",
    "That gives us the maximum likelihood estimate of $\\mu$ and $\\sigma$\n",
    "\n",
    "This gives us even more reason to solve problem #1 , since problem #1 gives us the likelihood so solving problem #1 is necessary , since we need to be able to calculate cost function to maximise it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>problem #1 : Find the probability of a sequence</h4>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Why need a special Algorithm ?</h4>\n",
    "\n",
    "lets try to solve this using only basic probability , once we do that we will get a better understanding of why basic probability is not enough and why a special algorithm is needed\n",
    "\n",
    "we know that :\n",
    "\n",
    "$$\\pi_i = p(z_1 = i)$$\n",
    "\n",
    "$$A_{ij} = p(z_t = j | z_{t-1} = i)$$\n",
    "\n",
    "$$B_{jk} = p(x_t = k | z_t = j)$$\n",
    "\n",
    "$$Given : x = \\{x_1,x_2,...,X_T\\} , Find : p(x)$$\n",
    "\n",
    "---\n",
    "\n",
    "now lets turn $p(x)$ into something we know , we can expand this so that it becomes the joint distribution $p(x_1,x_2,...,x_T)$\n",
    "\n",
    "But this does not involve the actual variables that we have $\\pi$,A,B , we can take a step towards this by invoking the hidden state sequence $z_1,...,z_T$ \n",
    "\n",
    "remember we know that for every observation we have there is also a corresponding hidden state and its the hidden states that are actually markov\n",
    "\n",
    "so we can expand this to include $x_1...x_T$ and $z_1...z_T$ , this gives us a huge joint distribution and for it to equal $p(x)$ we need to marginalise out the z (sum over all the $z$s)\n",
    "\n",
    "$$p(x) = \\sum^M_{z_1=1} \\sum^M_{z_2=1} \\cdots \\sum^M_{z_T=1} p(x_1,x_2,\\cdots,x_T,z_1,z_2,\\cdots,z_T)$$\n",
    "\n",
    "note : each z here would be summed over all possible values of the hidden state , that what we mean by marginalisation\n",
    "\n",
    "This step allows us to remember that our model has a structure, the structure is that each z depends only on the previous z , and each x depends only on the corresponding z at the same time\n",
    "\n",
    "we can make use of this structure to factorise this joint probability distribution into several conditional distributions (just apply bayes rules a bunch of times)\n",
    "\n",
    "$$p(x) = \\sum_{z_1} \\sum_{z_2} \\cdots \\sum_{z_T} p(z_1)p(x_1|z_1)\\prod^T_{t=2}p(z_t|z_{t-1})p(x_t|z_t)$$\n",
    "\n",
    "now plug in $\\pi$,A,B \n",
    "\n",
    "$$p(x) = \\sum_{z_1}\\sum_{z_2}\\cdots\\sum_{z_T}\\pi(z_1)B(z_1,x_1)\\prod^T_{t=2}A(z_{t-1},z_t)B(z_t,x_t)$$\n",
    "\n",
    "Now the question is , is this a good answer ?\n",
    "\n",
    "No , to see why lets think in terms of time complexity\n",
    "\n",
    "<ul>\n",
    "    <li>we have M possible hidden states (so each summation is 1...M)\n",
    "</li>\n",
    "    <li>The sequence is of size T (so we have T summations)</li>\n",
    "    <li>If we consider just the inner term inside all the summations we have to do an O(T) product </li>\n",
    "    <li>How many times do we have to do this computation : $M^T$ (T buckets with M possibilities in each bucket $\\rightarrow M^T$)</li>\n",
    "    <li>so in total this computation is $O(TM^T)$</li>\n",
    "</ul>\n",
    "\n",
    "in other words this computation grows exponentially , which is never good (so it does not scale)\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Improvement</h4>\n",
    "\n",
    "lets take an example to build some mathematical intuition for how it might work\n",
    "\n",
    "consider M=2,T=2 (remember $x_1$ and $x_2$ would be given) , then our calculation is as follows :\n",
    "\n",
    "$$p(x) = \\sum^2_{z_1=1} \\sum^2_{z_2=1} \\pi(z_1) B(z_1,x_1) A(z_1,z_2) B(z_2,x_2)$$\n",
    "\n",
    "lets try to write out all the terms explicitly :\n",
    "\n",
    "$$p(x) = \\pi(1)B(1,x_1)A(1,1)B(1,x_2) + \\\\ \\pi(1)B(1,x_1)A(1,2)B(2,x_2) + \\\\ \\pi(2)B(2,x_1)A(2,1)B(1,x_2) + \\\\ \\pi(2)B(2,x_1)A(2,2)B(2,x_2)$$\n",
    "\n",
    "we can see that the first two tems and last two terms have common factors , so we can write this as :\n",
    "\n",
    "$$p(x) = \\pi(1)B(1,x_1) \\left[ A(1,1) + B(1,x_2) + A(1,2) + B(2,x_2)\\right] + \\\\ \\pi(2)B(2,x_1) \\left[ A(2,1)B(1,x2) + A(2,2)B(2,x_2)\\right]$$\n",
    "\n",
    "This seems like some savings but still its not clear how thic can be advantageous , we need a bigger example\n",
    "\n",
    "consider M=2,T=3 \n",
    "\n",
    "$$p(x) = \\sum^2_{z_1=1}\\sum^2_{z_2=1}\\sum^2_{z_3=1} \\pi(z_1)B(z_1,x_1)A(z_1,z_2)B(z_2,x_2)A(z_2,z_3)B(z_3,x_3)$$\n",
    "\n",
    "again lets try to expand each of the individual terms , note that now we have $2^3 = 8$ terms , so we can already see the effect of exponential growth , we now have to double the terms we had before , and each of those terms have a larger product , after expanding we get :\n",
    "\n",
    "$$p(x) = \\pi(1)B(1,x_1)A(1,1)B(1,x_2)A(1,1)B(1,x_3) + \\\\ \\pi(1)B(1,x_1)A(1,1)B(1,x_2)A(1,2)B(2,x_3) + \\\\ \\pi(1)B(1,x_1)A(1,2)B(2,x_2)A(2,1)B(1,x_3) + \\\\ \\pi(1)B(1,x_1)A(1,2)B(2,x_2)A(2,2)B(2,x_3) +  \\\\ \\pi(2)B(2,x_1)A(2,1)B(1,x_2)A(1,1)B(1,x_3) + \\\\ \\pi(2)B(2,x_1)A(2,1)B(1,x_2)A(1,2)B(2,x_3) + \\\\ \\pi(2)B(2,x_1)A(2,2)B(2,x_2)A(2,1)B(1,x_3) + \\\\ \\pi(2)B(2,x_1)A(2,2)B(2,x_2)A(2,2)B(2,x_3) $$\n",
    "\n",
    "As before we will again try to factor out this expression , we see the same pattern as before where the first half of the terms have the same initial parts as does the second half :\n",
    "\n",
    "$$p(x) = \\pi(1)B(1,x_1) \\left[ \\color{blue}{\\boxed{A(1,1)B(1,x_2)} } A(1,1)B(1,x_3) + \\\\ \\color{blue}{\\boxed{A(1,1)B(1,x_2)} }  A(1,2)B(2,x_3) + \\\\ \\color{green}{\\boxed{A(1,2)B(2,x_2)} }A(2,1)B(1,x_3) + \\\\ \\color{green}{\\boxed{A(1,2)B(2,x_2)} } A(2,2)B(2,x_3) \\right] + \\\\ \\pi(2)B(2,x_1) \\left[ \\color{orange}{\\boxed{A(2,1)B(1,x_2)} } A(1,1)B(1,x_3) + \\\\ \\color{orange}{\\boxed{A(2,1)B(1,x_2)} }  A(1,2)B(2,x_3) + \\\\ \\color{brown}{\\boxed{A(2,2)B(2,x_2)} }A(2,1)B(1,x_3) + \\\\ \\color{brown}{\\boxed{A(2,2)B(2,x_2)} } A(2,2)B(2,x_3) \\right]$$\n",
    "\n",
    "but here is the interesting part , inside each of the two factors , we see the opportunity to factor even more \n",
    "\n",
    "Lets write the expression with more factoring (Nested Factoring) :\n",
    "\n",
    "$$p(x) = \\pi(1)B(1,x_1) \\\\ \\left[  A(1,1)B(1,x_2) \\{ A(1,1)B(1,x_3) + A(1,2)B(2,x_3)\\} + \\\\ A(1,2)B(2,x_3)\\{ A(2,1)B(1,x_3) +  A(2,2)B(2,x_3)\\} \\right] \\\\ + \\\\ \\pi(2)B(2,x_1) \\\\ \\left[  A(2,1)B(1,x_2) \\{A(1,1)B(1,x_3) +  A(1,2)B(2,x_3)\\} + \\\\ A(2,2)B(2,x_2)\\{ A(2,1)B(1,x_3) +  A(2,2)B(2,x_3)\\} \\right]$$\n",
    "\n",
    "Now look at the term $\\pi(1)B(1,x_1)$ , doing this the old way we would have to calculate this 4 times , now we just have to compute it once , same for $\\pi(2)B(2,x_1)$\n",
    "\n",
    "Also consider the term $A(1,1)B(1,x_2)$ , before we had to calculate this product twice and now we just have to compute it once\n",
    "\n",
    "---\n",
    "\n",
    "<h4>More Improvement</h4>\n",
    "\n",
    "In the innermost terms we see that there is still some repitition that was not factored out , and if we factor these out , then we cannot factor the other terms out , seems we are stuck \n",
    "\n",
    "However , there is something even better we can do , which is to define this expression recursively !\n",
    "\n",
    "---\n",
    "\n",
    "Now that we are convinced that basic probability is not enough , and we got a basic understanding of what we need to do and the motivation behind it , we will discuss the solution to this problem from three different persepctives :\n",
    "\n",
    "<ol>\n",
    "    <li>Mathematical: use math and probability to manipulate the equation from before to arrive at a new answer</li>\n",
    "    <li>Algorithmic : what would the code/algorith look like ?</li>\n",
    "    <li>Graphical : use diagram/visualisation</li>\n",
    "</ol>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Mathematical Approach</h4>\n",
    "\n",
    "Lets go back to the symbollic variables we started with , this will help us see the pattern we need more clearly , here are the equations for T = 3 :\n",
    "\n",
    "we know that :\n",
    "\n",
    "$$p(x) = \\sum_{z_1}\\sum_{z_2}\\sum_{z_3}p(z_1)p(x_1|z_1)\\prod_{t=2}^3p(z_t|z_{t-1})p(x_t|z_t)$$\n",
    "\n",
    "expanding this we get :\n",
    "\n",
    "$$p(x) = \\sum_{z_1}\\sum_{z_2}\\sum_{z_3}p(z_1)p(x_1|z_1)p(z_2|z_1)p(x_2|z_2)p(z_3|z_2)p(x_3|z_3)$$\n",
    "\n",
    "Now , this equation can also be written as :\n",
    "\n",
    "$$p(x) = \\sum_{z_3}p(x_3|z_3) \\sum_{z_2}p(z_3|z_2)p(x_2|z_2)\\sum_{z_1}p(z_2|z_1)p(x_1|z_1)p(z_1)$$\n",
    "\n",
    "In this equation its clear that not all of the terms have to be inside all the summations\n",
    "\n",
    "for ex , any term that does not depend on $z_1$ , can be brought outside the $z_1$ summation\n",
    "\n",
    "Now lets try the same with T = 4 , and see if we can detect some pattern , we yeild:\n",
    "\n",
    "$$p(x) = \\sum_{z_4}p(x_4|z_4)\\sum_{z_3}p(z_4|z_3)p(x_3|z_3)\\sum_{z_2}p(z_3|z_2)p(x_2|z_2)\\sum_{z_1}p(z_2|z_1)p(x_1|z_1)p(z_1)$$\n",
    "\n",
    "it should be clear that there is some pattern as we move from one time step to the other , this brings us to the Algorithmic perspective\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Algorithmic Approach</h4>\n",
    "\n",
    "Now , we want to extract a recurive pattern\n",
    "\n",
    "for each inner summation , not counting the final time step , we see the pattern :\n",
    "\n",
    "$$p(x_{t+1}|z_{t+1}) \\sum_{z_t} p(z_{t+1}|z_t) \\left( p(x_t|z_t) \\sum_{z_{t-1}} \\cdots \\right)$$\n",
    "\n",
    "As we can see $\\left( p(x_t|z_t) \\sum_{z_{t-1}} \\cdots \\right)$ is where our recursiveness appears , lets assign that to a variable called $\\alpha$\n",
    "\n",
    "$$Let : \\alpha(t,z_t) = p(x_t|z_t)\\sum_{z_{t-1}} \\cdots $$\n",
    "\n",
    "Then we can represent each $\\alpha$ in terms of a previous $\\alpha$\n",
    "\n",
    "$$Then : \\alpha(t+1,z_{t+1}) = p(x_{t+1}|z_{t+1}) \\sum_{z_t} p(z_{t+1}|z_t) \\alpha(z_t)$$\n",
    "\n",
    "The base case happens at t = 1:\n",
    "\n",
    "$$\\alpha(1,z_1) = p(x_1|z_1)p(z_1)$$\n",
    "\n",
    "<h5>Using HMM parameters</h5>\n",
    "\n",
    "Now its useful to represent everything in terms of actual variables we are going to use in code ($\\pi,A,B$)\n",
    "\n",
    "Initialisation Step (base case) :\n",
    "\n",
    "$$\\alpha(t,i) = \\pi_i B(i,x_t) , for \\  t = 1 $$\n",
    "\n",
    "The time t reperesents the time step of the sequence we are refering to , and the index i represents which hidden state we are refering to\n",
    "\n",
    "Induction Step :\n",
    "\n",
    "$$\\alpha(t+1,j) = B(j,x_{t+1}) \\sum_{i=1}^M \\alpha(t,i)A(i,j) , for \\ t = 1 \\cdots T-1$$\n",
    "\n",
    "\n",
    "Termination Step:\n",
    "\n",
    "$$p(x) = \\sum^M_{i=1} \\alpha(T,i)$$\n",
    "\n",
    "Thats the leftmost summation we skipped when defining $\\alpha$ recursively\n",
    "\n",
    "This is what we called the Forward Algorithm ( still more detail yet to come )\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Time Complexity</h4>\n",
    "\n",
    "Now lets do a complexity Analysis , to do this it is most helpful to look at the induction step :\n",
    "\n",
    "$$\\alpha(t+1,j) = B(j,x_{t+1}) \\sum_{i=1}^M \\alpha(t,i)A(i,j) , for \\ t = 1 \\cdots T-1$$\n",
    "\n",
    "At each step , we need to calculate $\\alpha$ for each state j , where j = 1 $\\cdots$ M , so we need to calculate M $\\alpha$s for a given time step\n",
    "\n",
    "each of these calculations involve a sum over M terms (that i = 1 ... M ) and therefore each induction step is $O(M^2)$\n",
    "\n",
    "Since we have T time steps in total , then the overall algorithm is $O(TM^2)$ - thats polynomial !\n",
    "\n",
    "If this deduction was not clear enough recall our original expression (for ex T = 4):\n",
    "\n",
    "$$p(x) = \\sum_{z_4}p(x_4|z_4)\\sum_{z_3}p(z_4|z_3)p(x_3|z_3)\\sum_{z_2}p(z_3|z_2)p(x_2|z_2)\\sum_{z_1}p(z_2|z_1)p(x_1|z_1)p(z_1)$$\n",
    "\n",
    "as we can see each $\\alpha$ has both $z_t$ and $z_{t-1}$ (for ex $p(x_4|z_4)\\sum_\\limits{z_3}p(z_4|z_3)$) , both are inside a summation that goes from 1...M , thats the $O(M^2)$ , we have T $\\alpha$s , so we arrive at the $O(TM^2)$\n",
    "\n",
    "---\n",
    "\n",
    "</h4>Graphical Approach</h4>\n",
    "\n",
    "lets start by looking at a state diagram for the first time step of a sequence\n",
    "\n",
    "$$\\alpha(t,i) = \\pi_i B(i,x_t) , for \\ t = 1$$\n",
    "\n",
    "<img src='extras/25.8.PNG' width = '400'><img>\n",
    "\n",
    "we can say that we start in some null state , and we then transition to the initial state which is governed by the initial state distribution ($\\pi$)\n",
    "\n",
    "For simplicity lets assume we just have 2 hidden states (M=2)\n",
    "\n",
    "Once we have landed in a hidden state we just have to choose our observation which depends only on the observation probability (B) , which means that the observation depends only on the state we are currently in\n",
    "\n",
    "<h4>Sanity Check</h4>\n",
    "\n",
    "Lets try to caclulate the probability of a sequence of length 1  (initialisation + termination steps):\n",
    "\n",
    "$$p(x_1) = \\pi_1B(1,x_1) + \\pi_2B(2,x_1)$$\n",
    "\n",
    "it might not be clear that exactly this means , so we can replace $\\pi$ and $B$ with the actual probabilities they represent\n",
    "\n",
    "$$p(x_1) = p(z_1=1)p(x_1|z_1=1) + p(z_1=2)p(x_1|z_1 = 2)$$\n",
    "\n",
    "So $p(x_1)$ simplifies to the joint distribution $p(x_1,z_1)$ using bayes rule\n",
    "\n",
    "$$p(x_1) = \\sum^M_{z_1=1} p(z_1)p(x_1|z_1) = \\sum^M_{z_1=1}p(x_1,z_1)$$\n",
    "\n",
    "Then $p(x_1)$ is just the joint distribution $p(x_1,z_1)$ marginalised over $z_1$\n",
    "\n",
    "so everything is working as expected (at least for a sequence of length 1 )\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Middle (Induction) Step</h4>\n",
    "\n",
    "<img src = \"extras/25.9.PNG\" width='400'></img>\n",
    "\n",
    "Suppose that for hidden state in the previous time step t , we have calculated $\\alpha$ , think of $\\alpha$ as being the probability of being in that state (we will discuss it in detail later)\n",
    "\n",
    "what we can do here is , for the next time step t+1 , the probability of arriving in some state is just the probability we were in each of the previous states multiplied by their transition probabilities , so multiplying by the transition probabilities takes us from one state to the next\n",
    "\n",
    "It is clear that the states themselves arent the only thing that matters , our calculation for $\\alpha$ also involves the observations and not just the states , so when we arrive in some state z we can then use our observation matrix B to get the probability of producing the next observation at time t+1 \n",
    "\n",
    "$$\\alpha(t+1,j) = B(j,x_{t+1}) \\sum^M_{i=1} \\alpha(t,i)A(i,j) , for \\ t=1 \\cdots T-1$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>what does $\\alpha$ represent</h4>\n",
    "\n",
    "lets reconsider the sequence for T=3\n",
    "\n",
    "$$p(x) = \\sum_{z_3}p(x_3|z_3) \\sum_{z_2}p(z_3|z_2)p(x_2|z_2)\\sum_{z_1}p(z_2|z_1)\\boxed{p(x_1|z_1)p(z_1)}$$\n",
    "\n",
    "we can apply Bayes rule to combine conditional distributions\n",
    "\n",
    "sp first we can use the definition of $\\alpha$ for the initialisation step , then we can apply bayes rule to get the joint distribution $p(x_1,z_1)$\n",
    "\n",
    "$$\\alpha(1,z_1) = p(x_1|z_1)p(z_1) = p(x_1,z_1)$$\n",
    "\n",
    "Next we can combine the these terms :\n",
    "\n",
    "$$p(x) = \\sum_{z_3}p(x_3|z_3) \\sum_{z_2}p(z_3|z_2)\\boxed{p(x_2|z_2)\\sum_{z_1}p(z_2|z_1)p(x_1|z_1)p(z_1)}$$\n",
    "\n",
    "Again we can use bayes rule to combine the three innter most terms:\n",
    "\n",
    "$$p(x_1,z_1,z_2) = p(z_2|z_1)p(x_1|z_1)p(z_1)$$\n",
    "\n",
    "Since we take sum over $z_1$ we end up marginalising it out\n",
    "\n",
    "$$\\sum_{z_1}p(x_1,z_1,z_2) = p(x_1,z_2)$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\\alpha(2,z_2) = p(x_2|z_2)p(x_1,z_2) = p(x_1,x_2,z_2)$$\n",
    "\n",
    "lets combine the term we just found with the remaining term $p(z_3|z_2)$\n",
    "\n",
    "$$p(x) = \\sum_{z_3}p(x_3|z_3) \\sum_{z_2}\\boxed{p(z_3|z_2)}\\boxed{p(x_2|z_2)\\sum_{z_1}p(z_2|z_1)p(x_1|z_1)p(z_1)}$$\n",
    "\n",
    "using bayes rule we get\n",
    "\n",
    "$$ p(z_3|z_2)p(x_1,x_2,z_2) = p(x_1,x_2,z_2,z_3)$$\n",
    "\n",
    "then again since we sum over $z_2$ so it gets marganinalised out\n",
    "\n",
    "$$\\sum_{z_2} p(x_1,x_2,z_2,z_3) = p(x_1,x_2,z_3)$$\n",
    "\n",
    "then we can get $\\alpha$ at time step t = 3\n",
    "\n",
    "$$\\alpha(3,z_3) = p(x_3|z_3)p(x_1,x_2,z_3) = p(x_1,x_2,x_3,z_3)$$\n",
    "\n",
    "of course if we apply the termination step , we sum over $z_3$ so it gets marginalised and we get our joint probability , $p(x_1,x_2,x_3)$\n",
    "\n",
    "$$p(x_1,x_2,x_3) = \\sum_{z_3} p(x_1,x_2,x_3,z_3) = \\sum_{z_3}\\alpha(3,z_3) $$\n",
    "\n",
    "\n",
    "so in general :\n",
    "\n",
    "$$\\alpha(t,i) = p(x_1,x_2,...,x_t,z_t=i)$$\n",
    "\n",
    "and again \n",
    "\n",
    "$$p(x) = p(x_1,...,x_T) = \\sum^M_{i=1} p(x_1,...,x_T,z_T = i) = \\sum^M_{i=1} \\alpha(T,i)$$\n",
    "\n",
    "which is the general form of the expression above where we yeilded $p(x_1,x_2,x_3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the second part of the \"Forward-Backward Algorithm\" (obviously thats the Backward Algorithm)\n",
    "\n",
    "we didnt need the Backward Algorithm to solve our problem : finding p(x) , so why is it needed ?\n",
    "\n",
    "The backward algorithm is not required to find $p(x)$ , instead we are going to see that its kind of opposite to the forward algorithm , but it wont be useful until we train an HMM\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Backward Algorithm</h3>\n",
    "\n",
    "we are going to look at the backward algorithm in terms of the very same steps of the forward algorithm\n",
    "\n",
    "In particular , we are going to have some variable called $\\beta$ which would be defined to be some probability distribution\n",
    "\n",
    "Then we are going to have a recursice algorithm to find its values , the initialisation step will be used for the base case , and the induction step will be used for all other time steps , in ths case there isnt any need for termination step\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Initialisation Step</h3>\n",
    "\n",
    "Again , the backward Algorithm is the reverse of the forward algorithm , wherase the forward algorithm starts at the beginning of the sequence , the backward algorithm starts at the end\n",
    "\n",
    "so we define our initialisation step as follows :\n",
    "\n",
    "$$\\beta(T,i)=1$$\n",
    "\n",
    "note that unlike the forward algorithm , this value does not involve $\\pi,A,B$ , but just a constant 1\n",
    "\n",
    "Think of this as sort of a sentinel value that makes it consistent with the rest of the $\\beta$s , we will see exactly why its 1 when we see exactly what probability $\\beta$ represents\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Induction Step</h3>\n",
    "\n",
    "Now we are going backwards , so we define Induction Step for t = (T-1) ... 1 as the following recursive formula : \n",
    "\n",
    "$$\\beta(t,i) = \\sum^M_{j=1}A(i,j)B(j,x_{t+1})\\beta(t+1,j)$$\n",
    "\n",
    "again we are moving backwards , because we want to follow the convention that i is the first index in A and j is the second index in A so that we always $A(i,j)$ we have the index i on left and j on right , but we sum over j instead of i\n",
    "\n",
    "---\n",
    "\n",
    "<h4>what does $\\beta$ represent ?</h4>\n",
    "\n",
    "$$\\beta(t,i) = p(x_{t+1},...,x_T | z_t = i)$$\n",
    "\n",
    "$\\beta(t,i)$ is the probability of seeing the sequence at the future time steps $x_{t+1},...,x_T$ given that the hidden state at time t , $z_t$ = i\n",
    "\n",
    "This is unlike forward algorithm , where $\\alpha$ has both x and z at the same time steps \n",
    "\n",
    "Here we can see that for $\\beta$ that the time steps of x are only future times , wherase the time step for t\n",
    "\n",
    "now we see why $\\beta(T,i)=1$ , this is because at T the state can be i but there are no future observations since the last observation is $x_T$ , because no future observations exist , the value of this probability = 1 by default\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Does $\\beta$ make sense ?</h4>\n",
    "\n",
    "we can do the same factorisation as the forward algorithm , except reverse the order of the summations , that is isntead of having the eariler $z$s some on the inside and the later $z$s some on the outside , we just reverse where we put the summations\n",
    "\n",
    "Again lets try for a sequence where T = 3\n",
    "\n",
    "$$p(x) = \\sum_{z_1}\\sum_{z_2}\\sum_{z_3}p(z_1)p(x_1|z_1)p(z_2|z_1)p(x_2|z_2)p(z_3|z_2)p(x_3|z_3)$$\n",
    "\n",
    "this can be rewritten as :\n",
    "\n",
    "$$p(x) = \\sum_{z_1}p(z_1)p(x_1|z_1)\\sum_{z_2}p(z_2|z_1)p(x_2|z_2)\\sum_{z_3}p(z_3|z_2)p(x_3|z_3)$$\n",
    "\n",
    "from here its easy to see that this follows the same recursion that we saw in the $\\beta$ equation where in each sume we had A $\\times$ B $\\times$ Previous $\\beta$ (the previous beta at time t is $\\beta$ at t+1 since we are going backwards) , notice the pattern\n",
    "\n",
    "$$p(x) = \\sum_{z_1}\\pi_1B(z_1,x_1)\\color{blue}{\\boxed{\\color{black}{\\sum_{z_2}A(z_1,z_2)B(z_2,x_2)\\color{green}{\\boxed{\\color{black}{\\sum_{z_3}A(z_2,z_3)B(z_3,x_3)}}}}}}$$\n",
    "\n",
    "remember that the first $\\beta$ at time t = 3 is just equal to 1 , so we imagine that there is an imaginary 1 at the end of this sum\n",
    "\n",
    "---\n",
    "\n",
    "<h4>The Graphical Perspective</h4>\n",
    "\n",
    "Since we are going backwards we can start at the end of the sequence \n",
    "\n",
    "The main thing we have to remember is that for the backward algorithm , we are not concerned with observations at the current time\n",
    "\n",
    "so , At the final time step T , there aare no future observations to consider \n",
    "\n",
    "<img src=\"extras/25.10.PNG\" width=\"100\"></img>\n",
    "\n",
    "$$\\beta(T,i) = 1$$\n",
    "\n",
    "next we go to the 2nd last state and now is when we consider the last observation at time T ($x_T$) , we also sum over the transition probabilities from the second last state going to all the possible last states , as usual we can think of this as bayes rule with marginalisation , it does not really matter that we are going backwards since random variables in bayes rule dont have any order\n",
    "\n",
    "<img src=\"extras/25.11.PNG\" width=\"400\"></img>\n",
    "\n",
    "$$\\beta(t,i) = \\sum^M_{j=1} A(i,j)B(j,x_{t+1})\\beta(t+1,j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will discuss the viterbi Algorithm which is an algorithm that solves the 2nd problem of HMMs , that was : <strong>find the most likely sequence of hidden states given an observation sequence</strong>\n",
    "\n",
    "The first problem which we discussed previously was to find the probability of a sequence\n",
    "\n",
    "The third problem , which discuss next , is abiut how to train an HMM given a set of sequences (finding the parameters of the HMM)\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Most-Likely Sequence of Hidden States</h4>\n",
    "\n",
    "At a high level , what we are looking for is a sequence of states that best explains the observation (x) , now how do we translate this intuition into math ?\n",
    "\n",
    "we dont want p(x) since that sums over all possible values of z , instead what we want is :\n",
    "\n",
    "$$z^* =\\arg \\max_z p(z|x)$$\n",
    "\n",
    "This is a conditional probability , it is the probability of the hidden state sequence z given the observation sequence x\n",
    "\n",
    "so we want to find the $z$ that yeilds the maximimum $p(z|x)$ , that is the most probable states given the observation\n",
    "\n",
    "Now we can apply Bayes rule to arrive at :\n",
    "\n",
    "$$z^* = \\arg \\max_z p(z|x) = \\arg \\max_z \\frac{p(z,x)}{p(x)}$$\n",
    "\n",
    "$p(x)$ is independant of $z$ , thus any change in z has no effect on $x$ , and so $p(x)$ is constant wrt to $z$ and therefore maximising $p(z,x)$ is the same as maximising $p(z|x)$\n",
    "\n",
    "$$z^* = \\arg \\max_z p(z|x) = \\arg \\max_z \\frac{p(z,x)}{p(x)} = \\arg \\max_z p(z,x)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Relationship to Forward Algorithm</h4>\n",
    "\n",
    "Recall that this is what we had before but without summations over z , so all we need to do is not sum anything , so that the same expression we wrote before but without\n",
    "\n",
    "$$p(z,x) = p(z_1,\\ldots,z_T,x_1,\\ldots,x_T) = p(z_1)p(x_1|z_1) \\prod^T_{t=2} p(z_t|z_{t-1})p(x_t|z_t)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Algorithmic Complexity</h4>\n",
    "\n",
    "we recognise that in order to do this cacluclation we have to do $O(T)$ multiplications\n",
    "\n",
    "If we have an observation sequence , and a sequence of hidden states , and we want to find the joint probability <strong>value</strong> we just multiply all these terms together\n",
    "\n",
    "Its important to note that , while the previous formula was in terms of probability distribution , this formula is for obtaining a probability <strong>value</strong> , in other words , we are pluggin in specific values for $z$ and $x$ in order to come up with a number representing the probability of observing those sequences ($z,x$)\n",
    "\n",
    "---\n",
    "\n",
    "<h4>The Naive Approach</h4>\n",
    "\n",
    "Again lets try to approach the problem naively so we get an understanding of why we need to do something better\n",
    "\n",
    "lets try to come up with a brute force approach to finding the most likely sequence of hidden states , using the expression of $p(z,x)$ we described\n",
    "\n",
    "```python\n",
    "Given x (observation sequence)\n",
    "most_likely_sequence = None\n",
    "maximum_prob = 0\n",
    "for z in all_possible_state_sequences:\n",
    "    if p(z,x) > maximum_prob:\n",
    "        maximum_prob = p(z,x)\n",
    "        most_likely_sequence = z\n",
    "return most_likely_sequence\n",
    "```\n",
    "\n",
    "The hidden detail in this algorithm is , what are \"all_possible_state_sequences\" ?\n",
    "\n",
    "we have discussed this before , we have T buckets , each bucket can take M items , so that is $M^T$ possibilities , with each iteration having $O(T)$ multiplications in total the algorithm is $O(TM^T)$ , exponential growth :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to look at how the Vitebri Algorithm solces the problem of finding the most likely sequence of hidden states\n",
    "\n",
    "lets first consider a sequence of length T = 1\n",
    "\n",
    "Also assume we have M = 2 states, and K = 2 symbols (heads,tails)\n",
    "\n",
    "<img src=\"extras/25.12.PNG\" width=\"200\"></img>\n",
    "\n",
    "Suppose that in state 1 , we have a 90% probability of heads so :\n",
    "\n",
    "<ul>\n",
    "    <li>$B(1,heads) = 0.9$</li>\n",
    "    <li>$B(1,tails) = 0.1$</li>\n",
    "</ul>\n",
    "\n",
    "Suppose that in state 2 , we have a 10% probability of heads so :\n",
    "\n",
    "<ul>\n",
    "    <li>$B(2,heads) = 0.1$</li>\n",
    "    <li>$B(2,tails) = 0.9$</li>\n",
    "</ul>\n",
    "\n",
    "Assume that $\\pi = [0.5,0.5]$\n",
    "\n",
    "suppose that $x = \\{tails\\}$ , the question is what is the most likely state ?\n",
    "\n",
    "That should be pretty intuitive , the most likely state is $z = \\{2\\}$ since it has a 90% probability of showing tails \n",
    "\n",
    "note that the answer may change based on what $\\pi$ is\n",
    "\n",
    "remember that what we want is the joint distribution , lets talk about how we would calculate the joint distributions for a sequence of length 1\n",
    "\n",
    "if $z =\\{1\\}$ then the joint probability is :\n",
    "\n",
    "$$p(z,x) = \\pi(1) \\times B(1,tails) = 0.5 \\times 0.1 = 0.05$$\n",
    "\n",
    "if $z =\\{2\\}$ then the joint probability is :\n",
    "\n",
    "$$p(z,x) = \\pi(2) \\times B(2,tails) = 0.5 \\times 0.9 = 0.45$$\n",
    "\n",
    "clearly the second probability is larger \n",
    "\n",
    "---\n",
    "\n",
    "Next , lets consider a sequence of length T = 2 , we will need to define a state transition matrix\n",
    "\n",
    "<img src=\"extras/25.13.PNG\" width = \"400\"></img>\n",
    "\n",
    "now lets consider the observation sequence $x = \\{heads,tails\\}$ , we can enumerate the state sequences (still doing things the slow way) :$\\{11,12,21,22\\}$\n",
    "\n",
    "now lets calculate the joint probability for each of these possibilities\n",
    "\n",
    "for $z = \\{11\\}$\n",
    "\n",
    "$$\\pi(1) \\times B(1,heads) \\times A(1,1) \\times B(1,tails) = 0.5 \\times 0.9 \\times 0.8 \\times 0.1 = 0.036 $$\n",
    "\n",
    "for $z = \\{12\\}$\n",
    "\n",
    "$$\\pi(1) \\times B(1,heads) \\times A(1,2) \\times B(2,tails) = 0.5 \\times 0.9 \\times 0.2 \\times 0.9 = 0.081 $$\n",
    "\n",
    "for $z = \\{21\\}$\n",
    "\n",
    "$$\\pi(2) \\times B(2,heads) \\times A(2,1) \\times B(1,tails) = 0.5 \\times 0.1 \\times 0.4 \\times 0.1 = 0.002 $$\n",
    "\n",
    "for $z = \\{22\\}$\n",
    "\n",
    "$$\\pi(2) \\times B(2,heads) \\times A(2,2) \\times B(2,tails) = 0.5 \\times 0.1 \\times 0.6 \\times 0.0 = 0.027 $$\n",
    "\n",
    "so the winner is $z = \\{12\\}$\n",
    "\n",
    "now lets look at the calculations we did very carefully\n",
    "\n",
    "for $z = \\{11\\}$\n",
    "\n",
    "$$\\color{green}{\\pi(1) \\times B(1,heads)} \\times A(1,1) \\times B(1,tails) = 0.5 \\times 0.9 \\times 0.8 \\times 0.1 = 0.036 $$\n",
    "\n",
    "for $z = \\{12\\}$\n",
    "\n",
    "$$\\color{green}{\\pi(1) \\times B(1,heads)} \\times A(1,2) \\times B(2,tails) = 0.5 \\times 0.9 \\times 0.2 \\times 0.9 = 0.081 $$\n",
    "\n",
    "for $z = \\{21\\}$\n",
    "\n",
    "$$\\color{blue}{\\pi(2) \\times B(2,heads)} \\times A(2,1) \\times B(1,tails) = 0.5 \\times 0.1 \\times 0.4 \\times 0.1 = 0.002 $$\n",
    "\n",
    "for $z = \\{22\\}$\n",
    "\n",
    "$$\\color{blue}{\\pi(2) \\times B(2,heads)} \\times A(2,2) \\times B(2,tails) = 0.5 \\times 0.1 \\times 0.6 \\times 0.0 = 0.027 $$\n",
    "\n",
    "the question is , can we formulate this into an algorithm ?\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Finding the pattern</h4>\n",
    "\n",
    "For each state transition , we only need to consider how to get to the next state and make the next observation\n",
    "\n",
    "lets ask the following question : \n",
    "\n",
    "\"<strong>At time t</strong>, what is the <strong>maximum probability</strong> given that I have already visited the optimal sequence of states prior to time t , <strong>to arrive in state j ?</strong>\"\n",
    "\n",
    "we define this as $\\delta(t,j)$ , our claim is that $\\delta$ could be defined recursively such that is searches through all the $\\delta$s at the previous time step\n",
    "\n",
    "$$\\delta(t,j) = \\max_{i \\in\\{1\\ldots M\\}} \\{\\delta(t-1,i)A(i,j)\\}B(j,x(t))$$\n",
    "\n",
    "So what does the equation say on the RHS ?\n",
    "\n",
    "we just defined $\\delta$ , so $\\delta(t-1,j)$ is the maximum probability of arriving in state i at the previous time step (t-1)\n",
    "\n",
    "what we are doing is that it is searching through all possible previous states i and multiplying by $A(i,j)$ to get the probability of arriving in state $j$ from state $i$ , we also multiply by B(j,x(t)) since this is the joint distribution for z and x ( $p(z,x)$ )\n",
    "\n",
    "what we just saw was the <strong>Induction step</strong>\n",
    "\n",
    "Next we look at the initialisation and termination steps\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Initialisation Step</h4>\n",
    "\n",
    "To initialise $\\delta$ , remember its just the maximum joint probability of arriving in state j over all possible previous states , since there are no previous states for the first state there is no $\\max$ operation\n",
    "\n",
    "$$\\delta(1,j) = \\pi_jB(j,x(1))$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Termination Step</h4>\n",
    "\n",
    "This is just the maximum probability over the entire sequence\n",
    "\n",
    "we know that $\\delta(T,j)$ gives us the probability of ending up at state j at the final time step T , all we need to do is take the max over the final state to get the entire probability for the whole sequence \n",
    "\n",
    "$$p^* = \\max_{j\\in\\{1\\ldots M\\}} \\delta(T,j)$$\n",
    "\n",
    "notice how this algorithm is very similar to the forward algorithm except that we take then max instead of the sum\n",
    "\n",
    "Both forward adn Viterbi Algorithm are examples of <u>Dynamic Programming</u>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Is that correct ?</h4>\n",
    "\n",
    "How do we know that taking max over just most recent state will lead to best sequence globally ?\n",
    "\n",
    "suppose we find that {$z_2 = 2 \\rightarrow z_3 = 1$} is the best transition and {$z_1=z \\rightarrow z_2=2$} was best transition for getting to $z_2 = 2$ , how do we know that the sequence 121 is the optimum sequence  and not for ex 211 ?\n",
    "\n",
    "to answer this lets consider a longer example T=3 , our observation sequence {heads,tails,heads} , this is the same sequence as we had before just with an extra 'heads' at the end\n",
    "\n",
    "---\n",
    "\n",
    "<h4>heads,tails,heads</h4>\n",
    "\n",
    "lets use our recursive formula to calculate our best probability and most likey next state\n",
    "\n",
    "we already know (from previous calculations) that the best sequence of states for the first two observations is $\\{1,2\\}$\n",
    "\n",
    "we also know the value of $\\delta$ :\n",
    "\n",
    "$\\delta(2,1) = 0.036$ , which is the probability of ending up in state 1 at time t = 2\n",
    "\n",
    "$\\delta(2,2) = 0.036$ , which is the probability of ending up in state 2 at time t = 2\n",
    "\n",
    "notice how out of the 4 probabilities we were considering before only these 2 matter , because they give us the maximum probability of ending up in those respective states\n",
    "\n",
    "Now , we ecan calculate $\\delta(3,1)$ and $\\delta(3,2)$\n",
    "\n",
    "$\\delta(3,1) =$ max of:\n",
    "\n",
    "$\\delta(2,1) \\times A(1,1) \\times B(1,heads) = 0.036 \\times 0.8 \\times 0.9 = 0.02592$\n",
    "\n",
    "vs\n",
    "\n",
    "$\\delta(2,2) \\times A(2,1) \\times B(1,heads) = 0.081 \\times 0.4 \\times 0.9 = 0.02916$\n",
    "\n",
    "so $\\delta(3,1) = 0.02916$\n",
    "\n",
    "$\\delta(3,2) =$ max of:\n",
    "\n",
    "$\\delta(2,1) \\times A(1,2) \\times B(2,heads) = 0.036 \\times 0.2 \\times 0.1 = 0.00072$\n",
    "\n",
    "vs\n",
    "\n",
    "$\\delta(2,2) \\times A(2,2) \\times B(2,heads) = 0.081 \\times 0.6 \\times 0.1 = 0.00486$\n",
    "\n",
    "so $\\delta(3,2) = 0.00486$\n",
    "\n",
    "$\\delta(3,2) = 0.00486$\n",
    "\n",
    "note : when we take the max , this tells us which state to come from / transition to use , since we chose A(2,1) (in $\\delta(3,1)$) this means that the ideal state sequence in total is 121 (this also measn that we need some method to keep track of the optimum sequence so when asked we can return 121 )\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Why is it globally optimal ?</h4>\n",
    "\n",
    "This seems kind of like a greedy algorithm which we know sometimes does not lead to the optimal answer\n",
    "\n",
    "The key is to remember that : the state transitions are <strong>Markov</strong>\n",
    "\n",
    "it does not matter how I got to state 2 at time 2 , this wont affect the transition from state ate time 2 going to a new state at time 3 , and so wont affect the max for going to a new state at time 3\n",
    "\n",
    "---\n",
    "\n",
    "<h4>The Hidden State Sequence</h4>\n",
    "\n",
    "All we have done so far is calculate the probability for the best hidden state sequence , but this doesnot return what the best hidden state sequence actually is\n",
    "\n",
    "In order to do this we need to use <strong>backtracking</strong> , which is a common componnent of <u>dynamic programming</u>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Bakctracking</h4>\n",
    "\n",
    "To see how backtracking works , lets reconsider the induction step :\n",
    "\n",
    "$$\\delta(t,j) = \\max_{i \\in\\{1\\ldots M\\}} \\{\\delta(t-1,i)A(i,j)\\}B(j,x(t))$$\n",
    "\n",
    "we see that involves taking the max over all possible previous states , of course this also tells us that the arg max will tell use which state lead to this maximim value\n",
    "\n",
    "$$\\psi(t,j) = \\arg \\max_{i \\in \\{1\\ldots M\\} } \\{ \\delta(t-1,i)A(i,j)\\}$$\n",
    "\n",
    "note that we dont need to make use of B since that appears outside the max and is independant of the previous state (i)\n",
    "\n",
    "so what does $\\psi$ tell us ?\n",
    "\n",
    "It says : \"for the current time t what is the best previous state that takes us to state j at time t\"\n",
    "\n",
    "There are 2 states involved here :\n",
    "\n",
    "j , the current state , input $\\psi$\n",
    "\n",
    "returned state by function , which is the argmax over i , the best state to come from to arrive at j\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Initialisation Step</h4>\n",
    "\n",
    "we assume states are numbered 1...M\n",
    "\n",
    "$$\\psi(1,j) = 0$$\n",
    "\n",
    "so 0 is just a sentinel / null value (this value is never used)\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Termination Step</h4>\n",
    "\n",
    "To find the best state for the final time step we take the arg max instead of the max\n",
    "\n",
    "so we modify to :\n",
    "\n",
    "$$p* = \\max_{j \\in \\{1\\ldots M\\}} \\delta(T,j)$$\n",
    "\n",
    "to \n",
    "\n",
    "$$z(T)^* = \\arg \\max_{i \\in \\{1\\ldots M\\}} \\delta(T,i)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>The Optimal State Sequence</h4>\n",
    "\n",
    "Now , how do we actually recover the optimal state sequence ?\n",
    "\n",
    "$\\psi$ tells us the optimal previous state to arrive at every possible state at each time step , but what we need now is a single state at each time step , the single state that yeilds the maximum joint probability\n",
    "\n",
    "well , again , we know that $\\psi$ return the best previous state given the current state , so if $z(T)^*$ is the optimal state at time T , then we can use $\\psi$ to retreive the best state that we should have come from to arrive at $z(T)^*$\n",
    "\n",
    "$$z(T-1)^* = \\psi(T,z(T)^*)$$\n",
    "\n",
    "and of course we can repeat this pattern all the way down to the initial state using $\\psi$ to tell us , given the current state what is the best previous state\n",
    "\n",
    "since we move backwards from t=T...1 we call this <strong>backtracking</strong>\n",
    "\n",
    "$$z(t)^* = \\psi(t+1,z(t+1)^*)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to discuss how to train an HMM , that is how to find $\\pi,A,B$\n",
    "\n",
    "first , we will focus on the simplest way to train an HMM , this is not THE way to train an HMM that we will usually find in the literature\n",
    "\n",
    "we will start with the case where the hidden states are not really hidden during training\n",
    "\n",
    "Then we will go over a more difficult scenario where the hidden states actually are hidden\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Parts-of-speech tagging</h4>\n",
    "\n",
    "\n",
    "In POS tagging , the POS is considered to be the hidden state\n",
    "\n",
    "<img src=\"extras/25.14.PNG\" width=\"600\"></img>\n",
    "\n",
    "But importantly , when we collect our dataset that data doesnot have hidden states , since each word in our dataset will be labeld with POS tags (otherwise there would be nothing to train) , so the states would be given , the states will be observed\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Training an HMM the simple way</h4>\n",
    "\n",
    "so how do we train an HMM if we have both observed sequences and the corresponding hidden states sequences\n",
    "\n",
    "first consider the states , these are just a plain markov model , remember that $A(i,j)$ is just the probability of going to state j from state i\n",
    "\n",
    "$$A(i,j) = p(z_{t+1} = j | z_t = i)$$\n",
    "\n",
    "Therefore the Maximum Likelihood estimate of $A(i,j)$ is :\n",
    "\n",
    "$$A(i,j) = \\frac{count(i \\rightarrow j)}{count(i)}$$\n",
    "\n",
    "for ex , the probability of going from noun to verb , is just the number of times we observe the pattern \"noun verb\" divided by the total number of nouns in the dataset\n",
    "\n",
    "similarly , we can use the maximum likelihood estimate for $\\pi$:\n",
    "\n",
    "$$\\pi_i = \\frac{count(z_1=i)}{N}$$\n",
    "\n",
    "Thats saying that the maximum likelihood estimate for $\\pi_i$ is equal to the the number of sequences that start in state i / the total number of sequences in our dataset\n",
    "\n",
    "again same for B\n",
    "\n",
    "$$B(j,k) = \\frac{count(z=j \\land x = k)}{count(z=j)}$$\n",
    "\n",
    "Thats the number of times we observe symbol k when in state j / number of times we observe state j\n",
    "\n",
    "note : $\\land$ is the logical AND operator\n",
    "\n",
    "Once again , this method only applies when we can observe the hidden states\n",
    "\n",
    "Another example of this is \"speech-to-text\" , since the observations are sound clips , and the hidden states are the actual words being spoken , we can be sure that our dataset will come with both the sound and the corresponding words , thus the hidden states are observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next , we look at a more formal way of training Markov models , which will help us through the  theory of training a full HMM\n",
    "\n",
    "we saw before that each state transition can be estimated using simple counting\n",
    "\n",
    "$$A(i,j) = \\frac{count(i \\rightarrow j)}{count(i)}$$\n",
    "\n",
    "counting makes so much sense , but we never asked ourselves , how is this actually derived ?\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Review: Maximum Likelihood</h4>\n",
    "\n",
    "we know that our usual way of training a model is to use Maximum likelihood estimation , this involves setting up a likelihhod function , and then maximising the likelihood wrt the parameters of the model\n",
    "\n",
    "The simplest example may be the probability of heads and tails by flipping a coun a bunch of times , As we know this corresponds to a Bernoulli Distribution where we have binary outcomes \n",
    "\n",
    "if we let $\\theta = p(heads)$ (heads =1 , tails = 0) , then we can write down the PMF as :\n",
    "\n",
    "$$f(x;\\theta) = \\theta^x (1-\\theta)^{1-x}$$\n",
    "\n",
    "This just means that the probability of x being 1 is $\\theta$ and the probability of x being 0 is $1-\\theta$\n",
    "\n",
    "our likelihood function is just the product of the PMF value for each datapoint we collect\n",
    "\n",
    "$$L = \\prod^N_{i=1}f(x_i;\\theta) = \\prod_{i=1}^N \\theta^{x_i}(1-\\theta)^{(1-x_i)}$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Log-Likelihood</h4>\n",
    "\n",
    "As we know , we usually dont maximise the likelihood directly , instead we take the log of the likelihood and maximise that instead\n",
    "\n",
    "This is allowed since the log() is a monotonically increasing function\n",
    "\n",
    "$$l(\\theta) = \\sum^N_{i=1} \\{ x_i \\log \\theta + (1- x_i) \\log (1 - \\theta)\\}$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Finding the maximum</h4>\n",
    "\n",
    "As usual we can solve this type of problem using calculus , in order to maximise the log likelihood we take the derivative , set it to 0 , and solve for $\\theta$ \n",
    "\n",
    "$$\\frac{dl\\theta}{d\\theta} = \\frac{1}{\\theta} \\sum^N_{i=1} x_i - \\frac{1}{1-\\theta} \\sum^N_{i=1} (1-x_i) = 0$$\n",
    "\n",
    "As we expect this leads us to the answer that $\\theta$ is the sample mean of x\n",
    "\n",
    "$$\\theta = \\frac{1}{N}\\sum_{i=1}^N x_i$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>The PMF of a discrete distribution</h4>\n",
    "\n",
    "lets recall how we can express the PMF of a discrete distribution\n",
    "\n",
    "for bernoulli since there are only 2 possible outcomes there is only 1 parameter, so the pmf is just what we saw earlier\n",
    "\n",
    "But suppose we have M $\\geq$ 2 possible outcomes , then we can generalise the bernoulli distribution into a categorical distribution\n",
    "\n",
    "The PMF of the categorical distribution is the product of each individual category\n",
    "\n",
    "$$f(x;\\theta) = \\theta_1^{1(x=1)} \\theta_2^{1(x=2)} \\ldots \\theta_M^{1(x=M)} = \\prod_{i-1}^M \\theta_i^{1(x=i)} $$\n",
    "\n",
    "lets make sure this makes sense , for each input x , $d(x;\\theta)$ shoudl return the probability of that input (thats essentially what a PMF is)\n",
    "\n",
    "$1(a)$ is the indicator function , returns 1 if a = true , 0 otherwise\n",
    "\n",
    "of course x can be either 1,2,..M one at a time , so if we plug in x=1 we get back $\\theta_1$ and if we plug in x=2 we get back $\\theta_2$ , so this PMF function does exactly what we want\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Markov Model Likelihood</h4>\n",
    "\n",
    "Now we return to the problem of how to train an Markov Model , lets again start by setting up our likelihood function , we have two parameters we have to deal with $\\pi$ the initial state distribution and $A$ the transition probability matrix , first lets write the likelihood in terms of generic probability distributions\n",
    "\n",
    "$$L = \\prod_{n=1}^N p(z_1^{(n)}) \\prod_{t=2}^{T_n}p(z_t^{(n)}|z_{t-1}^{(n)})$$\n",
    "\n",
    "note : our data has multiple sequences , so we index that with n , there are N sampels indexed by n\n",
    "\n",
    "the probability of each sequence is $ p(z_1^{(n)}) \\prod\\limits_{t=2}^{T_n}p(z_t^{(n)}|z_{t-1}^{(n)})$\n",
    "\n",
    "The next thing we can do is express each distribution in terms of our Markon model parameters $\\pi,A$ , this follows from the discussion we had earlier about how to express the PMF of a categorical distribution\n",
    "\n",
    "$$p(z_1) = \\prod_{i=1}^M \\pi_i^{1(z_1=i)}$$\n",
    "\n",
    "$$p(z_t|z_{t-1}) = \\prod_{i=1}^M \\prod_{j=1}^M A_{ij}^{1(z_{t-1}=i \\land z_t = j)}$$\n",
    "\n",
    "Next we can write the likelihood in terms of our model parameters $\\pi,A$\n",
    "\n",
    "$$\\large L = \\prod_{n=1}^N \\prod_{i=1}^M \\pi_i^{1(z_1^{(n)}=i)} \\prod_{t=2}^{T_n}\\prod_{i=1}^M \\prod_{j=1}^M A_{ij}^{1(z_{t-1}^{(n)}=i \\land z_{t}^{(n)}=j)}$$\n",
    "\n",
    "next we can take the log to get log-likelihood , products turn into sum\n",
    "\n",
    "$$ \\large l(\\pi,A) = \\sum^N_{n=1} \\left\\{ \\sum_{i=1}^M 1(z_1^{(n)} = i) \\log \\pi_i + \\sum_{t=2}^{T_n} \\sum_{i=1}^M \\sum_{j=1}^M 1( z_{t-1}^{(n)}=i \\land z_t^{(n)} = j ) \\log A_{ij}\\right\\} $$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Constraints</h4>\n",
    "\n",
    "next we remember that $\\pi,A$ cant take just any values , in fact they are constrained , $\\pi$ must sm to 1 , and each row of $A$ must sum to 1\n",
    "\n",
    "$$\\sum_{i=1}\\pi_i = 1$$\n",
    "\n",
    "$$\\sum_{j=1}^M A_{ij} = 1 , \\forall i=1\\ldots M$$\n",
    "\n",
    "also values must be $\\geq$ 0 (sicne these are probabilities) \n",
    "\n",
    "$$\\pi_i \\geq 0 \\forall i = 1 \\ldots M$$\n",
    "\n",
    "$$A_{ij} \\geq 0 \\forall i = 1 \\ldots M$$\n",
    "\n",
    "But the inequality constraints are handled automatically so we can safely ignore them and the result will be non-negative anyway\n",
    "\n",
    "so we have a <strong>constrained</strong> optimisation problem\n",
    "    \n",
    "---\n",
    "\n",
    "<h4>Lagrange Multipliers</h4>\n",
    "\n",
    "In order to solve a constraint optimisation problem we need to apply the method of <strong>Lagrange Multipliers</strong>\n",
    "\n",
    "The general way to do this is to first create the Lagrangian , which is the original function we want to optimise + some new terms where we inroduce the lagrange multipliers multiplied by other functions representing the contraints\n",
    "\n",
    "we use $R$ to represent the Lagrangian since we already used L to represent the lokelihood\n",
    "\n",
    "$$R = l(\\pi,A) + \\sum^M_{i=1}\\alpha_i\\left(1-\\sum_{j=1}^M A_{ij}\\right) + \\pi_i\\left(1-\\sum_{i=1}^M\\pi_i\\right)$$\n",
    "\n",
    "our goal now would be to take the derivative of the Lagrangian , $R$ wrt to all of our parameters then we set those derivatives to 0 and solve for our parameters\n",
    "\n",
    "$$set \\ \\frac{\\partial R}{\\partial \\pi} = 0 , \\frac{\\partial R}{\\partial A} = 0 , \\frac{\\partial R}{\\partial \\alpha} = 0 , \\frac{\\partial R}{\\partial \\beta} = 0 , solve \\ for \\ \\pi , A$$\n",
    "\n",
    "---\n",
    "\n",
    "lets start by taking the derivative of $R$ wrt $A_{ij}$ and setting it to 0 :\n",
    "\n",
    "$$\\frac{\\partial R}{\\partial A_{ij}} = \\sum^N_{n=1} \\sum^{T_n}_{t=2} 1 \\left( z_{t-1}^{(n)} = i \\land z_t^{(n)} = j \\right) \\frac{1}{A_{ij}} - \\alpha_i = 0$$\n",
    "\n",
    "Then we can isolate $A_{ij}$\n",
    "\n",
    "$$A_{ij} = \\frac{1}{\\alpha_i} \\sum^N_{n=1}\\sum^{T_n}_{t=2} 1(z_{t-1}^{(n)} =i \\land z_t^{(n)} = j)$$\n",
    "\n",
    "The only variable we dont know is $\\alpha_i$ so lets solve for it\n",
    "\n",
    "---\n",
    "\n",
    "Taking the derivative of $R$ wrt $\\alpha$ just gives is our original constraint\n",
    "\n",
    "$$\\frac{\\partial R}{\\partial \\alpha_i} = 1 - \\sum_{j=1}^M A_{ij} = 0$$\n",
    "\n",
    "then we can take this equeation , set it to 0 , and plug in the expression for $A_{ij}$ that we found previously\n",
    "\n",
    "$$1 - \\sum^M_{j=1} \\frac{1}{\\alpha_i} \\sum^N_{n=1}\\sum^{T_n}_{t=2} 1 \\left( z_{t-1}^{(n)} = i \\land z_t^{(n)} = j \\right) = 0$$\n",
    "\n",
    "Now we can solve for $\\alpha$\n",
    "\n",
    "$$\\alpha_i = \\sum^M_{j=1}\\sum^N_{n=1}\\sum^{T_n}_{t=2} 1\\left( z_{t-1}^{(n)} = i \\land z_t^{(n)} = j \\right)$$\n",
    "\n",
    "summing over all possbile values of j simply means we dont care what j is , this is just as saying just count up how many times the previous state was i , so this can be rewritten as\n",
    "\n",
    "$$\\alpha_i = \\sum^N_{n=1}\\sum^{T_n}_{t=2}1(z_{t-1}^{(n)} = i)$$\n",
    "\n",
    "finally we can plug in $\\alpha_i$ into our expression for $A_{ij}$\n",
    "\n",
    "$$\\large A_{ij} = \\frac{\\sum\\limits^N_{n=1}\\sum\\limits^{T_n}_{t=2} 1(z_{t-1}^{(n)}=i \\land z_t^{(n)}=j)}{\\sum\\limits^N_{n=1}\\sum\\limits_{t=2}^{T_N}1(z_{t-1}^{(n)}=i)}$$\n",
    "\n",
    "which is just the count expression we have been using all along !\n",
    "\n",
    "The numerator is just the number of times we were in state i and transitioned to state j\n",
    "\n",
    "The denominator is the number of times we were in state i\n",
    "\n",
    "Therefore we recovered the original update for $A_{ij}$ , the difference now is we derived it rigorously\n",
    "\n",
    "---\n",
    "\n",
    "Now we do the same for $\\pi$\n",
    "\n",
    "$$\\frac{\\partial R}{\\partial \\pi_i} = \\sum^N_{n=1} 1\\left(z_1^{(n)} = i\\right) \\frac{1}{\\pi_i} - \\beta = 0$$\n",
    "\n",
    "$$\\pi_i = \\frac{1}{\\beta} \\sum^N_{n=1} 1\\left( z_1^{(n)} = i \\right)$$\n",
    "\n",
    "again we arrive at an expressino in terms of the lagrange multiplier , this time $\\beta$ , wo we need to solve for $\\beta$\n",
    "\n",
    "---\n",
    "\n",
    "$$\\frac{\\partial R}{\\partial \\beta} = 1 - \\sum^M_{i=1} \\pi_i = 0$$\n",
    "\n",
    "again we get our original constraint on $\\pi$ , next we lets plug-in the expressino we found for $\\pi$ and solve for $\\beta$\n",
    "\n",
    "$$1-\\sum^M_{i=1} \\frac{1}{\\beta} \\sum^N_{n=1} 1 \\left( z_1^{(n)} = i \\right) = 0$$\n",
    "\n",
    "again since we sum over all the possible values of i , the value of i doesnot matter , the output of the sum is always 1\n",
    "\n",
    "$$\\beta = \\sum_{n=1}^N \\sum_{i=1}^M 1 \\left( z_1^{(n)} = i \\right) = \\sum^N_{n=1} 1 = N$$\n",
    "\n",
    "finally we can get $\\pi$ :\n",
    "\n",
    "$$\\pi_i = \\frac{1}{N} \\sum_{n=1}^N 1 \\left( z_1^{(n)} = i\\right)$$\n",
    "\n",
    "so $\\pi_i$ is just the count of how many times we started a sequence in state i divided by the total number of sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to look at how we train a HMM in the most general case , where the hidden states are really unobserved\n",
    "\n",
    "we will begin by understanding how to train an HMM from an intuitive / mechanical perspective , this should be enough to implement HMM in code\n",
    "\n",
    "Then we will be deriving the algorithm from first principles , which is very nontrivial\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Training an HMM</h4>\n",
    "\n",
    "lets begin by remembering how we train an HMM when we know the hidden states \n",
    "\n",
    "$$\\pi_i = \\frac{count(z_1=i)}{N}$$\n",
    "\n",
    "$$A_{ij} = \\frac{count(z_t=i \\rightarrow z_{t+1} = j)}{count(z_t=i)}$$\n",
    "\n",
    "$$B_{jk} = \\frac{count(z_t = j \\land x_t = k)}{count(z_t = j)}$$\n",
    "\n",
    "so it was just a matter of counting , this is the basis for our intuition\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Bernoulli Distribution</h4>\n",
    "\n",
    "Here is another way we can look at these estimates , to keep things simple , lets reconsider the Bernoulli distribution , again we have only one parameter $\\theta$ which is the probability of success , we usualyy say success = 1 , fail = 0\n",
    "\n",
    "$$f_{Bernoulli}(x;\\theta) = \\theta^x \\left(1-\\theta \\right)^{1-x}$$\n",
    "\n",
    "$p(succcess) = \\theta $\n",
    "\n",
    "One important fact about this is that :\n",
    "\n",
    "$$E\\left[X\\right] = \\theta$$\n",
    "\n",
    "\n",
    "$$\\hat \\theta = \\frac{1}{N} \\sum^N_{i=1} x_i = \\frac{1}{N} \\sum^N_{i=1} 1(x_i=1) = \\frac{count(x=1)}{N}$$\n",
    "\n",
    "even though X can never actually take on the value $\\theta$ , (for ex if it is a fractional value : 0.5) , the expected value can still be a fractional value\n",
    "\n",
    "Also recall that the maximum likelihood estimate of $\\theta$ which we called $\\hat \\theta$ is just the sum of all $x$s divided by N , or in other words the sample mean , we can also write this in terms of counting , its the number of times that $x=1$ divided by N\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Categorical Distribution</h4>\n",
    "\n",
    "$$f_{categorical}(x;\\theta) = \\prod^{K}_{k=1}\\theta_k^{1(x=k)}$$\n",
    "remember what we said earlier :\n",
    "\n",
    "$$\\theta_k = p(x=k)$$\n",
    "\n",
    "We can see that its the same story as the Bernoulli Distribution\n",
    "\n",
    "$$\\hat \\theta_k = \\frac{count(x=k)}{N}$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Training an HMM</h4>\n",
    "\n",
    "using this knowledge we can rewrite our estimates of the parameters of the HMM in terms of probabilities rather than counts (which are the basis of our intuition)\n",
    "\n",
    "for $\\pi$ this is trivial\n",
    "\n",
    "$$\\pi_i = \\frac{count(z_1=i)}{N} = p(z_1 = i)$$\n",
    "\n",
    "for $A,B$ we can divide both numerator and denominator by $N$ which then gives us probabilities on both top and bottom\n",
    "\n",
    "$$A_{ij} = \\frac{count(z_t=i \\rightarrow z_{t+1} = j)}{count(z_t = i)} = \\frac{count(z_t=i \\rightarrow z_{t+1} = j)/N}{count(z_t = i)/N} = \\frac{p(z_t=i,z_{t+1}=j)}{p(z_t=i)}$$\n",
    "\n",
    "$$B_{jk} = \\frac{count(z_t=j \\land x_t = k)}{count(z_t = j)} = \\frac{count(z_t=j \\land x_t = k)/N}{count(z_t = j)/N} = \\frac{p(z_t=j,x_t=k)}{p(z_t=j)}$$\n",
    "\n",
    "of course this should make sense given Bayes rule , the numerator is just the joint probability and the denominator is just the marginal probability\n",
    "\n",
    "---\n",
    "\n",
    "if we look at this in more detail we realise that we are not quite done yet , we still have a pesky time variable t\n",
    "\n",
    "if we <strong>index by time</strong> , then we only count the expected number of transitions during that time\n",
    "\n",
    "$$p(z_t = i , z_{t+1} = j) = expected \\ \\# \\ of \\ times \\ i \\rightarrow \\ j \\ at \\ time \\ t$$\n",
    "\n",
    "but if we <strong>sum over time</strong> , we count all the expected number of transitions over all time\n",
    "\n",
    "$$\\sum_{t=1}^{T-1} p(z_t=i,z_{t+1}=j) = \\ expected \\ \\# \\ of \\ times \\ i \\rightarrow \\ j \\ in \\ total$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>A & B expected counts</h4>\n",
    "\n",
    "so what we end up with is the sum over time in the numerator and denominator for both A and B\n",
    "\n",
    "$$A_{ij} = \\frac{\\sum\\limits^{T-1}_{t=1}p(z_t=i,z_{t+1}=j)}{\\sum\\limits^{T-1}_{t=1} p(z_t = i)}$$\n",
    "\n",
    "$$B_{jk} = \\frac{\\sum\\limits^T_{t=1} p(z_t = j , x_t=k)}{\\sum\\limits^T_{t=1}p(z_t=j)}$$\n",
    "\n",
    "for B we sum all the way up to T because it doesnot depend on a time transition just a single time t\n",
    "\n",
    "note : there is no need to sum over time for $\\pi$ since $\\pi$ refers to the first time step only\n",
    "\n",
    "---\n",
    "\n",
    "<h4>How do we find these probabilities</h4>\n",
    "\n",
    "At this point we can work on defining these probabilities , we will begin by writing the answer , which is highly non-trivial :\n",
    "\n",
    "we define a new variable :\n",
    "\n",
    "$$\\xi_t(i,j) = p(z_t = i , z_{t+1} = j | x)$$\n",
    "\n",
    "Then we can apply Bayes rule :\n",
    "\n",
    "$$= \\frac{p(z_t=i,z_{t+1}=j,x)}{p(x)}$$\n",
    "\n",
    "then we can replace $p(x)$ with the probability on top marginalised over i and j\n",
    "\n",
    "$$ = \\frac{p(z_t=i,z_{t+1}=j,x)}{\\sum\\limits^M_{i=1}\\sum\\limits^M_{j=1}p(z_t=i,z_{t+1}=j,x)}$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>But how do we find this probability</h4>\n",
    "\n",
    "Luckily we can express this new variable in terms of existing variables\n",
    "\n",
    "$$\\xi_t(i,j) = \\frac{\\alpha_t(i) A(i,j)B(j,x(t+1))\\beta_{t+1}(j)}{\\sum\\limits^M_{i=1}\\sum\\limits^M_{j=1} \\alpha_t(i) A(i,j) B(j,x(t+1))\\beta_{t+1}(j)}$$\n",
    "\n",
    "Does this even make any sense !?\n",
    "\n",
    "---\n",
    "\n",
    "<h4>How does it work ?</h4>\n",
    "\n",
    "we can of course plug in each probability to make sure that this makes sense\n",
    "\n",
    "$$\\alpha_t(i)A(i,j)B(j,x(t+1))\\beta_{t+1}(j)$$\n",
    "\n",
    "$$= p(x_1,\\ldots,x_t,z_t=i)p(z_{t+1} =j|z_t = i)p(x_{t+1}|z_{t+1}=j)p(x_{t+2},\\ldots,x_T|z_{t+1}=j)$$\n",
    "\n",
    "$$=p(x_1,\\ldots,x_T,z_t=i,z_{t+1}=j)$$\n",
    "\n",
    "Another thing we can do is think of this in terms of a picture\n",
    "\n",
    "<img src=\"extras/25.15.PNG\" width = \"600\"><img>\n",
    "\n",
    "<span style=\"color: green;\"> $\\alpha$ represents the probability of seeing every observation up to time t then arriving in state i at time t</span>\n",
    "\n",
    "<span style=\"color: #33BBFF;\">from here we transition from state i to state j , thats represented by $A_{ij}$ </span>\n",
    "\n",
    "<span style=\"color: orange;\">from here we emit the symbol $x_{t+1}$ , thats represented by $B_{j,x(t+1)}$</span>\n",
    "\n",
    "<span style=\"color: purple;\">finally $\\beta$ takes care of the rest of the sequence , it represents the probability of seeing the observed sequence from $x_{t+2}$ and onwards and that we were in state j at time t+1</span>\n",
    "\n",
    "So this gives us the joint distribution with all the variables we are looking for\n",
    "\n",
    "next we are going to use $\\psi$ to re-estimate all our variables $\\pi,A,B$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Re-Estimating A</h4>\n",
    "\n",
    "lets rewrite the expression for A , this time we use the conditional probability , saying x is given , to make it explicit that these observations depend on observation sequence x\n",
    "\n",
    "$$A_{ij} = \\frac{\\sum\\limits^{T-1}_{t=1}p(z_t=i,z_{t+1}=j|x)}{\\sum\\limits^{T-1}_{t-1}p(z_t=i|x)}$$\n",
    "\n",
    "note : we are going to assume that we are learning from a single observation sequence , so there is no summing over N\n",
    "\n",
    "The numerator here is just the $\\xi$ variable that we defined\n",
    "\n",
    "$$\\xi(i,j) = p(z_i = i , z_{t+1} = j | x)$$\n",
    "\n",
    "The denominator is just the numerator marginalised over state j , this means we can take $\\xi$ and sum over all values of j , typically we assign this to a new variable $\\gamma$\n",
    "\n",
    "$$\\gamma_t(i) = p(z_t = i|x) = \\sum^M_{j=1}\\xi_t(i,j)$$\n",
    "\n",
    "then we can rewrite $A$ as :\n",
    "\n",
    "$$A_{ij} = \\frac{\\sum\\limits^{T-1}_{t=1}\\xi_t(i,j)}{\\sum\\limits^{T-1}_{t=1}\\gamma_t(i)}$$\n",
    "\n",
    "as we can see we end up marginalising over T , the idea is because there may be multiple of these transitions over the entire sequence we are going to marginalise over time , we can also think of this as calculating the expected count of these events\n",
    "\n",
    "$$A_{ij} = \\frac{\\sum\\limits^{T-1}_{t=1}\\xi_t(i,j)}{\\sum\\limits^{T-1}_{t=1}\\gamma_t(i)} = \\frac{expected \\ \\# \\ of \\ transitions \\ from \\ i \\rightarrow j}{expected \\ \\# \\ of \\ times \\ in \\ state \\ i}$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Re-Estimating $\\pi$</h4>\n",
    "\n",
    "At this point it is easy to see how we can re-estimate $\\pi$ , $\\pi_i$ is just the probability that we were in state i at time t=1 , well that basically $\\gamma_1$\n",
    "\n",
    "$$\\pi_i = \\gamma_1(i) = p(z_1 = i|x)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Re-Estimating B</h4>\n",
    "\n",
    "This one is a little more tricky , we can start with the probabilities , but now making it explicit that each probability depends on the observation sequence x\n",
    "\n",
    "$$B_{jk} = \\frac{\\sum\\limits^T_{t=1} p(z_t=j,x_t=k|x)}{\\sum\\limits^T_{t=1}p(z_t=j|x)}$$\n",
    "\n",
    "The trick here is to realise that we know x since that is the observation , we have observed it \n",
    "\n",
    "the denominator probability of being in state j at time t is just the $\\gamma$ variable\n",
    "\n",
    "therefore the probability of being in state j at time t while also observing the symbol k is just the sum of all the gammas but only when the observation was the symbol k , so we can make use of the indicator function again which only returns 1 when the argument is true and false otherwise\n",
    "\n",
    "so we can rewrite B as :\n",
    "\n",
    "$$B_{jk} = \\frac{\\sum\\limits^T_{t=1}\\gamma_t(j)1(x_t=k)}{\\sum\\limits^T_{t=1}\\gamma_t(j)}$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Important detail</h4>\n",
    "\n",
    "one thing to realise is that this is not like training a Markov Model\n",
    "\n",
    "In a markov model , we just take our observations and assign the parameters once through counting\n",
    "\n",
    "But here we have a unique situation , we essentialy have two groups of calculations\n",
    "\n",
    "<img src=\"extras/25.16.PNG\" width=\"300\"></img>\n",
    "\n",
    "<ul>\n",
    "    <li>Group 1 : calculate $\\xi,\\gamma$</li>\n",
    "    <li>Group 2 : calculate $\\pi,A,B$ from $\\xi,\\gamma$</li>\n",
    "    \n",
    "</ul>\n",
    "\n",
    "But here is the issue , $\\xi,\\gamma$ depend on A&B !\n",
    "\n",
    "In other words there is this circular dependancy between the first set of calculations and the second set of calculations\n",
    "\n",
    "---\n",
    "\n",
    "<h4>An Iterative Algorithm</h4>\n",
    "\n",
    "In reality this is an iterative algorithm\n",
    "\n",
    "what we have arrived at is a general algorithm known as EM expectation-maximisation (EM)\n",
    "\n",
    "EM is a general method , the specific instance of the EM algorithm for HMMs is called the <strong>Baum-Welch algorithm</strong>\n",
    "\n",
    "lets look at a psuedo code:\n",
    "\n",
    "```\n",
    "Loop until covergance:\n",
    "    #E-step\n",
    "    xi = ...\n",
    "    gamma = ...\n",
    "\n",
    "    # M-step\n",
    "    pi = ... , A = ... , B = ...\n",
    "```\n",
    "\n",
    "here is how it works\n",
    "\n",
    "we are going to have an outer loop which loops until convergance\n",
    "\n",
    "inside this loop we first perform the E-step / Expectation Step which means calculating $\\xi,\\gamma$\n",
    "\n",
    "After we do that we then re-estimae $\\pi,A,B$ in the M-step / Maximisation step\n",
    "\n",
    "and we keep doing this until convergance\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Notes</h4>\n",
    "\n",
    "It can be shown that EM always leads to monotonically increasing objective (in our case , this is the likelihood p(x))\n",
    "\n",
    "In reality , this might not always be the case due to numeric stability issues\n",
    "\n",
    "Another thing to consider is that this procedure leads only to local maximum and not global maximum , in fact there are no algorithms known to find global maximum\n",
    "\n",
    "one strategy is to do multiple restarts (as we do with k-means) , so we run multiple times and take the best answer , again , this is becasue we get a different result each time thanks to the fact that we can only find local maximums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to rigorously derive the HMM updates that we saw in the previous section\n",
    "\n",
    "To begin this discussion we have to talk about how the expectation maximisation algorithm works , we wont be deriving EM itself (check 16 : Gaussian Mixture models) , since its only the resulting steps that are important since thats waht we have to follow\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Expectation-Maximisation</h4>\n",
    "\n",
    "The EM algorithm is an algorithm applied to models where we want to do maximum-likelihood but we cant \n",
    "\n",
    "This is usually because we have to sum over latent variables , in other words :\n",
    "\n",
    "$$p(x) = \\sum_z p(z,x)$$\n",
    "\n",
    "practically speaking this is because we usually end up maximising the log-likelihood and there are no mathematical identities which allow us to simplify the log of a sum\n",
    "\n",
    "$$\\log p(x) = \\log \\sum_z = p(z,x)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Example : Gaussian Mixture Model</h4>\n",
    "\n",
    "One of the simplest examples of this scenario is the Gaussian mixture model\n",
    "\n",
    "Unlike a single gaussian , whose likelihood is easy to maximise in a closed form expression , we cant just take the log of the sums and maximise it wrt the parameters\n",
    "\n",
    "$$P_{GMM}(x) = \\sum_{i=1}^M \\pi_i p(x;\\mu_i,\\sigma_i^2)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>HMM Log-Likelihood</h4>\n",
    "\n",
    "lets write the HMM log-likelihood explicitly , this is what we would get as our objective function :\n",
    "\n",
    "$$J = log \\sum_{z_1} \\ldots \\sum_{z_T} p(z_1)p(x_1|z_1)\\prod_{t=2}^T p(z_t|z_{t-1})p(x_t|z_t)$$\n",
    "\n",
    "as we can see taking the log doesnot simplify this expression at all\n",
    "\n",
    "More importantly even if we do try to differentiate this wrt parameters and set the gradient to 0 , we would still not be able to arrive at a closed form solution for $\\pi,B$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>EM Approach</h4>\n",
    "\n",
    "<img src=\"extras/25.17.PNG\" width=\"500\"></img>\n",
    "\n",
    "The genreal EM approach is this , instead of optimising $p(x)$ directly , we instead find some other function which is a lower bound on $p(x)$ , this is the E-step (given the \"starting point\" , find the red curve)\n",
    "we maximise the surrogate function (red curve) instead wrt to the parameters , this is the M-step\n",
    "\n",
    "This is guaranteed to give us an improvement in our actual objective (green curve)\n",
    "\n",
    "The idea is maximising $p(x)$ our original objective is infeasable but maximising the surrogate function instead is easy\n",
    "\n",
    "Geometrically we can picture the process as follows\n",
    "\n",
    "first in step 1 the E-step , we have some setting of the parameters whatever they may be , they are randomly initialised an change everytime we run through the algorithm\n",
    "\n",
    "Then we find a lower bound function that touches the true objective function at this setting of the parameters\n",
    "\n",
    "in step 2 , the M-step we maximise this lower bound function wrt the parameters\n",
    "\n",
    "This also leads to an incrrease in our true objective as well , and we just keep repeating this process until we hit a local optimum\n",
    "\n",
    "---\n",
    "\n",
    "<h4>EM Approach in Math</h4>\n",
    "\n",
    "Mathematically here is how it works \n",
    "\n",
    "E-step:\n",
    "\n",
    "we calculate the posterior distribution $p(z|x)$ (z and x are sequences) , we will call this Q(z) \n",
    "\n",
    "$$Q(z) \\equiv p(z|x)$$\n",
    "\n",
    "note : this is done using old values of $\\pi,A,B$ , so for this $Q(z)$ function $\\pi,A,B$ are considered constant\n",
    "\n",
    "M-step:\n",
    "\n",
    "In the M-step we maximise the surrogate function involving $Q(z)$ and the joint distribution $p(z,x)$\n",
    "\n",
    "$$\\pi,A,B = \\arg \\max_{\\pi,A,B} = \\sum_z Q(z) \\log \\frac{p(z,x)}{Q(z)}$$\n",
    "\n",
    "importantly this maximisation also involves constraints , in particular :\n",
    "\n",
    "sum over $\\pi$ must be 1\n",
    "\n",
    "$$\\sum^M_{i=1} \\pi_i = 1$$\n",
    "\n",
    "sum over each row in A and B must be 1\n",
    "\n",
    "$$\\sum^M_{j=1}A_{ij} =1 , \\forall i=1\\ldots M$$\n",
    "\n",
    "$$\\sum^K_{k=1}B_{jk} =1 , \\forall j=1\\ldots M$$\n",
    "\n",
    "Now that we have our problem setup , the rest of the work is purely mechanical\n",
    "\n",
    "note : to start we are going to do this derivation with respect to a single sequence X , later we will look at variations of this as how to train HMMs with multiple observation sequences and contineous observations\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Simplify Objective</h4>\n",
    "\n",
    "lets start by writing up the objective and simplifying it\n",
    "\n",
    "our objective is as follows:\n",
    "\n",
    "$$\\pi,A,B = \\arg \\max_{\\pi,A,B} \\sum_z Q(z) \\log \\frac{p(z,x)}{Q(z)}$$\n",
    "\n",
    "the first thing we can do is apply the $\\log$ rule to seperate the numerator $p(z,x)$ and the denominator $Q(z)$\n",
    "\n",
    "$$ = \\arg \\max_{\\pi,A,B} \\sum_z Q(z) \\left\\{ \\log p(z,x) - \\log Q(z) \\right\\} $$\n",
    "\n",
    "since $Q(z)$ doesnot depend on the parameter , it can be removed from the objective and it can be removed from the argmax\n",
    "\n",
    "$$= \\arg \\max_{\\pi,A,B} \\sum_z Q(z) \\log p(z,x)$$\n",
    "\n",
    "next we can replace $p(z,x)$ with the expression involving HMM probabilities\n",
    "\n",
    "$$= \\arg \\max_{\\pi,A,B} \\sum_z Q(z) \\log p(z_1) p(x_1|z_1) \\prod^T_{t=2} p(z_t|z_{t-1})p(x_t|z_t)$$\n",
    "\n",
    "next we replace each probability with our parameters $\\pi,A,B$\n",
    "\n",
    "$$\\large = \\arg \\max_{\\pi,A,B} \\sum_z Q(z) \\left\\{ \\log \\pi_{z_1} + \\log B_{z_1x_1} + \\sum_{t=2} ^T \\left( \\log A_{z_t,z_{t-1}} + \\log B_{z_t,x_t}\\right) \\right\\}$$\n",
    "\n",
    "note that they are still indexed by arbitary random variables however\n",
    "\n",
    "In order to change them into actual probability distributions , we have to use products and indicator functions\n",
    "\n",
    "$$\\large = \\arg \\max_{\\pi,A,B}\\sum_z Q(z) \\left\\{ \\log \\prod^M_{j=1} \\pi_j^{1(z_1=j)} + \\log \\prod^M_{j=1} \\prod^K_{k=1} B_{jk}^{1(z_1=j \\land x_1 = k)} + \\\\ \\sum^T_{t=2}  \\left( \\log \\prod^M_{i=1} \\prod^M_{j=1} A_{ij}^{1(z_t = j \\land z_{t-1} = i)} + \\log \\prod^M_{j=1} \\prod^K_{k=1} B_{jk}^{1(z_t=j \\land x_t=k)} \\right) \\right\\}$$\n",
    "\n",
    "next we can use the log rule to turn all these products into sums , we can also conveniently cobine a few of these summations \n",
    "\n",
    "$$\\arg \\max_{\\pi,A,B} \\sum_z Q(z) \\left\\{ \\sum^M_{j=1} \\sum^K_{k=1} \\left( 1(z_1 = j) \\log \\pi_j +  1(z_1 = j \\land x_1 = k) \\log B_{jk}\\right) + \\\\  \\sum^T_{t=2} \\sum^M_{i=1}\\sum^M_{j=1}\\sum^K_{k=1} \\left( 1(z_t = j \\land z_{t-1} = i) \\log A_{ij} + 1(z_t = j \\land x_t = k) \\log B_{jk} \\right)\\right\\}$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Lagrangian</h4>\n",
    "\n",
    "Now that we have the final form of our objective , we can construct the Lagrangian\n",
    "\n",
    "we need 3 sets of lagrange multipliers , one for the $\\pi$ constraint , one for the $A$ constraint and one for the $B$ constraint\n",
    "\n",
    "we will use the letters $\\delta,\\varepsilon,\\zeta$ (since we already used $\\alpha$,$\\beta$,$\\gamma$ elsewhere in HMM model) , we use $R$ to represent  the Lagrangian\n",
    "\n",
    "$$R(\\pi,A,B,\\delta,\\varepsilon,\\zeta) =  \\sum_z Q(z) \\left\\{ \\sum^M_{j=1} \\sum^K_{k=1} \\left( 1(z_1 = j) \\log \\pi_j +  1(z_1 = j \\land x_1 = k) \\log B_{jk} \\right) + \\\\  \\sum^T_{t=2} \\sum^M_{i=1}\\sum^M_{j=1}\\sum^K_{k=1} \\left( 1(z_t = j \\land z_{t-1} = i) \\log A_{ij} + 1(z_t = j \\land x_t = k) \\log B_{jk} \\right)\\right\\} + \\\\ \\delta(1-\\sum^M_{i=1}\\pi_i) + \\sum_{i=1}^M \\varepsilon_i(1-\\sum^M_{j=1}A_{ij}) + \\sum^M_{j=1}\\zeta_j(1-\\sum^K_{k=1}B_{jk})$$\n",
    "\n",
    "The next step is to differentiate R wrt $\\pi,A,B,\\delta,\\varepsilon,\\zeta$\n",
    "\n",
    "---\n",
    "Since $\\pi$ goes with $\\delta$ , we will differentiate these first\n",
    "\n",
    "Differentiate R wrt $\\pi$\n",
    "\n",
    "$$\\frac{\\partial R}{\\partial \\pi_i} = Q(z_1)1(z_1 = i) \\frac{1}{\\pi_i} - \\delta = 0$$\n",
    "\n",
    "Isolate $\\pi$\n",
    "\n",
    "$$\\pi_i = \\frac{1}{\\delta}Q(z_1)1(z_1=i)$$\n",
    "\n",
    "Next we differentiate R wrt $\\delta$ (this is just the original constraint):\n",
    "\n",
    "$$\\frac{\\partial R}{\\partial \\delta} - 1 - \\sum^M_{i=1} \\pi_i = 0$$\n",
    "\n",
    "plug in $\\pi$ into this equation:\n",
    "\n",
    "$$1 - \\sum^M_{i=1} \\frac{1}{\\delta} Q(z_1) 1(z_1 = i) = 0$$\n",
    "\n",
    "Isolate $\\delta$:\n",
    "\n",
    "$$\\delta = \\sum^M_{i=1}Q(z_1)1(z_1=i)$$\n",
    "\n",
    "now replace what $\\delta$ in the expression of $\\pi$ with what we just found :\n",
    "\n",
    "$$\\pi_i = \\frac{Q(z_1)1(z_1=i)}{\\sum\\limits^M_{i=1}Q(z_1)1(z_1=i)}$$\n",
    "\n",
    "note : we will take care of Q later\n",
    "\n",
    "---\n",
    "\n",
    "Next we move on to A and $\\varepsilon$\n",
    "\n",
    "Differentiate R wrt A\n",
    "\n",
    "$$\\frac{\\partial R}{\\partial A_{ij}} = \\sum_z Q(z) \\sum^T_{t=2} 1(z_t = j \\land z_{t-1} = i) \\frac{1}{A_{ij}} - \\varepsilon_i = 0$$\n",
    "\n",
    "Isolate A\n",
    "\n",
    "$$A_{ij} = \\frac{1}{\\varepsilon_i}\\sum_z Q(z) \\sum^T_{t=2} 1(z_t = j \\land z_{t-1} = i)$$\n",
    "\n",
    "Differentiate R wrt $\\varepsilon$ (again just the original constraint)\n",
    "\n",
    "$$\\frac{\\partial R}{\\partial \\varepsilon_i} = 1 - \\sum^M_{j=1} A_{ij} = 0$$\n",
    "\n",
    "and again we plug in the expression for A:\n",
    "\n",
    "$$1-\\sum^M_{j=1} \\frac{1}{\\varepsilon_i}\\sum_z Q(z) \\sum_{t=2}^T 1(z_t = j \\land z_{t-1} = i ) = 0$$\n",
    "\n",
    "then we isolate $\\varepsilon$\n",
    "\n",
    "$$\\varepsilon_i = \\sum^M_{j=1}\\sum_zQ(z) \\sum^T_{t=2} 1(z_t = j \\land z_{t-1} = i)$$\n",
    "\n",
    "since we are summing an indicator variable over all values of j , this simply erases the  , since it is equivalent to say it doesnot matter what j is , just count 1 anyway\n",
    "\n",
    "$$\\varepsilon_i = \\sum_z Q(z) \\sum^T_{t=2} 1(z_{t-1} = i)$$\n",
    "\n",
    "now return to get an expression for the update of A\n",
    "\n",
    "$$A_{ij} = \\frac{\\sum\\limits_z Q(z) \\sum\\limits^T_{t=2} 1(z_t = j \\land z_{t-1} = i)}{\\sum\\limits_z Q(z) \\sum\\limits^T_{t=2} 1(z_{t-1} = i)}$$\n",
    "\n",
    "note thaat aside from Q , this looks almost exactly like our \"counting\" method\n",
    "\n",
    "---\n",
    "\n",
    "one last time for $B,\\zeta$\n",
    "\n",
    "Differentiate R wrt B :\n",
    "\n",
    "$$\\frac{\\partial R}{\\partial B_{jk}} = \\sum_z Q(z) \\sum^T_{t=1} 1(z_t= \\land x_t = k)\\frac{1}{b_{jk}} - \\zeta_j$$\n",
    "\n",
    "note that unlike A where we sum from t = 2 ... T , we can start from t = 1 here since the observation probability also applies to the first time step\n",
    "\n",
    "isolate $B_{jk}$ in terms of $\\zeta$\n",
    "\n",
    "$$B_{jk} = \\frac{1}{\\zeta_j} \\sum_z Q(z) \\sum^T_{t=1} 1(z_t=j \\land x_t = k)$$\n",
    "\n",
    "Differentiate R wrt $\\zeta$\n",
    "\n",
    "$$\\frac{\\partial R}{\\partial \\zeta_j} = 1 - \\sum^K_{k=1} B_{jk} = 0$$\n",
    "\n",
    "plug in the expresiion for B:\n",
    "\n",
    "$$1 - \\sum^K_{k=1} \\frac{1}{\\zeta_j} \\sum_z Q(z) \\sum^T_{t=1} 1(z_t = j \\land x_t = k) = 0$$\n",
    "\n",
    "now isolate $\\zeta$\n",
    "\n",
    "$$\\zeta_j = \\sum^K_{k=1} \\sum_z Q(z) \\sum^T_{t=1} 1(z_t = j \\land x_t = k)$$\n",
    "\n",
    "we can apply the same trich as before to simplify $\\zeta$ , since we sum over all possible values of k in an indicator , it simply means that we dont care what the value of K is\n",
    "\n",
    "$$\\zeta_j = \\sum_z Q(z) \\sum^T_{t=1} 1(z_t = j)$$\n",
    "\n",
    "finally we can return to the expression of B , plugging in what we just found to obtain the update for B :\n",
    "\n",
    "$$B_{j,k} = \\frac{\\sum\\limits_z Q(z) \\sum\\limits^T_{t=1} 1(z_t = j \\land x_t = k)}{\\sum\\limits_z Q(z) \\sum\\limits_{t-1}^T 1(z_t=j)}$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>What do we do with Q(z) ?</h4>\n",
    "\n",
    "recall that :\n",
    "\n",
    "$$Q(z) = p(z|x)$$\n",
    "\n",
    "lets use the numerator of A as an example :\n",
    "\n",
    "$$\\sum_z p(z|x) \\sum^T_{t=2} 1(z_t = j \\land z_{t-1} = i)$$\n",
    "\n",
    "this can be rewritten as :\n",
    "\n",
    "$$= \\sum^T_{t=2} \\sum_z 1(z_t = j \\land z_{t-1} = i) p(z|x)$$\n",
    "\n",
    "now lets think about the effect of multiplying by indicator dunction and summing over all values of z\n",
    "\n",
    "well , this is the same as saying , take te posterior $p(x)$ suover all the possible values of z except for $z_{t-1}$ and $z_{t}$ which must be set to i and j respectively\n",
    "\n",
    "$$ = \\sum^T_{t=2} \\sum_{z_1} \\ldots \\sum_{t-2} \\sum_{t+1} \\ldots \\sum_T p(z_1,z_2,\\ldots,z_{t-1} = i , z_{t} = j,\\ldots,z_T|x)$$\n",
    "\n",
    "of course this is the same as marginalising over all the $z$s except for $z_{t-1}$ and $z_t$\n",
    "\n",
    "$$= \\sum^T_{t=2} p(z_{t-1} = i , z_t = j | x )$$\n",
    "\n",
    "we can do a similar calculation for the denominator\n",
    "\n",
    "$$\\sum_z Q(z) \\sum^T_{t=2} 1(z_{t-1} = i)$$\n",
    "\n",
    "$$= \\sum_z p(z|x) \\sum^T_{t=2} 1(z_{t-1}=i)$$\n",
    "\n",
    "$$= \\sum^T_{t=2} p(z_{t-1} = i |x)$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<h4>We are done ! :) </h4>\n",
    "\n",
    "we already derived these probabilities and gave them names $\\xi$ and $\\gamma$\n",
    "\n",
    "$$\\xi(i,j) = p(z_t = i , z_{t+1} = j |x)$$\n",
    "\n",
    "$$\\gamma_t(i) = p(z_t = i|x) = \\sum^M_{j=1} \\xi_t(i,j)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>The HMM Updates</h4>\n",
    "\n",
    "<strong>E-step</strong>\n",
    "\n",
    "$$\\large \\xi_t(i,j) = \\frac{\\alpha_t(i)A(i,j)B(j,x(t+1))\\beta_{t+1}(j)}{\\sum\\limits^M_{i=1} \\sum\\limits^M_{j=1} \\alpha_t(i)A(i,j)B(j,x(t+1))\\beta_{t+1}(j)}$$\n",
    "\n",
    "$$\\large \\gamma_t(i) = \\sum^M_{j=1} \\xi_t(i,j) $$\n",
    "\n",
    "<strong>M-step</strong>\n",
    "\n",
    "$$\\large \\pi_i = \\gamma_1(i)$$\n",
    "\n",
    "$$\\large A_{ij} = \\frac{\\sum\\limits^{T-1}_{t=1}\\xi_t(i,j)}{\\sum\\limits^{T-1}_{t=1}\\gamma_t{i}}$$\n",
    "\n",
    "$$\\large B_{jk} = \\frac{\\sum\\limits^T_{t=1}\\gamma_t(j)1(x_t=k)}{\\sum\\limits^T_{t=1}\\gamma_t(j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to choose M (the number of hidden states) ?\n",
    "\n",
    "We have 2 scenarios which are dependant on the dataset :\n",
    "\n",
    "\n",
    "<strong>#1 </strong> M is already known , because the hidden states are not truley hidden\n",
    "\n",
    "<strong>#2 </strong> M is unknown so we treat it like a hyperparameter , and thus we need to use hyperparameter optimisation methods to find the ideal value of M\n",
    "\n",
    "---\n",
    "\n",
    "<h1>M is known</h1>\n",
    "\n",
    "Example : parts-of-speech tagging \n",
    "\n",
    "remember this is when we want to label each word as Noun,Verb,Adjective...\n",
    "\n",
    "<img src=\"extras/25.18.PNG\" width=\"300\"></img>\n",
    "\n",
    "In this scenario we know how many parts of speech there are because this has already been determined by linguists\n",
    "\n",
    "M = total number of POS that exist\n",
    "\n",
    "---\n",
    "\n",
    "<h1>M is unkown</h1>\n",
    "\n",
    "The solution amounts to nothing more than trial and error , to apply this principle we have 2 ways , the \"machine learning\" :) , and the \"statistics\" way :|\n",
    "\n",
    "<h4>The MAchine Learning Way</h4>\n",
    "\n",
    "split data into train and test sets\n",
    "\n",
    "Train HMM with different values of M (on the train set only)\n",
    "\n",
    "then we can plot the log-likelihood on the test set vs M\n",
    "\n",
    "<img src=\"extras/25.19.PNG\" width=\"300\"></img>\n",
    "\n",
    "since we would like a maximum likelihood model , we can choose the value of M that leads to the best likelihood\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Cross-Validation</h4>\n",
    "\n",
    "But why only have one train set and one test set ? \n",
    "\n",
    "what if our train set wasnt split in such a way that it was representitive of the test set\n",
    "\n",
    "A related approach is to generate multiple training and test sets : cross validation\n",
    "\n",
    "with K-fold Cross validation we pick a value for K (typically K = 5,8,10)\n",
    "\n",
    "then split the data into K peices\n",
    "\n",
    "we iterate K times , in each iteration k=1...K , we choose the k'th peice to be the test set , and all the other peices as the train set\n",
    "\n",
    "The final score is the average of the K test scores (which can be the log-likelihood) \n",
    "\n",
    "<img src=\"extras/25.20.PNG\" width=\"500\"></img>\n",
    "\n",
    "so how do we use Cross-Validation\n",
    "\n",
    "same as before , plot the cross validation score for different values of M , and pick the M that yeilds the highest score\n",
    "\n",
    "---\n",
    "\n",
    "<h4>The Statistics way</h4>\n",
    "\n",
    "In statistics a common model seelction method is AIC or BIC\n",
    "\n",
    "often statisticians print both , and both yeiild the same answer\n",
    "\n",
    "<ul>\n",
    "    <li>AIC = Akaike Information Criterion </li>\n",
    "    <li>BIC = Bayesian Information Criterion</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "$$AIC = 2p - 2 \\log L$$\n",
    "$$BIC = p \\log N - 2 \\log L$$\n",
    "$$L = \\log - likelihood$$\n",
    "$$p = number \\ of \\ parameters$$\n",
    "$$N = number \\ of \\ samples$$\n",
    "\n",
    "One advantage of these is that there is no need to train/test split all our data is train data\n",
    "\n",
    "The idea behind both of these is that we want to penalise a model for using too many parameters , thats why both of these are proportional to P the number of model parameters\n",
    "\n",
    "we would like to choose the model that has the minimum AIC or BIC (these usually seelct the same model)\n",
    "\n",
    "so Plot AIC or BIC vs M , then pick the best \"M\" \n",
    "\n",
    "---\n",
    "\n",
    "<h4>But why Penalise Model Complexity ?</h4>\n",
    "\n",
    "just a reminder of why we would like to penalise a model for using too many parameters\n",
    "\n",
    "recall the typical picture of train loss and test loss against model complexity\n",
    "\n",
    "<img src=\"extras/25.21.PNG\" width=\"300\"></img>\n",
    "\n",
    "The basic idea is , we can always increase the model complexity and obtain a better loss on the training data , thats because a highly complex model can fit almost anything\n",
    "\n",
    "However , this risks overfitting , when we overfit , this means that our model doesnot generalsie well , in other words it makes poor predictions on unseen data and obiously we dont want that since thats the data we truely care about\n",
    "\n",
    "Therefore we want the model complexity to be just right where the model is complex enough to capture the patterns in the data but not so complex that it is fitting to the noise in the train data without being unable to generalise well to unseen test data\n",
    "\n",
    "thats why we would like to penalise model complexity as AIC and BIC both do\n",
    "\n",
    "on the other hand , corss validation and train-test split already looks at unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to discuss how to train an HMM when we have multiple observation sequences\n",
    "\n",
    "lets first think about how we would represent this data\n",
    "\n",
    "usually when we have N samples of dimensionality D , we can put them in an NxD matrix\n",
    "\n",
    "But this becomes a problem when we are talking about sequences since sequences can be of variable length\n",
    "\n",
    "for ex: suppose we are looking at voice samples\n",
    "\n",
    "A sample of someone saying :\n",
    "\n",
    "\"Hello world\"\n",
    "\n",
    "is going to be much shorter than a sample of someone saying\n",
    "\n",
    "\"I learned a lot during Dr. Torki's classes on macine learning\"\n",
    "\n",
    "The simplest way is just to store each individual way as an element in a python list of length N\n",
    "\n",
    "Inside the python list we can have individual numpy arrays of any length , call the length of the nth element/sequence T(n) , n=1...N\n",
    "\n",
    "Each sequence will have its own $\\alpha,\\beta,\\ldots$ indexed by n\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Update Equations</h4>\n",
    "\n",
    "$$\\pi_i = \\frac{1}{N} \\sum^N_{n=1}\\frac{\\alpha_n(1,i)\\beta_n(1,i)}{P(n)}$$\n",
    "\n",
    "$$A(i,j) =  \\frac{\\sum\\limits^N_{n=1} \\frac{1}{P(n)} \\sum\\limits^{T(n)-1}_{t=1} \\alpha_n(t,i)A(i,j)B(j,x_n(t+1))\\beta_n(t+1,j)}{\\sum\\limits^N_{n=1}\\frac{1}{P(n)} \\sum\\limits^{T(n)-1}_{t=1} \\alpha_n(t,i)\\beta_n(t,i)}$$\n",
    "\n",
    "$$B(j,k) = \\frac{\\sum\\limits^N_{n=1} \\frac{1}{P(n)} \\sum\\limits^{T(n)}_{t=1} \\alpha_n(t,j) \\beta_n(t,j) \\ if \\ x_n(t) = k \\ , \\ else \\ 0}{\\sum\\limits^N_{n=1}\\frac{1}{P(n)}\\sum\\limits^{T(n)}_{t=1} \\alpha_n(t,j)\\beta_n(t,j)}$$\n",
    "\n",
    "whats interesting about these equations is that each of the inner sums gets multiplied by the inverse of the probability of the observation ($\\frac{1}{P(n)}$)\n",
    "\n",
    "That means that if , udner the current model , some observation we would like to model is very improbable then we will give the updates based on that observation more weight\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
