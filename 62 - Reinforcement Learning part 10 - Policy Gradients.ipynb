{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpFSxgP2fiJb"
   },
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbNULZVbfmr7"
   },
   "source": [
    "<h3>Policy Gradient Methods</h3>\n",
    "\n",
    "In this notebook of the series we are going to discuss another method of solving the control problem called policy gradient methods\n",
    "\n",
    "We have so far been parameterising the value function but it's plausible that we can parameterise the policy as well\n",
    "\n",
    "In particular we want to find the optimal policy $\\pi^*$\n",
    "\n",
    "At first this might seem like a weird idea compared to what we were doing before\n",
    "\n",
    "Recall that our current strategy is policy iteration\n",
    "\n",
    "We iteratively switch between policy evaluation which means finding the value function given the current, policy and policy improvement which means acting greedily with respect to the current value function\n",
    "\n",
    "We've seen that this converges so that we get the optimal value function for which the optimal policy is just taking the max of this optimal value function\n",
    "\n",
    "---\n",
    "\n",
    "So what would a parameterized policy look like?\n",
    "\n",
    "Well we know that the policy has to be some kind of probability $\\pi(a \\vert s)$ \n",
    "\n",
    "In particular we can score each action $a$ using a linear model or any other kind of model\n",
    "\n",
    "$$score_j = f(a_j,s,\\theta) (= \\varphi(s)^T\\theta_j \\ if \\ linear)$$\n",
    "\n",
    "And then as we know from deep learning we can use the softmax function to turn these actions scores into probabilities\n",
    "\n",
    "$$\\pi(a_j \\vert s) = \\frac{\\exp(score_j)}{\\sum_{j'} \\exp(score_{j'})}$$\n",
    "\n",
    "This ensures that all the probabilities sum to one\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Gradient Objective</h3>\n",
    "\n",
    "For a policy to be optimal it needs to have some objective\n",
    "\n",
    "This is something we should be used to from machine learning \n",
    "\n",
    "Most machine learning methods we've looked at start by trying to optimise some objective\n",
    "\n",
    "If our model is differentiable then we can use gradient descent or gradient ascent to reach our objective\n",
    "\n",
    "Because this notebook is on policy gradient methods, we are of course going to be taking a similar approach with respect to the policy\n",
    "\n",
    "The big question is What should this objective be?\n",
    "\n",
    "Consider that we start in some start state $s_0$ \n",
    "\n",
    "As we know we want to maximize the total return of the entire episode which is $V(s_0)$\n",
    "\n",
    "Also remember that the value function $V$ is also dependent on the policy $\\pi$, so we can explicitly show that by subscripting $V$ with $\\pi$ \n",
    "\n",
    "$$Performance : \\eta(\\theta_P) = V_\\pi(s_0)$$\n",
    "\n",
    "An unfortunate convention is that the letter $\\eta$ is used for the policy objective since we've used it for other purposes in past notebooks \n",
    "\n",
    "Just remember that $\\eta$ in this notebook means policy objective which we usually call the performance\n",
    "\n",
    "Note that $\\theta_p$ here means the parameters we are using to parameterize the policy \n",
    "\n",
    "We are subscripting the policy parameters with $p$ since the value function will also have a set of parameters which we'll call $\\theta_v$\n",
    "\n",
    "\n",
    "$$Policy : \\pi(a \\vert s,\\theta_p) = f(s;\\theta_p)$$\n",
    "\n",
    "$$Value \\ Function \\ Approximation : \\hat V_\\pi(s) = f(s;\\theta_v)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Gradient Methods</h3>\n",
    "\n",
    "The next few steps are unfortunately not straightforward at all\n",
    "\n",
    "However the interpretation makes intuitive sense\n",
    "\n",
    "The important part here is more about being able to implement the algorithm in code so that we have yet another tool for our reinforcement learning toolbox\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Gradient Theorem</h3>\n",
    "\n",
    "It can be shown that the gradient of the performance takes this form which is dependent on the gradient of the policy itself which is convenient\n",
    "\n",
    "$$\\nabla \\eta(\\theta_p) = E \\left[ \\sum_a Q_\\pi(s,a) \\nabla_{\\theta_p} \\pi(a \\vert s, \\theta_p) \\right] $$\n",
    "\n",
    "This is called the policy gradient theorem\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Gradient Methods</h3>\n",
    "\n",
    "What we can do is manipulate this equation by multiplying and dividing by $\\pi$\n",
    "\n",
    "$$\\nabla \\eta (\\theta_p) = E \\left[\\sum_a \\pi(a \\vert s, \\theta_p) Q_\\pi(s,a) \\nabla_{\\theta_p} \\pi(a \\vert s,\\theta_p) \\frac{1}{\\pi(a \\vert s,\\theta_p)}\\right]$$\n",
    "\n",
    "Once we do this we can see that the summation is actually just another expected value over $\\pi$\n",
    "\n",
    "$$\\nabla \\eta (\\theta_p) = E \\left[E \\left\\{Q_\\pi(s,a)\\nabla_{\\theta_p} \\pi(a \\vert s,\\theta_p) \\frac{1}{\\pi(a \\vert s, \\theta_p)}\\right\\}\\right]$$\n",
    "\n",
    "But the expected value of an expected value is still just an expected value, so we can make it one expected value\n",
    "\n",
    "$$\\nabla \\eta (\\theta_p) = E \\left[Q_\\pi(s,a) \\nabla_{\\theta_p} \\pi(a \\vert s,\\theta_p) \\frac{1}{\\pi(a \\vert s,\\theta_p)}\\right]$$\n",
    "\n",
    "---\n",
    "\n",
    "What we can further do is use an identity from calculus \n",
    "\n",
    "The gradient of $\\log f$ is the gradient of $f$ divided by $f$\n",
    "\n",
    "$$\\nabla \\log f(x) = \\frac{\\nabla f(x)}{f(x)}$$\n",
    "\n",
    "Apply this identity to\n",
    "\n",
    "$$\\nabla \\eta (\\theta_p) = E \\left[Q_\\pi(s,a) \\nabla_{\\theta_p} \\pi(a \\vert s,\\theta_p) \\frac{1}{\\pi(a \\vert s,\\theta_p)}\\right]$$\n",
    "\n",
    "to get \n",
    "\n",
    "$$\\nabla \\eta (\\theta_p) = E \\left[Q_\\pi(s,a) \\nabla_{\\theta_p} \\log \\pi(a \\vert s, \\theta_p)\\right]$$\n",
    "\n",
    "---\n",
    "\n",
    "The last step is to realize that $Q$ is actually the expected value of the return $G$\n",
    "\n",
    "So we can replace that with $G$ itself since it all goes inside the expected value\n",
    "\n",
    "$$\\nabla \\eta(\\theta_p) = E \\left[G \\nabla_{\\theta_p} \\log \\pi(a \\vert s,\\theta_p)\\right]$$\n",
    "\n",
    "---\n",
    "\n",
    "Now we have an expression full of stuff we can actually use \n",
    "\n",
    "$G$ which is the return we get from playing an episode and $\\pi$ which is our parameterized policy\n",
    "\n",
    "So what we would do is we would play an episode calculate the returns and then perform gradient ascent\n",
    "\n",
    "Notice that gradient ascent and not gradient descent because we are trying to maximize the total return not minimize it\n",
    "\n",
    "---\n",
    "\n",
    "In fact we could do this as batch gradient ascent because by the time the episode is over we have all the returns\n",
    "\n",
    "In fact this is suggested by the expected value symbol as well\n",
    "\n",
    "We know from before that an expected value can be approximated by sample mean\n",
    "\n",
    "$$E(X) \\approx \\frac{1}{N} \\sum^N_{n=1} X_n$$\n",
    "\n",
    "$$\\nabla \\eta (\\theta_p) \\approx \\frac{1}{T} \\sum^T_{t=1} G_t \\nabla_{\\theta_p} \\log \\pi(a_t \\vert s_t,\\theta_p)$$\n",
    "\n",
    "---\n",
    "\n",
    "But also remember that tensorflow is going to take gradients for us \n",
    "\n",
    "In particular we want just one expression we can pass in as the cost to the optimizer \n",
    "\n",
    "To turn what we have into that form, we realize that $G$ is a constant so it can be moved inside the gradient\n",
    "\n",
    "$$\\frac{1}{T} \\sum^T_{t=1} G_t \\nabla_{\\theta_p} \\log \\pi(a_t \\vert s_t, \\theta_p) = \\frac{1}{T} \\sum^T_{t=1} \\nabla_{\\theta_p} G_t \\log \\pi(a_t \\vert s_t,\\theta_p)$$\n",
    "\n",
    "---\n",
    "\n",
    "We also know that the derivative of a sum is just the sum of all the individual derivatives\n",
    "\n",
    "So we can move the gradient outside the sum\n",
    "\n",
    "$$\\frac{1}{T} \\sum^T_{t=1} \\nabla_{\\theta_p} G_t \\log \\pi(a_t \\vert s_t,\\theta_p) = \\frac{1}{T} \\nabla_{\\theta_p} \\sum^T_{t=1} G_t \\log \\pi (a_t \\vert s_t,\\theta_p)$$\n",
    "\n",
    "---\n",
    "\n",
    "And finally we know that $\\frac{1}{T}$ is a meaningless constant because it can be absorbed into the learning rate, so we can get rid of that too\n",
    "\n",
    "And so finally we have an expression for the thing we want to maximize\n",
    "\n",
    "$$maximise : \\sum^T_{t=1} G_t \\log \\pi(a_t \\vert s_t, \\theta_p)$$\n",
    "\n",
    "$$minimise : - \\sum^T_{t=1} G_t \\log \\pi (a_t \\vert s_t,\\theta_p)$$\n",
    "\n",
    "Since tensorflow optimizers only have a minimised function we can minimize the negative of this \n",
    "\n",
    "And to be clear capital T represents the length of an episode and the index lowercase t represents the t-th timestep of an episode \n",
    "\n",
    "Because this involves the sum of return's over an entire episode, this is a Monte-Carlo method\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Intuition</h3>\n",
    "\n",
    "To gain better intuition about the gradient ascent update rule, it helps to look at what it would look like if we were to do stochastic gradient descent or in other words the update for just one return, one state and one action\n",
    "\n",
    "$$\\theta_{p,t+1} = \\theta_{p,t} + \\alpha G_t \\frac{\\nabla \\pi(a_t \\vert s_t)}{\\pi(a_t \\vert s_t)} (= \\theta_{p,t} + \\alpha G_t \\nabla \\log \\pi(a_t \\vert s_t))$$\n",
    "\n",
    "So there are three terms here that affect the new value of $\\theta$ \n",
    "\n",
    "The return $G$, the gradient of $\\pi$, $\\nabla \\pi(a_t \\vert s_t)$ and $pi$ itself $\\pi(a_t \\vert s_t)$\n",
    "\n",
    "Remember that $\\pi$ is the probability of choosing an action $a$ given state $s$ using the current policy\n",
    "\n",
    "---\n",
    "\n",
    "First consider $F$ the return\n",
    "\n",
    "We are moving in a direction proportional to $G$\n",
    "\n",
    "The bigger $G$ is the bigger step we take\n",
    "\n",
    "This is good because we want to maximize our reward\n",
    "\n",
    "Second consider $\\pi$ the probability of choosing action\n",
    "\n",
    "We are moving in a direction inversely proportional to $pi$\n",
    "\n",
    "This is good because if $pi$ is small but the return is good then we can take an even bigger step in that direction\n",
    "\n",
    "And finally the gradient of $\\pi$, $\\nabla \\pi$ is a vector so that gives us the actual direction we want to go\n",
    "\n",
    "The gradient tells us the direction of greatest increase in $\\pi$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>What about V(s)?</h3>\n",
    "\n",
    "We'll notice that earlier in this section, we mentioned using an approximation of $V(S)$ as well but so far that hasn't come into play\n",
    "\n",
    "One common modification of the policy gradient that we are going to use is to add a baseline\n",
    "\n",
    "So instead of our constant being just $G$ it'll be $G-V(s)$ our prediction of the value at state $s$\n",
    "\n",
    "$$maximise : \\sum^T_{t=1} (G - V(s_t)) \\log \\pi(a_t \\vert s_t, \\theta_p)$$\n",
    "\n",
    "The baseline can actually be any function that depends only on $s$, but of course since we already know about $V$, it seems the most appropriate choice\n",
    "\n",
    "We call this difference between $G$ and $V$ the advantage\n",
    "\n",
    "The reason we want to add a baseline is because it has been shown to have a significant effect on the variance of the update rule\n",
    "\n",
    "This in turn has been shown to speed up learning\n",
    "\n",
    "---\n",
    "\n",
    "The update parameters of $V$ of course just use gradient descent as usual\n",
    "\n",
    "$$\\theta_{V,t+1} = \\theta_{V,t} + \\alpha(G_t - V_t) \\nabla V_t$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>What about TD methods?</h3>\n",
    "\n",
    "A natural question at this point is can we convert this from a Monte Carlo method to a TD method so that we don't have to wait for an episode to end before doing any updates\n",
    "\n",
    "Of course this is possible and in reinforcement learning this has a special name, The actor critic method\n",
    "\n",
    "It's called actor critic because we think of the policy as the actor and the TD error which depends on the value estimate as the critic\n",
    "\n",
    "So in the updates, all we do is replace $G$ with the one step estimate of $G$\n",
    "\n",
    "$$\\theta_{p,t+1} = \\theta_{p,t} + \\alpha(r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)) \\nabla \\log \\pi(a_t \\vert s_t)$$\n",
    "\n",
    "$$\\theta_{V,t+1} = \\theta_{V,t} + \\alpha(r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)) \\nabla V(s_t)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Why use it?</h3>\n",
    "\n",
    "Now that we've gone through the heavy parts of the policy gradient method let's talk about why we might want to use it\n",
    "\n",
    "We know that the policy gradient method yields a probabilistic policy \n",
    "\n",
    "This should be reminiscent of epsilon greedy which is also probabilistic\n",
    "\n",
    "However it should be clear why the policy gradient method is more expressive \n",
    "\n",
    "With epsilon greedy all the suboptimal actions have the same probability of happening even though one might be better than the other \n",
    "\n",
    "With the policy gradient method, we can model this betterness directly\n",
    "\n",
    "For example it might actually be optimal to do action one 90% of the time action two 8% of the time and action 3 only 2% of the time\n",
    "\n",
    "---\n",
    "\n",
    "In addition we should keep in mind that states themselves can be stochastic\n",
    "\n",
    "One of the sources of this randomness is that the state does not give us the full information about the environment\n",
    "\n",
    "For example in blackjack we don't know the dealer's next card\n",
    "\n",
    "So the optimal action needs to be probabilistic to account for different possibilities\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Summary</h3>\n",
    "\n",
    "Now let's summarize this section since there was a lot of information in it\n",
    "\n",
    "First we saw that we can parameterize the policy so that in effect we get a probabilistic policy using a softmax output\n",
    "\n",
    "Next we saw that the objective that the policy tries to optimize is the expected return from the start state\n",
    "\n",
    "In other words this is the expected return over the entire episode\n",
    "\n",
    "We call this objective the performance\n",
    "\n",
    "Next we looked at the policy gradient theorem\n",
    "\n",
    "We manipulated the results of the policy gradient theorem in order to give us a single cost function that we could then input into  tensorflow which is going to be helpful during implementation \n",
    "\n",
    "Next, we looked at a modification of the policy creating an algorithm that uses a baseline and we call this difference between the return and the baseline the advantage \n",
    "\n",
    "We then looked at the actor critic method which uses TD updates instead of Monte-Carlo updates\n",
    "\n",
    "Finally we discussed why we might want to use policy gradient methods rather than policy iteration\n",
    "\n",
    "It allows us to explicitly model arbitrary probabilistic policies when a probabilistic policy could in fact be the optimal policy\n",
    "\n",
    "This could in turn be because of the fact that the state transitions are probabilistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0dNsnW86dug"
   },
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kx8Gl8zJ6gBJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxJBQtyo6jh5"
   },
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn-4dZi16nIH"
   },
   "source": [
    "In this section we're going to extend our knowledge of the policy gradient method by looking at continuous action spaces\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Continuous Action Spaces</h3>\n",
    "\n",
    "Remember that both of the environments we've looked at so far cartpole and mountain car both have discrete action spaces\n",
    "\n",
    "Luckily there is an environment called `MountainCarContinuous-v0` that gives us continuous action spaces, wo we have something to test this out on\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Gradient Method</h3>\n",
    "\n",
    "So if we think about our current policy model that allows us to choose from a discrete action space what's the main idea\n",
    "\n",
    "The main idea is that the output of the model is a probability distribution that we can then sample an action from\n",
    "\n",
    "So if we want to have a continuous action space to sample from how might we go about creating a distribution for that, what distribution might be appropriate?\n",
    "\n",
    "Well technically we could choose any continuous distribution but the gaussian seems like a good place to start\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Continuous Policy</h3>\n",
    "\n",
    "Remember that gaussian has two parameters the mean and the variance\n",
    "\n",
    "$$\\pi(a \\vert s) = \\frac{1}{\\sqrt{2 \\pi \\nu}} e^{-\\frac{1}{2\\nu}(a-\\mu)^2} = N(a;\\mu,\\nu)$$\n",
    "\n",
    "So in order to create a parameterized policy we would parameterize the mean and variance\n",
    "\n",
    "Let's assume for the purposes of this section that both the mean and variance are linear in their parameters\n",
    "\n",
    "So in other words \n",
    "\n",
    "$$\\mu(s;\\theta_\\mu) = \\theta^T_\\mu \\varphi(s)$$\n",
    "\n",
    "$$\\nu(s;\\theta_\\nu) = \\exp(\\theta_\\nu^T \\varphi(s))$$\n",
    "\n",
    "Alternatively we could use the softplus function instead of the exponential\n",
    "\n",
    "$$Alternative : \\nu(s;\\theta_v) = softplus(\\theta_v^T \\varphi(s))$$\n",
    "\n",
    "$$softplus(a) = \\ln(1+\\exp(a))$$\n",
    "\n",
    "---\n",
    "\n",
    "So why would we want to use the exponential or softplus after multiplying the feature vector by $\\theta_\\nu$?\n",
    "\n",
    "Well remember that the variance must be positive but using gradient descent theta is unbounded\n",
    "\n",
    "Therefore in order to make sure the variance is always positive we can exponentiate the output of the dot product or use the softplus\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Softplus</h3>\n",
    "\n",
    "Recall that the softplus is asymptotically the same as `ReLU` but it can actually go down to zero which is useful\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6c/Rectifier_and_softplus_functions.svg\" width=\"450\">\n",
    "\n",
    "This might be a little better than the exponential since the exponential grows very fast if its input is large\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Gradient</h3>\n",
    "\n",
    "Once we've defined the model we have to figure out how to update it\n",
    "\n",
    "Luckily the principles of the policy gradient method don't change just because we've redefined the policy\n",
    "\n",
    "$$\\theta = \\theta + \\alpha \\sum^T_{t=1} (G_t - V(s_t)) \\nabla \\log \\pi(a_t \\vert s_t)$$\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha(r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)) \\nabla \\log \\pi(a_t \\vert s_t)$$\n",
    "\n",
    "We still have the same cost and updates as before\n",
    "\n",
    "And of course we can replace plain gradient descent with any of its modifications like `AdaGrad` or `RMSprop`\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Worth repeating</h3>\n",
    "\n",
    "This is an important idea so it's worth saying again \n",
    "\n",
    "The policy gradient method doesn't change just because we redefine the policy\n",
    "\n",
    "The policy can be any function we make it, and the policy gradient method still applies\n",
    "\n",
    "So we can have any kind of output distribution any kind of parameterization\n",
    "\n",
    "The method remains the same\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Final thought</h3>\n",
    "\n",
    "One last thing to think about \n",
    "\n",
    "This wouldn't be possible if we didn't have policy gradient methods\n",
    "\n",
    "Consider regular $Q$-learning with function approximation \n",
    "\n",
    "Function approximation of the action value $Q$ lets us handle infinite states spaces but not infinite action spaces\n",
    "\n",
    "Remember our earlier strategy\n",
    "\n",
    "We created a different linear model for each action and then took the $arg max$ in order to determine what action to do at any step\n",
    "\n",
    "Deep $Q$-learning is the same where you have a neural network for multiple output nodes one for each separate action \n",
    "\n",
    "Because we need a separate output or model for each action, this clearly can't scale to an infinite number of actions\n",
    "\n",
    "So with $Q$-learning a continuous action space is impossible but with policy gradient methods it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7TKb9oXFEoT"
   },
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwTWzQQCGUjG"
   },
   "source": [
    "In this section we're going to go through more details on how we're going to apply a parameterised policy to the continuous version of mountain car\n",
    "\n",
    "---\n",
    "\n",
    "<h3>MountainCarContinuous-v0</h3>\n",
    "\n",
    "It's good to go over what the differences are between discrete mountain car and continuous mountain car since they are not obvious \n",
    "\n",
    "Specifically the reward structure for mountain car continuous is different\n",
    "\n",
    "We can also see this information on the wiki page <a href=\"https://github.com/openai/gym/wiki/MountainCarContinuous-v0\">here</a>\n",
    "\n",
    "So in continuous mountain car if we get to the goal point then we automatically get a reward of 100\n",
    "\n",
    "So unlike a discrete mounting car our reward can be positive \n",
    "\n",
    "However subtracted from the 100 is the\n",
    "sum of squared actions we take\n",
    "\n",
    "So if the magnitude of your actions is larger each step we take is penalized more\n",
    "\n",
    "So our agent should be incentivized to take smaller actions\n",
    "\n",
    "The environment is considered solved if we can reach an average total reward of 90 over 100 consecutive episodes\n",
    "\n",
    "Note that while the action space is continuous our action must be between  -1 and 1\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Gradient?</h3>\n",
    "\n",
    "Next thing is, while we could implement the policy gradient theory directly we are instead going to take another approach\n",
    "\n",
    "The problem with gradient descent or gradient ascent is that it doesn't seem to converge nicely i this scenario\n",
    "\n",
    "What we'll do instead is a special type of random search called hill climbing\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Hill Climbing</h3>\n",
    "\n",
    "Hill climbing works as follows\n",
    "\n",
    "Suppose we only have two parameters so that we can show the parameters on a 2D grid\n",
    "\n",
    "First we initialize the parameters randomly, this is the green dot\n",
    "\n",
    "<img src=\"extras/62.0.PNG\" width=\"400\">\n",
    "\n",
    "Then we play one episode or a few episodes with these parameters \n",
    "\n",
    "Playing fewer episodes will increase the variance of our estimates and playing more episodes will decrease the variance yielding better estimates but it will take longer\n",
    "\n",
    "The reason we want to play a few estimates is to get an idea if these parameters are good or not\n",
    "\n",
    "If the parameters are better than the best parameters we found so far then we set these as the best parameters, otherwise we stick with the origina\n",
    "\n",
    "---\n",
    "\n",
    "So next thing we do is we generate a new set of parameters by randomly perturbing the current best parameters\n",
    "\n",
    "<img src=\"extras/62.1.PNG\" width=\"400\">\n",
    "\n",
    "So basically this new set of parameters will be in the neighborhood of the current best parameters\n",
    "\n",
    "We then test these new parameters to see if they yield a good average return\n",
    "\n",
    "If they are then we set these to be the current best parameters and so on\n",
    "\n",
    "<img src=\"extras/62.2.PNG\" width=\"400\">\n",
    "\n",
    "---\n",
    "\n",
    "Eventually will have tested many sets of parameters sort of walking on a path up a hill hence the term hill climbing\n",
    "\n",
    "<img src=\"extras/62.3.PNG\" width=\"400\">\n",
    "\n",
    "Note that the path is always going to go in an increasing direction of the average return\n",
    "\n",
    "---\n",
    "\n",
    "<h3>When did we first see this?</h3>\n",
    "\n",
    "We may also recall from deep learning notebooks that this is one of the methods we suggested could be used to find a good set of hyper parameters for training a neural network\n",
    "\n",
    "So this is in fact just another optimization technique not unique to reinforcement learning but particularly helpful in some situations\n",
    "\n",
    "We will see that this method generally yields more consistent results than gradient descent and it beats trying to do a hyper parameter search\n",
    "\n",
    "However since we have all the skills you need to build a parameterized policy and perform gradient ascent on the performance we are encouraged to we will try and see if we can find good hyperparameters\n",
    "\n",
    "We may try using both neural networks and linear models with radial basis function kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jcx0I7526nQw"
   },
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EG2_qC_EqJt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZUGwIGZFJZP"
   },
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtASWKEmFLci"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MctN8OUMFM8F"
   },
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPnAxAxyFP_L"
   },
   "source": [
    "In this section we are going to summarize everything we've learned in this notebook which was focused on the policy gradient method\n",
    "\n",
    "---\n",
    "\n",
    "<h3>PG Summary</h3>\n",
    "\n",
    "The difference between policy gradient methods and non-policy gradient methods is that instead of just modeling the value function ($V(a)$ or $Q(s,a)$) we also create a model for the policy $\\pi(a \\vert s)$\n",
    "\n",
    "This allows us to create a probabilistic policy naturally rather than doing something like epsilon greedy where we have to choose epsilon which we might end up doing suboptimally\n",
    "\n",
    "---\n",
    "\n",
    "We started this notebook by just looking at how we would parameterize the policy or in other words make up a function to model the policy with some learnable parameters\n",
    "\n",
    "Then we asked well how can we optimize this policy?\n",
    "\n",
    "For that we needed to create an objective function for the policy\n",
    "\n",
    "Once we did that we saw that it was just business as usual, learned by playing some episodes and doing gradient descent\n",
    "\n",
    "---\n",
    "\n",
    "One benefit of using policy grittier methods is that it very naturally extends to continuous action spaces\n",
    "\n",
    "So instead of doing softmax the output layer of our model we could just output a mean and variance representing a gaussian and then sample from that gaussian to get an action"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOQhvs6xOO2Pg59IaJE/1zd",
   "collapsed_sections": [
    "MpFSxgP2fiJb",
    "a0dNsnW86dug",
    "xxJBQtyo6jh5",
    "z7TKb9oXFEoT",
    "Jcx0I7526nQw",
    "lZUGwIGZFJZP",
    "MctN8OUMFM8F"
   ],
   "mount_file_id": "14-iKN4yv82U5PzgSYuvphp0r6Km4h6Er",
   "name": "62 - Reinforcement Learning part 10 - Policy Gradients.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
