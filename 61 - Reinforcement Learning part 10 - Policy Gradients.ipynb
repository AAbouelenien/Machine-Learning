{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to discuss another method of solving the control problem called policy gradient methods\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Gradient Methods</h3>\n",
    "\n",
    "We have so far been parameterising the value function but it's plausible that we can parameterise the policy as well\n",
    "\n",
    "In particular we want to find the optimal policy $\\pi^*$\n",
    "\n",
    "At first this might seem like a weird idea compared to what we were doing before\n",
    "\n",
    "Recall that our current strategy is policy iteration\n",
    "\n",
    "We iteratively switch between policy evaluation which means finding the value function given the current policy and policy improvement which means acting greedily with respect to the current value function\n",
    "\n",
    "We've seen that this converges so that we get the optimal value function for which the optimal policy is just taking the arg max of this optimal value function\n",
    "\n",
    "---\n",
    "\n",
    "So what would a parameterize policy look like\n",
    "\n",
    "Well we know that the policy has to be some kind of probability $\\pi(a \\vert s)$ \n",
    "\n",
    "In particular we can score each action $a$ using a linear model or any other kind of model\n",
    "\n",
    "$$\\large score_j = f(a_j,s,\\theta) (= \\varphi(s)^T\\theta_j \\text{ if linear})$$\n",
    "\n",
    "And then as we know from deep learning we can use the softmax function to turn these actions scores\n",
    "into probabilities\n",
    "\n",
    "$$\\large \\pi(a_j \\vert s) = \\frac{exp(score_j)}{\\sum_{j^\\prime} exp(score_{j^\\prime})}$$\n",
    "\n",
    "This ensures that all the probabilities sum to one\n",
    "\n",
    "---\n",
    "\n",
    "For a policy to be optimal it needs to have some objective\n",
    "\n",
    "This is something we should be used to from machine learning \n",
    "\n",
    "Most machine learning methods we've looked at start by trying to optimize some objective\n",
    "\n",
    "If our model is differentiable then we can use gradient descent or gradient assent to reach our objective\n",
    "\n",
    "Because this notebook is on policy gradient methods, we are of course going to be taking a similar approach with respect to the policy\n",
    "\n",
    "The big question is, What should this objective be?\n",
    "\n",
    "---\n",
    "\n",
    "Consider that we start in some start state as not as you know we want to maximize the total return of\n",
    "\n",
    "the entire episode which is of not.\n",
    "\n",
    "Also remember that the value function V is also dependent on the policy pi.\n",
    "\n",
    "So we can explicitly show that by subscripting value of pi and unfortunate convention is that the letter\n",
    "\n",
    "Eira is used for the policy objective.\n",
    "\n",
    "Since we've used it for other purposes in past courses just remember that Ada in this course means policy\n",
    "\n",
    "objective which we usually call the performance.\n",
    "\n",
    "Note that theta here means the parameters we are using to parameterize the policy we are subscripting\n",
    "\n",
    "the policy parameters with P since the value function will also have a set of parameters which we'll\n",
    "\n",
    "call theta V.\n",
    "\n",
    "The next few steps are unfortunately not straightforward at all.\n",
    "\n",
    "However the interpretation makes intuitive sense.\n",
    "\n",
    "So if you want to just skim this lecture that's fine.\n",
    "\n",
    "The important part here is more about being able to implement the algorithm in code so that you have\n",
    "\n",
    "yet another tool for your reinforcement learning toolbox.\n",
    "\n",
    "It can be shown that the gradient of the performance takes this form which is dependent on the gradient\n",
    "\n",
    "of the policy itself which is convenient.\n",
    "\n",
    "This is called the policy gradient theorem.\n",
    "\n",
    "What you can do is manipulate this equation by multiplying and dividing by pi.\n",
    "\n",
    "Once we do this we can see that this summation is actually just another expected value over PI.\n",
    "\n",
    "But the expected value of an expected value is still just an expected value.\n",
    "\n",
    "So we can make it one expected value\n",
    "\n",
    "what we can further do is use an identity from calculus the gradient of log f is the gradient of f divided\n",
    "\n",
    "by F..\n",
    "\n",
    "The last step is to realize that Q is actually the expected value of the returned G.\n",
    "\n",
    "So we can replace that with P itself since it all goes inside the expected value.\n",
    "\n",
    "Now we have an expression full of stuff we can actually use G which is the return we get from playing\n",
    "\n",
    "an episode in pi which is our parameterize policy.\n",
    "\n",
    "So we would do is we would play an episode calculate the returns and then perform gradient ascent.\n",
    "\n",
    "Notice that gradient ascent and not gradient descent because we are trying to maximize the total return\n",
    "\n",
    "not minimize it.\n",
    "\n",
    "In fact you could do this as bad gradient ascent because by the time the episode is over you have all\n",
    "\n",
    "the returns.\n",
    "\n",
    "In fact this is suggested by the expected value symbol as well.\n",
    "\n",
    "We know from before that an expected value can be approximated by sample mean\n",
    "\n",
    "but also remember that tensor flow in Vienna are going to take gradients for us in particular we want\n",
    "\n",
    "just one expression we can pass in as the cost to the optimizer to turn what we have into that form.\n",
    "\n",
    "We realize that G is a constant so it can be moved inside the gradient.\n",
    "\n",
    "We also know that the derivative of a sum is just the sum of all the individual derivatives.\n",
    "\n",
    "So we can move the gradient outside the some.\n",
    "\n",
    "And finally we know that one over t is a meaningless constant because it can be absorbed into a learning\n",
    "\n",
    "rate.\n",
    "\n",
    "So we can get rid of that too.\n",
    "\n",
    "And so finally we have an expression for the thing we want to maximize.\n",
    "\n",
    "Since tensor flow optimizers only have a minimised function we can minimize the negative of this and\n",
    "\n",
    "to be clear capital-T represents the length of an episode and the index lowercase t represents the T\n",
    "\n",
    "of timestep of an episode because this involves the sum of return's over an entire episode.\n",
    "\n",
    "This is a Monte-Carlo method\n",
    "\n",
    "to gain better intuition about the gradient ascent update rule.\n",
    "\n",
    "It helps to look at what it would look like if we were to do stochastic gradient descent or in other\n",
    "\n",
    "words the update for just one return or one state in one action.\n",
    "\n",
    "So there are three terms here that affect the new value of theta the return the gradient of Pi and pi\n",
    "\n",
    "itself.\n",
    "\n",
    "Remember that pi is the probability of choosing an action a given status using the current policy.\n",
    "\n",
    "First consider g the return.\n",
    "\n",
    "We are moving in a direction proportional to G.\n",
    "\n",
    "The bigger G is the bigger step we take.\n",
    "\n",
    "This is good because we want to maximize our reward.\n",
    "\n",
    "Second consider pi the probability of choosing action.\n",
    "\n",
    "We are moving in a direction inversely proportional to pi.\n",
    "\n",
    "This is good because if pi is small but the return is good then we can take an even bigger step in that\n",
    "\n",
    "direction.\n",
    "\n",
    "And finally the gradient of Pi is a vector so that gives us the actual direction we want to go.\n",
    "\n",
    "The gradient tells us the direction of greatest increase in PI\n",
    "\n",
    "you'll notice that earlier in this lecture I mentioned using an approximation of V of S as well but\n",
    "\n",
    "so far that hasn't come into play.\n",
    "\n",
    "One common modification of the policy gradient that we are going to use is to add a baseline.\n",
    "\n",
    "So instead of our constant being just g it'll be g minus vivax our prediction of the value add status.\n",
    "\n",
    "The baseline can actually be any function that depends only on s but of course since we already know\n",
    "\n",
    "about V.\n",
    "\n",
    "It seems the most appropriate choice.\n",
    "\n",
    "We call this difference between G and V.\n",
    "\n",
    "The advantage.\n",
    "\n",
    "The reason we want to add a baseline is because it has been shown to have a significant effect on the\n",
    "\n",
    "variance of the update rule.\n",
    "\n",
    "This in turn has been shown to speed up learning\n",
    "\n",
    "the update parameters of V of course just use gradient descent as usual\n",
    "\n",
    "a natural question at this point is can you convert this from a Monte Carlo method to a TV method so\n",
    "\n",
    "that you don't have to wait for an episode to end before doing any updates.\n",
    "\n",
    "Of course this is possible and in reinforcement learning this has a special name.\n",
    "\n",
    "The actor critic method it's called actor critic because we think of the policy as the actor and the\n",
    "\n",
    "teacher which depends on the value estimate as the critic.\n",
    "\n",
    "So when the updates.\n",
    "\n",
    "All we do is replace G with the one step estimate of G.\n",
    "\n",
    "Now that we've gone through the heavy parts of the policy gradient method let's talk about why you might\n",
    "\n",
    "want to use it.\n",
    "\n",
    "We know that the policy gradient method yields a probabilistic policy that should be reminiscent of\n",
    "\n",
    "epsilon greedy which is also probabilistic.\n",
    "\n",
    "However it should be clear why the policy gradient method is more expressive with Epsilon greedy.\n",
    "\n",
    "All the suboptimal actions have the same probability of happening even though one might be better than\n",
    "\n",
    "the other with the policy gradient method.\n",
    "\n",
    "We can model this betterness directly.\n",
    "\n",
    "For example it might actually be optimal to do action one.\n",
    "\n",
    "90 percent of the time action to 8 percent of the time and action 3 only 2 percent of the time.\n",
    "\n",
    "In addition we should keep in mind that states themselves can be stochastic.\n",
    "\n",
    "One of the sources of this randomness is that the state does not give you the full information about\n",
    "\n",
    "the environment.\n",
    "\n",
    "For example in blackjack you don't know the dealer's next card.\n",
    "\n",
    "So the optimal action needs to be probabilistic to account for different possibilities.\n",
    "\n",
    "Now let's summarize this lecture's since there was a lot of information in it.\n",
    "\n",
    "First we saw that we can parameterize the policy so that in effect we get a probabilistic policy using\n",
    "\n",
    "a soft max output.\n",
    "\n",
    "Next we saw that the objective that the policy tries to optimize is the expected return from the start\n",
    "\n",
    "state.\n",
    "\n",
    "In other words this is the expected return over the entire episode.\n",
    "\n",
    "We call this objective the performance.\n",
    "\n",
    "Next we looked at the policy gradient theorem.\n",
    "\n",
    "We manipulated the results of the policy gradient theorem in order to give us a single cost function\n",
    "\n",
    "that we could then input into s.a.a tensor flow which is going to be helpful during implementation next.\n",
    "\n",
    "We looked at a modification of the policy creating an algorithm that uses a baseline and we call this\n",
    "\n",
    "difference between the return and the baseline.\n",
    "\n",
    "The advantage we then looked at the actor critic method which uses TDA updates instead of Monte-Carlo\n",
    "\n",
    "updates.\n",
    "\n",
    "Finally we discussed why we might want to use policy gradient methods rather than policy iteration.\n",
    "\n",
    "It allows us to explicitly model arbitrary probabilistic policies when a probabilistic policy could\n",
    "\n",
    "in fact be the optimal policy.\n",
    "\n",
    "This could in turn be because of the fact that the state transitions are probabilistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\varphi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
