{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we introduced many hyperparameters which need to be optimised , and this will only get worse as neural network gets more and more complex , so the first thing we will consider is hyperparameters optimisation\n",
    "\n",
    "Here are all the hyperparameters we learned about so far\n",
    "\n",
    "<ul>\n",
    "<li>learning rate / decay rate</li>\n",
    "<li>momentum</li>\n",
    "<li>regularisation</li>\n",
    " <li>hidden layer size</li>\n",
    "<li>number of hidden layers</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Cross-validation</h3>\n",
    "\n",
    "splut data into K parts , do K iterations - test on one part train on the other K-1 parts\n",
    "\n",
    "in code it might look something like :\n",
    "\n",
    "```python\n",
    "for k in range(K):\n",
    "    training_data = data[:k] + data[k+1:]\n",
    "    model = train(trainin_data)\n",
    "    score = test([data[k]])\n",
    "    \n",
    "```\n",
    "\n",
    "average the K scores , then choose the parameters with highest average scode\n",
    "\n",
    "keep the test set out of here , in Cross-validation we 'test' on validation set\n",
    "\n",
    "we can also use sklearn.cross_validation.KFold which does the data splitting\n",
    "\n",
    "\n",
    "<h3>Grid Search</h3>\n",
    "\n",
    "Exhaustive search , so we choose a set of each of the hyperparameters we want to try then we try every possible combination \n",
    "\n",
    "very slow , but since each odel is independant , there is a good opportunity for parallelization , frameworks like Hadoop/Spark are ideal here\n",
    "\n",
    "```python\n",
    "learning_rates = [0.1,0.01,0.001,0.0001,0.00001]\n",
    "momentums = [1,0.1,0.01,0.001]\n",
    "regularisations = [1,0.1,0.01]\n",
    "\n",
    "for lr in learning_rate:\n",
    "    for mu in momentums:\n",
    "        for reg in regularisations:\n",
    "            score = corss_validation(lr,mu,reg,data)\n",
    "```\n",
    "\n",
    "<h3>Radom Search</h3>\n",
    "\n",
    "Instead of looking at every possibility , move in random directions until score is improved\n",
    "\n",
    "```python\n",
    "theta = random position in hyperparameter space\n",
    "\n",
    "score1 = cross_validation(theta,theta)\n",
    "\n",
    "for i in range(max_iteration):\n",
    "    next_theta = sample from hypersphere around theta\n",
    "    \n",
    "    score2 = cross_validation(next_theta,data)\n",
    "    \n",
    "    if score2 is better than score1:\n",
    "        theta = next_theta\n",
    "```\n",
    "\n",
    "note : In geometry of higher dimensions, a hypersphere is the set of points at a constant distance from a given point called its centre\n",
    "\n",
    "note : still we need to ask what is the best combination to use , Nesterov momentum + RMSprop or maybe Regular momentum with exponential decay ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to discuss how we sample random numbers when doing grid/random search , this is not as straightforward as it might seem\n",
    "\n",
    "for ex , suppsoe we want to randomly sample the learning rate , we know that the difference between 0.001 and 0.0011 is no that significant \n",
    "\n",
    "generally we would like to try numbers on a log scale , $10^{-2},10^{-3},10^{-4},etc...$\n",
    "\n",
    "So sampling unformly is problematic , to understand this lets draw a number line :\n",
    "\n",
    "<img src=\"extras/20.18.PNG\"><img>\n",
    "\n",
    "as we can see , if we sample uniformally most of what we sample (the green area) is on the same scale of 0.1 and everytinh else is under-represented\n",
    "\n",
    "To fix this , we sample uniformly from the limits , for ex (-7,-1) , so $R \\sim U(-7,-1)$ , then Learning rate = $10^R$\n",
    "\n",
    "once again , lets draw the number line :\n",
    "\n",
    "<img src=\"extras/20.19.PNG\"><img>\n",
    "\n",
    "This can also be used for hyperparams like decay , we want to try numbers like 0.9,0.99,0.999\n",
    "\n",
    "$0.9 = 1 - 10^{-1}$\n",
    "\n",
    "$0.99 = 1 - 10^{-2}$\n",
    "\n",
    "$0.999 = 1 - 10^{-3}$\n",
    "\n",
    "Decay rate = $1 - 10^{R}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Code</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets try to code grid and random search\n",
    "# we will use the spiral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spiral():\n",
    "    # first use poral co-ordinates\n",
    "    # radius increases\n",
    "    radii = np.linspace(1,10,100)\n",
    "    thetas = np.empty((6,100))\n",
    "    # theta also increase , so curvature increase\n",
    "    for i in range(6): # we have 6 spirals\n",
    "        theta_start = np.pi*i/3.0 # 0,60,120,180,240,300\n",
    "        theta_end = theta_start + np.pi/2 # each spiral has a semi-circular sahpe\n",
    "        thetas[i] = np.linspace(theta_start,theta_end,100)\n",
    "     \n",
    "    # next cartesian co-ordinates\n",
    "    x1 = np.empty((6,100))\n",
    "    x2 = np.empty((6,100))\n",
    "    \n",
    "    for i in range(6):\n",
    "        x1[i] = radii * np.cos(thetas[i])\n",
    "        x2[i] = radii * np.sin(thetas[i])\n",
    "        \n",
    "    X = np.empty((600,2))\n",
    "    X[:,0] = x1.flatten()\n",
    "    X[:,1] = x2.flatten()\n",
    "    \n",
    "    # add some noise\n",
    "    X += np.random.randn(600,2)*0.5\n",
    "    \n",
    "    # targets\n",
    "    Y = np.array([0]*100+[1]*100+[0]*100+[1]*100+[0]*100+[1]*100)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = get_spiral() \n",
    "X,Y = shuffle(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters to test\n",
    "hidden_layer_sizes = [[300],[100,100],[50,50,50]]\n",
    "learning_rates = [1e-4,1e-3,1e-2]\n",
    "l2_penalties = [0.,0.1,1.0]\n",
    "momentums = [0.9,0.99,0.999]\n",
    "best = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(len(X)*0.7) # 70% train 30% test\n",
    "Xtrain , Ytrain = X[:N],Y[:N]\n",
    "Xtest , Ytest = X[N:],Y[N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.01, 'l2': 1.0, 'validation score': 0.9722222222222222, 'momentum': 0.999, 'hidden layer size': [50, 50, 50]}\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "for lr in learning_rates:\n",
    "    for l2 in l2_penalties:\n",
    "        for size in hidden_layer_sizes:\n",
    "            for mu in momentums:\n",
    "                model = MLPClassifier(hidden_layer_sizes = size ,learning_rate_init=lr,alpha=l2,momentum=mu,max_iter=3000)\n",
    "                model.fit(Xtrain,Ytrain)\n",
    "                score = model.score(Xtest,Ytest)\n",
    "                if score > best_score:\n",
    "                    best_Score = score\n",
    "                    best[\"lr\"] = lr\n",
    "                    best[\"l2\"] = l2\n",
    "                    best[\"validation score\"] = score\n",
    "                    best[\"momentum\"] = mu\n",
    "                    best[\"hidden layer size\"] = size\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets try random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 30\n",
    "hidden_layer_size = [20,20] \n",
    "log_lr = -1 # we search through log scale , learning rate 10**log_lr\n",
    "log_l2 = -1 # same for l2 penalty\n",
    "log_mu = -1 # same for momentum , momentum = 1-10**log_mu\n",
    "best = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may be warned that MLP did not converge yet\n",
    "# we will ignore this warning\n",
    "# this may require several runs due to randomness\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'log lr': -2, 'log l2': -4, 'validation score': 0.9777777777777777, 'log momentum': -2, 'hidden layer size': [20, 20, 20]}\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "for _ in range(max_iter):\n",
    "    model = MLPClassifier(hidden_layer_sizes = hidden_layer_size ,learning_rate_init=10**log_lr,alpha=10**log_l2,momentum=1-10**log_mu,max_iter=3000)\n",
    "    model.fit(Xtrain,Ytrain)\n",
    "    score = model.score(Xtest,Ytest)\n",
    "    if score > best_score:\n",
    "        best_Score = score\n",
    "        best[\"log lr\"] = log_lr\n",
    "        best[\"log l2\"] = log_l2\n",
    "        best[\"validation score\"] = score\n",
    "        best[\"log momentum\"] = log_mu\n",
    "        best[\"hidden layer size\"] = hidden_layer_size\n",
    "    # now lets randomly select hyperparameters\n",
    "    # imagine each hyperparameter as being a dimension and so the complete set of hyperparameters is a vector\n",
    "    # now we are searching within a hypersphere around this vector\n",
    "    # in other words we are going to go +/- a certain amount in each direction/parameter\n",
    "    size = len(best[\"hidden layer size\"]) + np.random.randint(-1,2) # -1,0 or 1\n",
    "    size = max(1,size)\n",
    "    units = best[\"hidden layer size\"][0]\n",
    "    units = units + np.random.randint(-1,2)*10\n",
    "    units = max(10,units)\n",
    "    hidden_layer_size = [units]*size\n",
    "    log_lr = best[\"log lr\"] + np.random.randint(-1,2)\n",
    "    log_l2 = best[\"log l2\"] + np.random.randint(-1,2)\n",
    "    log_mu = best[\"log momentum\"] + np.random.randint(-1,2)\n",
    "    # max mu = 1 , max log_mu = 0\n",
    "    log_mu = min(0,log_mu)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to discuss a limitation of Grid search and we can solve it\n",
    "\n",
    "lets say we have two hyperparameters to optimise , the learning rate and RMSProp epsilon \n",
    "\n",
    "<img src='extras/20.1.PNG' width = 300></img>\n",
    "\n",
    "lets say that epsilon has no effect on performance , what happens is that we end up trying 5 different epsilons over learning rates thats 5x the necessay work , geometrically , the result over each column is the same , what can we do about that ?\n",
    "\n",
    "<img src='extras/20.2.PNG' width = 300></img>\n",
    "\n",
    "select random points within the grid , this greatly reduces the chance that we are going to duplicate work unnecessariy (we might get weird learning rates as 0.178923 but thats okay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we want to discuss weight initialisation\n",
    "\n",
    "first we want to talk about vanishing and exploding gradients\n",
    "\n",
    "<h3>Vanishing Gradients</h3>\n",
    "\n",
    "We know that when it comes to neural network , deeper is better (this has been observed experimentally)\n",
    "\n",
    "but here comes a problem , remeber that to calculate derivative we use chain rule , in that we multiply several derivatives by each other \n",
    "\n",
    "<img src='extras/20.3.PNG' width=300><img>\n",
    "\n",
    "lets assume we use the sigmoid activation function , we notice two things , first its derivative approaches 0 very quickly , so a deep network using sigmoid will have many derivatives close to 0 and hence trains slowly , secondly the maximum value of derivative = 0.25 , so even if we get the maximum value of derivative at each layer (0.25) , we are still diminishing the value of the gradient by a quarter at every layer \n",
    "\n",
    "\n",
    "<h3>Exploding Gradient</h3>\n",
    "\n",
    "Again what we are doing during backporpagation is multiplying lots og numbers , if these numbers > 1 , then the gradient explodes !\n",
    "\n",
    "This is also a bad scenario since our weights will become infinity which will not help us make predictions\n",
    "\n",
    "If weights are too small they vanish if they are too big they explode , so we need to initialise them to the just right values (of course there are other ways to solve these problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets talk about different strategies for weight initialisation\n",
    "\n",
    "<h3>Initialising to 0 (or constant)</h3>\n",
    "\n",
    "This is common in linear models (ex. linear regression) and works fine , but not with a neural netowrk\n",
    "\n",
    "consider and AMM with 1 hidden layer updates :\n",
    "\n",
    "W : input to hidden weights\n",
    "\n",
    "V : hidden to output weights\n",
    "\n",
    "Z : $activation(W^Tx+B)$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial V} = Z^T(Y-T)$ , we can clearly see that if W is all 0s , and we use tanh as our activation function , then Z is all 0s , so V doesnot get updated since $\\frac{\\partial J}{\\partial V}$ is always 0 , same goes for W since $\\frac{\\partial J}{\\partial W} = X^T\\{(Y-T)V^T \\odot (1-Z^2)\\}$\n",
    "\n",
    "If we use sigmoid , then Z will be all 0.5 , the weights will change but in an undesirable way , in particular there is going to be symmetry along one axis (which will also happen in case of any constant)\n",
    "\n",
    "lets try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the same from 17-feedforward\n",
    "Nclass = 500\n",
    "# 3 gaussian clouds\n",
    "X1 = np.random.randn(Nclass,2) + np.array([0,-2])\n",
    "X2 = np.random.randn(Nclass,2) + np.array([2,2])\n",
    "X3 = np.random.randn(Nclass,2) + np.array([-2,2])\n",
    "X = np.concatenate((X1,X2,X3))\n",
    "# labels\n",
    "Y = np.array([0]*Nclass+[1]*Nclass+[2]*Nclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(output):\n",
    "    output = np.exp(output)\n",
    "    output = output / output.sum(axis=1,keepdims=True)\n",
    "    return output\n",
    "\n",
    "def sigmoid(a):\n",
    "    return 1/(1+np.exp(-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "M = 3\n",
    "K = 3\n",
    "\n",
    "# we want targets to be NxK matrix , so we one hot encode the targets\n",
    "# we need to one hot encode labels\n",
    "def one_hot_encode(Y):\n",
    "    N = len(Y)\n",
    "    D = Y.max()+1\n",
    "    encoded = np.zeros((N,D))\n",
    "    for i in range(N):\n",
    "        encoded[i,Y[i]] = 1\n",
    "    return encoded\n",
    "\n",
    "T = one_hot_encode(Y)\n",
    "\n",
    "# next we define weights and bias terms\n",
    "W = np.ones((D,M))\n",
    "b = np.ones(M)\n",
    "\n",
    "V = np.ones((M,K))\n",
    "c = np.ones(K)\n",
    "\n",
    "\n",
    "def forward(X,W,b,act_fn):\n",
    "    out = X@W+b\n",
    "    act = act_fn(out)\n",
    "    return act\n",
    "\n",
    "    \n",
    "epochs = 1000\n",
    "\n",
    "lr = 0.01 \n",
    "\n",
    "for i in range(epochs):\n",
    "    # feedforward\n",
    "    Z = forward(X,W,b,sigmoid)\n",
    "    y = forward(Z,V,c,softmax)\n",
    "    \n",
    "    #calculate gradients\n",
    "    delta = T-y\n",
    "    grad_V = Z.T@(delta)\n",
    "    grad_c = delta.sum(axis=0)\n",
    "    \n",
    "    Z_prime = Z*(1-Z)\n",
    "    grad_b = ((delta@V.T)*Z_prime)\n",
    "    grad_W = X.T@grad_b\n",
    "    grad_b = grad_b.sum(axis=0)\n",
    "        \n",
    "    # now we perform gradient ascent\n",
    "    b += lr*grad_b\n",
    "    c += lr*grad_c\n",
    "    W += lr*grad_W\n",
    "    V += lr*grad_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W [[3.09306886 3.09306886 3.09306886]\n",
      " [3.12243033 3.12243033 3.12243033]] \n",
      "\n",
      "V [[-5.34827982  5.22694209  3.12133773]\n",
      " [-5.34827982  5.22694209  3.12133773]\n",
      " [-5.34827982  5.22694209  3.12133773]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"W\",W,\"\\n\")\n",
    "print(\"V\",V,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same effect can be observed if we use tanh/sigmoid and for any constant , we always get symmetry\n",
    "\n",
    "notice how all the columns in W and V are the same  , this means that all weights are the same and thus all nodes will calculate the same feature , here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.43949001  0.43949001  0.43949001]\n",
      " [-8.67957415 -8.67957415 -8.67957415]\n",
      " [-1.20503873 -1.20503873 -1.20503873]\n",
      " ...\n",
      " [-5.47571103 -5.47571103 -5.47571103]\n",
      " [-0.98075621 -0.98075621 -0.98075621]\n",
      " [ 0.87645632  0.87645632  0.87645632]]\n"
     ]
    }
   ],
   "source": [
    "print(X@W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all units calculate the same feature , its like having only one unit in that layer!\n",
    "\n",
    "We can see this clealy since all the columns are the same , this means that adding more units will not increase the expressiveness of the network\n",
    "\n",
    "Initialising randomly allows us to break this symmetry and hence make use of all hidden units in network\n",
    "\n",
    "<h3>Initialise Randomly</h3>\n",
    "\n",
    "Now that we know that we need to initialise randomly , what distribution should they come from and what are the parameters of this distribution ?\n",
    "\n",
    "consider linear regression:\n",
    "\n",
    "$$y = w_1x_1 + w_2x_2 + ...$$\n",
    "\n",
    "we already know that the variance of all the xs is one since we normalised the training data to have 0 mean and unit variance\n",
    "\n",
    "$$var(x_i) = 1$$\n",
    "\n",
    "we would also like the output of this model to have variance 1 since in a neural network this is going to feed in into subsequent layers\n",
    "\n",
    "$$var(y) = 1$$\n",
    "\n",
    "since each x and each w is IID(independant and identically distributed) we can say:\n",
    "\n",
    "$$var(y) = var(w_1)var(x_1)+var(w_2)var(x_2)+...$$\n",
    "\n",
    "we know that $var(x_i) = 1$ , and since we initialise all the ws the same way they all have the same variance var(w) and so for each vector x of dimenstionality D we get :\n",
    "\n",
    "$$var(y) = D * var(w)$$\n",
    "\n",
    "so in order to make the var(y) = 1 , we need var(w) = $\\frac{1}{D}$\n",
    "\n",
    "in code we can acheive this by:\n",
    "\n",
    "```python\n",
    "np.random.randn(D)/np.sqrt(D)\n",
    "```\n",
    "\n",
    "now of course the nonlinear activation function does change the variance but this is just an approximation \n",
    "\n",
    "so the most important thing is for the weight initialisation not to be constant , the weights need to be initialised randomly\n",
    "\n",
    "second most important thing is fo weights to be small enough , too large weights lead to a very steep gradient and hence NaNs\n",
    "\n",
    "learning rate , training algorithm , and architecture are probably a higher prioirity , once again weights need to be tandom and small\n",
    "\n",
    "now lets go look over some general rules that we can apply , for a network layer , let M1 be the input size (also called fain-in) and M2 be the output size (also called fan-out)\n",
    "\n",
    "<h4>Method #1</h4>\n",
    "\n",
    "just set standard deviation to 0.01\n",
    "\n",
    "```python\n",
    "W = np.random.randn(M1,M2)*0.01\n",
    "```\n",
    "\n",
    "<h4>Method #2</h4>\n",
    "\n",
    "\n",
    "```python\n",
    "var = 2 / (M1+M2)\n",
    "W = np.random.randn(M1,M2) * np.sqrt(var)\n",
    "```\n",
    "$\\frac{1}{var}$ = average of fan-in and fan-out , typically used for tanh() , called Xavier (Glorot) Normal Initialiser\n",
    "\n",
    "<h4>Method #3</h4>\n",
    "\n",
    "\n",
    "```python\n",
    "var = 1/M1\n",
    "W = np.random.randn(M1,M2) * np.sqrt(var)\n",
    "```\n",
    "also for tanh()\n",
    "\n",
    "\n",
    "\n",
    "<h4>Method #4</h4>\n",
    "\n",
    "\n",
    "```python\n",
    "var = 2/M1\n",
    "W = np.random.randn(M1,M2) * np.sqrt(var)\n",
    "```\n",
    "for relu() , called He Normal \n",
    "\n",
    "\n",
    "<h4>Glorot Uniform</h4>\n",
    "\n",
    "Draw from uniform distribution , limit = $\\sqrt{\\frac{6}{M1+M2}}$\n",
    "\n",
    "$W \\sim U(-limit,+limit)$\n",
    "\n",
    "<h4>LeCun Uniform</h4>\n",
    "\n",
    "Draw from uniform distribution , limit = \n",
    "$\\sqrt{\\frac{3}{M1}}$\n",
    "\n",
    "$W \\sim U(-limit,+limit)$\n",
    "\n",
    "\n",
    "Bias terms can be initialised to 0 or random , doesnot really matter , we mostly care about breaking symmetry , randomly initialising weight matrix accomplished that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a quick note on Local Minima vs Global Minima\n",
    "\n",
    "<img src='extras/20.4.PNG'><img>\n",
    "\n",
    "Researchers have reasoned that any point where gradient is 0 , we are much more likely to be at a saddle point\n",
    "\n",
    "<img src='extras/20.5.PNG' width = 300><img>\n",
    "\n",
    "recall that a saddle point in 2 dimensions has a minimum along 1 axis and a maximum along the other axis\n",
    "\n",
    "while doing gradient descent , we are very unlickly to be going along the precise direction of going down to the minimum ,  more likely we will be moving along both axis at the same time and hence sliding of the saddle and so we do not get stuck , hence saddles are not a problem\n",
    "\n",
    "but why , for very high number of dimensions , are we unlikely to reach a global minima ?\n",
    "\n",
    "well this is a probability problem , if derivative = 0 , we have 2 choices for each axis min or max (remember each parameter is a dimension)\n",
    "\n",
    "If we have 1 million parameter , hence 1 million dimension the probability of a global minimum , a minimum in ALL 1 million dimension is $0.5^{1 \\ million}$ , which is basically 0\n",
    "\n",
    "so researchers nowadays dont really care about local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next , we discuss some Modern Regularisation Techniques \n",
    "\n",
    "We already know about:\n",
    "\n",
    "L1: encourages sparsity (=0)\n",
    "\n",
    "L2: encourages small weights(~=0)\n",
    "\n",
    "$$L_{regularised} = L_{likelihood} + \\lambda \\vert \\theta \\vert ^p$$\n",
    "\n",
    "Next is Dropout regularisation which adds no term on cost , instead it works by randomly dropping out nodes during training\n",
    "\n",
    "<img src = 'extras/20.6.PNG' width=500></img>\n",
    "\n",
    "This has the effect of making it so that any hidden layer unit cant rely on just one input feature because at any time that node could be dropped\n",
    "\n",
    "we will see how dropout emulates ensemble of neural networks \n",
    "\n",
    "What is an ensemble ? imagine instead of randomly dropping off nodes during training , we create several instances of neural networks with these different structures and train them all , then our prediction is the average of the predictions of each individual neural network , thats an ensemble\n",
    "\n",
    "we will also discuss am ethod called Noise Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets discuss in depth how dropping nodes randomly in a neural network performs regularisation and emulates an ensemble of neural networks\n",
    "\n",
    "The basic idea of Ensembles is to train a group of prediction models , that are different , then average their prediction or take majority vote , the result is better accuracy\n",
    "\n",
    "There are many ways in which models are different , one way is that each model is trained on a subset of the data (also good if algoirthm doesnot scale) , so if we have 1 million points , we can train train 10 different decision trees each on 100k points\n",
    "\n",
    "Another way is to not use all the features , so if we have 100 features , each of the 10 trees train on 10 features\n",
    "\n",
    "if we combine 2 methods , if shape of X.shape = (1 million,100) , each decision tree trains on x.shape = (100k,10)\n",
    "\n",
    "This happens to perform better than one decision tree :) , dropout is more like the version of ensembling where we choose a set of features\n",
    "\n",
    "<h3>Dropout</h3>\n",
    "\n",
    "we are going to use only a subset of features , but not only on input layer , rather at every layer  \n",
    "\n",
    "at every layer we are going to choose randomly which nodes to drop , we use a probability p(drop) or p(keep) = 1-p(drop) to tell us the probability of dropping or keeping a node\n",
    "\n",
    "typical values for p(keep) = 0.8 for input layer and 0.5 for hidden layers , note that we only drop layers during training \n",
    "\n",
    "next is prediction , in ensemble we had 10 decision trees to make a prediction , but in dropout we only need one , we will see how we can use only one network to 'emulate' and ensemble (not create one)\n",
    "\n",
    "The way we make Predictions is that instead of dropping nodes , we multiply the output of a layer by its p(keep) , which effectively shrinks all the values at that layer something similar to what L2 regularisation does (it makes the weights smaller so that effectively all the values become smaller)\n",
    "\n",
    "so if we have 1 hidden layer X $\\rightarrow$ Z $\\rightarrow$ Y\n",
    "\n",
    "X_drop = p(keep|layer1)*X\n",
    "\n",
    "Z = f(X_drop@W + b)\n",
    "\n",
    "Z_drop = p(keep|layer2)*Z\n",
    "\n",
    "Y = softmax(Z_drop@V+c)\n",
    "\n",
    "lets consider how this is an ensemble , if we have N nodes in a neural network (N = # inputs + # hiddens) with each node having 2 states : drop/keep then we have a total of $2^N$ different possible configuration , in other words we are approximating an ensemble of $2^N$ different neural networks which may would have been infeasable (detailed discussion in next Math section)\n",
    "\n",
    "at a low level implementation we dont actually drop nodes , rather we multiply by 0 , anything that comes after will be multiplied by 0 which is still 0 so we get the same effect of dropping nodes\n",
    "\n",
    "if our input is NxD matrix then we need a mask of 0s and 1s of size NxD\n",
    "\n",
    "in numpy we can make a mask this way:\n",
    "\n",
    "```python\n",
    "p_keep = 0.8\n",
    "mask = np.random.binomial(n=1,p=p_keep,size=X.shape)\n",
    "```\n",
    "\n",
    "then for example Z = f((X*mask).dot(W)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be quick\n",
    "# we will only change the forward function so that we multiply the matrix by a mask\n",
    "# we will reuse the code on MNIST from 19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the same from 17-feedforward\n",
    "Nclass = 500\n",
    "\n",
    "# 3 gaussian clouds\n",
    "X1 = np.random.randn(Nclass,2) + np.array([0,-2])\n",
    "X2 = np.random.randn(Nclass,2) + np.array([2,2])\n",
    "X3 = np.random.randn(Nclass,2) + np.array([-2,2])\n",
    "\n",
    "X = np.concatenate((X1,X2,X3))\n",
    "# labels\n",
    "Y = np.array([0]*Nclass+[1]*Nclass+[2]*Nclass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(output):\n",
    "    output = np.exp(output)\n",
    "    output = output / output.sum(axis=1,keepdims=True)\n",
    "    return output\n",
    "\n",
    "def sigmoid(a):\n",
    "    return 1/(1+np.exp(-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "M = 3\n",
    "K = 3\n",
    "\n",
    "p_keep = [0.8,0.5]\n",
    "\n",
    "# we want targets to be NxK matrix , so we one hot encode the targets\n",
    "# we need to one hot encode labels\n",
    "def one_hot_encode(Y):\n",
    "    N = len(Y)\n",
    "    D = Y.max()+1\n",
    "    encoded = np.zeros((N,D))\n",
    "    for i in range(N):\n",
    "        encoded[i,Y[i]] = 1\n",
    "    return encoded\n",
    "\n",
    "T = one_hot_encode(Y)\n",
    "\n",
    "# next we define weights and bias terms\n",
    "W = np.random.randn(D,M)\n",
    "b = np.random.randn(M)\n",
    "\n",
    "V = np.random.randn(M,K)\n",
    "c = np.random.randn(K)\n",
    "\n",
    "# here we change forward\n",
    "def forward(X,W,b,act_fn,p_keep):\n",
    "    mask = mask = np.random.binomial(n=1,p=p_keep,size=X.shape)\n",
    "    out = (X*mask)@W+b\n",
    "    act = act_fn(out)\n",
    "    return act\n",
    "\n",
    "    \n",
    "epochs = 1000\n",
    "\n",
    "lr = 0.0001 \n",
    "\n",
    "for i in range(epochs):\n",
    "    # feedforward\n",
    "    Z = forward(X,W,b,sigmoid,p_keep[0])\n",
    "    y = forward(Z,V,c,softmax,p_keep[1])\n",
    "    \n",
    "    #calculate gradients\n",
    "    delta = T-y\n",
    "    grad_V = Z.T@(delta)\n",
    "    grad_c = delta.sum(axis=0)\n",
    "    \n",
    "    Z_prime = Z*(1-Z)\n",
    "    grad_b = ((delta@V.T)*Z_prime)\n",
    "    grad_W = X.T@grad_b\n",
    "    grad_b = grad_b.sum(axis=0)\n",
    "        \n",
    "    # now we perform gradient ascent\n",
    "    b += lr*grad_b\n",
    "    c += lr*grad_c\n",
    "    W += lr*grad_W\n",
    "    V += lr*grad_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "# now lets get predictions and caluclate accuracy\n",
    "Z = forward(X*p_keep[0],W,b,sigmoid,1)\n",
    "y = forward(Z*p_keep[1],V,c,softmax,1)\n",
    "preds = np.argmax(y,axis=1)\n",
    "print(\"accuracy: \",np.mean(preds==Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to understand how multiplying p(keep) when getting predictions emulates an ensemble of neural networks where the probability of dropping a node is 1 - p(keep)\n",
    "\n",
    "lets consider a portion of a neural network :\n",
    "\n",
    "3 inputs , 1 output\n",
    "\n",
    "All x are 1 , all w are 1\n",
    "\n",
    "p(keep) = $\\frac{2}{3}$\n",
    "\n",
    "<img src='extras/20.7.PNG' width=\"200\"><img>\n",
    "\n",
    "Again , each node can be either on/odd or kept/dropped , so we have 3 bodes , 2 possibilities , which is $2^3$ = 8 different configurations\n",
    "\n",
    "<img src = 'extras/20.8.PNG' width = '500'><img>\n",
    "\n",
    "Dark circle $\\rightarrow$ off , light circle $\\rightarrow$ on\n",
    "\n",
    "The key is to recognise that the probability of each configuration is not equivalent (not $\\frac{1}{8}$)\n",
    "\n",
    "remember p(keep) , the probability of a node being on , = 2/3 , so p(drop) , the probability of a node being off , = 1/3\n",
    "\n",
    "so the probability of a configuration where all 3 nodes are off :\n",
    "\n",
    "<img src = 'extras/20.9.PNG' width = '100'><img>\n",
    "\n",
    "$= p(drop)p(drop)p(drop) = \\frac{1}{3} \\times \\frac{1}{3} \\times \\frac{1}{3} = \\frac{1}{27}$\n",
    "\n",
    "the probability of a configuration where 1 node is on and 2 are off (as any of these):\n",
    "\n",
    "<img src = 'extras/20.10.PNG' width = '300'><img>\n",
    "\n",
    "$= p(drop)p(drop)p(drop) = \\frac{1}{3} \\times \\frac{1}{3} \\times \\frac{2}{3} = \\frac{2}{27}$\n",
    "\n",
    "So here is a diagram with the probability of each configuration :\n",
    "\n",
    "<img src = 'extras/20.11.PNG' width = '500'><img>\n",
    "\n",
    "for each of these configurations , it is easy to calculate output value (we assumed all inputs and weights are 1)\n",
    "\n",
    "If 0 nodes are \"on\" : 0\n",
    "\n",
    "If 1 nodes is \"on\" : 1x1 = 1\n",
    "\n",
    "If 2 nodes are \"on\" : 1x1 + 1x1 = 2\n",
    "\n",
    "If 3 nodes are \"on\" : 1x1 + 1x1 + 1x1 = 3\n",
    "\n",
    "<img src = 'extras/20.12.PNG' width = '500'><img>\n",
    "\n",
    "Now lets \"pretend\" we used an ensemble of 27 base models , and each model showed up the expected number of times\n",
    "\n",
    "for example this configuration\n",
    "\n",
    "<img src = 'extras/20.13.PNG' width = '100'><img>\n",
    "\n",
    "shows up 8 times since the probability of occuring is 8/27 (so 8 of our 27 models have this configuration)\n",
    "\n",
    "The ensemble output is the average of all the base model outputs :\n",
    "\n",
    "output $= ( 0\\times1 + 1\\times2 + 1\\times2 + 1\\times2 + 2\\times4 + 2\\times4 + 2\\times4 + 2\\times4 + 3\\times8 ) / 27 = 2$\n",
    "\n",
    "each term here is : output value x # of times configuration occurs\n",
    "\n",
    "we could have calculated this as an expected value of X\n",
    "\n",
    "$$E(X) = \\sum_i x_i p(x_i)$$\n",
    "\n",
    "then we get the same answer :\n",
    "\n",
    "$0 \\times \\frac{1}{27} + 1 \\times \\frac{2}{27} + 1 \\times \\frac{2}{27} + 1 \\times \\frac{2}{27} + 2 \\times \\frac{4}{27} + 2 \\times \\frac{4}{27} + 2 \\times \\frac{4}{27} + 3 \\times \\frac{8}{27}  = 2$\n",
    "\n",
    "now imagine if we just multiplied inputs by p(keep) (2/3) \n",
    "\n",
    "output $= \\frac{2}{3}\\times1 + \\frac{2}{3}\\times1 + \\frac{2}{3}\\times1  = 2 $\n",
    "\n",
    "we get the same answer by multiplying x by p(keep) vs generating all the possible configurations , calculating their output and then finding the expected average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is another regularisation technique , Noise injection\n",
    "\n",
    "rather than diving deep in mathematics we will just discuss the intuition\n",
    "\n",
    "<h3>Noise Injection</h3>\n",
    "\n",
    "without noise injection :\n",
    "\n",
    "train(X,Y)\n",
    "\n",
    "with noise injection :\n",
    "\n",
    "noise ~ N(0,small noise variance)\n",
    "\n",
    "train(X + noise , Y)\n",
    "\n",
    "so each time we call train function we add randomly generated guassian noise to X\n",
    "\n",
    "the noise is 0 mean and small variance , the variance is a hyperparameter\n",
    "\n",
    "now why does this help ?\n",
    "\n",
    "lets first recall the Underfit and Overfit scenarios\n",
    "\n",
    "<img src='extras/20.14.PNG' width='500'></img>\n",
    "\n",
    "on the left we have underfitting because the decision boundary is too simple and does not capture the complexit y of the real pattern , on the right we have overfitting because the decision boundary is too complex and ir looks like it is capturing random noise variations in data\n",
    "\n",
    "Neural networks are more susceptible to the overfitting scenario due to large number of parameters and flexibility in terms of the functions they can represent , so we want to focus on stopping overfitting\n",
    "\n",
    "now lets consider the noise-injected data\n",
    "\n",
    "<img src='extras/20.15.PNG' width='200'></img>\n",
    "\n",
    "now that we added gaussain noise to the data , we can see that the decision boundary is forced to balance between two sides and by doing so it needs to get simpler (less twists and turns ) , so it gets the boundary away from the overfitting situation and closer to the underfitting situation\n",
    "\n",
    "one special type of noise which we can add to neural networks is adversarial examples , we will discuss the general idea and leave the details for later\n",
    "\n",
    "Adverserial examples are generally done on image datasets , we can take a picture that a neural network does very well on and add an imperceptible amount of noise to it , so that it looks exactly the same to the human eye , but the result is that it tricks the neyral network into thinking that it is something else (even though that neural network has been already trained)\n",
    "\n",
    "So if we know how to generate adverserial examples , instead og adding random noise we can add intelligently designed noise\n",
    "\n",
    "Another way to do noise inhjection is to add noise to the weights , this makes the neural network more robhust to small changes in weights\n",
    "\n",
    "Y = f(X;W+noise) , noise ~ N(0,small noise variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will discuss Batch normalisation , first we begin with a high level overview\n",
    "\n",
    "we know by now that normalising our data  for ML models is typically a good idea , that when we make all input features have mean 0 variance 1\n",
    "\n",
    "this is accomplished by subtracting mean and dividing by standard deviation of training data , input_data = (data - mean)/std_dev\n",
    "\n",
    "recall that the reason we want to do this is because it is where our non-linear activation functions are more active/dynamic (where they change the most)\n",
    "\n",
    "\n",
    "<img src = 'extras/20.16.PNG' width = 300></img>\n",
    "\n",
    "Now , instead ogf batch normalisation being a part of data processing stage , we make it a part of neural network , so the neural network is going to do the data normalisation \n",
    "\n",
    "we recall that each layer of the neural network is basically logistic regression , so we do normalisation before each layer of neural network\n",
    "\n",
    "This is just a high level overview , next we get into the technical details\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into Batch normalisation ,  we need to introduce \"Expnentially-Smoothed Averages\"\n",
    "\n",
    "lets begin by considering how to calculate then average , if we have N points then we can get the sample mean by:\n",
    "\n",
    "$$\\bar X_N = \\frac{1}{N}X_n$$\n",
    "\n",
    "Now lets sat we have a large amount of data that cant fit into memory all at same time , can we still calculate the mean ?\n",
    "\n",
    "what we can do is read one data point at a time , then delete each data point after we looked at it , mathematically we begin from :\n",
    "\n",
    "$$\\bar X_N = \\frac{1}{N}X_n$$\n",
    "\n",
    "which could be manipulated to yeild:\n",
    "\n",
    "$$\\frac{1}{N}\\left[ (N-1) \\bar X_{N-1} + X_N\\right]$$\n",
    "\n",
    "use simpler symbols , here Y is our output , and t is our time step\n",
    "\n",
    "$$Y(t) = \\left(1-\\frac{1}{t}\\right)Y(t-1) + \\frac{1}{t}X(t)$$\n",
    "\n",
    "now we need to consider the previous equation , think about the $\\frac{1}{t}$ in $\\frac{1}{t}X(t)$ , it says that as t grows larger , the current sample has less and less effect on the total mean\n",
    "\n",
    "makes sense as we collect more Xs the total number of datapoints grows , so each new X has less and less influence on current Y\n",
    "\n",
    "but we also decrease the influence of the previous Y by $(1-\\frac{1}{t})$ , so the current Y is part of the old Y + part of the newsest X , this , in the end , balances out to give the exact sample mean of X\n",
    "\n",
    "for convenience lets call the $\\frac{1}{t}$ term $\\alpha(t)$\n",
    "\n",
    "now lets say that we dont want $\\alpha$ to equal $\\frac{1}{t}$ , lets say we want each datapoint to matter equally at the time we see it , so we set $\\alpha$ to a constant\n",
    "\n",
    "$$Y(t) = \\left( 1-\\alpha \\right) Y(t-1) + \\alpha X(t)$$\n",
    " \n",
    "(of course $\\alpha$ needs to be $0 < \\alpha < 1$ or we may end up negating previous mean)\n",
    "\n",
    "This gives us what we call \"exponential smoothed avaerage\" , we can see why its called \"exponential when we express it only in terms of X\"\n",
    "\n",
    "$$Y(t) = (1-\\alpha)Y(t-1) + \\alpha X(t)$$\n",
    "\n",
    "$$Y(t) = {(1-\\alpha)}^t Y(0) + \\alpha \\sum^{t-1}_{\\tau = 0} (1-\\alpha)^\\tau X (t-\\tau)$$\n",
    "\n",
    "(same as how we solve recurrence in algorithms)\n",
    "\n",
    "now does this still give us the mean or in other words the expected value of X ?\n",
    "\n",
    "well if we take the expected value of everyting we see that we arrive at the expected value of X\n",
    "\n",
    "recall the definition of expected value E(X):\n",
    "\n",
    "$$E(X) = \\sum x_ip(x_i)$$\n",
    "\n",
    "so we take $Y(0)$ and each $X(t-\\tau)$ as datapoints ($x_i$) an we want to prove all their coeffecients ($p(x_i)$) = 1\n",
    "\n",
    "so we want to prove , for any value of $\\tau$: \n",
    "\n",
    "$$ (1-\\alpha)^\\tau + \\alpha \\sum^{t-1}_{\\tau = 0} (1-\\alpha) = 1$$\n",
    "\n",
    "the summation is equivalent to a gemetric series where a = $\\alpha$ and $r = (1-\\alpha)$: \n",
    "\n",
    "$$ \\alpha \\sum^{t-1}_{\\tau = 0} (1-\\alpha) = \\sum^{t-1}_{\\tau=0} \\alpha (1-\\alpha)^\\tau $$\n",
    "\n",
    "we can use the rule for sum of geometric series :\n",
    "$$\\sum_{k=0}^n ar^k =  \\left( \\frac{1 - r^{n+1}}{1-r}\\right)$$ \n",
    "\n",
    "to get\n",
    "\n",
    "$$ \\alpha \\sum^{t-1}_{\\tau = 0} (1-\\alpha) = \\alpha \\left( \\frac{1 - (1-\\alpha)^\\tau}{1-(1-\\alpha)} \\right) = \\alpha \\left( \\frac{1 - (1-\\alpha)^\\tau}{\\alpha} \\right) = 1 - (1-\\alpha)^\\tau $$\n",
    "\n",
    "now lets plug this back into :\n",
    "\n",
    "$$ (1-\\alpha)^\\tau + \\alpha \\sum^{t-1}_{\\tau = 0} (1-\\alpha) = 1$$\n",
    "\n",
    "to get :\n",
    "\n",
    "$$ (1-\\alpha)^\\tau + 1 - (1-\\alpha)^\\tau = 1 \\ ; \\ 1 = 1$$\n",
    "\n",
    "so this is a valid expectation value for X\n",
    "\n",
    "lets review again what we said :\n",
    "\n",
    "$$\\bar X_N = \\frac{1}{N}X_n$$\n",
    "\n",
    "$$Y(t) = \\left(1-\\frac{1}{t}\\right)Y(t-1) + \\frac{1}{t}X(t)$$\n",
    "\n",
    "In case of the simple mean all data points are equi-probable and thus all points equally influnce the mean , this is clear in the first formula , in the second formula one might be tempted to beleive that the later the point comes the smaller the influence , but since we also take only a part of the previous mean eventually it balances out , of course this is a valid E(X) \n",
    "\n",
    "$$Y(t) = {(1-\\alpha)}^t Y(0) + \\alpha \\sum^{t-1}_{\\tau = 0} (1-\\alpha)^\\tau X (t-\\tau)$$\n",
    "\n",
    "The exponential smoothed average is also another valid E(X) , this time newer points are given higher weights and so they have more influence on the mean\n",
    "\n",
    "if X is not a fixed distribution , rather it changes with time , then this may be a better way to calculate mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get into the details of how Batch normalisation work , recall that :\n",
    "\n",
    "<ul>\n",
    "<li>we like data to be normalised</li>\n",
    "<li>$z = \\frac{x-\\mu}{\\sigma}$</li>\n",
    "<li>Batch normalisation is building in normalisation into the neural net</li>\n",
    "</ul>\n",
    "\n",
    "The name \"Batch Normalisation\" comes from the fact that we assume that we will do batch gradient descent , so during training we consider a small batch of data for each gradient descent step\n",
    "\n",
    "so the mean and standard deviation we calculate are the sample mean and sample standard deviation of this batch\n",
    "\n",
    "<ul>\n",
    "<li>$X_B = $ next batch of data</li>\n",
    "<li>$\\mu_B =$ mean($X_B$)</li>\n",
    "<li>$\\sigma_B=$ std($X_B$)</li>\n",
    "<li>$Y_B = \\frac{X_B - \\mu_B}{\\sigma_B}$</li>\n",
    "</ul>\n",
    "\n",
    "Keep in mind that this only applies to training , since only in training will we have batches of data , for testing we will do something else\n",
    "\n",
    "note that batch normalisation happens just before we pass it through the activation function\n",
    "\n",
    "<img src = 'extras/20.17.PNG' width=600><img>\n",
    "\n",
    "so we first do the linear transformaion then do batch normalisation then pass it through activation function\n",
    "\n",
    "for the rest of this section we will refer to the input of batch norm as 'X' and its output as 'Y'\n",
    "\n",
    "seems simple so far , again here are the steps :\n",
    "\n",
    "<ul>\n",
    "<li>$X_B = $ next batch of data</li>\n",
    "<li>$\\mu_B =$ mean($X_B$)</li>\n",
    "<li>$\\sigma^2_B=$ var($X_B$)</li>\n",
    "<li>$Y_B = \\frac{X_B - \\mu_B}{\\sqrt{\\sigma^2_B + \\varepsilon}}$</li>\n",
    "</ul>\n",
    "\n",
    "note how we add a small number $\\varepsilon$ to the denominator so that we avoid dividing by 0 if var = 0\n",
    "\n",
    "Yet we are missing one step which may seem counter-intuitive , after going through all the trouble to normalise data , we scale it back to someting else !\n",
    "\n",
    "$$\\hat X_B = \\frac{X_B - \\mu_B}{\\sqrt{\\sigma^2_B + \\varepsilon}}$$\n",
    "\n",
    "$$Y = \\gamma \\hat X_B + \\beta$$\n",
    "\n",
    "so we give the data a different mean and a different standard deviation , we call the second scale parameter $\\gamma$ and the second location parameter $\\beta$\n",
    "\n",
    "why do we \"un-standardise\" data ? simply because standardization may not be good (we dont know)\n",
    "\n",
    "So we let gradient descent figure out what is best by updating $\\gamma , \\beta$ , so $\\gamma , \\beta$ will be updated using backpropagation just like other wieghts\n",
    "\n",
    "so suppose that standardisation is good , then the neural network will learn that $\\gamma = 1$ and $\\beta = 0$ , but if standardisation is not good , then the neural netowrk will learn better $\\gamma,\\beta$ that would yeild better results\n",
    "\n",
    "But what about testing ? suppose we want to make a prediction for 1 data point , if we subtract its mean (which is just itself) , we get the vector 0!\n",
    "\n",
    "Would be better if we kept track of all the means and all the standard deviations we saw during training (remember we do batch gradient descent , each batch has its own mean and standard deviation) , then we can use all these means and standard deviations to calculate a global mean and a global standard deviation , then we can use these during test times\n",
    "\n",
    "in fact this is what we will exactly do , we will keep a running mean and a running variance , for this we use exponentially-smoothed average (like we saw in RMSprop and Adam)\n",
    "\n",
    "for each batch B:\n",
    "\n",
    "$$\\mu = decay \\times \\mu + (1-decay) \\times \\mu_B $$\n",
    "$$\\sigma^2 = decay \\times \\sigma^2 + (1-decay) \\times \\sigma_B^2$$\n",
    "\n",
    "here $\\mu , \\sigma^2$ are the global mean and variance , $\\mu_B , \\sigma^2_B$ are the mean and variance of this batch\n",
    "\n",
    "of course we can theoretically just use $\\mu$ = mean($X_{train}$) , $\\sigma^2$ = var($X_{train}$) , that is the actual sample mean and sample variance of all training data , but may not scale for very large data\n",
    "\n",
    "so calculation during test mode :\n",
    "\n",
    "$$\\hat x_{test} = \\frac{(x_{test}-\\mu)}{\\sqrt{\\sigma^2 + \\varepsilon}}$$\n",
    "\n",
    "$$y_{test} = \\gamma \\hat x_{test} + \\beta$$\n",
    "\n",
    "$\\mu$,$\\sigma^2$ collected during training\n",
    "\n",
    "<h3>shape</h3>\n",
    "\n",
    "note that these are all element-wise operations , so if the layer has output size M , then gamma and beta will be of size M\n",
    "\n",
    "<h3>Theory</h3>\n",
    "\n",
    "now why does batch normalisatino actually help ?\n",
    "\n",
    "The authors of the original paper posed a problem called \"internal covariate shift\"\n",
    "\n",
    "sounds complicated , but essentially covariate is just another name for input features , so we may call these X , and by shift they mean that , as we perform batch gradient descent , what might happen is that the distribution of X might change as we train the network\n",
    "\n",
    "Now what happens when the data shifts , is that the weights now have to adjust to compensate because what they expected the data to look like before is not what it looks like now , so if input data changed , all weights in front of it will have to change , the authors argued that this increases training time , since it requires lowering the learning rate and careful weight initialisation\n",
    "\n",
    "But if we do batch normalisation , and the data in each layer is normalised (assume ideal case where all data is mean 0 and variance 1) , now the weights wont have to adapt to different looking data because all data looks the same \n",
    "\n",
    "The authors showed that this has two effects :\n",
    "\n",
    "<ul>\n",
    "<li>increase learning rate $\\rightarrow$ faster training</li>\n",
    "<li>Acts as regulariser , since inputs wont take on extreme values , neither will the weights\n",
    "    <ul><li>in fact it has been demonstrated that (sometimes) using BN eleminates the need for dropout since BN already is already doing something like regularisation</li>\n",
    "    <li>However as it stands , Dropout is still being used</li></ul></li>\n",
    "</ul>\n",
    "\n",
    "note : usually a layer has params (W,b) , nut since BN already shifts data we no longer need b !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to discuss another perspective of how batch normalisation does regularisation , in a sense , batch norm is a kind of 'noise injection'\n",
    "\n",
    "when we are doing batch norm , we use $\\mu_B,\\sigma_B$ as if they were the true values $\\mu_{TRUE},\\sigma_{TRUE}$ which of course we can never really know\n",
    "\n",
    "but if we did know $\\mu_{TRUE},\\sigma_{TRUE}$ and we used these during training and testing then training and testing would be exactly the same \n",
    "\n",
    "$$ \\hat X = \\frac{X - \\mu_{TRUE}}{\\sigma_{TRUE}} $$\n",
    "\n",
    "$$ Y = \\hat X \\gamma + \\beta $$\n",
    "\n",
    "recall that the mechanism of batch norm is the same during training and testing , the only difference is how we calculate $\\mu,\\sigma$\n",
    "\n",
    "now one way to think of an estimate is :\n",
    "       \n",
    "    Estimate = True Value + Noise\n",
    "      \n",
    "$$\\mu_B = \\mu_{TRUE} + \\varepsilon$$\n",
    "$$\\sigma_B = \\sigma_{TRUE} + \\varepsilon$$\n",
    "\n",
    "so batch norm is a way of doing noise injection , at the level of $\\mu_B , \\sigma_B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the next part we will introduce tensorflow and pytorch\n",
    "# instead of calculating gradients ourselves , these libraries do automatic differentiation\n",
    "# also have very useful built in functions\n",
    "# so we leave batch normalisation to the next part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
