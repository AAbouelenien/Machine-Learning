{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhJNhLRYXWIf"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.layers import Input,Dense,Conv2D,Flatten\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icWS17WHXWIk"
   },
   "outputs": [],
   "source": [
    "MAX_EXPERIENCES = 500000\n",
    "MIN_EXPERIENCES = 50000\n",
    "TARGET_UPDATE_PERIOD = 10000\n",
    "IM_SIZE = 84\n",
    "K = 4 #env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ju8FEfoCXWIo"
   },
   "outputs": [],
   "source": [
    "class ImageTransformer:\n",
    "    # transfrom image from 210,160,3\n",
    "    # to 84,84\n",
    "    def transform(self,state):\n",
    "        state = tf.image.rgb_to_grayscale(state)\n",
    "        state = tf.image.crop_to_bounding_box(state,34, 0, 160, 160)\n",
    "        state = tf.image.resize(state,[IM_SIZE, IM_SIZE],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        state = tf.squeeze(state)\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97ak3jUmXWIr"
   },
   "outputs": [],
   "source": [
    "def update_state(state, obs_small):\n",
    "    # throw oldest frame\n",
    "    # append newest to end\n",
    "    return np.append(state[:,:,1:],np.expand_dims(obs_small,2),axis=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJyO0L1RXWIu"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self,size=MAX_EXPERIENCES,frame_height=IM_SIZE,frame_width = IM_SIZE,agent_history_length=4,batch_size=32):\n",
    "        self.size =size\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "        \n",
    "        self.actions = np.empty(self.size,dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size,dtype=np.float32)\n",
    "        self.frames = np.empty((self.size,self.frame_height,self.frame_width),dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size,dtype=np.bool)\n",
    "        # pre-allocate memory for batch\n",
    "        self.states = np.empty((self.batch_size,self.agent_history_length,self.frame_height,self.frame_width),dtype=np.uint8) \n",
    "        self.new_states = np.empty((self.batch_size,self.agent_history_length,self.frame_height,self.frame_width),dtype=np.uint8) \n",
    "        self.indices = np.empty(self.batch_size,dtype=np.int32)\n",
    "        \n",
    "    def add_experience(self,action,frame,reward,terminal):\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.count = max(self.count,self.current+1)\n",
    "        self.current = (self.current+1) % self.size\n",
    "        \n",
    "    def _get_state(self,index):\n",
    "        #frames [t-3,t+1[\n",
    "        return self.frames[index-self.agent_history_length+1:index+1, ...]\n",
    "    \n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                # start at agent_history_length since when sampling , \n",
    "                # this frame is taken along the previous agent_history_length-1 frames\n",
    "                index = random.randint(self.agent_history_length, self.count - 1)\n",
    "                # current is a circular index\n",
    "                # reaching here means current made a cycle (see final comment)\n",
    "                # in which case index = current-1 is newest experience , while index = current is oldest\n",
    "                # thus having experinces before and after index is invalid since these are non-consecutive\n",
    "                # if >= current was empty index would be less than current since count be current+1\n",
    "                \n",
    "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
    "                    continue\n",
    "                # same goes here\n",
    "                # if any of the experiences mark the end of the episode , the next experience is surely of a new episode\n",
    "                # thus these are also non-consecutive experiences\n",
    "                if self.terminal_flags[index - self.agent_history_length:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "          \n",
    "    def get_minibatch(self):\n",
    "        self._get_valid_indices()\n",
    "        \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(idx - 1) # s (4 consecutive frames , s is last)\n",
    "            self.new_states[i] = self._get_state(idx) # s'\n",
    "        # states : N,T,H,W ----> N,H,W,T so it suits tensorflow , T acts like channels\n",
    "        return np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.new_states, axes=(0, 2, 3, 1)), self.terminal_flags[self.indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVGy4HQPXWIx"
   },
   "outputs": [],
   "source": [
    "def loss(targets,actions,yhat):\n",
    "    selected_action_values = tf.reduce_sum(yhat * tf.one_hot(actions, K),axis=1)\n",
    "    #cost = tf.reduce_mean(tf.square(targets - selected_action_values))\n",
    "    cost = tf.reduce_mean(tf.keras.losses.Huber()(targets, selected_action_values))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOgP10TZXWI0"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model,opt,inputs,actions,targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        yhat = model(inputs, training=True)\n",
    "        loss_value = loss(targets,actions,yhat)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MurAh9NGXWI4"
   },
   "outputs": [],
   "source": [
    "def DQN(K,conv_layer_sizes,hidden_layer_sizes): # returns the DQN model\n",
    "    input_ = Input(shape=(IM_SIZE,IM_SIZE,4))\n",
    "    x = input_/255.0\n",
    "    for num_output_filters,filtersz,poolsz in conv_layer_sizes:\n",
    "        x = Conv2D(num_output_filters,filtersz,poolsz,activation='relu')(x)\n",
    "    x = Flatten()(x)   \n",
    "    for M in hidden_layer_sizes:\n",
    "        x = Dense(M,activation='relu')(x)\n",
    "    output = Dense(K,activation='relu')(x)\n",
    "    model = Model(inputs=input_,outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwqdTab0XWI7"
   },
   "outputs": [],
   "source": [
    "def copy_weights(from_,to):\n",
    "    weights = [w.numpy() for w in from_.weights]\n",
    "    to.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tjmz_-JXWI-"
   },
   "outputs": [],
   "source": [
    "def sample_action(model,x,eps):\n",
    "    if np.random.random() < eps:\n",
    "        return np.random.choice(K)\n",
    "    else:\n",
    "        x = np.expand_dims(x,axis=0)\n",
    "        return np.argmax(model(x).numpy()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqhGvMrlXWJB"
   },
   "outputs": [],
   "source": [
    "def train(model,opt,target_model,rb,gamma,batch_size):\n",
    "    states, actions, rewards, next_states, dones = rb.get_minibatch()\n",
    "    next_Qs = target_model.predict(next_states)\n",
    "    next_Q = np.max(next_Qs,axis=1)\n",
    "    targets = rewards + np.invert(dones).astype(np.float32) * gamma * next_Q\n",
    "    loss = train_step(model,opt,states,actions,targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urT2d1sjXWJE"
   },
   "outputs": [],
   "source": [
    "def play_one(env,total_t,rb,model,opt,target_model,im_transformer,gamma,batch_size,epsilon,epsilon_change,epsilon_min):\n",
    "    t0 = datetime.now()\n",
    "    obs = env.reset()\n",
    "    obs_small = im_transformer.transform(obs)\n",
    "    state = np.stack([obs_small]*4,axis=2)\n",
    "    loss = None\n",
    "    \n",
    "    total_time_training = 0\n",
    "    num_steps_in_episode = 0\n",
    "    episode_reward = 0\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        if total_t % TARGET_UPDATE_PERIOD == 0:\n",
    "            copy_weights(model,target_model)\n",
    "            print(\"Copied model parameters to target network. total_t = %s, period = %s\" % (total_t, TARGET_UPDATE_PERIOD))\n",
    "            \n",
    "        action = sample_action(model,state,epsilon)\n",
    "        obs,reward,done,_ = env.step(action)\n",
    "        obs_small = im_transformer.transform(obs)\n",
    "        next_state = update_state(state,obs_small)\n",
    "\n",
    "        episode_reward += reward\n",
    "        rb.add_experience(action, obs_small, reward, done)\n",
    "\n",
    "        t0_2 = datetime.now()\n",
    "        loss = train(model, opt,target_model, rb, gamma, batch_size)\n",
    "        dt = datetime.now() - t0_2\n",
    "\n",
    "        # More debugging info\n",
    "        total_time_training += dt.total_seconds()\n",
    "        num_steps_in_episode += 1\n",
    "\n",
    "        state = next_state\n",
    "        total_t += 1\n",
    "\n",
    "        epsilon = max(epsilon - epsilon_change, epsilon_min)\n",
    "\n",
    "    return total_t, episode_reward, (datetime.now() - t0), num_steps_in_episode, total_time_training/num_steps_in_episode, epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KaM72YZOXWJH"
   },
   "outputs": [],
   "source": [
    "conv_layer_sizes = [(32, 8, 4), (64, 4, 2), (64, 3, 1)]\n",
    "hidden_layer_sizes = [512]\n",
    "gamma = 0.99\n",
    "batch_sz = 32\n",
    "num_episodes = 3500\n",
    "total_t = 0\n",
    "rb = ReplayMemory()\n",
    "episode_rewards = np.zeros(num_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ZIGd44FXWJK"
   },
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_change = (epsilon - epsilon_min) / 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SiBLpBG2XWJP"
   },
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")\n",
    "model = DQN(K=K,conv_layer_sizes=conv_layer_sizes,hidden_layer_sizes=hidden_layer_sizes)\n",
    "target_model = DQN(K=K,conv_layer_sizes=conv_layer_sizes,hidden_layer_sizes=hidden_layer_sizes)\n",
    "im_transformer = ImageTransformer()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TaRs4MnXWJS"
   },
   "outputs": [],
   "source": [
    "# populate reolay buffer\n",
    "obs = env.reset()\n",
    "for i in range(MIN_EXPERIENCES):\n",
    "    action = np.random.choice(K)\n",
    "    obs,reward,done,_ = env.step(action)\n",
    "    obs_small = im_transformer.transform(obs)\n",
    "    rb.add_experience(action,obs_small,reward,done)\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlKTD0qFXWJV"
   },
   "outputs": [],
   "source": [
    "def running_avg(totalrewards):\n",
    "    # average results over 100 episodes\n",
    "    N = len(totalrewards)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = totalrewards[max(0,t-100):(t+1)].mean()\n",
    "    return running_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Du2JMeNPXWJY",
    "outputId": "e454f992-5200-4a44-e3dd-f1290b490500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model parameters to target network. total_t = 0, period = 10000\n",
      "Episode: 0 Duration: 0:00:08.882938 Num steps: 176 Reward: 0.0 Training time per step: 0.045 Avg Reward (Last 100): 0.000 Epsilon: 1.000\n",
      "Episode: 1 Duration: 0:00:07.190657 Num steps: 183 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 0.000 Epsilon: 0.999\n",
      "Episode: 2 Duration: 0:00:10.567375 Num steps: 262 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 0.333 Epsilon: 0.999\n",
      "Episode: 3 Duration: 0:00:07.105578 Num steps: 172 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 0.250 Epsilon: 0.999\n",
      "Episode: 4 Duration: 0:00:11.954805 Num steps: 295 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 0.600 Epsilon: 0.998\n",
      "Episode: 5 Duration: 0:00:10.080000 Num steps: 257 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 0.667 Epsilon: 0.998\n",
      "Episode: 6 Duration: 0:00:10.889602 Num steps: 276 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 0.857 Epsilon: 0.997\n",
      "Episode: 7 Duration: 0:00:11.240731 Num steps: 288 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.000 Epsilon: 0.997\n",
      "Episode: 8 Duration: 0:00:06.904636 Num steps: 175 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 0.889 Epsilon: 0.996\n",
      "Episode: 9 Duration: 0:00:07.962229 Num steps: 203 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 0.900 Epsilon: 0.996\n",
      "Episode: 10 Duration: 0:00:09.845893 Num steps: 250 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 0.909 Epsilon: 0.995\n",
      "Episode: 11 Duration: 0:00:08.883188 Num steps: 227 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 0.917 Epsilon: 0.995\n",
      "Episode: 12 Duration: 0:00:10.340396 Num steps: 257 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.000 Epsilon: 0.995\n",
      "Episode: 13 Duration: 0:00:12.560128 Num steps: 319 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.071 Epsilon: 0.994\n",
      "Episode: 14 Duration: 0:00:06.720679 Num steps: 172 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.000 Epsilon: 0.994\n",
      "Episode: 15 Duration: 0:00:10.873543 Num steps: 276 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.062 Epsilon: 0.993\n",
      "Episode: 16 Duration: 0:00:10.783774 Num steps: 275 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.118 Epsilon: 0.993\n",
      "Episode: 17 Duration: 0:00:15.914175 Num steps: 406 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.278 Epsilon: 0.992\n",
      "Episode: 18 Duration: 0:00:10.940921 Num steps: 277 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.316 Epsilon: 0.991\n",
      "Episode: 19 Duration: 0:00:12.205232 Num steps: 312 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.400 Epsilon: 0.991\n",
      "Episode: 20 Duration: 0:00:09.778279 Num steps: 251 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.381 Epsilon: 0.990\n",
      "Episode: 21 Duration: 0:00:06.740028 Num steps: 172 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.318 Epsilon: 0.990\n",
      "Episode: 22 Duration: 0:00:09.270635 Num steps: 238 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.304 Epsilon: 0.990\n",
      "Episode: 23 Duration: 0:00:06.522116 Num steps: 166 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.250 Epsilon: 0.989\n",
      "Episode: 24 Duration: 0:00:09.828320 Num steps: 252 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.240 Epsilon: 0.989\n",
      "Episode: 25 Duration: 0:00:08.795837 Num steps: 223 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.231 Epsilon: 0.989\n",
      "Episode: 26 Duration: 0:00:08.203433 Num steps: 191 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.185 Epsilon: 0.988\n",
      "Episode: 27 Duration: 0:00:12.522724 Num steps: 314 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.250 Epsilon: 0.988\n",
      "Episode: 28 Duration: 0:00:09.464468 Num steps: 227 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.241 Epsilon: 0.987\n",
      "Episode: 29 Duration: 0:00:11.570211 Num steps: 273 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.267 Epsilon: 0.987\n",
      "Episode: 30 Duration: 0:00:07.532065 Num steps: 172 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.226 Epsilon: 0.986\n",
      "Episode: 31 Duration: 0:00:14.592425 Num steps: 347 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.281 Epsilon: 0.986\n",
      "Episode: 32 Duration: 0:00:18.536120 Num steps: 414 Reward: 4.0 Training time per step: 0.040 Avg Reward (Last 100): 1.364 Epsilon: 0.985\n",
      "Episode: 33 Duration: 0:00:19.576813 Num steps: 458 Reward: 5.0 Training time per step: 0.038 Avg Reward (Last 100): 1.471 Epsilon: 0.984\n",
      "Episode: 34 Duration: 0:00:23.036718 Num steps: 548 Reward: 9.0 Training time per step: 0.037 Avg Reward (Last 100): 1.686 Epsilon: 0.983\n",
      "Episode: 35 Duration: 0:00:08.867079 Num steps: 212 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.667 Epsilon: 0.983\n",
      "Episode: 36 Duration: 0:00:10.279975 Num steps: 245 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.649 Epsilon: 0.982\n",
      "Copied model parameters to target network. total_t = 10000, period = 10000\n",
      "Episode: 37 Duration: 0:00:13.001056 Num steps: 325 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.658 Epsilon: 0.982\n",
      "Episode: 38 Duration: 0:00:11.651807 Num steps: 266 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.667 Epsilon: 0.981\n",
      "Episode: 39 Duration: 0:00:08.346605 Num steps: 213 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.650 Epsilon: 0.981\n",
      "Episode: 40 Duration: 0:00:11.787739 Num steps: 303 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.659 Epsilon: 0.980\n",
      "Episode: 41 Duration: 0:00:10.582190 Num steps: 267 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.667 Epsilon: 0.980\n",
      "Episode: 42 Duration: 0:00:06.905410 Num steps: 176 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.628 Epsilon: 0.980\n",
      "Episode: 43 Duration: 0:00:14.353793 Num steps: 365 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.659 Epsilon: 0.979\n",
      "Episode: 44 Duration: 0:00:09.665736 Num steps: 237 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.644 Epsilon: 0.979\n",
      "Episode: 45 Duration: 0:00:06.951554 Num steps: 176 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.609 Epsilon: 0.978\n",
      "Episode: 46 Duration: 0:00:08.483967 Num steps: 217 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.596 Epsilon: 0.978\n",
      "Episode: 47 Duration: 0:00:09.434920 Num steps: 238 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.583 Epsilon: 0.977\n",
      "Episode: 48 Duration: 0:00:09.233346 Num steps: 235 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.571 Epsilon: 0.977\n",
      "Episode: 49 Duration: 0:00:10.866783 Num steps: 277 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.580 Epsilon: 0.976\n",
      "Episode: 50 Duration: 0:00:19.382440 Num steps: 493 Reward: 5.0 Training time per step: 0.034 Avg Reward (Last 100): 1.647 Epsilon: 0.976\n",
      "Episode: 51 Duration: 0:00:14.395582 Num steps: 368 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.673 Epsilon: 0.975\n",
      "Episode: 52 Duration: 0:00:09.678908 Num steps: 249 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.660 Epsilon: 0.975\n",
      "Episode: 53 Duration: 0:00:10.097745 Num steps: 232 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.648 Epsilon: 0.974\n",
      "Episode: 54 Duration: 0:00:12.355891 Num steps: 302 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.655 Epsilon: 0.974\n",
      "Episode: 55 Duration: 0:00:13.663075 Num steps: 322 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.679 Epsilon: 0.973\n",
      "Episode: 56 Duration: 0:00:09.836552 Num steps: 245 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.667 Epsilon: 0.973\n",
      "Episode: 57 Duration: 0:00:08.059287 Num steps: 205 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.655 Epsilon: 0.972\n",
      "Episode: 58 Duration: 0:00:10.805529 Num steps: 254 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.644 Epsilon: 0.972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 59 Duration: 0:00:08.800456 Num steps: 203 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.633 Epsilon: 0.971\n",
      "Episode: 60 Duration: 0:00:11.409208 Num steps: 269 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.639 Epsilon: 0.971\n",
      "Episode: 61 Duration: 0:00:12.153274 Num steps: 295 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.645 Epsilon: 0.970\n",
      "Episode: 62 Duration: 0:00:10.588947 Num steps: 270 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.651 Epsilon: 0.970\n",
      "Episode: 63 Duration: 0:00:11.846984 Num steps: 306 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.672 Epsilon: 0.969\n",
      "Episode: 64 Duration: 0:00:06.813523 Num steps: 174 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.646 Epsilon: 0.969\n",
      "Episode: 65 Duration: 0:00:09.486357 Num steps: 230 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.636 Epsilon: 0.969\n",
      "Episode: 66 Duration: 0:00:11.160422 Num steps: 235 Reward: 1.0 Training time per step: 0.042 Avg Reward (Last 100): 1.627 Epsilon: 0.968\n",
      "Episode: 67 Duration: 0:00:09.358870 Num steps: 168 Reward: 0.0 Training time per step: 0.050 Avg Reward (Last 100): 1.603 Epsilon: 0.968\n",
      "Episode: 68 Duration: 0:00:13.491462 Num steps: 264 Reward: 1.0 Training time per step: 0.045 Avg Reward (Last 100): 1.594 Epsilon: 0.967\n",
      "Episode: 69 Duration: 0:00:12.170560 Num steps: 269 Reward: 2.0 Training time per step: 0.040 Avg Reward (Last 100): 1.600 Epsilon: 0.967\n",
      "Episode: 70 Duration: 0:00:15.994473 Num steps: 339 Reward: 3.0 Training time per step: 0.042 Avg Reward (Last 100): 1.620 Epsilon: 0.966\n",
      "Episode: 71 Duration: 0:00:08.007792 Num steps: 182 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.597 Epsilon: 0.966\n",
      "Episode: 72 Duration: 0:00:08.837734 Num steps: 173 Reward: 0.0 Training time per step: 0.045 Avg Reward (Last 100): 1.575 Epsilon: 0.966\n",
      "Episode: 73 Duration: 0:00:14.187558 Num steps: 318 Reward: 3.0 Training time per step: 0.039 Avg Reward (Last 100): 1.595 Epsilon: 0.965\n",
      "Episode: 74 Duration: 0:00:07.068869 Num steps: 169 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.573 Epsilon: 0.965\n",
      "Episode: 75 Duration: 0:00:07.247181 Num steps: 176 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.553 Epsilon: 0.964\n",
      "Episode: 76 Duration: 0:00:07.189733 Num steps: 176 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.532 Epsilon: 0.964\n",
      "Copied model parameters to target network. total_t = 20000, period = 10000\n",
      "Episode: 77 Duration: 0:00:07.542102 Num steps: 185 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.513 Epsilon: 0.964\n",
      "Episode: 78 Duration: 0:00:14.486269 Num steps: 352 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.532 Epsilon: 0.963\n",
      "Episode: 79 Duration: 0:00:06.962634 Num steps: 168 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.512 Epsilon: 0.963\n",
      "Episode: 80 Duration: 0:00:12.688093 Num steps: 303 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.519 Epsilon: 0.962\n",
      "Episode: 81 Duration: 0:00:11.612452 Num steps: 263 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.524 Epsilon: 0.962\n",
      "Episode: 82 Duration: 0:00:08.852691 Num steps: 208 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.518 Epsilon: 0.961\n",
      "Episode: 83 Duration: 0:00:07.180584 Num steps: 173 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.500 Epsilon: 0.961\n",
      "Episode: 84 Duration: 0:00:13.969499 Num steps: 344 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.518 Epsilon: 0.961\n",
      "Episode: 85 Duration: 0:00:10.236818 Num steps: 250 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.523 Epsilon: 0.960\n",
      "Episode: 86 Duration: 0:00:11.636936 Num steps: 269 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.529 Epsilon: 0.960\n",
      "Episode: 87 Duration: 0:00:10.679269 Num steps: 255 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.523 Epsilon: 0.959\n",
      "Episode: 88 Duration: 0:00:14.267359 Num steps: 333 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.539 Epsilon: 0.959\n",
      "Episode: 89 Duration: 0:00:07.776783 Num steps: 185 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.522 Epsilon: 0.958\n",
      "Episode: 90 Duration: 0:00:11.874019 Num steps: 275 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.527 Epsilon: 0.958\n",
      "Episode: 91 Duration: 0:00:12.103159 Num steps: 294 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.533 Epsilon: 0.957\n",
      "Episode: 92 Duration: 0:00:11.962415 Num steps: 285 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.538 Epsilon: 0.957\n",
      "Episode: 93 Duration: 0:00:07.784921 Num steps: 178 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.521 Epsilon: 0.956\n",
      "Episode: 94 Duration: 0:00:11.068391 Num steps: 269 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.526 Epsilon: 0.956\n",
      "Episode: 95 Duration: 0:00:10.133760 Num steps: 238 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.521 Epsilon: 0.955\n",
      "Episode: 96 Duration: 0:00:07.342182 Num steps: 177 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.505 Epsilon: 0.955\n",
      "Episode: 97 Duration: 0:00:09.756653 Num steps: 232 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.500 Epsilon: 0.955\n",
      "Episode: 98 Duration: 0:00:15.430756 Num steps: 368 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.515 Epsilon: 0.954\n",
      "Episode: 99 Duration: 0:00:08.554402 Num steps: 201 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.510 Epsilon: 0.954\n",
      "Episode: 100 Duration: 0:00:10.030254 Num steps: 245 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.505 Epsilon: 0.953\n",
      "Episode: 101 Duration: 0:00:07.011986 Num steps: 168 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.505 Epsilon: 0.953\n",
      "Episode: 102 Duration: 0:00:17.446292 Num steps: 416 Reward: 4.0 Training time per step: 0.037 Avg Reward (Last 100): 1.545 Epsilon: 0.952\n",
      "Episode: 103 Duration: 0:00:10.714878 Num steps: 259 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.554 Epsilon: 0.952\n",
      "Episode: 104 Duration: 0:00:07.409343 Num steps: 175 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.554 Epsilon: 0.951\n",
      "Episode: 105 Duration: 0:00:10.497508 Num steps: 250 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.554 Epsilon: 0.951\n",
      "Episode: 106 Duration: 0:00:08.174345 Num steps: 204 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.554 Epsilon: 0.951\n",
      "Episode: 107 Duration: 0:00:07.071386 Num steps: 176 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.535 Epsilon: 0.950\n",
      "Episode: 108 Duration: 0:00:11.033396 Num steps: 277 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.535 Epsilon: 0.950\n",
      "Episode: 109 Duration: 0:00:09.402847 Num steps: 232 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.545 Epsilon: 0.949\n",
      "Episode: 110 Duration: 0:00:14.521902 Num steps: 365 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.564 Epsilon: 0.949\n",
      "Episode: 111 Duration: 0:00:06.748999 Num steps: 168 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.554 Epsilon: 0.948\n",
      "Episode: 112 Duration: 0:00:09.199332 Num steps: 232 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.554 Epsilon: 0.948\n",
      "Episode: 113 Duration: 0:00:09.331335 Num steps: 235 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.545 Epsilon: 0.948\n",
      "Episode: 114 Duration: 0:00:08.410456 Num steps: 214 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.535 Epsilon: 0.947\n",
      "Episode: 115 Duration: 0:00:08.845652 Num steps: 225 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.545 Epsilon: 0.947\n",
      "Episode: 116 Duration: 0:00:15.758622 Num steps: 398 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.564 Epsilon: 0.946\n",
      "Copied model parameters to target network. total_t = 30000, period = 10000\n",
      "Episode: 117 Duration: 0:00:13.544068 Num steps: 334 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.574 Epsilon: 0.945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 118 Duration: 0:00:14.990181 Num steps: 381 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.574 Epsilon: 0.945\n",
      "Episode: 119 Duration: 0:00:09.310085 Num steps: 236 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.564 Epsilon: 0.944\n",
      "Episode: 120 Duration: 0:00:07.043348 Num steps: 178 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.535 Epsilon: 0.944\n",
      "Episode: 121 Duration: 0:00:15.467662 Num steps: 389 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.564 Epsilon: 0.943\n",
      "Episode: 122 Duration: 0:00:06.958900 Num steps: 179 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.564 Epsilon: 0.943\n",
      "Episode: 123 Duration: 0:00:08.433597 Num steps: 211 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.564 Epsilon: 0.943\n",
      "Episode: 124 Duration: 0:00:09.350227 Num steps: 237 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.574 Epsilon: 0.942\n",
      "Episode: 125 Duration: 0:00:08.405126 Num steps: 212 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.574 Epsilon: 0.942\n",
      "Episode: 126 Duration: 0:00:09.125040 Num steps: 231 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.574 Epsilon: 0.941\n",
      "Episode: 127 Duration: 0:00:06.963706 Num steps: 177 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.574 Epsilon: 0.941\n",
      "Episode: 128 Duration: 0:00:07.975251 Num steps: 203 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.554 Epsilon: 0.941\n",
      "Episode: 129 Duration: 0:00:09.702078 Num steps: 242 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.554 Epsilon: 0.940\n",
      "Episode: 130 Duration: 0:00:06.862698 Num steps: 174 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.535 Epsilon: 0.940\n",
      "Episode: 131 Duration: 0:00:09.510205 Num steps: 241 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.545 Epsilon: 0.940\n",
      "Episode: 132 Duration: 0:00:12.399183 Num steps: 308 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.535 Epsilon: 0.939\n",
      "Episode: 133 Duration: 0:00:12.315670 Num steps: 316 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.525 Epsilon: 0.938\n",
      "Episode: 134 Duration: 0:00:09.909722 Num steps: 250 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.485 Epsilon: 0.938\n",
      "Episode: 135 Duration: 0:00:10.269280 Num steps: 262 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.416 Epsilon: 0.937\n",
      "Episode: 136 Duration: 0:00:12.715929 Num steps: 321 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.426 Epsilon: 0.937\n",
      "Episode: 137 Duration: 0:00:08.029552 Num steps: 204 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.426 Epsilon: 0.937\n",
      "Episode: 138 Duration: 0:00:06.570314 Num steps: 165 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.406 Epsilon: 0.936\n",
      "Episode: 139 Duration: 0:00:08.740398 Num steps: 222 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.396 Epsilon: 0.936\n",
      "Episode: 140 Duration: 0:00:07.175267 Num steps: 181 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.386 Epsilon: 0.935\n",
      "Episode: 141 Duration: 0:00:07.326935 Num steps: 186 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.366 Epsilon: 0.935\n",
      "Episode: 142 Duration: 0:00:08.301191 Num steps: 209 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.356 Epsilon: 0.935\n",
      "Episode: 143 Duration: 0:00:09.305382 Num steps: 234 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.366 Epsilon: 0.934\n",
      "Episode: 144 Duration: 0:00:07.257195 Num steps: 183 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.337 Epsilon: 0.934\n",
      "Episode: 145 Duration: 0:00:17.844144 Num steps: 457 Reward: 5.0 Training time per step: 0.034 Avg Reward (Last 100): 1.376 Epsilon: 0.933\n",
      "Episode: 146 Duration: 0:00:09.488153 Num steps: 239 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.386 Epsilon: 0.933\n",
      "Episode: 147 Duration: 0:00:08.685216 Num steps: 222 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.386 Epsilon: 0.932\n",
      "Episode: 148 Duration: 0:00:13.198287 Num steps: 331 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.932\n",
      "Episode: 149 Duration: 0:00:07.175558 Num steps: 181 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.396 Epsilon: 0.931\n",
      "Episode: 150 Duration: 0:00:16.969628 Num steps: 430 Reward: 5.0 Training time per step: 0.034 Avg Reward (Last 100): 1.426 Epsilon: 0.931\n",
      "Episode: 151 Duration: 0:00:07.541182 Num steps: 193 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.376 Epsilon: 0.930\n",
      "Episode: 152 Duration: 0:00:11.458818 Num steps: 290 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.366 Epsilon: 0.930\n",
      "Episode: 153 Duration: 0:00:07.478029 Num steps: 186 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.356 Epsilon: 0.929\n",
      "Episode: 154 Duration: 0:00:13.774932 Num steps: 352 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.376 Epsilon: 0.929\n",
      "Episode: 155 Duration: 0:00:11.587464 Num steps: 294 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.376 Epsilon: 0.928\n",
      "Episode: 156 Duration: 0:00:06.798899 Num steps: 173 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.347 Epsilon: 0.928\n",
      "Copied model parameters to target network. total_t = 40000, period = 10000\n",
      "Episode: 157 Duration: 0:00:08.783622 Num steps: 223 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.347 Epsilon: 0.928\n",
      "Episode: 158 Duration: 0:00:08.834759 Num steps: 224 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.347 Epsilon: 0.927\n",
      "Episode: 159 Duration: 0:00:13.298642 Num steps: 336 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.366 Epsilon: 0.927\n",
      "Episode: 160 Duration: 0:00:08.468943 Num steps: 211 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.366 Epsilon: 0.926\n",
      "Episode: 161 Duration: 0:00:07.310308 Num steps: 185 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.347 Epsilon: 0.926\n",
      "Episode: 162 Duration: 0:00:13.480535 Num steps: 344 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.356 Epsilon: 0.925\n",
      "Episode: 163 Duration: 0:00:09.149403 Num steps: 229 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.347 Epsilon: 0.925\n",
      "Episode: 164 Duration: 0:00:07.222223 Num steps: 182 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.317 Epsilon: 0.925\n",
      "Episode: 165 Duration: 0:00:13.012300 Num steps: 328 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.337 Epsilon: 0.924\n",
      "Episode: 166 Duration: 0:00:06.834296 Num steps: 173 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.924\n",
      "Episode: 167 Duration: 0:00:07.053383 Num steps: 178 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.923\n",
      "Episode: 168 Duration: 0:00:09.312520 Num steps: 233 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.327 Epsilon: 0.923\n",
      "Episode: 169 Duration: 0:00:08.701699 Num steps: 222 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.922\n",
      "Episode: 170 Duration: 0:00:08.718445 Num steps: 222 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.922\n",
      "Episode: 171 Duration: 0:00:09.549062 Num steps: 243 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.922\n",
      "Episode: 172 Duration: 0:00:14.143644 Num steps: 360 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.921\n",
      "Episode: 173 Duration: 0:00:07.062559 Num steps: 179 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.921\n",
      "Episode: 174 Duration: 0:00:12.546758 Num steps: 319 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.920\n",
      "Episode: 175 Duration: 0:00:10.969279 Num steps: 277 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.347 Epsilon: 0.920\n",
      "Episode: 176 Duration: 0:00:06.981413 Num steps: 175 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.347 Epsilon: 0.919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 177 Duration: 0:00:09.738998 Num steps: 247 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.366 Epsilon: 0.919\n",
      "Episode: 178 Duration: 0:00:06.783107 Num steps: 169 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.366 Epsilon: 0.919\n",
      "Episode: 179 Duration: 0:00:07.194038 Num steps: 177 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.337 Epsilon: 0.918\n",
      "Episode: 180 Duration: 0:00:11.264668 Num steps: 274 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.356 Epsilon: 0.918\n",
      "Episode: 181 Duration: 0:00:06.977211 Num steps: 176 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.337 Epsilon: 0.917\n",
      "Episode: 182 Duration: 0:00:06.867669 Num steps: 174 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.917\n",
      "Episode: 183 Duration: 0:00:10.676216 Num steps: 269 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.917\n",
      "Episode: 184 Duration: 0:00:11.211767 Num steps: 284 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.347 Epsilon: 0.916\n",
      "Episode: 185 Duration: 0:00:13.055710 Num steps: 329 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.337 Epsilon: 0.916\n",
      "Episode: 186 Duration: 0:00:07.244649 Num steps: 183 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.915\n",
      "Episode: 187 Duration: 0:00:09.507914 Num steps: 241 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.307 Epsilon: 0.915\n",
      "Episode: 188 Duration: 0:00:06.812662 Num steps: 173 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.914\n",
      "Episode: 189 Duration: 0:00:11.715664 Num steps: 296 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.914\n",
      "Episode: 190 Duration: 0:00:11.272551 Num steps: 282 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.307 Epsilon: 0.913\n",
      "Episode: 191 Duration: 0:00:13.944211 Num steps: 352 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.913\n",
      "Episode: 192 Duration: 0:00:15.035605 Num steps: 375 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.337 Epsilon: 0.912\n",
      "Episode: 193 Duration: 0:00:11.046714 Num steps: 280 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.337 Epsilon: 0.912\n",
      "Episode: 194 Duration: 0:00:11.098386 Num steps: 279 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.356 Epsilon: 0.911\n",
      "Episode: 195 Duration: 0:00:11.935906 Num steps: 304 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.356 Epsilon: 0.911\n",
      "Episode: 196 Duration: 0:00:10.845239 Num steps: 272 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.356 Epsilon: 0.910\n",
      "Copied model parameters to target network. total_t = 50000, period = 10000\n",
      "Episode: 197 Duration: 0:00:07.244915 Num steps: 183 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.356 Epsilon: 0.910\n",
      "Episode: 198 Duration: 0:00:07.850594 Num steps: 198 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.356 Epsilon: 0.909\n",
      "Episode: 199 Duration: 0:00:07.308782 Num steps: 185 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.909\n",
      "Episode: 200 Duration: 0:00:06.595350 Num steps: 167 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.909\n",
      "Episode: 201 Duration: 0:00:16.727828 Num steps: 425 Reward: 5.0 Training time per step: 0.034 Avg Reward (Last 100): 1.356 Epsilon: 0.908\n",
      "Episode: 202 Duration: 0:00:09.627225 Num steps: 242 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.366 Epsilon: 0.908\n",
      "Episode: 203 Duration: 0:00:07.575179 Num steps: 191 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.907\n",
      "Episode: 204 Duration: 0:00:10.966042 Num steps: 275 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.327 Epsilon: 0.907\n",
      "Episode: 205 Duration: 0:00:06.856087 Num steps: 173 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.906\n",
      "Episode: 206 Duration: 0:00:07.090275 Num steps: 179 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.307 Epsilon: 0.906\n",
      "Episode: 207 Duration: 0:00:10.888459 Num steps: 276 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.906\n",
      "Episode: 208 Duration: 0:00:07.273174 Num steps: 183 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.905\n",
      "Episode: 209 Duration: 0:00:12.282668 Num steps: 305 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.317 Epsilon: 0.905\n",
      "Episode: 210 Duration: 0:00:06.988718 Num steps: 176 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.307 Epsilon: 0.904\n",
      "Episode: 211 Duration: 0:00:09.570001 Num steps: 242 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.904\n",
      "Episode: 212 Duration: 0:00:11.120499 Num steps: 276 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.307 Epsilon: 0.903\n",
      "Episode: 213 Duration: 0:00:13.779048 Num steps: 347 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.903\n",
      "Episode: 214 Duration: 0:00:07.327616 Num steps: 178 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.317 Epsilon: 0.902\n",
      "Episode: 215 Duration: 0:00:08.714462 Num steps: 218 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.317 Epsilon: 0.902\n",
      "Episode: 216 Duration: 0:00:07.198373 Num steps: 174 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.307 Epsilon: 0.902\n",
      "Episode: 217 Duration: 0:00:15.581488 Num steps: 389 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.307 Epsilon: 0.901\n",
      "Episode: 218 Duration: 0:00:12.038415 Num steps: 301 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.901\n",
      "Episode: 219 Duration: 0:00:12.119035 Num steps: 306 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.900\n",
      "Episode: 220 Duration: 0:00:09.179752 Num steps: 229 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.277 Epsilon: 0.900\n",
      "Episode: 221 Duration: 0:00:11.294189 Num steps: 286 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.899\n",
      "Episode: 222 Duration: 0:00:11.572730 Num steps: 291 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.899\n",
      "Episode: 223 Duration: 0:00:21.545212 Num steps: 539 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 1.337 Epsilon: 0.898\n",
      "Episode: 224 Duration: 0:00:12.042780 Num steps: 305 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.347 Epsilon: 0.897\n",
      "Episode: 225 Duration: 0:00:19.646089 Num steps: 493 Reward: 6.0 Training time per step: 0.034 Avg Reward (Last 100): 1.396 Epsilon: 0.896\n",
      "Episode: 226 Duration: 0:00:06.756338 Num steps: 170 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.386 Epsilon: 0.896\n",
      "Episode: 227 Duration: 0:00:11.641279 Num steps: 290 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.396 Epsilon: 0.895\n",
      "Episode: 228 Duration: 0:00:12.164061 Num steps: 311 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.416 Epsilon: 0.895\n",
      "Episode: 229 Duration: 0:00:06.890292 Num steps: 173 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.406 Epsilon: 0.894\n",
      "Episode: 230 Duration: 0:00:09.476126 Num steps: 236 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.894\n",
      "Episode: 231 Duration: 0:00:11.267197 Num steps: 285 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.426 Epsilon: 0.893\n",
      "Episode: 232 Duration: 0:00:08.647871 Num steps: 218 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.426 Epsilon: 0.893\n",
      "Episode: 233 Duration: 0:00:10.914708 Num steps: 274 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.426 Epsilon: 0.893\n",
      "Episode: 234 Duration: 0:00:09.812866 Num steps: 248 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.406 Epsilon: 0.892\n",
      "Copied model parameters to target network. total_t = 60000, period = 10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 235 Duration: 0:00:10.627387 Num steps: 264 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.892\n",
      "Episode: 236 Duration: 0:00:11.498547 Num steps: 287 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.406 Epsilon: 0.891\n",
      "Episode: 237 Duration: 0:00:12.252604 Num steps: 306 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.891\n",
      "Episode: 238 Duration: 0:00:12.648799 Num steps: 317 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.426 Epsilon: 0.890\n",
      "Episode: 239 Duration: 0:00:18.561332 Num steps: 470 Reward: 5.0 Training time per step: 0.034 Avg Reward (Last 100): 1.475 Epsilon: 0.889\n",
      "Episode: 240 Duration: 0:00:09.339780 Num steps: 236 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.475 Epsilon: 0.889\n",
      "Episode: 241 Duration: 0:00:13.038895 Num steps: 325 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.505 Epsilon: 0.888\n",
      "Episode: 242 Duration: 0:00:10.522124 Num steps: 263 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.525 Epsilon: 0.888\n",
      "Episode: 243 Duration: 0:00:10.649421 Num steps: 269 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.535 Epsilon: 0.887\n",
      "Episode: 244 Duration: 0:00:11.479426 Num steps: 287 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.545 Epsilon: 0.887\n",
      "Episode: 245 Duration: 0:00:09.800248 Num steps: 247 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.554 Epsilon: 0.886\n",
      "Episode: 246 Duration: 0:00:13.847094 Num steps: 345 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.535 Epsilon: 0.886\n",
      "Episode: 247 Duration: 0:00:07.164376 Num steps: 180 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.525 Epsilon: 0.885\n",
      "Episode: 248 Duration: 0:00:06.882420 Num steps: 173 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.515 Epsilon: 0.885\n",
      "Episode: 249 Duration: 0:00:09.772893 Num steps: 246 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.505 Epsilon: 0.885\n",
      "Episode: 250 Duration: 0:00:06.929679 Num steps: 174 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.505 Epsilon: 0.884\n",
      "Episode: 251 Duration: 0:00:10.591549 Num steps: 264 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.475 Epsilon: 0.884\n",
      "Episode: 252 Duration: 0:00:16.565884 Num steps: 417 Reward: 6.0 Training time per step: 0.034 Avg Reward (Last 100): 1.535 Epsilon: 0.883\n",
      "Episode: 253 Duration: 0:00:10.842487 Num steps: 274 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.535 Epsilon: 0.883\n",
      "Episode: 254 Duration: 0:00:15.031053 Num steps: 375 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.564 Epsilon: 0.882\n",
      "Episode: 255 Duration: 0:00:15.442602 Num steps: 390 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.574 Epsilon: 0.881\n",
      "Episode: 256 Duration: 0:00:09.596798 Num steps: 240 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.564 Epsilon: 0.881\n",
      "Episode: 257 Duration: 0:00:07.247408 Num steps: 180 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.564 Epsilon: 0.880\n",
      "Episode: 258 Duration: 0:00:12.704113 Num steps: 314 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.574 Epsilon: 0.880\n",
      "Episode: 259 Duration: 0:00:13.457859 Num steps: 335 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.584 Epsilon: 0.879\n",
      "Episode: 260 Duration: 0:00:12.403447 Num steps: 311 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.584 Epsilon: 0.879\n",
      "Episode: 261 Duration: 0:00:17.222823 Num steps: 429 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 1.624 Epsilon: 0.878\n",
      "Episode: 262 Duration: 0:00:08.866354 Num steps: 220 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.634 Epsilon: 0.878\n",
      "Episode: 263 Duration: 0:00:11.646996 Num steps: 293 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.624 Epsilon: 0.877\n",
      "Episode: 264 Duration: 0:00:13.586331 Num steps: 340 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.644 Epsilon: 0.876\n",
      "Episode: 265 Duration: 0:00:07.540189 Num steps: 190 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.644 Epsilon: 0.876\n",
      "Episode: 266 Duration: 0:00:10.194442 Num steps: 253 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.634 Epsilon: 0.876\n",
      "Episode: 267 Duration: 0:00:13.007392 Num steps: 327 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.653 Epsilon: 0.875\n",
      "Episode: 268 Duration: 0:00:08.592652 Num steps: 211 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.663 Epsilon: 0.875\n",
      "Episode: 269 Duration: 0:00:07.631000 Num steps: 192 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.653 Epsilon: 0.874\n",
      "Copied model parameters to target network. total_t = 70000, period = 10000\n",
      "Episode: 270 Duration: 0:00:10.773352 Num steps: 270 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.663 Epsilon: 0.874\n",
      "Episode: 271 Duration: 0:00:09.119147 Num steps: 230 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.663 Epsilon: 0.873\n",
      "Episode: 272 Duration: 0:00:08.633927 Num steps: 215 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.663 Epsilon: 0.873\n",
      "Episode: 273 Duration: 0:00:09.708276 Num steps: 244 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.644 Epsilon: 0.873\n",
      "Episode: 274 Duration: 0:00:07.205458 Num steps: 180 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.644 Epsilon: 0.872\n",
      "Episode: 275 Duration: 0:00:14.546076 Num steps: 366 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.644 Epsilon: 0.872\n",
      "Episode: 276 Duration: 0:00:12.583827 Num steps: 314 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.653 Epsilon: 0.871\n",
      "Episode: 277 Duration: 0:00:13.715647 Num steps: 345 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.683 Epsilon: 0.870\n",
      "Episode: 278 Duration: 0:00:08.340111 Num steps: 211 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.673 Epsilon: 0.870\n",
      "Episode: 279 Duration: 0:00:12.727055 Num steps: 319 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.703 Epsilon: 0.869\n",
      "Episode: 280 Duration: 0:00:07.226848 Num steps: 181 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.703 Epsilon: 0.869\n",
      "Episode: 281 Duration: 0:00:07.139577 Num steps: 179 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.683 Epsilon: 0.869\n",
      "Episode: 282 Duration: 0:00:11.401441 Num steps: 287 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.703 Epsilon: 0.868\n",
      "Episode: 283 Duration: 0:00:08.853440 Num steps: 222 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.713 Epsilon: 0.868\n",
      "Episode: 284 Duration: 0:00:10.734285 Num steps: 269 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.713 Epsilon: 0.867\n",
      "Episode: 285 Duration: 0:00:11.080436 Num steps: 281 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.713 Epsilon: 0.867\n",
      "Episode: 286 Duration: 0:00:06.710091 Num steps: 169 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.693 Epsilon: 0.867\n",
      "Episode: 287 Duration: 0:00:07.734684 Num steps: 194 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.693 Epsilon: 0.866\n",
      "Episode: 288 Duration: 0:00:06.996204 Num steps: 175 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.683 Epsilon: 0.866\n",
      "Episode: 289 Duration: 0:00:07.382940 Num steps: 185 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.683 Epsilon: 0.866\n",
      "Episode: 290 Duration: 0:00:07.460458 Num steps: 188 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.663 Epsilon: 0.865\n",
      "Episode: 291 Duration: 0:00:15.093781 Num steps: 376 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.683 Epsilon: 0.865\n",
      "Episode: 292 Duration: 0:00:08.559598 Num steps: 214 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.663 Epsilon: 0.864\n",
      "Episode: 293 Duration: 0:00:07.715558 Num steps: 181 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.624 Epsilon: 0.864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 294 Duration: 0:00:11.478538 Num steps: 279 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.624 Epsilon: 0.863\n",
      "Episode: 295 Duration: 0:00:09.498566 Num steps: 240 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.614 Epsilon: 0.863\n",
      "Episode: 296 Duration: 0:00:06.842028 Num steps: 172 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.594 Epsilon: 0.863\n",
      "Episode: 297 Duration: 0:00:10.101992 Num steps: 250 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.594 Epsilon: 0.862\n",
      "Episode: 298 Duration: 0:00:08.038200 Num steps: 203 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.594 Epsilon: 0.862\n",
      "Episode: 299 Duration: 0:00:11.791135 Num steps: 299 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.604 Epsilon: 0.861\n",
      "Episode: 300 Duration: 0:00:07.309941 Num steps: 184 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.604 Epsilon: 0.861\n",
      "Episode: 301 Duration: 0:00:14.347183 Num steps: 361 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.634 Epsilon: 0.860\n",
      "Episode: 302 Duration: 0:00:10.719852 Num steps: 268 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.604 Epsilon: 0.860\n",
      "Episode: 303 Duration: 0:00:11.025354 Num steps: 280 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.614 Epsilon: 0.859\n",
      "Episode: 304 Duration: 0:00:07.431307 Num steps: 186 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.614 Epsilon: 0.859\n",
      "Episode: 305 Duration: 0:00:12.662392 Num steps: 317 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.624 Epsilon: 0.858\n",
      "Episode: 306 Duration: 0:00:07.438466 Num steps: 186 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.624 Epsilon: 0.858\n",
      "Episode: 307 Duration: 0:00:12.457948 Num steps: 307 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.644 Epsilon: 0.857\n",
      "Episode: 308 Duration: 0:00:07.264177 Num steps: 183 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.624 Epsilon: 0.857\n",
      "Episode: 309 Duration: 0:00:08.440253 Num steps: 212 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.634 Epsilon: 0.857\n",
      "Episode: 310 Duration: 0:00:13.395607 Num steps: 331 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.644 Epsilon: 0.856\n",
      "Copied model parameters to target network. total_t = 80000, period = 10000\n",
      "Episode: 311 Duration: 0:00:08.516766 Num steps: 210 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.653 Epsilon: 0.856\n",
      "Episode: 312 Duration: 0:00:07.180885 Num steps: 179 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.644 Epsilon: 0.855\n",
      "Episode: 313 Duration: 0:00:10.037808 Num steps: 252 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.634 Epsilon: 0.855\n",
      "Episode: 314 Duration: 0:00:11.074582 Num steps: 276 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.624 Epsilon: 0.855\n",
      "Episode: 315 Duration: 0:00:06.975587 Num steps: 174 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.624 Epsilon: 0.854\n",
      "Episode: 316 Duration: 0:00:06.480201 Num steps: 162 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.614 Epsilon: 0.854\n",
      "Episode: 317 Duration: 0:00:10.882310 Num steps: 272 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.634 Epsilon: 0.853\n",
      "Episode: 318 Duration: 0:00:06.698458 Num steps: 167 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.594 Epsilon: 0.853\n",
      "Episode: 319 Duration: 0:00:07.527836 Num steps: 187 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.574 Epsilon: 0.853\n",
      "Episode: 320 Duration: 0:00:07.339553 Num steps: 183 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.554 Epsilon: 0.852\n",
      "Episode: 321 Duration: 0:00:09.443197 Num steps: 236 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.554 Epsilon: 0.852\n",
      "Episode: 322 Duration: 0:00:07.346309 Num steps: 184 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.535 Epsilon: 0.852\n",
      "Episode: 323 Duration: 0:00:06.723373 Num steps: 167 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.515 Epsilon: 0.851\n",
      "Episode: 324 Duration: 0:00:06.963849 Num steps: 174 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.455 Epsilon: 0.851\n",
      "Episode: 325 Duration: 0:00:12.429255 Num steps: 311 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.465 Epsilon: 0.851\n",
      "Episode: 326 Duration: 0:00:11.856271 Num steps: 295 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.426 Epsilon: 0.850\n",
      "Episode: 327 Duration: 0:00:09.060136 Num steps: 228 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.436 Epsilon: 0.850\n",
      "Episode: 328 Duration: 0:00:12.569823 Num steps: 314 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.436 Epsilon: 0.849\n",
      "Episode: 329 Duration: 0:00:06.790889 Num steps: 173 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.416 Epsilon: 0.849\n",
      "Episode: 330 Duration: 0:00:08.793004 Num steps: 216 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.426 Epsilon: 0.848\n",
      "Episode: 331 Duration: 0:00:06.933185 Num steps: 174 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.416 Epsilon: 0.848\n",
      "Episode: 332 Duration: 0:00:08.138629 Num steps: 200 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.396 Epsilon: 0.848\n",
      "Episode: 333 Duration: 0:00:15.961308 Num steps: 398 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.426 Epsilon: 0.847\n",
      "Episode: 334 Duration: 0:00:06.835644 Num steps: 172 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.406 Epsilon: 0.847\n",
      "Episode: 335 Duration: 0:00:12.912248 Num steps: 324 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.416 Epsilon: 0.846\n",
      "Episode: 336 Duration: 0:00:07.603766 Num steps: 190 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.406 Epsilon: 0.846\n",
      "Episode: 337 Duration: 0:00:11.024059 Num steps: 280 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.406 Epsilon: 0.845\n",
      "Episode: 338 Duration: 0:00:09.309399 Num steps: 233 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.396 Epsilon: 0.845\n",
      "Episode: 339 Duration: 0:00:06.913242 Num steps: 173 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.366 Epsilon: 0.844\n",
      "Episode: 340 Duration: 0:00:07.003517 Num steps: 175 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.844\n",
      "Episode: 341 Duration: 0:00:07.026701 Num steps: 176 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.307 Epsilon: 0.844\n",
      "Episode: 342 Duration: 0:00:09.792836 Num steps: 244 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.287 Epsilon: 0.843\n",
      "Episode: 343 Duration: 0:00:20.056864 Num steps: 505 Reward: 6.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.842\n",
      "Episode: 344 Duration: 0:00:08.318731 Num steps: 205 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.317 Epsilon: 0.842\n",
      "Episode: 345 Duration: 0:00:11.358813 Num steps: 287 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.842\n",
      "Episode: 346 Duration: 0:00:11.808289 Num steps: 295 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.841\n",
      "Episode: 347 Duration: 0:00:10.995248 Num steps: 276 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.841\n",
      "Episode: 348 Duration: 0:00:07.097153 Num steps: 178 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.840\n",
      "Episode: 349 Duration: 0:00:08.697212 Num steps: 219 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.840\n",
      "Episode: 350 Duration: 0:00:11.313156 Num steps: 282 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.327 Epsilon: 0.839\n",
      "Episode: 351 Duration: 0:00:06.954627 Num steps: 174 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.839\n",
      "Episode: 352 Duration: 0:00:10.984941 Num steps: 279 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model parameters to target network. total_t = 90000, period = 10000\n",
      "Episode: 353 Duration: 0:00:19.885785 Num steps: 488 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 1.327 Epsilon: 0.838\n",
      "Episode: 354 Duration: 0:00:06.885548 Num steps: 168 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.307 Epsilon: 0.837\n",
      "Episode: 355 Duration: 0:00:09.808150 Num steps: 242 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.287 Epsilon: 0.837\n",
      "Episode: 356 Duration: 0:00:17.353870 Num steps: 430 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.287 Epsilon: 0.836\n",
      "Episode: 357 Duration: 0:00:09.539876 Num steps: 238 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.836\n",
      "Episode: 358 Duration: 0:00:07.490492 Num steps: 187 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.835\n",
      "Episode: 359 Duration: 0:00:08.325078 Num steps: 208 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.835\n",
      "Episode: 360 Duration: 0:00:12.462687 Num steps: 311 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.834\n",
      "Episode: 361 Duration: 0:00:10.038307 Num steps: 252 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.267 Epsilon: 0.834\n",
      "Episode: 362 Duration: 0:00:06.309907 Num steps: 159 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.218 Epsilon: 0.834\n",
      "Episode: 363 Duration: 0:00:07.478709 Num steps: 187 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.208 Epsilon: 0.833\n",
      "Episode: 364 Duration: 0:00:10.089080 Num steps: 252 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.198 Epsilon: 0.833\n",
      "Episode: 365 Duration: 0:00:09.317099 Num steps: 234 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.178 Epsilon: 0.832\n",
      "Episode: 366 Duration: 0:00:09.834782 Num steps: 246 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.188 Epsilon: 0.832\n",
      "Episode: 367 Duration: 0:00:08.970157 Num steps: 225 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.188 Epsilon: 0.832\n",
      "Episode: 368 Duration: 0:00:08.109221 Num steps: 199 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.168 Epsilon: 0.831\n",
      "Episode: 369 Duration: 0:00:11.472945 Num steps: 283 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.178 Epsilon: 0.831\n",
      "Episode: 370 Duration: 0:00:06.879821 Num steps: 171 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.178 Epsilon: 0.830\n",
      "Episode: 371 Duration: 0:00:14.298196 Num steps: 358 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.188 Epsilon: 0.830\n",
      "Episode: 372 Duration: 0:00:10.324343 Num steps: 260 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.198 Epsilon: 0.829\n",
      "Episode: 373 Duration: 0:00:07.010141 Num steps: 175 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.188 Epsilon: 0.829\n",
      "Episode: 374 Duration: 0:00:12.136576 Num steps: 302 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.198 Epsilon: 0.828\n",
      "Episode: 375 Duration: 0:00:14.111670 Num steps: 352 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.228 Epsilon: 0.828\n",
      "Episode: 376 Duration: 0:00:07.018557 Num steps: 175 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.198 Epsilon: 0.828\n",
      "Episode: 377 Duration: 0:00:09.848804 Num steps: 248 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.178 Epsilon: 0.827\n",
      "Episode: 378 Duration: 0:00:14.935235 Num steps: 371 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.188 Epsilon: 0.826\n",
      "Episode: 379 Duration: 0:00:17.593753 Num steps: 439 Reward: 5.0 Training time per step: 0.034 Avg Reward (Last 100): 1.228 Epsilon: 0.826\n",
      "Episode: 380 Duration: 0:00:07.685544 Num steps: 192 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.198 Epsilon: 0.825\n",
      "Episode: 381 Duration: 0:00:07.909084 Num steps: 200 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.208 Epsilon: 0.825\n",
      "Episode: 382 Duration: 0:00:10.784824 Num steps: 270 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.228 Epsilon: 0.824\n",
      "Episode: 383 Duration: 0:00:09.879771 Num steps: 245 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.218 Epsilon: 0.824\n",
      "Episode: 384 Duration: 0:00:07.759463 Num steps: 187 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.208 Epsilon: 0.824\n",
      "Episode: 385 Duration: 0:00:06.612347 Num steps: 165 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.188 Epsilon: 0.823\n",
      "Episode: 386 Duration: 0:00:15.087061 Num steps: 376 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.208 Epsilon: 0.823\n",
      "Episode: 387 Duration: 0:00:08.411229 Num steps: 210 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.218 Epsilon: 0.822\n",
      "Episode: 388 Duration: 0:00:09.479441 Num steps: 239 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.228 Epsilon: 0.822\n",
      "Episode: 389 Duration: 0:00:10.077854 Num steps: 251 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.238 Epsilon: 0.821\n",
      "Episode: 390 Duration: 0:00:06.986050 Num steps: 174 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.238 Epsilon: 0.821\n",
      "Episode: 391 Duration: 0:00:06.897771 Num steps: 169 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.238 Epsilon: 0.821\n",
      "Episode: 392 Duration: 0:00:12.046728 Num steps: 298 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.218 Epsilon: 0.820\n",
      "Copied model parameters to target network. total_t = 100000, period = 10000\n",
      "Episode: 393 Duration: 0:00:11.449931 Num steps: 283 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.228 Epsilon: 0.820\n",
      "Episode: 394 Duration: 0:00:07.900858 Num steps: 196 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.228 Epsilon: 0.819\n",
      "Episode: 395 Duration: 0:00:14.166110 Num steps: 355 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.238 Epsilon: 0.819\n",
      "Episode: 396 Duration: 0:00:09.596971 Num steps: 240 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.238 Epsilon: 0.818\n",
      "Episode: 397 Duration: 0:00:06.834598 Num steps: 169 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.238 Epsilon: 0.818\n",
      "Episode: 398 Duration: 0:00:10.705503 Num steps: 269 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.248 Epsilon: 0.818\n",
      "Episode: 399 Duration: 0:00:13.889497 Num steps: 342 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.277 Epsilon: 0.817\n",
      "Episode: 400 Duration: 0:00:06.528893 Num steps: 164 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.257 Epsilon: 0.817\n",
      "Episode: 401 Duration: 0:00:08.897712 Num steps: 223 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.267 Epsilon: 0.816\n",
      "Episode: 402 Duration: 0:00:07.104672 Num steps: 176 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.238 Epsilon: 0.816\n",
      "Episode: 403 Duration: 0:00:09.253824 Num steps: 232 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.228 Epsilon: 0.816\n",
      "Episode: 404 Duration: 0:00:09.307423 Num steps: 230 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.218 Epsilon: 0.815\n",
      "Episode: 405 Duration: 0:00:06.624132 Num steps: 165 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.218 Epsilon: 0.815\n",
      "Episode: 406 Duration: 0:00:09.070392 Num steps: 228 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.198 Epsilon: 0.814\n",
      "Episode: 407 Duration: 0:00:12.908611 Num steps: 322 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.218 Epsilon: 0.814\n",
      "Episode: 408 Duration: 0:00:09.982529 Num steps: 251 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.208 Epsilon: 0.813\n",
      "Episode: 409 Duration: 0:00:07.076285 Num steps: 178 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.208 Epsilon: 0.813\n",
      "Episode: 410 Duration: 0:00:08.269589 Num steps: 206 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.208 Epsilon: 0.813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 411 Duration: 0:00:07.034347 Num steps: 177 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.178 Epsilon: 0.812\n",
      "Episode: 412 Duration: 0:00:10.784930 Num steps: 267 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.188 Epsilon: 0.812\n",
      "Episode: 413 Duration: 0:00:09.150136 Num steps: 230 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.198 Epsilon: 0.811\n",
      "Episode: 414 Duration: 0:00:07.138704 Num steps: 178 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.188 Epsilon: 0.811\n",
      "Episode: 415 Duration: 0:00:07.287293 Num steps: 181 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.168 Epsilon: 0.811\n",
      "Episode: 416 Duration: 0:00:07.026985 Num steps: 174 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.168 Epsilon: 0.810\n",
      "Episode: 417 Duration: 0:00:11.724731 Num steps: 285 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.188 Epsilon: 0.810\n",
      "Episode: 418 Duration: 0:00:07.254719 Num steps: 172 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.168 Epsilon: 0.810\n",
      "Episode: 419 Duration: 0:00:06.963893 Num steps: 174 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.168 Epsilon: 0.809\n",
      "Episode: 420 Duration: 0:00:12.940635 Num steps: 322 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.198 Epsilon: 0.809\n",
      "Episode: 421 Duration: 0:00:09.051979 Num steps: 227 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.208 Epsilon: 0.808\n",
      "Episode: 422 Duration: 0:00:10.221398 Num steps: 255 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.208 Epsilon: 0.808\n",
      "Episode: 423 Duration: 0:00:13.861549 Num steps: 344 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.238 Epsilon: 0.807\n",
      "Episode: 424 Duration: 0:00:15.199737 Num steps: 377 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.277 Epsilon: 0.807\n",
      "Episode: 425 Duration: 0:00:11.923088 Num steps: 301 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.806\n",
      "Episode: 426 Duration: 0:00:09.757004 Num steps: 234 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.277 Epsilon: 0.806\n",
      "Episode: 427 Duration: 0:00:10.917073 Num steps: 265 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.277 Epsilon: 0.805\n",
      "Episode: 428 Duration: 0:00:07.092972 Num steps: 173 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.805\n",
      "Episode: 429 Duration: 0:00:11.197577 Num steps: 276 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.804\n",
      "Episode: 430 Duration: 0:00:09.208362 Num steps: 215 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.277 Epsilon: 0.804\n",
      "Episode: 431 Duration: 0:00:07.357927 Num steps: 181 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.804\n",
      "Episode: 432 Duration: 0:00:07.507371 Num steps: 186 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.267 Epsilon: 0.803\n",
      "Episode: 433 Duration: 0:00:14.800109 Num steps: 368 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.803\n",
      "Episode: 434 Duration: 0:00:06.432602 Num steps: 160 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.257 Epsilon: 0.802\n",
      "Copied model parameters to target network. total_t = 110000, period = 10000\n",
      "Episode: 435 Duration: 0:00:12.872548 Num steps: 324 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.802\n",
      "Episode: 436 Duration: 0:00:09.382542 Num steps: 231 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.277 Epsilon: 0.801\n",
      "Episode: 437 Duration: 0:00:07.651016 Num steps: 191 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.801\n",
      "Episode: 438 Duration: 0:00:10.442118 Num steps: 261 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.801\n",
      "Episode: 439 Duration: 0:00:06.828095 Num steps: 169 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.267 Epsilon: 0.800\n",
      "Episode: 440 Duration: 0:00:07.290485 Num steps: 180 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.800\n",
      "Episode: 441 Duration: 0:00:13.498604 Num steps: 335 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.799\n",
      "Episode: 442 Duration: 0:00:09.537297 Num steps: 238 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.307 Epsilon: 0.799\n",
      "Episode: 443 Duration: 0:00:12.158364 Num steps: 302 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.327 Epsilon: 0.798\n",
      "Episode: 444 Duration: 0:00:08.426250 Num steps: 209 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.798\n",
      "Episode: 445 Duration: 0:00:08.711857 Num steps: 215 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.798\n",
      "Episode: 446 Duration: 0:00:08.007732 Num steps: 201 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.257 Epsilon: 0.797\n",
      "Episode: 447 Duration: 0:00:08.812894 Num steps: 218 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.248 Epsilon: 0.797\n",
      "Episode: 448 Duration: 0:00:09.593705 Num steps: 237 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.238 Epsilon: 0.796\n",
      "Episode: 449 Duration: 0:00:15.497587 Num steps: 386 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.796\n",
      "Episode: 450 Duration: 0:00:14.674545 Num steps: 367 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.795\n",
      "Episode: 451 Duration: 0:00:11.315737 Num steps: 281 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.795\n",
      "Episode: 452 Duration: 0:00:08.967895 Num steps: 219 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.307 Epsilon: 0.794\n",
      "Episode: 453 Duration: 0:00:07.111620 Num steps: 168 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.287 Epsilon: 0.794\n",
      "Episode: 454 Duration: 0:00:16.768919 Num steps: 411 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.793\n",
      "Episode: 455 Duration: 0:00:07.155374 Num steps: 176 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.793\n",
      "Episode: 456 Duration: 0:00:11.266488 Num steps: 282 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.792\n",
      "Episode: 457 Duration: 0:00:07.020333 Num steps: 174 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.238 Epsilon: 0.792\n",
      "Episode: 458 Duration: 0:00:10.933205 Num steps: 271 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.248 Epsilon: 0.791\n",
      "Episode: 459 Duration: 0:00:14.565389 Num steps: 363 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.791\n",
      "Episode: 460 Duration: 0:00:08.965886 Num steps: 226 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.790\n",
      "Episode: 461 Duration: 0:00:10.979332 Num steps: 276 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.790\n",
      "Episode: 462 Duration: 0:00:11.272583 Num steps: 281 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.789\n",
      "Episode: 463 Duration: 0:00:10.403441 Num steps: 256 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.297 Epsilon: 0.789\n",
      "Episode: 464 Duration: 0:00:08.427403 Num steps: 212 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.307 Epsilon: 0.789\n",
      "Episode: 465 Duration: 0:00:06.807267 Num steps: 170 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.788\n",
      "Episode: 466 Duration: 0:00:12.201810 Num steps: 302 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.307 Epsilon: 0.788\n",
      "Episode: 467 Duration: 0:00:11.318493 Num steps: 282 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.787\n",
      "Episode: 468 Duration: 0:00:06.914467 Num steps: 173 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.307 Epsilon: 0.787\n",
      "Episode: 469 Duration: 0:00:09.555692 Num steps: 223 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.317 Epsilon: 0.787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 470 Duration: 0:00:07.694251 Num steps: 179 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.297 Epsilon: 0.786\n",
      "Episode: 471 Duration: 0:00:08.026129 Num steps: 201 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.786\n",
      "Episode: 472 Duration: 0:00:13.535103 Num steps: 322 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.287 Epsilon: 0.785\n",
      "Episode: 473 Duration: 0:00:14.707966 Num steps: 340 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.297 Epsilon: 0.785\n",
      "Episode: 474 Duration: 0:00:07.375237 Num steps: 174 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.297 Epsilon: 0.784\n",
      "Copied model parameters to target network. total_t = 120000, period = 10000\n",
      "Episode: 475 Duration: 0:00:17.816208 Num steps: 422 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 1.317 Epsilon: 0.784\n",
      "Episode: 476 Duration: 0:00:14.108099 Num steps: 350 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.783\n",
      "Episode: 477 Duration: 0:00:08.141680 Num steps: 189 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.317 Epsilon: 0.783\n",
      "Episode: 478 Duration: 0:00:07.982217 Num steps: 189 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.307 Epsilon: 0.782\n",
      "Episode: 479 Duration: 0:00:10.147741 Num steps: 244 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.277 Epsilon: 0.782\n",
      "Episode: 480 Duration: 0:00:08.091546 Num steps: 179 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.228 Epsilon: 0.781\n",
      "Episode: 481 Duration: 0:00:09.532373 Num steps: 224 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.238 Epsilon: 0.781\n",
      "Episode: 482 Duration: 0:00:12.333786 Num steps: 287 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.248 Epsilon: 0.781\n",
      "Episode: 483 Duration: 0:00:13.243684 Num steps: 305 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.257 Epsilon: 0.780\n",
      "Episode: 484 Duration: 0:00:07.671448 Num steps: 176 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.248 Epsilon: 0.780\n",
      "Episode: 485 Duration: 0:00:16.255014 Num steps: 372 Reward: 4.0 Training time per step: 0.038 Avg Reward (Last 100): 1.287 Epsilon: 0.779\n",
      "Episode: 486 Duration: 0:00:11.173838 Num steps: 267 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.307 Epsilon: 0.779\n",
      "Episode: 487 Duration: 0:00:09.223581 Num steps: 227 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.277 Epsilon: 0.778\n",
      "Episode: 488 Duration: 0:00:07.131555 Num steps: 175 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.778\n",
      "Episode: 489 Duration: 0:00:09.836598 Num steps: 237 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.777\n",
      "Episode: 490 Duration: 0:00:06.994441 Num steps: 172 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.257 Epsilon: 0.777\n",
      "Episode: 491 Duration: 0:00:12.322938 Num steps: 304 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.277 Epsilon: 0.777\n",
      "Episode: 492 Duration: 0:00:11.350505 Num steps: 278 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.297 Epsilon: 0.776\n",
      "Episode: 493 Duration: 0:00:13.301201 Num steps: 331 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.307 Epsilon: 0.775\n",
      "Episode: 494 Duration: 0:00:13.878249 Num steps: 345 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.775\n",
      "Episode: 495 Duration: 0:00:07.093651 Num steps: 175 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.775\n",
      "Episode: 496 Duration: 0:00:09.514030 Num steps: 234 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.297 Epsilon: 0.774\n",
      "Episode: 497 Duration: 0:00:13.214585 Num steps: 303 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.307 Epsilon: 0.774\n",
      "Episode: 498 Duration: 0:00:12.742472 Num steps: 307 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.327 Epsilon: 0.773\n",
      "Episode: 499 Duration: 0:00:07.105277 Num steps: 173 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.307 Epsilon: 0.773\n",
      "Episode: 500 Duration: 0:00:09.930659 Num steps: 246 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.772\n",
      "Episode: 501 Duration: 0:00:07.304911 Num steps: 179 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.287 Epsilon: 0.772\n",
      "Episode: 502 Duration: 0:00:06.879167 Num steps: 170 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.277 Epsilon: 0.772\n",
      "Episode: 503 Duration: 0:00:09.881339 Num steps: 245 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.771\n",
      "Episode: 504 Duration: 0:00:07.333487 Num steps: 181 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.277 Epsilon: 0.771\n",
      "Episode: 505 Duration: 0:00:06.883043 Num steps: 170 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.771\n",
      "Episode: 506 Duration: 0:00:10.044759 Num steps: 245 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.277 Epsilon: 0.770\n",
      "Episode: 507 Duration: 0:00:07.598765 Num steps: 189 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.267 Epsilon: 0.770\n",
      "Episode: 508 Duration: 0:00:10.655887 Num steps: 267 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.267 Epsilon: 0.769\n",
      "Episode: 509 Duration: 0:00:07.182864 Num steps: 178 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.257 Epsilon: 0.769\n",
      "Episode: 510 Duration: 0:00:06.817112 Num steps: 169 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.257 Epsilon: 0.769\n",
      "Episode: 511 Duration: 0:00:07.342711 Num steps: 182 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.248 Epsilon: 0.768\n",
      "Episode: 512 Duration: 0:00:07.742023 Num steps: 192 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.248 Epsilon: 0.768\n",
      "Episode: 513 Duration: 0:00:17.224534 Num steps: 342 Reward: 3.0 Training time per step: 0.044 Avg Reward (Last 100): 1.257 Epsilon: 0.767\n",
      "Episode: 514 Duration: 0:00:09.932578 Num steps: 177 Reward: 0.0 Training time per step: 0.049 Avg Reward (Last 100): 1.248 Epsilon: 0.767\n",
      "Episode: 515 Duration: 0:00:13.112613 Num steps: 270 Reward: 2.0 Training time per step: 0.042 Avg Reward (Last 100): 1.267 Epsilon: 0.767\n",
      "Episode: 516 Duration: 0:00:07.672547 Num steps: 179 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.267 Epsilon: 0.766\n",
      "Copied model parameters to target network. total_t = 130000, period = 10000\n",
      "Episode: 517 Duration: 0:00:10.292766 Num steps: 231 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.277 Epsilon: 0.766\n",
      "Episode: 518 Duration: 0:00:08.950905 Num steps: 216 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.765\n",
      "Episode: 519 Duration: 0:00:12.553437 Num steps: 299 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.287 Epsilon: 0.765\n",
      "Episode: 520 Duration: 0:00:07.260786 Num steps: 180 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.765\n",
      "Episode: 521 Duration: 0:00:09.973816 Num steps: 246 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.764\n",
      "Episode: 522 Duration: 0:00:12.256129 Num steps: 303 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.287 Epsilon: 0.764\n",
      "Episode: 523 Duration: 0:00:14.900590 Num steps: 364 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.307 Epsilon: 0.763\n",
      "Episode: 524 Duration: 0:00:09.097693 Num steps: 225 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.763\n",
      "Episode: 525 Duration: 0:00:09.533556 Num steps: 236 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.257 Epsilon: 0.762\n",
      "Episode: 526 Duration: 0:00:07.096896 Num steps: 175 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.238 Epsilon: 0.762\n",
      "Episode: 527 Duration: 0:00:14.078089 Num steps: 347 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.257 Epsilon: 0.761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 528 Duration: 0:00:13.755391 Num steps: 340 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.267 Epsilon: 0.761\n",
      "Episode: 529 Duration: 0:00:09.660136 Num steps: 239 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.760\n",
      "Episode: 530 Duration: 0:00:11.810902 Num steps: 289 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.277 Epsilon: 0.760\n",
      "Episode: 531 Duration: 0:00:08.529419 Num steps: 211 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.759\n",
      "Episode: 532 Duration: 0:00:08.047204 Num steps: 200 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.759\n",
      "Episode: 533 Duration: 0:00:09.658303 Num steps: 234 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.287 Epsilon: 0.758\n",
      "Episode: 534 Duration: 0:00:07.417380 Num steps: 181 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.257 Epsilon: 0.758\n",
      "Episode: 535 Duration: 0:00:06.702140 Num steps: 165 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.257 Epsilon: 0.758\n",
      "Episode: 536 Duration: 0:00:16.885884 Num steps: 419 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.267 Epsilon: 0.757\n",
      "Episode: 537 Duration: 0:00:09.101698 Num steps: 222 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.757\n",
      "Episode: 538 Duration: 0:00:07.248139 Num steps: 179 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.267 Epsilon: 0.756\n",
      "Episode: 539 Duration: 0:00:11.892816 Num steps: 296 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.267 Epsilon: 0.756\n",
      "Episode: 540 Duration: 0:00:06.491346 Num steps: 160 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.756\n",
      "Episode: 541 Duration: 0:00:07.295365 Num steps: 179 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.755\n",
      "Episode: 542 Duration: 0:00:18.126117 Num steps: 445 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 1.287 Epsilon: 0.754\n",
      "Episode: 543 Duration: 0:00:08.422998 Num steps: 210 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.754\n",
      "Episode: 544 Duration: 0:00:11.756215 Num steps: 293 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.753\n",
      "Episode: 545 Duration: 0:00:13.261084 Num steps: 327 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.297 Epsilon: 0.753\n",
      "Episode: 546 Duration: 0:00:12.392151 Num steps: 305 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.317 Epsilon: 0.752\n",
      "Episode: 547 Duration: 0:00:10.887861 Num steps: 270 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.327 Epsilon: 0.752\n",
      "Episode: 548 Duration: 0:00:17.339692 Num steps: 427 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.356 Epsilon: 0.751\n",
      "Episode: 549 Duration: 0:00:14.256084 Num steps: 353 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.376 Epsilon: 0.750\n",
      "Episode: 550 Duration: 0:00:08.156609 Num steps: 202 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.347 Epsilon: 0.750\n",
      "Episode: 551 Duration: 0:00:07.292594 Num steps: 180 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.750\n",
      "Episode: 552 Duration: 0:00:06.677452 Num steps: 165 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.749\n",
      "Episode: 553 Duration: 0:00:07.633088 Num steps: 166 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.287 Epsilon: 0.749\n",
      "Episode: 554 Duration: 0:00:09.600133 Num steps: 235 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.297 Epsilon: 0.749\n",
      "Episode: 555 Duration: 0:00:07.309329 Num steps: 181 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.257 Epsilon: 0.748\n",
      "Copied model parameters to target network. total_t = 140000, period = 10000\n",
      "Episode: 556 Duration: 0:00:12.606041 Num steps: 314 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.748\n",
      "Episode: 557 Duration: 0:00:14.325250 Num steps: 354 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.307 Epsilon: 0.747\n",
      "Episode: 558 Duration: 0:00:06.732437 Num steps: 166 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.307 Epsilon: 0.747\n",
      "Episode: 559 Duration: 0:00:07.198956 Num steps: 179 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.747\n",
      "Episode: 560 Duration: 0:00:09.573517 Num steps: 235 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.257 Epsilon: 0.746\n",
      "Episode: 561 Duration: 0:00:08.940040 Num steps: 224 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.257 Epsilon: 0.746\n",
      "Episode: 562 Duration: 0:00:07.198582 Num steps: 179 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.238 Epsilon: 0.745\n",
      "Episode: 563 Duration: 0:00:16.324351 Num steps: 408 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.257 Epsilon: 0.745\n",
      "Episode: 564 Duration: 0:00:07.312195 Num steps: 183 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.248 Epsilon: 0.744\n",
      "Episode: 565 Duration: 0:00:11.915663 Num steps: 295 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.257 Epsilon: 0.744\n",
      "Episode: 566 Duration: 0:00:06.796195 Num steps: 168 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.257 Epsilon: 0.744\n",
      "Episode: 567 Duration: 0:00:11.740856 Num steps: 296 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.257 Epsilon: 0.743\n",
      "Episode: 568 Duration: 0:00:06.937327 Num steps: 173 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.238 Epsilon: 0.743\n",
      "Episode: 569 Duration: 0:00:12.096618 Num steps: 293 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.257 Epsilon: 0.742\n",
      "Episode: 570 Duration: 0:00:10.158078 Num steps: 252 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.267 Epsilon: 0.742\n",
      "Episode: 571 Duration: 0:00:08.327976 Num steps: 206 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.741\n",
      "Episode: 572 Duration: 0:00:07.036225 Num steps: 175 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.741\n",
      "Episode: 573 Duration: 0:00:08.216453 Num steps: 205 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.257 Epsilon: 0.741\n",
      "Episode: 574 Duration: 0:00:11.289078 Num steps: 281 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.248 Epsilon: 0.740\n",
      "Episode: 575 Duration: 0:00:16.393914 Num steps: 407 Reward: 5.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.739\n",
      "Episode: 576 Duration: 0:00:08.532180 Num steps: 213 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.267 Epsilon: 0.739\n",
      "Episode: 577 Duration: 0:00:22.612523 Num steps: 562 Reward: 7.0 Training time per step: 0.034 Avg Reward (Last 100): 1.307 Epsilon: 0.738\n",
      "Episode: 578 Duration: 0:00:07.442288 Num steps: 180 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.307 Epsilon: 0.738\n",
      "Episode: 579 Duration: 0:00:08.858663 Num steps: 218 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.317 Epsilon: 0.737\n",
      "Episode: 580 Duration: 0:00:10.112879 Num steps: 252 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.737\n",
      "Episode: 581 Duration: 0:00:07.212165 Num steps: 178 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.317 Epsilon: 0.737\n",
      "Episode: 582 Duration: 0:00:09.652304 Num steps: 241 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.736\n",
      "Episode: 583 Duration: 0:00:10.480418 Num steps: 256 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.317 Epsilon: 0.736\n",
      "Episode: 584 Duration: 0:00:07.222437 Num steps: 181 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.735\n",
      "Episode: 585 Duration: 0:00:06.827279 Num steps: 170 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.287 Epsilon: 0.735\n",
      "Episode: 586 Duration: 0:00:10.376205 Num steps: 260 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.257 Epsilon: 0.735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 587 Duration: 0:00:16.845853 Num steps: 420 Reward: 4.0 Training time per step: 0.034 Avg Reward (Last 100): 1.277 Epsilon: 0.734\n",
      "Episode: 588 Duration: 0:00:07.239993 Num steps: 180 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.267 Epsilon: 0.733\n",
      "Episode: 589 Duration: 0:00:14.852760 Num steps: 300 Reward: 2.0 Training time per step: 0.043 Avg Reward (Last 100): 1.287 Epsilon: 0.733\n",
      "Episode: 590 Duration: 0:00:17.013490 Num steps: 335 Reward: 3.0 Training time per step: 0.044 Avg Reward (Last 100): 1.307 Epsilon: 0.732\n",
      "Episode: 591 Duration: 0:00:14.247081 Num steps: 285 Reward: 2.0 Training time per step: 0.043 Avg Reward (Last 100): 1.327 Epsilon: 0.732\n",
      "Episode: 592 Duration: 0:00:26.669002 Num steps: 519 Reward: 6.0 Training time per step: 0.044 Avg Reward (Last 100): 1.366 Epsilon: 0.731\n",
      "Episode: 593 Duration: 0:00:10.343924 Num steps: 223 Reward: 1.0 Training time per step: 0.040 Avg Reward (Last 100): 1.356 Epsilon: 0.730\n",
      "Copied model parameters to target network. total_t = 150000, period = 10000\n",
      "Episode: 594 Duration: 0:00:21.088797 Num steps: 440 Reward: 5.0 Training time per step: 0.041 Avg Reward (Last 100): 1.376 Epsilon: 0.730\n",
      "Episode: 595 Duration: 0:00:12.888581 Num steps: 241 Reward: 1.0 Training time per step: 0.046 Avg Reward (Last 100): 1.356 Epsilon: 0.729\n",
      "Episode: 596 Duration: 0:00:08.102855 Num steps: 168 Reward: 0.0 Training time per step: 0.041 Avg Reward (Last 100): 1.356 Epsilon: 0.729\n",
      "Episode: 597 Duration: 0:00:07.798780 Num steps: 170 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.347 Epsilon: 0.729\n",
      "Episode: 598 Duration: 0:00:07.114557 Num steps: 172 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.327 Epsilon: 0.728\n",
      "Episode: 599 Duration: 0:00:07.097756 Num steps: 172 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.307 Epsilon: 0.728\n",
      "Episode: 600 Duration: 0:00:06.846383 Num steps: 167 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.307 Epsilon: 0.728\n",
      "Episode: 601 Duration: 0:00:07.397592 Num steps: 182 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.297 Epsilon: 0.727\n",
      "Episode: 602 Duration: 0:00:09.379136 Num steps: 231 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.307 Epsilon: 0.727\n",
      "Episode: 603 Duration: 0:00:07.118021 Num steps: 174 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.307 Epsilon: 0.727\n",
      "Episode: 604 Duration: 0:00:11.258711 Num steps: 276 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.317 Epsilon: 0.726\n",
      "Episode: 605 Duration: 0:00:09.870307 Num steps: 240 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.327 Epsilon: 0.726\n",
      "Episode: 606 Duration: 0:00:12.351415 Num steps: 298 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.347 Epsilon: 0.725\n",
      "Episode: 607 Duration: 0:00:13.704279 Num steps: 333 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.356 Epsilon: 0.725\n",
      "Episode: 608 Duration: 0:00:08.767914 Num steps: 211 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.366 Epsilon: 0.724\n",
      "Episode: 609 Duration: 0:00:15.515560 Num steps: 378 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.386 Epsilon: 0.724\n",
      "Episode: 610 Duration: 0:00:10.941722 Num steps: 267 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.396 Epsilon: 0.723\n",
      "Episode: 611 Duration: 0:00:07.285520 Num steps: 177 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.396 Epsilon: 0.723\n",
      "Episode: 612 Duration: 0:00:06.762296 Num steps: 166 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.396 Epsilon: 0.722\n",
      "Episode: 613 Duration: 0:00:08.338208 Num steps: 181 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.396 Epsilon: 0.722\n",
      "Episode: 614 Duration: 0:00:10.302980 Num steps: 244 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.376 Epsilon: 0.722\n",
      "Episode: 615 Duration: 0:00:16.802400 Num steps: 403 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.416 Epsilon: 0.721\n",
      "Episode: 616 Duration: 0:00:15.110721 Num steps: 365 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.426 Epsilon: 0.720\n",
      "Episode: 617 Duration: 0:00:07.350969 Num steps: 173 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.426 Epsilon: 0.720\n",
      "Episode: 618 Duration: 0:00:07.092296 Num steps: 172 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.416 Epsilon: 0.720\n",
      "Episode: 619 Duration: 0:00:11.153219 Num steps: 271 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.426 Epsilon: 0.719\n",
      "Episode: 620 Duration: 0:00:11.268957 Num steps: 278 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.426 Epsilon: 0.719\n",
      "Episode: 621 Duration: 0:00:11.829454 Num steps: 287 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.446 Epsilon: 0.718\n",
      "Episode: 622 Duration: 0:00:07.445389 Num steps: 180 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.426 Epsilon: 0.718\n",
      "Episode: 623 Duration: 0:00:09.925192 Num steps: 245 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.416 Epsilon: 0.717\n",
      "Episode: 624 Duration: 0:00:13.098206 Num steps: 318 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.717\n",
      "Episode: 625 Duration: 0:00:08.558371 Num steps: 210 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.406 Epsilon: 0.716\n",
      "Episode: 626 Duration: 0:00:06.972557 Num steps: 170 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.396 Epsilon: 0.716\n",
      "Episode: 627 Duration: 0:00:07.280438 Num steps: 180 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.396 Epsilon: 0.716\n",
      "Episode: 628 Duration: 0:00:06.872912 Num steps: 169 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.366 Epsilon: 0.716\n",
      "Episode: 629 Duration: 0:00:12.463074 Num steps: 304 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.356 Epsilon: 0.715\n",
      "Episode: 630 Duration: 0:00:09.978496 Num steps: 248 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.356 Epsilon: 0.715\n",
      "Episode: 631 Duration: 0:00:07.123264 Num steps: 175 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.337 Epsilon: 0.714\n",
      "Episode: 632 Duration: 0:00:08.013686 Num steps: 180 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.327 Epsilon: 0.714\n",
      "Episode: 633 Duration: 0:00:14.710421 Num steps: 305 Reward: 2.0 Training time per step: 0.042 Avg Reward (Last 100): 1.347 Epsilon: 0.713\n",
      "Episode: 634 Duration: 0:00:13.499579 Num steps: 280 Reward: 2.0 Training time per step: 0.042 Avg Reward (Last 100): 1.356 Epsilon: 0.713\n",
      "Episode: 635 Duration: 0:00:07.889229 Num steps: 178 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.356 Epsilon: 0.713\n",
      "Episode: 636 Duration: 0:00:08.066321 Num steps: 182 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.356 Epsilon: 0.712\n",
      "Copied model parameters to target network. total_t = 160000, period = 10000\n",
      "Episode: 637 Duration: 0:00:10.633171 Num steps: 207 Reward: 1.0 Training time per step: 0.044 Avg Reward (Last 100): 1.327 Epsilon: 0.712\n",
      "Episode: 638 Duration: 0:00:09.496520 Num steps: 193 Reward: 0.0 Training time per step: 0.042 Avg Reward (Last 100): 1.317 Epsilon: 0.711\n",
      "Episode: 639 Duration: 0:00:12.605810 Num steps: 255 Reward: 1.0 Training time per step: 0.043 Avg Reward (Last 100): 1.327 Epsilon: 0.711\n",
      "Episode: 640 Duration: 0:00:10.016792 Num steps: 175 Reward: 0.0 Training time per step: 0.050 Avg Reward (Last 100): 1.307 Epsilon: 0.711\n",
      "Episode: 641 Duration: 0:00:07.656455 Num steps: 163 Reward: 0.0 Training time per step: 0.040 Avg Reward (Last 100): 1.307 Epsilon: 0.710\n",
      "Episode: 642 Duration: 0:00:16.549468 Num steps: 372 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.337 Epsilon: 0.710\n",
      "Episode: 643 Duration: 0:00:10.228877 Num steps: 251 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.297 Epsilon: 0.709\n",
      "Episode: 644 Duration: 0:00:09.804935 Num steps: 232 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.297 Epsilon: 0.709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 645 Duration: 0:00:13.536898 Num steps: 306 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.297 Epsilon: 0.708\n",
      "Episode: 646 Duration: 0:00:13.364604 Num steps: 305 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.287 Epsilon: 0.708\n",
      "Episode: 647 Duration: 0:00:10.743572 Num steps: 247 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.277 Epsilon: 0.707\n",
      "Episode: 648 Duration: 0:00:10.951745 Num steps: 252 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.267 Epsilon: 0.707\n",
      "Episode: 649 Duration: 0:00:12.609700 Num steps: 283 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.248 Epsilon: 0.706\n",
      "Episode: 650 Duration: 0:00:07.739970 Num steps: 178 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.218 Epsilon: 0.706\n",
      "Episode: 651 Duration: 0:00:13.835878 Num steps: 318 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.238 Epsilon: 0.705\n",
      "Episode: 652 Duration: 0:00:09.932870 Num steps: 228 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.248 Epsilon: 0.705\n",
      "Episode: 653 Duration: 0:00:14.598254 Num steps: 331 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.277 Epsilon: 0.704\n",
      "Episode: 654 Duration: 0:00:12.244019 Num steps: 281 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.297 Epsilon: 0.704\n",
      "Episode: 655 Duration: 0:00:12.207124 Num steps: 276 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.307 Epsilon: 0.703\n",
      "Episode: 656 Duration: 0:00:19.571991 Num steps: 451 Reward: 5.0 Training time per step: 0.037 Avg Reward (Last 100): 1.356 Epsilon: 0.703\n",
      "Episode: 657 Duration: 0:00:12.287224 Num steps: 278 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.347 Epsilon: 0.702\n",
      "Episode: 658 Duration: 0:00:10.071247 Num steps: 232 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.317 Epsilon: 0.702\n",
      "Episode: 659 Duration: 0:00:07.602196 Num steps: 175 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.317 Epsilon: 0.701\n",
      "Episode: 660 Duration: 0:00:10.251165 Num steps: 234 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.327 Epsilon: 0.701\n",
      "Episode: 661 Duration: 0:00:07.885268 Num steps: 182 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.317 Epsilon: 0.701\n",
      "Episode: 662 Duration: 0:00:14.916643 Num steps: 342 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.337 Epsilon: 0.700\n",
      "Episode: 663 Duration: 0:00:14.045770 Num steps: 322 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.366 Epsilon: 0.699\n",
      "Episode: 664 Duration: 0:00:07.614329 Num steps: 172 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.327 Epsilon: 0.699\n",
      "Episode: 665 Duration: 0:00:10.040510 Num steps: 230 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.337 Epsilon: 0.699\n",
      "Episode: 666 Duration: 0:00:10.128029 Num steps: 233 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.327 Epsilon: 0.698\n",
      "Episode: 667 Duration: 0:00:08.817953 Num steps: 203 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.337 Epsilon: 0.698\n",
      "Episode: 668 Duration: 0:00:19.700662 Num steps: 451 Reward: 4.0 Training time per step: 0.037 Avg Reward (Last 100): 1.356 Epsilon: 0.697\n",
      "Episode: 669 Duration: 0:00:06.970593 Num steps: 167 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.356 Epsilon: 0.697\n",
      "Episode: 670 Duration: 0:00:10.181936 Num steps: 246 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.347 Epsilon: 0.696\n",
      "Episode: 671 Duration: 0:00:11.046244 Num steps: 268 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.347 Epsilon: 0.696\n",
      "Episode: 672 Duration: 0:00:10.990032 Num steps: 270 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.356 Epsilon: 0.695\n",
      "Episode: 673 Duration: 0:00:14.119210 Num steps: 313 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.376 Epsilon: 0.695\n",
      "Episode: 674 Duration: 0:00:13.871865 Num steps: 318 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.406 Epsilon: 0.694\n",
      "Copied model parameters to target network. total_t = 170000, period = 10000\n",
      "Episode: 675 Duration: 0:00:08.239555 Num steps: 188 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.386 Epsilon: 0.694\n",
      "Episode: 676 Duration: 0:00:12.810169 Num steps: 295 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.356 Epsilon: 0.693\n",
      "Episode: 677 Duration: 0:00:09.804728 Num steps: 224 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.356 Epsilon: 0.693\n",
      "Episode: 678 Duration: 0:00:09.416690 Num steps: 212 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.297 Epsilon: 0.693\n",
      "Episode: 679 Duration: 0:00:07.539917 Num steps: 172 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.297 Epsilon: 0.692\n",
      "Episode: 680 Duration: 0:00:07.317933 Num steps: 162 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.287 Epsilon: 0.692\n",
      "Episode: 681 Duration: 0:00:07.618035 Num steps: 165 Reward: 0.0 Training time per step: 0.040 Avg Reward (Last 100): 1.277 Epsilon: 0.692\n",
      "Episode: 682 Duration: 0:00:09.449714 Num steps: 208 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.287 Epsilon: 0.691\n",
      "Episode: 683 Duration: 0:00:09.766163 Num steps: 217 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.287 Epsilon: 0.691\n",
      "Episode: 684 Duration: 0:00:15.381259 Num steps: 348 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.297 Epsilon: 0.690\n",
      "Episode: 685 Duration: 0:00:07.817735 Num steps: 180 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.297 Epsilon: 0.690\n",
      "Episode: 686 Duration: 0:00:07.840673 Num steps: 178 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.297 Epsilon: 0.690\n",
      "Episode: 687 Duration: 0:00:15.439124 Num steps: 356 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.317 Epsilon: 0.689\n",
      "Episode: 688 Duration: 0:00:10.347234 Num steps: 237 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.287 Epsilon: 0.689\n",
      "Episode: 689 Duration: 0:00:07.166363 Num steps: 165 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.287 Epsilon: 0.688\n",
      "Episode: 690 Duration: 0:00:12.827758 Num steps: 294 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.287 Epsilon: 0.688\n",
      "Episode: 691 Duration: 0:00:10.419433 Num steps: 235 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.267 Epsilon: 0.687\n",
      "Episode: 692 Duration: 0:00:08.007898 Num steps: 184 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.248 Epsilon: 0.687\n",
      "Episode: 693 Duration: 0:00:11.548767 Num steps: 267 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.208 Epsilon: 0.687\n",
      "Episode: 694 Duration: 0:00:15.209064 Num steps: 349 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.228 Epsilon: 0.686\n",
      "Episode: 695 Duration: 0:00:07.365199 Num steps: 168 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.178 Epsilon: 0.686\n",
      "Episode: 696 Duration: 0:00:13.315484 Num steps: 304 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.198 Epsilon: 0.685\n",
      "Episode: 697 Duration: 0:00:08.019950 Num steps: 185 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.198 Epsilon: 0.685\n",
      "Episode: 698 Duration: 0:00:07.220206 Num steps: 164 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.198 Epsilon: 0.684\n",
      "Episode: 699 Duration: 0:00:07.790044 Num steps: 181 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.198 Epsilon: 0.684\n",
      "Episode: 700 Duration: 0:00:12.585349 Num steps: 289 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.218 Epsilon: 0.684\n",
      "Episode: 701 Duration: 0:00:07.429725 Num steps: 171 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.218 Epsilon: 0.683\n",
      "Episode: 702 Duration: 0:00:16.000479 Num steps: 351 Reward: 3.0 Training time per step: 0.039 Avg Reward (Last 100): 1.248 Epsilon: 0.683\n",
      "Episode: 703 Duration: 0:00:10.389832 Num steps: 234 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.248 Epsilon: 0.682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 704 Duration: 0:00:12.967513 Num steps: 292 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.277 Epsilon: 0.682\n",
      "Episode: 705 Duration: 0:00:07.307279 Num steps: 166 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.257 Epsilon: 0.681\n",
      "Episode: 706 Duration: 0:00:11.866780 Num steps: 272 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.267 Epsilon: 0.681\n",
      "Episode: 707 Duration: 0:00:07.494979 Num steps: 171 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.248 Epsilon: 0.681\n",
      "Episode: 708 Duration: 0:00:13.605922 Num steps: 310 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.257 Epsilon: 0.680\n",
      "Episode: 709 Duration: 0:00:11.524226 Num steps: 265 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.267 Epsilon: 0.680\n",
      "Episode: 710 Duration: 0:00:07.365112 Num steps: 168 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.228 Epsilon: 0.679\n",
      "Episode: 711 Duration: 0:00:18.537553 Num steps: 423 Reward: 4.0 Training time per step: 0.037 Avg Reward (Last 100): 1.257 Epsilon: 0.679\n",
      "Episode: 712 Duration: 0:00:09.357613 Num steps: 208 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.267 Epsilon: 0.678\n",
      "Episode: 713 Duration: 0:00:11.431002 Num steps: 260 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.287 Epsilon: 0.678\n",
      "Episode: 714 Duration: 0:00:11.910410 Num steps: 269 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.307 Epsilon: 0.677\n",
      "Episode: 715 Duration: 0:00:10.333309 Num steps: 237 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.307 Epsilon: 0.677\n",
      "Episode: 716 Duration: 0:00:19.103842 Num steps: 439 Reward: 4.0 Training time per step: 0.037 Avg Reward (Last 100): 1.307 Epsilon: 0.676\n",
      "Copied model parameters to target network. total_t = 180000, period = 10000\n",
      "Episode: 717 Duration: 0:00:13.434288 Num steps: 306 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.307 Epsilon: 0.675\n",
      "Episode: 718 Duration: 0:00:11.683878 Num steps: 269 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.327 Epsilon: 0.675\n",
      "Episode: 719 Duration: 0:00:07.934361 Num steps: 183 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.327 Epsilon: 0.675\n",
      "Episode: 720 Duration: 0:00:12.926335 Num steps: 284 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.327 Epsilon: 0.674\n",
      "Episode: 721 Duration: 0:00:10.241216 Num steps: 233 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.317 Epsilon: 0.674\n",
      "Episode: 722 Duration: 0:00:09.141901 Num steps: 210 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.307 Epsilon: 0.673\n",
      "Episode: 723 Duration: 0:00:13.565879 Num steps: 310 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.327 Epsilon: 0.673\n",
      "Episode: 724 Duration: 0:00:11.036289 Num steps: 250 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.327 Epsilon: 0.672\n",
      "Episode: 725 Duration: 0:00:13.761083 Num steps: 318 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.337 Epsilon: 0.672\n",
      "Episode: 726 Duration: 0:00:10.768433 Num steps: 243 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.337 Epsilon: 0.671\n",
      "Episode: 727 Duration: 0:00:11.540100 Num steps: 259 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.356 Epsilon: 0.671\n",
      "Episode: 728 Duration: 0:00:14.800150 Num steps: 336 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.386 Epsilon: 0.670\n",
      "Episode: 729 Duration: 0:00:09.130388 Num steps: 205 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.396 Epsilon: 0.670\n",
      "Episode: 730 Duration: 0:00:12.756112 Num steps: 267 Reward: 2.0 Training time per step: 0.041 Avg Reward (Last 100): 1.396 Epsilon: 0.669\n",
      "Episode: 731 Duration: 0:00:09.557135 Num steps: 215 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.396 Epsilon: 0.669\n",
      "Episode: 732 Duration: 0:00:07.712202 Num steps: 177 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.396 Epsilon: 0.669\n",
      "Episode: 733 Duration: 0:00:16.161982 Num steps: 367 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.426 Epsilon: 0.668\n",
      "Episode: 734 Duration: 0:00:15.326874 Num steps: 346 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.436 Epsilon: 0.667\n",
      "Episode: 735 Duration: 0:00:14.314034 Num steps: 318 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.436 Epsilon: 0.667\n",
      "Episode: 736 Duration: 0:00:10.285601 Num steps: 230 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.446 Epsilon: 0.666\n",
      "Episode: 737 Duration: 0:00:11.515266 Num steps: 259 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.455 Epsilon: 0.666\n",
      "Episode: 738 Duration: 0:00:10.024721 Num steps: 227 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.455 Epsilon: 0.666\n",
      "Episode: 739 Duration: 0:00:10.994200 Num steps: 245 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.475 Epsilon: 0.665\n",
      "Episode: 740 Duration: 0:00:12.804076 Num steps: 292 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.485 Epsilon: 0.665\n",
      "Episode: 741 Duration: 0:00:18.855071 Num steps: 410 Reward: 4.0 Training time per step: 0.039 Avg Reward (Last 100): 1.525 Epsilon: 0.664\n",
      "Episode: 742 Duration: 0:00:07.644541 Num steps: 169 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.525 Epsilon: 0.664\n",
      "Episode: 743 Duration: 0:00:12.936021 Num steps: 288 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.515 Epsilon: 0.663\n",
      "Episode: 744 Duration: 0:00:15.747421 Num steps: 356 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.535 Epsilon: 0.662\n",
      "Episode: 745 Duration: 0:00:11.935402 Num steps: 246 Reward: 1.0 Training time per step: 0.041 Avg Reward (Last 100): 1.535 Epsilon: 0.662\n",
      "Episode: 746 Duration: 0:00:13.459642 Num steps: 291 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.535 Epsilon: 0.661\n",
      "Episode: 747 Duration: 0:00:08.819395 Num steps: 202 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.525 Epsilon: 0.661\n",
      "Episode: 748 Duration: 0:00:14.670969 Num steps: 299 Reward: 2.0 Training time per step: 0.042 Avg Reward (Last 100): 1.535 Epsilon: 0.661\n",
      "Episode: 749 Duration: 0:00:10.119216 Num steps: 208 Reward: 1.0 Training time per step: 0.042 Avg Reward (Last 100): 1.535 Epsilon: 0.660\n",
      "Episode: 750 Duration: 0:00:11.288231 Num steps: 273 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.535 Epsilon: 0.660\n",
      "Episode: 751 Duration: 0:00:12.284566 Num steps: 299 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.554 Epsilon: 0.659\n",
      "Episode: 752 Duration: 0:00:10.641257 Num steps: 260 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.535 Epsilon: 0.659\n",
      "Episode: 753 Duration: 0:00:09.575531 Num steps: 231 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.535 Epsilon: 0.658\n",
      "Copied model parameters to target network. total_t = 190000, period = 10000\n",
      "Episode: 754 Duration: 0:00:11.125761 Num steps: 271 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.525 Epsilon: 0.658\n",
      "Episode: 755 Duration: 0:00:11.669895 Num steps: 278 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.525 Epsilon: 0.657\n",
      "Episode: 756 Duration: 0:00:15.324565 Num steps: 374 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.545 Epsilon: 0.657\n",
      "Episode: 757 Duration: 0:00:12.306441 Num steps: 303 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.515 Epsilon: 0.656\n",
      "Episode: 758 Duration: 0:00:12.613488 Num steps: 308 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.525 Epsilon: 0.655\n",
      "Episode: 759 Duration: 0:00:06.964980 Num steps: 170 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.515 Epsilon: 0.655\n",
      "Episode: 760 Duration: 0:00:07.228652 Num steps: 177 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.515 Epsilon: 0.655\n",
      "Episode: 761 Duration: 0:00:09.412668 Num steps: 230 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.515 Epsilon: 0.654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 762 Duration: 0:00:11.115315 Num steps: 251 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.525 Epsilon: 0.654\n",
      "Episode: 763 Duration: 0:00:10.015373 Num steps: 222 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.505 Epsilon: 0.654\n",
      "Episode: 764 Duration: 0:00:17.186650 Num steps: 385 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.505 Epsilon: 0.653\n",
      "Episode: 765 Duration: 0:00:11.324693 Num steps: 258 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.525 Epsilon: 0.652\n",
      "Episode: 766 Duration: 0:00:12.646944 Num steps: 288 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.535 Epsilon: 0.652\n",
      "Episode: 767 Duration: 0:00:12.906423 Num steps: 294 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.554 Epsilon: 0.651\n",
      "Episode: 768 Duration: 0:00:09.040129 Num steps: 205 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.554 Epsilon: 0.651\n",
      "Episode: 769 Duration: 0:00:10.890743 Num steps: 250 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.535 Epsilon: 0.651\n",
      "Episode: 770 Duration: 0:00:07.474691 Num steps: 170 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.535 Epsilon: 0.650\n",
      "Episode: 771 Duration: 0:00:09.395506 Num steps: 216 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.535 Epsilon: 0.650\n",
      "Episode: 772 Duration: 0:00:08.820298 Num steps: 203 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.525 Epsilon: 0.649\n",
      "Episode: 773 Duration: 0:00:11.857905 Num steps: 270 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.525 Epsilon: 0.649\n",
      "Episode: 774 Duration: 0:00:07.387790 Num steps: 167 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.505 Epsilon: 0.649\n",
      "Episode: 775 Duration: 0:00:14.946257 Num steps: 342 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.505 Epsilon: 0.648\n",
      "Episode: 776 Duration: 0:00:08.108857 Num steps: 185 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.505 Epsilon: 0.648\n",
      "Episode: 777 Duration: 0:00:09.110861 Num steps: 208 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.495 Epsilon: 0.647\n",
      "Episode: 778 Duration: 0:00:15.169842 Num steps: 348 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.515 Epsilon: 0.647\n",
      "Episode: 779 Duration: 0:00:07.462515 Num steps: 172 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.505 Epsilon: 0.646\n",
      "Episode: 780 Duration: 0:00:08.005274 Num steps: 169 Reward: 0.0 Training time per step: 0.040 Avg Reward (Last 100): 1.505 Epsilon: 0.646\n",
      "Episode: 781 Duration: 0:00:18.854982 Num steps: 430 Reward: 4.0 Training time per step: 0.037 Avg Reward (Last 100): 1.545 Epsilon: 0.645\n",
      "Episode: 782 Duration: 0:00:10.197366 Num steps: 229 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.554 Epsilon: 0.645\n",
      "Episode: 783 Duration: 0:00:10.252667 Num steps: 233 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.554 Epsilon: 0.645\n",
      "Episode: 784 Duration: 0:00:15.812829 Num steps: 355 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.574 Epsilon: 0.644\n",
      "Episode: 785 Duration: 0:00:07.808228 Num steps: 178 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.545 Epsilon: 0.644\n",
      "Episode: 786 Duration: 0:00:11.292033 Num steps: 254 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.564 Epsilon: 0.643\n",
      "Episode: 787 Duration: 0:00:07.770338 Num steps: 178 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.564 Epsilon: 0.643\n",
      "Episode: 788 Duration: 0:00:09.677120 Num steps: 216 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.545 Epsilon: 0.642\n",
      "Episode: 789 Duration: 0:00:07.332327 Num steps: 165 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.535 Epsilon: 0.642\n",
      "Episode: 790 Duration: 0:00:15.463710 Num steps: 350 Reward: 4.0 Training time per step: 0.038 Avg Reward (Last 100): 1.574 Epsilon: 0.641\n",
      "Episode: 791 Duration: 0:00:11.858262 Num steps: 273 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.564 Epsilon: 0.641\n",
      "Episode: 792 Duration: 0:00:09.169319 Num steps: 203 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.564 Epsilon: 0.641\n",
      "Episode: 793 Duration: 0:00:10.411592 Num steps: 235 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.574 Epsilon: 0.640\n",
      "Copied model parameters to target network. total_t = 200000, period = 10000\n",
      "Episode: 794 Duration: 0:00:13.629693 Num steps: 314 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.584 Epsilon: 0.640\n",
      "Episode: 795 Duration: 0:00:07.617279 Num steps: 172 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.554 Epsilon: 0.639\n",
      "Episode: 796 Duration: 0:00:10.165710 Num steps: 230 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.564 Epsilon: 0.639\n",
      "Episode: 797 Duration: 0:00:07.417286 Num steps: 164 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.535 Epsilon: 0.639\n",
      "Episode: 798 Duration: 0:00:09.256391 Num steps: 211 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.545 Epsilon: 0.638\n",
      "Episode: 799 Duration: 0:00:09.469588 Num steps: 218 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.554 Epsilon: 0.638\n",
      "Episode: 800 Duration: 0:00:15.977327 Num steps: 361 Reward: 4.0 Training time per step: 0.037 Avg Reward (Last 100): 1.594 Epsilon: 0.637\n",
      "Episode: 801 Duration: 0:00:07.453859 Num steps: 169 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.574 Epsilon: 0.637\n",
      "Episode: 802 Duration: 0:00:07.567956 Num steps: 167 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.574 Epsilon: 0.637\n",
      "Episode: 803 Duration: 0:00:07.107659 Num steps: 161 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.545 Epsilon: 0.636\n",
      "Episode: 804 Duration: 0:00:09.677400 Num steps: 222 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.545 Epsilon: 0.636\n",
      "Episode: 805 Duration: 0:00:08.299518 Num steps: 188 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.515 Epsilon: 0.636\n",
      "Episode: 806 Duration: 0:00:08.969256 Num steps: 204 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.525 Epsilon: 0.635\n",
      "Episode: 807 Duration: 0:00:07.542241 Num steps: 173 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.505 Epsilon: 0.635\n",
      "Episode: 808 Duration: 0:00:12.303143 Num steps: 279 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.525 Epsilon: 0.634\n",
      "Episode: 809 Duration: 0:00:14.269681 Num steps: 325 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.525 Epsilon: 0.634\n",
      "Episode: 810 Duration: 0:00:08.347342 Num steps: 187 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.505 Epsilon: 0.633\n",
      "Episode: 811 Duration: 0:00:07.587230 Num steps: 167 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.505 Epsilon: 0.633\n",
      "Episode: 812 Duration: 0:00:15.284700 Num steps: 342 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.495 Epsilon: 0.633\n",
      "Episode: 813 Duration: 0:00:11.646729 Num steps: 262 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.505 Epsilon: 0.632\n",
      "Episode: 814 Duration: 0:00:13.388180 Num steps: 308 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.505 Epsilon: 0.632\n",
      "Episode: 815 Duration: 0:00:07.415107 Num steps: 167 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.485 Epsilon: 0.631\n",
      "Episode: 816 Duration: 0:00:07.999517 Num steps: 179 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.475 Epsilon: 0.631\n",
      "Episode: 817 Duration: 0:00:08.139418 Num steps: 173 Reward: 0.0 Training time per step: 0.040 Avg Reward (Last 100): 1.436 Epsilon: 0.631\n",
      "Episode: 818 Duration: 0:00:13.993625 Num steps: 299 Reward: 3.0 Training time per step: 0.040 Avg Reward (Last 100): 1.436 Epsilon: 0.630\n",
      "Episode: 819 Duration: 0:00:08.016895 Num steps: 175 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.416 Epsilon: 0.630\n",
      "Episode: 820 Duration: 0:00:15.351594 Num steps: 341 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.446 Epsilon: 0.629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 821 Duration: 0:00:10.542777 Num steps: 231 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.436 Epsilon: 0.629\n",
      "Episode: 822 Duration: 0:00:14.605177 Num steps: 302 Reward: 2.0 Training time per step: 0.041 Avg Reward (Last 100): 1.446 Epsilon: 0.628\n",
      "Episode: 823 Duration: 0:00:08.280129 Num steps: 168 Reward: 0.0 Training time per step: 0.042 Avg Reward (Last 100): 1.436 Epsilon: 0.628\n",
      "Episode: 824 Duration: 0:00:08.797469 Num steps: 175 Reward: 0.0 Training time per step: 0.043 Avg Reward (Last 100): 1.416 Epsilon: 0.628\n",
      "Episode: 825 Duration: 0:00:09.873044 Num steps: 204 Reward: 0.0 Training time per step: 0.041 Avg Reward (Last 100): 1.406 Epsilon: 0.627\n",
      "Episode: 826 Duration: 0:00:10.336840 Num steps: 228 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.386 Epsilon: 0.627\n",
      "Episode: 827 Duration: 0:00:11.170792 Num steps: 227 Reward: 1.0 Training time per step: 0.042 Avg Reward (Last 100): 1.386 Epsilon: 0.626\n",
      "Episode: 828 Duration: 0:00:11.982110 Num steps: 236 Reward: 1.0 Training time per step: 0.043 Avg Reward (Last 100): 1.376 Epsilon: 0.626\n",
      "Episode: 829 Duration: 0:00:08.358683 Num steps: 174 Reward: 0.0 Training time per step: 0.041 Avg Reward (Last 100): 1.347 Epsilon: 0.626\n",
      "Episode: 830 Duration: 0:00:07.818650 Num steps: 166 Reward: 0.0 Training time per step: 0.040 Avg Reward (Last 100): 1.337 Epsilon: 0.625\n",
      "Episode: 831 Duration: 0:00:15.016060 Num steps: 323 Reward: 3.0 Training time per step: 0.039 Avg Reward (Last 100): 1.347 Epsilon: 0.625\n",
      "Episode: 832 Duration: 0:00:13.497396 Num steps: 285 Reward: 2.0 Training time per step: 0.040 Avg Reward (Last 100): 1.356 Epsilon: 0.624\n",
      "Episode: 833 Duration: 0:00:12.082354 Num steps: 274 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.376 Epsilon: 0.624\n",
      "Episode: 834 Duration: 0:00:11.974549 Num steps: 269 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.366 Epsilon: 0.623\n",
      "Episode: 835 Duration: 0:00:09.066204 Num steps: 200 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.347 Epsilon: 0.623\n",
      "Episode: 836 Duration: 0:00:11.508745 Num steps: 262 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.347 Epsilon: 0.622\n",
      "Copied model parameters to target network. total_t = 210000, period = 10000\n",
      "Episode: 837 Duration: 0:00:15.375484 Num steps: 336 Reward: 3.0 Training time per step: 0.039 Avg Reward (Last 100): 1.366 Epsilon: 0.622\n",
      "Episode: 838 Duration: 0:00:07.601536 Num steps: 168 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.356 Epsilon: 0.622\n",
      "Episode: 839 Duration: 0:00:18.921014 Num steps: 401 Reward: 4.0 Training time per step: 0.040 Avg Reward (Last 100): 1.386 Epsilon: 0.621\n",
      "Episode: 840 Duration: 0:00:07.322214 Num steps: 164 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.366 Epsilon: 0.620\n",
      "Episode: 841 Duration: 0:00:10.971005 Num steps: 230 Reward: 1.0 Training time per step: 0.041 Avg Reward (Last 100): 1.356 Epsilon: 0.620\n",
      "Episode: 842 Duration: 0:00:07.870093 Num steps: 174 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.317 Epsilon: 0.620\n",
      "Episode: 843 Duration: 0:00:12.012507 Num steps: 257 Reward: 1.0 Training time per step: 0.040 Avg Reward (Last 100): 1.327 Epsilon: 0.619\n",
      "Episode: 844 Duration: 0:00:18.018054 Num steps: 395 Reward: 4.0 Training time per step: 0.039 Avg Reward (Last 100): 1.347 Epsilon: 0.619\n",
      "Episode: 845 Duration: 0:00:12.150510 Num steps: 278 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.337 Epsilon: 0.618\n",
      "Episode: 846 Duration: 0:00:14.142978 Num steps: 318 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.356 Epsilon: 0.618\n",
      "Episode: 847 Duration: 0:00:10.740617 Num steps: 239 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.356 Epsilon: 0.617\n",
      "Episode: 848 Duration: 0:00:10.446528 Num steps: 233 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.356 Epsilon: 0.617\n",
      "Episode: 849 Duration: 0:00:07.290079 Num steps: 166 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.337 Epsilon: 0.616\n",
      "Episode: 850 Duration: 0:00:09.923234 Num steps: 226 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.337 Epsilon: 0.616\n",
      "Episode: 851 Duration: 0:00:09.228386 Num steps: 206 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.327 Epsilon: 0.616\n",
      "Episode: 852 Duration: 0:00:14.388336 Num steps: 326 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.337 Epsilon: 0.615\n",
      "Episode: 853 Duration: 0:00:07.629171 Num steps: 171 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.327 Epsilon: 0.615\n",
      "Episode: 854 Duration: 0:00:10.282398 Num steps: 221 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.327 Epsilon: 0.614\n",
      "Episode: 855 Duration: 0:00:12.567406 Num steps: 279 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.327 Epsilon: 0.614\n",
      "Episode: 856 Duration: 0:00:08.413194 Num steps: 183 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.307 Epsilon: 0.613\n",
      "Episode: 857 Duration: 0:00:08.080893 Num steps: 176 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.267 Epsilon: 0.613\n",
      "Episode: 858 Duration: 0:00:08.692285 Num steps: 195 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.248 Epsilon: 0.613\n",
      "Episode: 859 Duration: 0:00:10.523913 Num steps: 240 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.228 Epsilon: 0.612\n",
      "Episode: 860 Duration: 0:00:13.829437 Num steps: 315 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.248 Epsilon: 0.612\n",
      "Episode: 861 Duration: 0:00:11.294949 Num steps: 255 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.257 Epsilon: 0.611\n",
      "Episode: 862 Duration: 0:00:14.259745 Num steps: 323 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.277 Epsilon: 0.611\n",
      "Episode: 863 Duration: 0:00:13.919248 Num steps: 309 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.287 Epsilon: 0.610\n",
      "Episode: 864 Duration: 0:00:10.002141 Num steps: 225 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.287 Epsilon: 0.610\n",
      "Episode: 865 Duration: 0:00:14.861619 Num steps: 331 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.287 Epsilon: 0.609\n",
      "Episode: 866 Duration: 0:00:09.941382 Num steps: 221 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.277 Epsilon: 0.609\n",
      "Episode: 867 Duration: 0:00:10.854027 Num steps: 236 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.267 Epsilon: 0.608\n",
      "Episode: 868 Duration: 0:00:08.216232 Num steps: 187 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.238 Epsilon: 0.608\n",
      "Episode: 869 Duration: 0:00:09.992707 Num steps: 225 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.238 Epsilon: 0.608\n",
      "Episode: 870 Duration: 0:00:10.002251 Num steps: 228 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.228 Epsilon: 0.607\n",
      "Episode: 871 Duration: 0:00:10.383480 Num steps: 237 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.238 Epsilon: 0.607\n",
      "Episode: 872 Duration: 0:00:14.379145 Num steps: 322 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.257 Epsilon: 0.606\n",
      "Episode: 873 Duration: 0:00:07.216585 Num steps: 162 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.248 Epsilon: 0.606\n",
      "Episode: 874 Duration: 0:00:11.608283 Num steps: 267 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.248 Epsilon: 0.605\n",
      "Episode: 875 Duration: 0:00:12.018205 Num steps: 271 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.267 Epsilon: 0.605\n",
      "Episode: 876 Duration: 0:00:10.247047 Num steps: 234 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.248 Epsilon: 0.605\n",
      "Episode: 877 Duration: 0:00:11.148719 Num steps: 244 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.257 Epsilon: 0.604\n",
      "Copied model parameters to target network. total_t = 220000, period = 10000\n",
      "Episode: 878 Duration: 0:00:07.962183 Num steps: 178 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.248 Epsilon: 0.604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 879 Duration: 0:00:12.159280 Num steps: 276 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.238 Epsilon: 0.603\n",
      "Episode: 880 Duration: 0:00:12.731615 Num steps: 288 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.257 Epsilon: 0.603\n",
      "Episode: 881 Duration: 0:00:07.529477 Num steps: 173 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.257 Epsilon: 0.602\n",
      "Episode: 882 Duration: 0:00:10.324211 Num steps: 232 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.228 Epsilon: 0.602\n",
      "Episode: 883 Duration: 0:00:07.308299 Num steps: 165 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.218 Epsilon: 0.602\n",
      "Episode: 884 Duration: 0:00:07.657609 Num steps: 173 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.208 Epsilon: 0.601\n",
      "Episode: 885 Duration: 0:00:09.053263 Num steps: 206 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.188 Epsilon: 0.601\n",
      "Episode: 886 Duration: 0:00:07.538405 Num steps: 169 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.188 Epsilon: 0.601\n",
      "Episode: 887 Duration: 0:00:07.514183 Num steps: 170 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.168 Epsilon: 0.600\n",
      "Episode: 888 Duration: 0:00:10.211714 Num steps: 223 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.178 Epsilon: 0.600\n",
      "Episode: 889 Duration: 0:00:12.770909 Num steps: 274 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.188 Epsilon: 0.600\n",
      "Episode: 890 Duration: 0:00:12.312605 Num steps: 229 Reward: 1.0 Training time per step: 0.046 Avg Reward (Last 100): 1.198 Epsilon: 0.599\n",
      "Episode: 891 Duration: 0:00:11.416541 Num steps: 236 Reward: 1.0 Training time per step: 0.041 Avg Reward (Last 100): 1.168 Epsilon: 0.599\n",
      "Episode: 892 Duration: 0:00:10.955916 Num steps: 248 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.168 Epsilon: 0.598\n",
      "Episode: 893 Duration: 0:00:07.789560 Num steps: 188 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.158 Epsilon: 0.598\n",
      "Episode: 894 Duration: 0:00:07.442768 Num steps: 179 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.149 Epsilon: 0.598\n",
      "Episode: 895 Duration: 0:00:18.492937 Num steps: 397 Reward: 4.0 Training time per step: 0.039 Avg Reward (Last 100): 1.158 Epsilon: 0.597\n",
      "Episode: 896 Duration: 0:00:08.707964 Num steps: 168 Reward: 0.0 Training time per step: 0.045 Avg Reward (Last 100): 1.158 Epsilon: 0.597\n",
      "Episode: 897 Duration: 0:00:10.914709 Num steps: 212 Reward: 1.0 Training time per step: 0.044 Avg Reward (Last 100): 1.158 Epsilon: 0.596\n",
      "Episode: 898 Duration: 0:00:07.703715 Num steps: 171 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.158 Epsilon: 0.596\n",
      "Episode: 899 Duration: 0:00:10.689845 Num steps: 228 Reward: 1.0 Training time per step: 0.040 Avg Reward (Last 100): 1.158 Epsilon: 0.595\n",
      "Episode: 900 Duration: 0:00:11.712018 Num steps: 249 Reward: 1.0 Training time per step: 0.040 Avg Reward (Last 100): 1.158 Epsilon: 0.595\n",
      "Episode: 901 Duration: 0:00:07.601909 Num steps: 163 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.119 Epsilon: 0.595\n",
      "Episode: 902 Duration: 0:00:13.956386 Num steps: 313 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.139 Epsilon: 0.594\n",
      "Episode: 903 Duration: 0:00:12.652474 Num steps: 291 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.158 Epsilon: 0.594\n",
      "Episode: 904 Duration: 0:00:11.048858 Num steps: 252 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.178 Epsilon: 0.593\n",
      "Episode: 905 Duration: 0:00:09.726138 Num steps: 236 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.178 Epsilon: 0.593\n",
      "Episode: 906 Duration: 0:00:14.403415 Num steps: 346 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.208 Epsilon: 0.592\n",
      "Episode: 907 Duration: 0:00:14.217519 Num steps: 330 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.228 Epsilon: 0.592\n",
      "Episode: 908 Duration: 0:00:16.958352 Num steps: 409 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.591\n",
      "Episode: 909 Duration: 0:00:07.615042 Num steps: 183 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.248 Epsilon: 0.590\n",
      "Episode: 910 Duration: 0:00:06.910815 Num steps: 163 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.218 Epsilon: 0.590\n",
      "Episode: 911 Duration: 0:00:07.737583 Num steps: 185 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.218 Epsilon: 0.590\n",
      "Episode: 912 Duration: 0:00:07.646250 Num steps: 181 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.218 Epsilon: 0.590\n",
      "Episode: 913 Duration: 0:00:12.246482 Num steps: 293 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.208 Epsilon: 0.589\n",
      "Episode: 914 Duration: 0:00:11.660752 Num steps: 280 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.208 Epsilon: 0.589\n",
      "Episode: 915 Duration: 0:00:07.520778 Num steps: 182 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.188 Epsilon: 0.588\n",
      "Episode: 916 Duration: 0:00:12.805329 Num steps: 305 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.208 Epsilon: 0.588\n",
      "Episode: 917 Duration: 0:00:10.663477 Num steps: 250 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.218 Epsilon: 0.587\n",
      "Episode: 918 Duration: 0:00:07.426399 Num steps: 175 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.218 Epsilon: 0.587\n",
      "Episode: 919 Duration: 0:00:11.278581 Num steps: 268 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.208 Epsilon: 0.586\n",
      "Copied model parameters to target network. total_t = 230000, period = 10000\n",
      "Episode: 920 Duration: 0:00:11.548586 Num steps: 273 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.228 Epsilon: 0.586\n",
      "Episode: 921 Duration: 0:00:07.530256 Num steps: 178 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.198 Epsilon: 0.586\n",
      "Episode: 922 Duration: 0:00:07.484229 Num steps: 179 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.188 Epsilon: 0.585\n",
      "Episode: 923 Duration: 0:00:11.388252 Num steps: 268 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.188 Epsilon: 0.585\n",
      "Episode: 924 Duration: 0:00:07.584409 Num steps: 181 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.188 Epsilon: 0.584\n",
      "Episode: 925 Duration: 0:00:08.072505 Num steps: 193 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.188 Epsilon: 0.584\n",
      "Episode: 926 Duration: 0:00:07.884530 Num steps: 187 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.188 Epsilon: 0.584\n",
      "Episode: 927 Duration: 0:00:13.221610 Num steps: 309 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.198 Epsilon: 0.583\n",
      "Episode: 928 Duration: 0:00:08.532436 Num steps: 201 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.188 Epsilon: 0.583\n",
      "Episode: 929 Duration: 0:00:09.513341 Num steps: 230 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.188 Epsilon: 0.582\n",
      "Episode: 930 Duration: 0:00:07.497932 Num steps: 174 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.188 Epsilon: 0.582\n",
      "Episode: 931 Duration: 0:00:15.403700 Num steps: 369 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.228 Epsilon: 0.581\n",
      "Episode: 932 Duration: 0:00:11.237567 Num steps: 265 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.218 Epsilon: 0.581\n",
      "Episode: 933 Duration: 0:00:08.798468 Num steps: 204 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.198 Epsilon: 0.581\n",
      "Episode: 934 Duration: 0:00:09.776561 Num steps: 234 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.188 Epsilon: 0.580\n",
      "Episode: 935 Duration: 0:00:09.004638 Num steps: 214 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.178 Epsilon: 0.580\n",
      "Episode: 936 Duration: 0:00:07.335694 Num steps: 169 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.168 Epsilon: 0.580\n",
      "Episode: 937 Duration: 0:00:07.786232 Num steps: 187 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.149 Epsilon: 0.579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 938 Duration: 0:00:12.335887 Num steps: 293 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.139 Epsilon: 0.579\n",
      "Episode: 939 Duration: 0:00:10.237875 Num steps: 239 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.149 Epsilon: 0.578\n",
      "Episode: 940 Duration: 0:00:10.102492 Num steps: 244 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.119 Epsilon: 0.578\n",
      "Episode: 941 Duration: 0:00:09.498885 Num steps: 222 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.129 Epsilon: 0.577\n",
      "Episode: 942 Duration: 0:00:08.704825 Num steps: 199 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.119 Epsilon: 0.577\n",
      "Episode: 943 Duration: 0:00:11.695812 Num steps: 255 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.129 Epsilon: 0.577\n",
      "Episode: 944 Duration: 0:00:10.432810 Num steps: 234 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.129 Epsilon: 0.576\n",
      "Episode: 945 Duration: 0:00:12.947565 Num steps: 288 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.109 Epsilon: 0.576\n",
      "Episode: 946 Duration: 0:00:09.381628 Num steps: 212 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.099 Epsilon: 0.575\n",
      "Episode: 947 Duration: 0:00:09.440446 Num steps: 207 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.079 Epsilon: 0.575\n",
      "Episode: 948 Duration: 0:00:13.267632 Num steps: 287 Reward: 3.0 Training time per step: 0.039 Avg Reward (Last 100): 1.089 Epsilon: 0.574\n",
      "Episode: 949 Duration: 0:00:07.730971 Num steps: 177 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.079 Epsilon: 0.574\n",
      "Episode: 950 Duration: 0:00:07.971923 Num steps: 180 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.079 Epsilon: 0.574\n",
      "Episode: 951 Duration: 0:00:09.633146 Num steps: 210 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.079 Epsilon: 0.573\n",
      "Episode: 952 Duration: 0:00:09.729561 Num steps: 209 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.079 Epsilon: 0.573\n",
      "Episode: 953 Duration: 0:00:10.192507 Num steps: 226 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.059 Epsilon: 0.573\n",
      "Episode: 954 Duration: 0:00:12.928590 Num steps: 267 Reward: 2.0 Training time per step: 0.041 Avg Reward (Last 100): 1.079 Epsilon: 0.572\n",
      "Episode: 955 Duration: 0:00:16.151029 Num steps: 354 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.089 Epsilon: 0.571\n",
      "Episode: 956 Duration: 0:00:07.410201 Num steps: 168 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.069 Epsilon: 0.571\n",
      "Episode: 957 Duration: 0:00:11.810869 Num steps: 276 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.089 Epsilon: 0.571\n",
      "Episode: 958 Duration: 0:00:13.889369 Num steps: 323 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.119 Epsilon: 0.570\n",
      "Episode: 959 Duration: 0:00:14.310527 Num steps: 311 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.139 Epsilon: 0.569\n",
      "Episode: 960 Duration: 0:00:09.387529 Num steps: 211 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.139 Epsilon: 0.569\n",
      "Episode: 961 Duration: 0:00:18.941288 Num steps: 432 Reward: 5.0 Training time per step: 0.037 Avg Reward (Last 100): 1.168 Epsilon: 0.568\n",
      "Copied model parameters to target network. total_t = 240000, period = 10000\n",
      "Episode: 962 Duration: 0:00:09.898326 Num steps: 236 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.168 Epsilon: 0.568\n",
      "Episode: 963 Duration: 0:00:10.960837 Num steps: 266 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.158 Epsilon: 0.567\n",
      "Episode: 964 Duration: 0:00:08.939300 Num steps: 209 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.149 Epsilon: 0.567\n",
      "Episode: 965 Duration: 0:00:13.199769 Num steps: 317 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.168 Epsilon: 0.566\n",
      "Episode: 966 Duration: 0:00:07.418621 Num steps: 178 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.139 Epsilon: 0.566\n",
      "Episode: 967 Duration: 0:00:07.367229 Num steps: 177 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.129 Epsilon: 0.566\n",
      "Episode: 968 Duration: 0:00:11.543062 Num steps: 263 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.139 Epsilon: 0.565\n",
      "Episode: 969 Duration: 0:00:15.061135 Num steps: 358 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.168 Epsilon: 0.565\n",
      "Episode: 970 Duration: 0:00:13.353956 Num steps: 271 Reward: 2.0 Training time per step: 0.042 Avg Reward (Last 100): 1.178 Epsilon: 0.564\n",
      "Episode: 971 Duration: 0:00:10.789356 Num steps: 244 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.178 Epsilon: 0.564\n",
      "Episode: 972 Duration: 0:00:07.755781 Num steps: 172 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.168 Epsilon: 0.563\n",
      "Episode: 973 Duration: 0:00:12.746494 Num steps: 275 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.158 Epsilon: 0.563\n",
      "Episode: 974 Duration: 0:00:15.394601 Num steps: 364 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.188 Epsilon: 0.562\n",
      "Episode: 975 Duration: 0:00:11.673207 Num steps: 281 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.188 Epsilon: 0.562\n",
      "Episode: 976 Duration: 0:00:13.233930 Num steps: 299 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.188 Epsilon: 0.561\n",
      "Episode: 977 Duration: 0:00:11.140708 Num steps: 249 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.198 Epsilon: 0.561\n",
      "Episode: 978 Duration: 0:00:09.187698 Num steps: 176 Reward: 0.0 Training time per step: 0.045 Avg Reward (Last 100): 1.188 Epsilon: 0.561\n",
      "Episode: 979 Duration: 0:00:10.622360 Num steps: 223 Reward: 1.0 Training time per step: 0.040 Avg Reward (Last 100): 1.198 Epsilon: 0.560\n",
      "Episode: 980 Duration: 0:00:07.184195 Num steps: 164 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.178 Epsilon: 0.560\n",
      "Episode: 981 Duration: 0:00:17.081920 Num steps: 377 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.188 Epsilon: 0.559\n",
      "Episode: 982 Duration: 0:00:10.194182 Num steps: 220 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.198 Epsilon: 0.559\n",
      "Episode: 983 Duration: 0:00:11.890338 Num steps: 268 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.208 Epsilon: 0.558\n",
      "Episode: 984 Duration: 0:00:19.500386 Num steps: 420 Reward: 4.0 Training time per step: 0.039 Avg Reward (Last 100): 1.248 Epsilon: 0.558\n",
      "Episode: 985 Duration: 0:00:09.826985 Num steps: 231 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.257 Epsilon: 0.557\n",
      "Episode: 986 Duration: 0:00:15.305590 Num steps: 347 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 1.277 Epsilon: 0.556\n",
      "Episode: 987 Duration: 0:00:11.823207 Num steps: 261 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.297 Epsilon: 0.556\n",
      "Episode: 988 Duration: 0:00:07.540232 Num steps: 171 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.297 Epsilon: 0.556\n",
      "Episode: 989 Duration: 0:00:08.544547 Num steps: 179 Reward: 0.0 Training time per step: 0.041 Avg Reward (Last 100): 1.287 Epsilon: 0.555\n",
      "Episode: 990 Duration: 0:00:15.313779 Num steps: 314 Reward: 2.0 Training time per step: 0.041 Avg Reward (Last 100): 1.287 Epsilon: 0.555\n",
      "Episode: 991 Duration: 0:00:15.577410 Num steps: 334 Reward: 3.0 Training time per step: 0.039 Avg Reward (Last 100): 1.307 Epsilon: 0.554\n",
      "Episode: 992 Duration: 0:00:07.682935 Num steps: 159 Reward: 0.0 Training time per step: 0.041 Avg Reward (Last 100): 1.297 Epsilon: 0.554\n",
      "Episode: 993 Duration: 0:00:18.018950 Num steps: 390 Reward: 4.0 Training time per step: 0.039 Avg Reward (Last 100): 1.327 Epsilon: 0.553\n",
      "Episode: 994 Duration: 0:00:14.461369 Num steps: 311 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.347 Epsilon: 0.553\n",
      "Episode: 995 Duration: 0:00:12.680089 Num steps: 275 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.366 Epsilon: 0.552\n",
      "Episode: 996 Duration: 0:00:10.950568 Num steps: 234 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.337 Epsilon: 0.552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 997 Duration: 0:00:11.140311 Num steps: 246 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.347 Epsilon: 0.551\n",
      "Episode: 998 Duration: 0:00:07.751316 Num steps: 171 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.337 Epsilon: 0.551\n",
      "Episode: 999 Duration: 0:00:10.455906 Num steps: 226 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.347 Epsilon: 0.551\n",
      "Episode: 1000 Duration: 0:00:13.932227 Num steps: 305 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.356 Epsilon: 0.550\n",
      "Copied model parameters to target network. total_t = 250000, period = 10000\n",
      "Episode: 1001 Duration: 0:00:13.256742 Num steps: 294 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.366 Epsilon: 0.550\n",
      "Episode: 1002 Duration: 0:00:13.744895 Num steps: 295 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.386 Epsilon: 0.549\n",
      "Episode: 1003 Duration: 0:00:10.319052 Num steps: 230 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.376 Epsilon: 0.549\n",
      "Episode: 1004 Duration: 0:00:21.475884 Num steps: 465 Reward: 8.0 Training time per step: 0.039 Avg Reward (Last 100): 1.436 Epsilon: 0.548\n",
      "Episode: 1005 Duration: 0:00:07.849777 Num steps: 173 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.416 Epsilon: 0.547\n",
      "Episode: 1006 Duration: 0:00:07.959994 Num steps: 177 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.406 Epsilon: 0.547\n",
      "Episode: 1007 Duration: 0:00:09.460517 Num steps: 205 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.376 Epsilon: 0.547\n",
      "Episode: 1008 Duration: 0:00:16.795067 Num steps: 373 Reward: 4.0 Training time per step: 0.038 Avg Reward (Last 100): 1.386 Epsilon: 0.546\n",
      "Episode: 1009 Duration: 0:00:10.736605 Num steps: 238 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.356 Epsilon: 0.546\n",
      "Episode: 1010 Duration: 0:00:22.706726 Num steps: 496 Reward: 6.0 Training time per step: 0.039 Avg Reward (Last 100): 1.416 Epsilon: 0.545\n",
      "Episode: 1011 Duration: 0:00:16.846472 Num steps: 370 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.446 Epsilon: 0.544\n",
      "Episode: 1012 Duration: 0:00:16.776159 Num steps: 371 Reward: 4.0 Training time per step: 0.038 Avg Reward (Last 100): 1.485 Epsilon: 0.543\n",
      "Episode: 1013 Duration: 0:00:08.088900 Num steps: 174 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.485 Epsilon: 0.543\n",
      "Episode: 1014 Duration: 0:00:15.025551 Num steps: 328 Reward: 3.0 Training time per step: 0.039 Avg Reward (Last 100): 1.495 Epsilon: 0.542\n",
      "Episode: 1015 Duration: 0:00:11.809657 Num steps: 269 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.495 Epsilon: 0.542\n",
      "Episode: 1016 Duration: 0:00:11.945845 Num steps: 257 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.505 Epsilon: 0.542\n",
      "Episode: 1017 Duration: 0:00:09.465026 Num steps: 212 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.495 Epsilon: 0.541\n",
      "Episode: 1018 Duration: 0:00:20.222743 Num steps: 456 Reward: 6.0 Training time per step: 0.037 Avg Reward (Last 100): 1.545 Epsilon: 0.540\n",
      "Episode: 1019 Duration: 0:00:14.866665 Num steps: 333 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.574 Epsilon: 0.540\n",
      "Episode: 1020 Duration: 0:00:09.640116 Num steps: 216 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.564 Epsilon: 0.539\n",
      "Episode: 1021 Duration: 0:00:13.112337 Num steps: 287 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.564 Epsilon: 0.539\n",
      "Episode: 1022 Duration: 0:00:18.389569 Num steps: 413 Reward: 4.0 Training time per step: 0.038 Avg Reward (Last 100): 1.604 Epsilon: 0.538\n",
      "Episode: 1023 Duration: 0:00:11.028449 Num steps: 237 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.614 Epsilon: 0.538\n",
      "Episode: 1024 Duration: 0:00:09.467944 Num steps: 203 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.604 Epsilon: 0.537\n",
      "Episode: 1025 Duration: 0:00:09.982534 Num steps: 223 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.614 Epsilon: 0.537\n",
      "Episode: 1026 Duration: 0:00:08.759208 Num steps: 189 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.614 Epsilon: 0.537\n",
      "Episode: 1027 Duration: 0:00:07.853410 Num steps: 171 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.614 Epsilon: 0.536\n",
      "Episode: 1028 Duration: 0:00:08.193321 Num steps: 182 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.594 Epsilon: 0.536\n",
      "Episode: 1029 Duration: 0:00:07.566004 Num steps: 169 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.594 Epsilon: 0.536\n",
      "Episode: 1030 Duration: 0:00:08.655882 Num steps: 183 Reward: 0.0 Training time per step: 0.040 Avg Reward (Last 100): 1.584 Epsilon: 0.535\n",
      "Episode: 1031 Duration: 0:00:07.782343 Num steps: 174 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.584 Epsilon: 0.535\n",
      "Episode: 1032 Duration: 0:00:08.193953 Num steps: 182 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.545 Epsilon: 0.535\n",
      "Episode: 1033 Duration: 0:00:18.022129 Num steps: 399 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.554 Epsilon: 0.534\n",
      "Episode: 1034 Duration: 0:00:10.236027 Num steps: 228 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.564 Epsilon: 0.534\n",
      "Episode: 1035 Duration: 0:00:11.558462 Num steps: 256 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.574 Epsilon: 0.533\n",
      "Episode: 1036 Duration: 0:00:09.363808 Num steps: 204 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.564 Epsilon: 0.533\n",
      "Episode: 1037 Duration: 0:00:08.136602 Num steps: 180 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.564 Epsilon: 0.532\n",
      "Episode: 1038 Duration: 0:00:08.262591 Num steps: 176 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.564 Epsilon: 0.532\n",
      "Copied model parameters to target network. total_t = 260000, period = 10000\n",
      "Episode: 1039 Duration: 0:00:19.220601 Num steps: 422 Reward: 4.0 Training time per step: 0.038 Avg Reward (Last 100): 1.584 Epsilon: 0.531\n",
      "Episode: 1040 Duration: 0:00:08.914574 Num steps: 198 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.574 Epsilon: 0.531\n",
      "Episode: 1041 Duration: 0:00:13.048221 Num steps: 292 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.584 Epsilon: 0.530\n",
      "Episode: 1042 Duration: 0:00:13.398354 Num steps: 290 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.594 Epsilon: 0.530\n",
      "Episode: 1043 Duration: 0:00:07.852776 Num steps: 174 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.594 Epsilon: 0.530\n",
      "Episode: 1044 Duration: 0:00:10.657626 Num steps: 241 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.594 Epsilon: 0.529\n",
      "Episode: 1045 Duration: 0:00:14.406152 Num steps: 312 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.604 Epsilon: 0.529\n",
      "Episode: 1046 Duration: 0:00:13.727685 Num steps: 308 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.604 Epsilon: 0.528\n",
      "Episode: 1047 Duration: 0:00:07.464774 Num steps: 160 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.594 Epsilon: 0.528\n",
      "Episode: 1048 Duration: 0:00:15.277755 Num steps: 336 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.614 Epsilon: 0.527\n",
      "Episode: 1049 Duration: 0:00:13.276692 Num steps: 300 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.604 Epsilon: 0.527\n",
      "Episode: 1050 Duration: 0:00:11.836995 Num steps: 247 Reward: 1.0 Training time per step: 0.040 Avg Reward (Last 100): 1.614 Epsilon: 0.526\n",
      "Episode: 1051 Duration: 0:00:16.095026 Num steps: 356 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.644 Epsilon: 0.526\n",
      "Episode: 1052 Duration: 0:00:07.934021 Num steps: 176 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.634 Epsilon: 0.525\n",
      "Episode: 1053 Duration: 0:00:07.754251 Num steps: 166 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.624 Epsilon: 0.525\n",
      "Episode: 1054 Duration: 0:00:11.450739 Num steps: 246 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.624 Epsilon: 0.524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1055 Duration: 0:00:14.903361 Num steps: 323 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.624 Epsilon: 0.524\n",
      "Episode: 1056 Duration: 0:00:13.824408 Num steps: 295 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.624 Epsilon: 0.523\n",
      "Episode: 1057 Duration: 0:00:07.321049 Num steps: 165 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.624 Epsilon: 0.523\n",
      "Episode: 1058 Duration: 0:00:15.581411 Num steps: 340 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.634 Epsilon: 0.522\n",
      "Episode: 1059 Duration: 0:00:14.183440 Num steps: 319 Reward: 4.0 Training time per step: 0.037 Avg Reward (Last 100): 1.644 Epsilon: 0.522\n",
      "Episode: 1060 Duration: 0:00:12.685384 Num steps: 282 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.634 Epsilon: 0.521\n",
      "Episode: 1061 Duration: 0:00:11.273437 Num steps: 245 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.634 Epsilon: 0.521\n",
      "Episode: 1062 Duration: 0:00:07.724499 Num steps: 172 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.584 Epsilon: 0.521\n",
      "Episode: 1063 Duration: 0:00:14.101434 Num steps: 314 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.594 Epsilon: 0.520\n",
      "Episode: 1064 Duration: 0:00:07.540895 Num steps: 163 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.574 Epsilon: 0.520\n",
      "Episode: 1065 Duration: 0:00:10.777776 Num steps: 240 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.574 Epsilon: 0.519\n",
      "Episode: 1066 Duration: 0:00:09.647170 Num steps: 219 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.554 Epsilon: 0.519\n",
      "Episode: 1067 Duration: 0:00:11.713732 Num steps: 252 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.564 Epsilon: 0.518\n",
      "Episode: 1068 Duration: 0:00:11.514973 Num steps: 260 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.584 Epsilon: 0.518\n",
      "Episode: 1069 Duration: 0:00:10.113373 Num steps: 226 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.574 Epsilon: 0.518\n",
      "Episode: 1070 Duration: 0:00:13.241459 Num steps: 285 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.564 Epsilon: 0.517\n",
      "Episode: 1071 Duration: 0:00:11.841056 Num steps: 266 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.564 Epsilon: 0.517\n",
      "Episode: 1072 Duration: 0:00:13.762351 Num steps: 305 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.574 Epsilon: 0.516\n",
      "Episode: 1073 Duration: 0:00:16.663226 Num steps: 370 Reward: 4.0 Training time per step: 0.038 Avg Reward (Last 100): 1.614 Epsilon: 0.515\n",
      "Episode: 1074 Duration: 0:00:09.178803 Num steps: 206 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.604 Epsilon: 0.515\n",
      "Episode: 1075 Duration: 0:00:10.452435 Num steps: 227 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.584 Epsilon: 0.515\n",
      "Copied model parameters to target network. total_t = 270000, period = 10000\n",
      "Episode: 1076 Duration: 0:00:16.481258 Num steps: 367 Reward: 3.0 Training time per step: 0.038 Avg Reward (Last 100): 1.594 Epsilon: 0.514\n",
      "Episode: 1077 Duration: 0:00:08.661213 Num steps: 186 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.574 Epsilon: 0.514\n",
      "Episode: 1078 Duration: 0:00:10.802598 Num steps: 229 Reward: 1.0 Training time per step: 0.040 Avg Reward (Last 100): 1.564 Epsilon: 0.513\n",
      "Episode: 1079 Duration: 0:00:07.902296 Num steps: 172 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.564 Epsilon: 0.513\n",
      "Episode: 1080 Duration: 0:00:23.676383 Num steps: 519 Reward: 5.0 Training time per step: 0.039 Avg Reward (Last 100): 1.604 Epsilon: 0.512\n",
      "Episode: 1081 Duration: 0:00:13.160036 Num steps: 294 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.624 Epsilon: 0.511\n",
      "Episode: 1082 Duration: 0:00:17.701527 Num steps: 381 Reward: 3.0 Training time per step: 0.039 Avg Reward (Last 100): 1.624 Epsilon: 0.511\n",
      "Episode: 1083 Duration: 0:00:10.941627 Num steps: 224 Reward: 1.0 Training time per step: 0.041 Avg Reward (Last 100): 1.624 Epsilon: 0.510\n",
      "Episode: 1084 Duration: 0:00:14.562216 Num steps: 305 Reward: 2.0 Training time per step: 0.040 Avg Reward (Last 100): 1.624 Epsilon: 0.510\n",
      "Episode: 1085 Duration: 0:00:08.804216 Num steps: 187 Reward: 0.0 Training time per step: 0.040 Avg Reward (Last 100): 1.584 Epsilon: 0.509\n",
      "Episode: 1086 Duration: 0:00:10.701754 Num steps: 234 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.584 Epsilon: 0.509\n",
      "Episode: 1087 Duration: 0:00:23.723300 Num steps: 472 Reward: 6.0 Training time per step: 0.042 Avg Reward (Last 100): 1.614 Epsilon: 0.508\n",
      "Episode: 1088 Duration: 0:00:07.964024 Num steps: 181 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.594 Epsilon: 0.508\n",
      "Episode: 1089 Duration: 0:00:10.644626 Num steps: 232 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.604 Epsilon: 0.507\n",
      "Episode: 1090 Duration: 0:00:08.997735 Num steps: 195 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.604 Epsilon: 0.507\n",
      "Episode: 1091 Duration: 0:00:10.603626 Num steps: 196 Reward: 0.0 Training time per step: 0.046 Avg Reward (Last 100): 1.584 Epsilon: 0.507\n",
      "Episode: 1092 Duration: 0:00:13.747792 Num steps: 295 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.574 Epsilon: 0.506\n",
      "Episode: 1093 Duration: 0:00:08.603242 Num steps: 194 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.574 Epsilon: 0.506\n",
      "Episode: 1094 Duration: 0:00:16.955962 Num steps: 377 Reward: 4.0 Training time per step: 0.038 Avg Reward (Last 100): 1.574 Epsilon: 0.505\n",
      "Episode: 1095 Duration: 0:00:11.409272 Num steps: 246 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.564 Epsilon: 0.505\n",
      "Episode: 1096 Duration: 0:00:09.624859 Num steps: 206 Reward: 0.0 Training time per step: 0.039 Avg Reward (Last 100): 1.545 Epsilon: 0.504\n",
      "Episode: 1097 Duration: 0:00:13.667995 Num steps: 288 Reward: 2.0 Training time per step: 0.040 Avg Reward (Last 100): 1.554 Epsilon: 0.504\n",
      "Episode: 1098 Duration: 0:00:09.482340 Num steps: 218 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.554 Epsilon: 0.503\n",
      "Episode: 1099 Duration: 0:00:11.863912 Num steps: 279 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.574 Epsilon: 0.503\n",
      "Episode: 1100 Duration: 0:00:15.688869 Num steps: 366 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.594 Epsilon: 0.502\n",
      "Episode: 1101 Duration: 0:00:08.586105 Num steps: 202 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.584 Epsilon: 0.502\n",
      "Episode: 1102 Duration: 0:00:13.453871 Num steps: 297 Reward: 2.0 Training time per step: 0.038 Avg Reward (Last 100): 1.584 Epsilon: 0.501\n",
      "Episode: 1103 Duration: 0:00:16.761047 Num steps: 343 Reward: 3.0 Training time per step: 0.041 Avg Reward (Last 100): 1.594 Epsilon: 0.501\n",
      "Episode: 1104 Duration: 0:00:07.455395 Num steps: 163 Reward: 0.0 Training time per step: 0.038 Avg Reward (Last 100): 1.584 Epsilon: 0.500\n",
      "Episode: 1105 Duration: 0:00:13.097790 Num steps: 298 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.525 Epsilon: 0.500\n",
      "Episode: 1106 Duration: 0:00:07.341230 Num steps: 167 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.525 Epsilon: 0.500\n",
      "Episode: 1107 Duration: 0:00:16.506349 Num steps: 394 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.564 Epsilon: 0.499\n",
      "Episode: 1108 Duration: 0:00:07.481976 Num steps: 176 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.564 Epsilon: 0.499\n",
      "Episode: 1109 Duration: 0:00:10.994980 Num steps: 256 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.545 Epsilon: 0.498\n",
      "Episode: 1110 Duration: 0:00:08.499049 Num steps: 205 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.545 Epsilon: 0.498\n",
      "Episode: 1111 Duration: 0:00:14.242977 Num steps: 341 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.505 Epsilon: 0.497\n",
      "Episode: 1112 Duration: 0:00:06.883170 Num steps: 166 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.475 Epsilon: 0.497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1113 Duration: 0:00:08.949173 Num steps: 212 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.446 Epsilon: 0.496\n",
      "Episode: 1114 Duration: 0:00:07.684648 Num steps: 178 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.446 Epsilon: 0.496\n",
      "Copied model parameters to target network. total_t = 280000, period = 10000\n",
      "Episode: 1115 Duration: 0:00:11.828070 Num steps: 253 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.426 Epsilon: 0.496\n",
      "Episode: 1116 Duration: 0:00:07.141294 Num steps: 164 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.406 Epsilon: 0.495\n",
      "Episode: 1117 Duration: 0:00:07.453188 Num steps: 179 Reward: 0.0 Training time per step: 0.034 Avg Reward (Last 100): 1.396 Epsilon: 0.495\n",
      "Episode: 1118 Duration: 0:00:09.640220 Num steps: 230 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.396 Epsilon: 0.495\n",
      "Episode: 1119 Duration: 0:00:11.539188 Num steps: 269 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.347 Epsilon: 0.494\n",
      "Episode: 1120 Duration: 0:00:13.303619 Num steps: 319 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.337 Epsilon: 0.494\n",
      "Episode: 1121 Duration: 0:00:12.835293 Num steps: 300 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.347 Epsilon: 0.493\n",
      "Episode: 1122 Duration: 0:00:09.590194 Num steps: 206 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.337 Epsilon: 0.493\n",
      "Episode: 1123 Duration: 0:00:09.507284 Num steps: 214 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.307 Epsilon: 0.492\n",
      "Episode: 1124 Duration: 0:00:10.404033 Num steps: 232 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 1.307 Epsilon: 0.492\n",
      "Episode: 1125 Duration: 0:00:14.283794 Num steps: 336 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.327 Epsilon: 0.491\n",
      "Episode: 1126 Duration: 0:00:12.135086 Num steps: 288 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.337 Epsilon: 0.491\n",
      "Episode: 1127 Duration: 0:00:14.815529 Num steps: 342 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.366 Epsilon: 0.490\n",
      "Episode: 1128 Duration: 0:00:18.067826 Num steps: 430 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.489\n",
      "Episode: 1129 Duration: 0:00:10.690682 Num steps: 243 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.426 Epsilon: 0.489\n",
      "Episode: 1130 Duration: 0:00:10.996907 Num steps: 254 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.446 Epsilon: 0.488\n",
      "Episode: 1131 Duration: 0:00:12.090120 Num steps: 283 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.465 Epsilon: 0.488\n",
      "Episode: 1132 Duration: 0:00:07.046027 Num steps: 163 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.465 Epsilon: 0.488\n",
      "Episode: 1133 Duration: 0:00:07.704998 Num steps: 179 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.465 Epsilon: 0.487\n",
      "Episode: 1134 Duration: 0:00:10.777302 Num steps: 256 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.446 Epsilon: 0.487\n",
      "Episode: 1135 Duration: 0:00:07.592380 Num steps: 179 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.436 Epsilon: 0.487\n",
      "Episode: 1136 Duration: 0:00:07.515485 Num steps: 172 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.416 Epsilon: 0.486\n",
      "Episode: 1137 Duration: 0:00:09.601437 Num steps: 228 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.426 Epsilon: 0.486\n",
      "Episode: 1138 Duration: 0:00:07.967019 Num steps: 189 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.426 Epsilon: 0.486\n",
      "Episode: 1139 Duration: 0:00:08.930124 Num steps: 213 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.436 Epsilon: 0.485\n",
      "Episode: 1140 Duration: 0:00:07.439914 Num steps: 170 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.396 Epsilon: 0.485\n",
      "Episode: 1141 Duration: 0:00:09.919810 Num steps: 232 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.484\n",
      "Episode: 1142 Duration: 0:00:12.507731 Num steps: 299 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.484\n",
      "Episode: 1143 Duration: 0:00:11.257485 Num steps: 261 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.396 Epsilon: 0.483\n",
      "Episode: 1144 Duration: 0:00:12.612502 Num steps: 303 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.416 Epsilon: 0.483\n",
      "Episode: 1145 Duration: 0:00:09.177504 Num steps: 214 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.416 Epsilon: 0.482\n",
      "Episode: 1146 Duration: 0:00:08.804809 Num steps: 204 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.396 Epsilon: 0.482\n",
      "Episode: 1147 Duration: 0:00:07.938770 Num steps: 188 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.376 Epsilon: 0.482\n",
      "Episode: 1148 Duration: 0:00:14.471022 Num steps: 346 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.481\n",
      "Episode: 1149 Duration: 0:00:11.116137 Num steps: 261 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.396 Epsilon: 0.481\n",
      "Episode: 1150 Duration: 0:00:11.082261 Num steps: 262 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.386 Epsilon: 0.480\n",
      "Episode: 1151 Duration: 0:00:16.838940 Num steps: 391 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.406 Epsilon: 0.480\n",
      "Episode: 1152 Duration: 0:00:11.870596 Num steps: 282 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.396 Epsilon: 0.479\n",
      "Episode: 1153 Duration: 0:00:10.079708 Num steps: 237 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.479\n",
      "Episode: 1154 Duration: 0:00:07.544720 Num steps: 180 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.478\n",
      "Copied model parameters to target network. total_t = 290000, period = 10000\n",
      "Episode: 1155 Duration: 0:00:10.225627 Num steps: 238 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.406 Epsilon: 0.478\n",
      "Episode: 1156 Duration: 0:00:16.477613 Num steps: 390 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.426 Epsilon: 0.477\n",
      "Episode: 1157 Duration: 0:00:09.003832 Num steps: 206 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.406 Epsilon: 0.477\n",
      "Episode: 1158 Duration: 0:00:07.481524 Num steps: 175 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.476\n",
      "Episode: 1159 Duration: 0:00:12.952984 Num steps: 309 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.476\n",
      "Episode: 1160 Duration: 0:00:10.805815 Num steps: 232 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.376 Epsilon: 0.475\n",
      "Episode: 1161 Duration: 0:00:11.946022 Num steps: 279 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.386 Epsilon: 0.475\n",
      "Episode: 1162 Duration: 0:00:09.258568 Num steps: 222 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.386 Epsilon: 0.475\n",
      "Episode: 1163 Duration: 0:00:09.347213 Num steps: 221 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.396 Epsilon: 0.474\n",
      "Episode: 1164 Duration: 0:00:08.009122 Num steps: 185 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.376 Epsilon: 0.474\n",
      "Episode: 1165 Duration: 0:00:08.217421 Num steps: 191 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.376 Epsilon: 0.473\n",
      "Episode: 1166 Duration: 0:00:10.194402 Num steps: 240 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.376 Epsilon: 0.473\n",
      "Episode: 1167 Duration: 0:00:17.752690 Num steps: 414 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.396 Epsilon: 0.472\n",
      "Episode: 1168 Duration: 0:00:11.511448 Num steps: 249 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.396 Epsilon: 0.472\n",
      "Episode: 1169 Duration: 0:00:09.980757 Num steps: 213 Reward: 1.0 Training time per step: 0.039 Avg Reward (Last 100): 1.386 Epsilon: 0.471\n",
      "Episode: 1170 Duration: 0:00:13.722813 Num steps: 291 Reward: 2.0 Training time per step: 0.039 Avg Reward (Last 100): 1.396 Epsilon: 0.471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1171 Duration: 0:00:07.704512 Num steps: 172 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.376 Epsilon: 0.471\n",
      "Episode: 1172 Duration: 0:00:11.148317 Num steps: 251 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 1.366 Epsilon: 0.470\n",
      "Episode: 1173 Duration: 0:00:11.970523 Num steps: 280 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.366 Epsilon: 0.470\n",
      "Episode: 1174 Duration: 0:00:09.084908 Num steps: 217 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.337 Epsilon: 0.469\n",
      "Episode: 1175 Duration: 0:00:08.648950 Num steps: 194 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.327 Epsilon: 0.469\n",
      "Episode: 1176 Duration: 0:00:09.583264 Num steps: 226 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.327 Epsilon: 0.469\n",
      "Episode: 1177 Duration: 0:00:08.848117 Num steps: 210 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.307 Epsilon: 0.468\n",
      "Episode: 1178 Duration: 0:00:09.562373 Num steps: 226 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.317 Epsilon: 0.468\n",
      "Episode: 1179 Duration: 0:00:07.661999 Num steps: 176 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.307 Epsilon: 0.467\n",
      "Episode: 1180 Duration: 0:00:11.430195 Num steps: 269 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.317 Epsilon: 0.467\n",
      "Episode: 1181 Duration: 0:00:12.831170 Num steps: 304 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.287 Epsilon: 0.466\n",
      "Episode: 1182 Duration: 0:00:08.782025 Num steps: 202 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.267 Epsilon: 0.466\n",
      "Episode: 1183 Duration: 0:00:07.268041 Num steps: 168 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.238 Epsilon: 0.466\n",
      "Episode: 1184 Duration: 0:00:10.753232 Num steps: 258 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.238 Epsilon: 0.465\n",
      "Episode: 1185 Duration: 0:00:08.734030 Num steps: 200 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.218 Epsilon: 0.465\n",
      "Episode: 1186 Duration: 0:00:07.157218 Num steps: 168 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.218 Epsilon: 0.465\n",
      "Episode: 1187 Duration: 0:00:15.230593 Num steps: 360 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.238 Epsilon: 0.464\n",
      "Episode: 1188 Duration: 0:00:09.378969 Num steps: 219 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.178 Epsilon: 0.464\n",
      "Episode: 1189 Duration: 0:00:16.222333 Num steps: 372 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.208 Epsilon: 0.463\n",
      "Episode: 1190 Duration: 0:00:07.064289 Num steps: 166 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.198 Epsilon: 0.463\n",
      "Episode: 1191 Duration: 0:00:07.621641 Num steps: 177 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.198 Epsilon: 0.462\n",
      "Episode: 1192 Duration: 0:00:13.159202 Num steps: 303 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.218 Epsilon: 0.462\n",
      "Episode: 1193 Duration: 0:00:09.151375 Num steps: 215 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.198 Epsilon: 0.461\n",
      "Episode: 1194 Duration: 0:00:14.002102 Num steps: 328 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.228 Epsilon: 0.461\n",
      "Episode: 1195 Duration: 0:00:09.449378 Num steps: 218 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.198 Epsilon: 0.460\n",
      "Copied model parameters to target network. total_t = 300000, period = 10000\n",
      "Episode: 1196 Duration: 0:00:15.322703 Num steps: 363 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.228 Epsilon: 0.460\n",
      "Episode: 1197 Duration: 0:00:08.098328 Num steps: 186 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.228 Epsilon: 0.459\n",
      "Episode: 1198 Duration: 0:00:08.810614 Num steps: 206 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.218 Epsilon: 0.459\n",
      "Episode: 1199 Duration: 0:00:11.315610 Num steps: 264 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.228 Epsilon: 0.459\n",
      "Episode: 1200 Duration: 0:00:10.358667 Num steps: 242 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.218 Epsilon: 0.458\n",
      "Episode: 1201 Duration: 0:00:09.941247 Num steps: 229 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.198 Epsilon: 0.458\n",
      "Episode: 1202 Duration: 0:00:17.602927 Num steps: 420 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.228 Epsilon: 0.457\n",
      "Episode: 1203 Duration: 0:00:11.355697 Num steps: 262 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.218 Epsilon: 0.456\n",
      "Episode: 1204 Duration: 0:00:13.982700 Num steps: 332 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.218 Epsilon: 0.456\n",
      "Episode: 1205 Duration: 0:00:08.728622 Num steps: 204 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.228 Epsilon: 0.455\n",
      "Episode: 1206 Duration: 0:00:11.547529 Num steps: 269 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.228 Epsilon: 0.455\n",
      "Episode: 1207 Duration: 0:00:09.048290 Num steps: 214 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.238 Epsilon: 0.455\n",
      "Episode: 1208 Duration: 0:00:14.584102 Num steps: 344 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.228 Epsilon: 0.454\n",
      "Episode: 1209 Duration: 0:00:08.129166 Num steps: 186 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.228 Epsilon: 0.454\n",
      "Episode: 1210 Duration: 0:00:14.996979 Num steps: 354 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.238 Epsilon: 0.453\n",
      "Episode: 1211 Duration: 0:00:07.633328 Num steps: 181 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.228 Epsilon: 0.453\n",
      "Episode: 1212 Duration: 0:00:10.623072 Num steps: 244 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.218 Epsilon: 0.452\n",
      "Episode: 1213 Duration: 0:00:07.942250 Num steps: 183 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.218 Epsilon: 0.452\n",
      "Episode: 1214 Duration: 0:00:13.763994 Num steps: 329 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.228 Epsilon: 0.451\n",
      "Episode: 1215 Duration: 0:00:08.475253 Num steps: 196 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.228 Epsilon: 0.451\n",
      "Episode: 1216 Duration: 0:00:08.748627 Num steps: 200 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.218 Epsilon: 0.451\n",
      "Episode: 1217 Duration: 0:00:09.939431 Num steps: 236 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.228 Epsilon: 0.450\n",
      "Episode: 1218 Duration: 0:00:09.143366 Num steps: 210 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.238 Epsilon: 0.450\n",
      "Episode: 1219 Duration: 0:00:16.170240 Num steps: 374 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 1.267 Epsilon: 0.449\n",
      "Episode: 1220 Duration: 0:00:08.937230 Num steps: 205 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.267 Epsilon: 0.449\n",
      "Episode: 1221 Duration: 0:00:11.823730 Num steps: 271 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.267 Epsilon: 0.448\n",
      "Episode: 1222 Duration: 0:00:12.166571 Num steps: 286 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.448\n",
      "Episode: 1223 Duration: 0:00:09.275603 Num steps: 217 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.447\n",
      "Episode: 1224 Duration: 0:00:12.072377 Num steps: 280 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.277 Epsilon: 0.447\n",
      "Episode: 1225 Duration: 0:00:09.757862 Num steps: 229 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.277 Epsilon: 0.446\n",
      "Episode: 1226 Duration: 0:00:12.423225 Num steps: 292 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.267 Epsilon: 0.446\n",
      "Episode: 1227 Duration: 0:00:10.287966 Num steps: 238 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.257 Epsilon: 0.446\n",
      "Episode: 1228 Duration: 0:00:10.850922 Num steps: 253 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.238 Epsilon: 0.445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1229 Duration: 0:00:10.726240 Num steps: 255 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.218 Epsilon: 0.445\n",
      "Episode: 1230 Duration: 0:00:09.020876 Num steps: 207 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.208 Epsilon: 0.444\n",
      "Episode: 1231 Duration: 0:00:11.828819 Num steps: 276 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.208 Epsilon: 0.444\n",
      "Episode: 1232 Duration: 0:00:14.576123 Num steps: 336 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.218 Epsilon: 0.443\n",
      "Episode: 1233 Duration: 0:00:09.290857 Num steps: 218 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.228 Epsilon: 0.443\n",
      "Episode: 1234 Duration: 0:00:13.004648 Num steps: 308 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.248 Epsilon: 0.442\n",
      "Copied model parameters to target network. total_t = 310000, period = 10000\n",
      "Episode: 1235 Duration: 0:00:16.763070 Num steps: 392 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.277 Epsilon: 0.441\n",
      "Episode: 1236 Duration: 0:00:10.875044 Num steps: 257 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.297 Epsilon: 0.441\n",
      "Episode: 1237 Duration: 0:00:08.944504 Num steps: 212 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.297 Epsilon: 0.441\n",
      "Episode: 1238 Duration: 0:00:14.126489 Num steps: 324 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.307 Epsilon: 0.440\n",
      "Episode: 1239 Duration: 0:00:08.641017 Num steps: 205 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.317 Epsilon: 0.440\n",
      "Episode: 1240 Duration: 0:00:10.423810 Num steps: 245 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.327 Epsilon: 0.439\n",
      "Episode: 1241 Duration: 0:00:17.529025 Num steps: 404 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 1.366 Epsilon: 0.439\n",
      "Episode: 1242 Duration: 0:00:08.985442 Num steps: 213 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.366 Epsilon: 0.438\n",
      "Episode: 1243 Duration: 0:00:13.110400 Num steps: 302 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.366 Epsilon: 0.438\n",
      "Episode: 1244 Duration: 0:00:10.329567 Num steps: 242 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.366 Epsilon: 0.437\n",
      "Episode: 1245 Duration: 0:00:10.093902 Num steps: 241 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 1.356 Epsilon: 0.437\n",
      "Episode: 1246 Duration: 0:00:11.411737 Num steps: 261 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.366 Epsilon: 0.436\n",
      "Episode: 1247 Duration: 0:00:10.282030 Num steps: 240 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.376 Epsilon: 0.436\n",
      "Episode: 1248 Duration: 0:00:13.275622 Num steps: 314 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.396 Epsilon: 0.435\n",
      "Episode: 1249 Duration: 0:00:12.463173 Num steps: 292 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.386 Epsilon: 0.435\n",
      "Episode: 1250 Duration: 0:00:07.356142 Num steps: 171 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.366 Epsilon: 0.434\n",
      "Episode: 1251 Duration: 0:00:11.616085 Num steps: 274 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.376 Epsilon: 0.434\n",
      "Episode: 1252 Duration: 0:00:13.588376 Num steps: 314 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.376 Epsilon: 0.433\n",
      "Episode: 1253 Duration: 0:00:06.730623 Num steps: 156 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 1.356 Epsilon: 0.433\n",
      "Episode: 1254 Duration: 0:00:08.249706 Num steps: 196 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.347 Epsilon: 0.433\n",
      "Episode: 1255 Duration: 0:00:11.238773 Num steps: 261 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.366 Epsilon: 0.432\n",
      "Episode: 1256 Duration: 0:00:10.328066 Num steps: 242 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.366 Epsilon: 0.432\n",
      "Episode: 1257 Duration: 0:00:11.337871 Num steps: 267 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.347 Epsilon: 0.431\n",
      "Episode: 1258 Duration: 0:00:17.567005 Num steps: 407 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 1.386 Epsilon: 0.431\n",
      "Episode: 1259 Duration: 0:00:07.541221 Num steps: 176 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.386 Epsilon: 0.430\n",
      "Episode: 1260 Duration: 0:00:12.296870 Num steps: 291 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.376 Epsilon: 0.430\n",
      "Episode: 1261 Duration: 0:00:13.324354 Num steps: 307 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.386 Epsilon: 0.429\n",
      "Episode: 1262 Duration: 0:00:11.912606 Num steps: 283 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.386 Epsilon: 0.429\n",
      "Episode: 1263 Duration: 0:00:07.540147 Num steps: 177 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.376 Epsilon: 0.428\n",
      "Episode: 1264 Duration: 0:00:11.550528 Num steps: 267 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.376 Epsilon: 0.428\n",
      "Episode: 1265 Duration: 0:00:13.075077 Num steps: 309 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.406 Epsilon: 0.427\n",
      "Episode: 1266 Duration: 0:00:13.958002 Num steps: 324 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.436 Epsilon: 0.427\n",
      "Episode: 1267 Duration: 0:00:12.373535 Num steps: 286 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.446 Epsilon: 0.426\n",
      "Episode: 1268 Duration: 0:00:10.014177 Num steps: 239 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.426 Epsilon: 0.426\n",
      "Episode: 1269 Duration: 0:00:15.420052 Num steps: 355 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 1.455 Epsilon: 0.425\n",
      "Episode: 1270 Duration: 0:00:12.787277 Num steps: 301 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.465 Epsilon: 0.425\n",
      "Episode: 1271 Duration: 0:00:13.540772 Num steps: 316 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.465 Epsilon: 0.424\n",
      "Copied model parameters to target network. total_t = 320000, period = 10000\n",
      "Episode: 1272 Duration: 0:00:16.487729 Num steps: 386 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.495 Epsilon: 0.423\n",
      "Episode: 1273 Duration: 0:00:13.976276 Num steps: 318 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.505 Epsilon: 0.423\n",
      "Episode: 1274 Duration: 0:00:08.690678 Num steps: 191 Reward: 0.0 Training time per step: 0.037 Avg Reward (Last 100): 1.485 Epsilon: 0.422\n",
      "Episode: 1275 Duration: 0:00:16.858401 Num steps: 398 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.515 Epsilon: 0.422\n",
      "Episode: 1276 Duration: 0:00:16.620482 Num steps: 380 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 1.554 Epsilon: 0.421\n",
      "Episode: 1277 Duration: 0:00:10.947025 Num steps: 255 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.564 Epsilon: 0.421\n",
      "Episode: 1278 Duration: 0:00:15.406485 Num steps: 361 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.584 Epsilon: 0.420\n",
      "Episode: 1279 Duration: 0:00:13.025505 Num steps: 306 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.594 Epsilon: 0.419\n",
      "Episode: 1280 Duration: 0:00:07.703814 Num steps: 181 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.594 Epsilon: 0.419\n",
      "Episode: 1281 Duration: 0:00:16.147537 Num steps: 373 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 1.624 Epsilon: 0.418\n",
      "Episode: 1282 Duration: 0:00:08.732290 Num steps: 205 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.604 Epsilon: 0.418\n",
      "Episode: 1283 Duration: 0:00:17.077823 Num steps: 396 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 1.644 Epsilon: 0.417\n",
      "Episode: 1284 Duration: 0:00:13.620516 Num steps: 319 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.663 Epsilon: 0.417\n",
      "Episode: 1285 Duration: 0:00:14.567802 Num steps: 337 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.683 Epsilon: 0.416\n",
      "Episode: 1286 Duration: 0:00:13.031960 Num steps: 310 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.703 Epsilon: 0.416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1287 Duration: 0:00:14.178281 Num steps: 334 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.723 Epsilon: 0.415\n",
      "Episode: 1288 Duration: 0:00:14.267038 Num steps: 326 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.723 Epsilon: 0.414\n",
      "Episode: 1289 Duration: 0:00:24.188125 Num steps: 565 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 1.792 Epsilon: 0.413\n",
      "Episode: 1290 Duration: 0:00:09.428403 Num steps: 219 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.772 Epsilon: 0.413\n",
      "Episode: 1291 Duration: 0:00:11.977309 Num steps: 284 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.792 Epsilon: 0.412\n",
      "Episode: 1292 Duration: 0:00:11.671595 Num steps: 277 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.802 Epsilon: 0.412\n",
      "Episode: 1293 Duration: 0:00:11.284248 Num steps: 266 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.792 Epsilon: 0.412\n",
      "Episode: 1294 Duration: 0:00:11.850344 Num steps: 279 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.812 Epsilon: 0.411\n",
      "Episode: 1295 Duration: 0:00:15.693607 Num steps: 362 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 1.822 Epsilon: 0.410\n",
      "Episode: 1296 Duration: 0:00:13.931874 Num steps: 333 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.832 Epsilon: 0.410\n",
      "Episode: 1297 Duration: 0:00:11.468752 Num steps: 260 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.802 Epsilon: 0.409\n",
      "Episode: 1298 Duration: 0:00:10.123031 Num steps: 240 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 1.812 Epsilon: 0.409\n",
      "Episode: 1299 Duration: 0:00:14.736442 Num steps: 344 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.832 Epsilon: 0.408\n",
      "Episode: 1300 Duration: 0:00:13.030179 Num steps: 293 Reward: 2.0 Training time per step: 0.037 Avg Reward (Last 100): 1.832 Epsilon: 0.408\n",
      "Episode: 1301 Duration: 0:00:18.149253 Num steps: 426 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.861 Epsilon: 0.407\n",
      "Episode: 1302 Duration: 0:00:08.936063 Num steps: 205 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.861 Epsilon: 0.407\n",
      "Copied model parameters to target network. total_t = 330000, period = 10000\n",
      "Episode: 1303 Duration: 0:00:13.878809 Num steps: 323 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.851 Epsilon: 0.406\n",
      "Episode: 1304 Duration: 0:00:14.403695 Num steps: 331 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.871 Epsilon: 0.405\n",
      "Episode: 1305 Duration: 0:00:13.439543 Num steps: 319 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 1.871 Epsilon: 0.405\n",
      "Episode: 1306 Duration: 0:00:12.458121 Num steps: 296 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 1.881 Epsilon: 0.404\n",
      "Episode: 1307 Duration: 0:00:13.849489 Num steps: 319 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 1.891 Epsilon: 0.404\n",
      "Episode: 1308 Duration: 0:00:13.086299 Num steps: 307 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.901 Epsilon: 0.403\n",
      "Episode: 1309 Duration: 0:00:12.321563 Num steps: 285 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 1.891 Epsilon: 0.403\n",
      "Episode: 1310 Duration: 0:00:09.304125 Num steps: 218 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 1.891 Epsilon: 0.402\n",
      "Episode: 1311 Duration: 0:00:19.248593 Num steps: 451 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 1.911 Epsilon: 0.401\n",
      "Episode: 1312 Duration: 0:00:12.036149 Num steps: 278 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 1.921 Epsilon: 0.401\n",
      "Episode: 1313 Duration: 0:00:16.901070 Num steps: 396 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 1.950 Epsilon: 0.400\n",
      "Episode: 1314 Duration: 0:00:13.003100 Num steps: 301 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 1.970 Epsilon: 0.400\n",
      "Episode: 1315 Duration: 0:00:20.549783 Num steps: 481 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 2.000 Epsilon: 0.399\n",
      "Episode: 1316 Duration: 0:00:20.519988 Num steps: 473 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 2.050 Epsilon: 0.398\n",
      "Episode: 1317 Duration: 0:00:19.596433 Num steps: 454 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 2.089 Epsilon: 0.397\n",
      "Episode: 1318 Duration: 0:00:12.270346 Num steps: 284 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.089 Epsilon: 0.397\n",
      "Episode: 1319 Duration: 0:00:13.075601 Num steps: 310 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.099 Epsilon: 0.396\n",
      "Episode: 1320 Duration: 0:00:12.288717 Num steps: 281 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 2.079 Epsilon: 0.396\n",
      "Episode: 1321 Duration: 0:00:09.202624 Num steps: 217 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 2.069 Epsilon: 0.395\n",
      "Episode: 1322 Duration: 0:00:15.474708 Num steps: 357 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.079 Epsilon: 0.395\n",
      "Episode: 1323 Duration: 0:00:12.753282 Num steps: 294 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.079 Epsilon: 0.394\n",
      "Episode: 1324 Duration: 0:00:13.749895 Num steps: 321 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.089 Epsilon: 0.393\n",
      "Episode: 1325 Duration: 0:00:13.944731 Num steps: 323 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.089 Epsilon: 0.393\n",
      "Episode: 1326 Duration: 0:00:14.304373 Num steps: 336 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.099 Epsilon: 0.392\n",
      "Episode: 1327 Duration: 0:00:12.312846 Num steps: 288 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.099 Epsilon: 0.392\n",
      "Episode: 1328 Duration: 0:00:15.808042 Num steps: 373 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.119 Epsilon: 0.391\n",
      "Episode: 1329 Duration: 0:00:17.167964 Num steps: 399 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 2.149 Epsilon: 0.390\n",
      "Episode: 1330 Duration: 0:00:10.328086 Num steps: 242 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.139 Epsilon: 0.390\n",
      "Episode: 1331 Duration: 0:00:18.078923 Num steps: 416 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 2.178 Epsilon: 0.389\n",
      "Episode: 1332 Duration: 0:00:12.813443 Num steps: 297 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.188 Epsilon: 0.389\n",
      "Episode: 1333 Duration: 0:00:09.640405 Num steps: 225 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 2.158 Epsilon: 0.388\n",
      "Copied model parameters to target network. total_t = 340000, period = 10000\n",
      "Episode: 1334 Duration: 0:00:11.220380 Num steps: 254 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 2.158 Epsilon: 0.388\n",
      "Episode: 1335 Duration: 0:00:10.477016 Num steps: 246 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.149 Epsilon: 0.387\n",
      "Episode: 1336 Duration: 0:00:11.400682 Num steps: 268 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.129 Epsilon: 0.387\n",
      "Episode: 1337 Duration: 0:00:16.868887 Num steps: 391 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 2.149 Epsilon: 0.386\n",
      "Episode: 1338 Duration: 0:00:13.006700 Num steps: 306 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.168 Epsilon: 0.386\n",
      "Episode: 1339 Duration: 0:00:12.634406 Num steps: 292 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.168 Epsilon: 0.385\n",
      "Episode: 1340 Duration: 0:00:11.052085 Num steps: 260 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.178 Epsilon: 0.385\n",
      "Episode: 1341 Duration: 0:00:09.664295 Num steps: 228 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.168 Epsilon: 0.384\n",
      "Episode: 1342 Duration: 0:00:11.642135 Num steps: 266 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 2.139 Epsilon: 0.384\n",
      "Episode: 1343 Duration: 0:00:14.379452 Num steps: 340 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.158 Epsilon: 0.383\n",
      "Episode: 1344 Duration: 0:00:12.756639 Num steps: 288 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 2.158 Epsilon: 0.383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1345 Duration: 0:00:11.285190 Num steps: 262 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.158 Epsilon: 0.382\n",
      "Episode: 1346 Duration: 0:00:11.168881 Num steps: 262 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.168 Epsilon: 0.382\n",
      "Episode: 1347 Duration: 0:00:11.983751 Num steps: 277 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 2.168 Epsilon: 0.381\n",
      "Episode: 1348 Duration: 0:00:09.818490 Num steps: 226 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 2.158 Epsilon: 0.381\n",
      "Episode: 1349 Duration: 0:00:09.818892 Num steps: 231 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.149 Epsilon: 0.380\n",
      "Episode: 1350 Duration: 0:00:10.062965 Num steps: 233 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 2.129 Epsilon: 0.380\n",
      "Episode: 1351 Duration: 0:00:13.519989 Num steps: 315 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.149 Epsilon: 0.379\n",
      "Episode: 1352 Duration: 0:00:07.531596 Num steps: 177 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 2.129 Epsilon: 0.379\n",
      "Episode: 1353 Duration: 0:00:11.781563 Num steps: 272 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 2.109 Epsilon: 0.379\n",
      "Episode: 1354 Duration: 0:00:15.460864 Num steps: 357 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 2.149 Epsilon: 0.378\n",
      "Episode: 1355 Duration: 0:00:10.471562 Num steps: 248 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 2.158 Epsilon: 0.377\n",
      "Episode: 1356 Duration: 0:00:10.740942 Num steps: 248 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 2.149 Epsilon: 0.377\n",
      "Episode: 1357 Duration: 0:00:13.113511 Num steps: 304 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.158 Epsilon: 0.376\n",
      "Episode: 1358 Duration: 0:00:10.773737 Num steps: 254 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.158 Epsilon: 0.376\n",
      "Episode: 1359 Duration: 0:00:10.860661 Num steps: 248 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 2.139 Epsilon: 0.376\n",
      "Episode: 1360 Duration: 0:00:15.850192 Num steps: 359 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 2.178 Epsilon: 0.375\n",
      "Episode: 1361 Duration: 0:00:12.187232 Num steps: 266 Reward: 1.0 Training time per step: 0.038 Avg Reward (Last 100): 2.168 Epsilon: 0.374\n",
      "Episode: 1362 Duration: 0:00:15.937518 Num steps: 374 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.178 Epsilon: 0.374\n",
      "Episode: 1363 Duration: 0:00:12.534196 Num steps: 291 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.178 Epsilon: 0.373\n",
      "Episode: 1364 Duration: 0:00:11.641175 Num steps: 270 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.198 Epsilon: 0.373\n",
      "Episode: 1365 Duration: 0:00:09.700549 Num steps: 227 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.198 Epsilon: 0.372\n",
      "Episode: 1366 Duration: 0:00:10.469212 Num steps: 245 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.178 Epsilon: 0.372\n",
      "Episode: 1367 Duration: 0:00:09.881468 Num steps: 227 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 2.158 Epsilon: 0.371\n",
      "Episode: 1368 Duration: 0:00:12.331950 Num steps: 281 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 2.158 Epsilon: 0.371\n",
      "Episode: 1369 Duration: 0:00:09.172538 Num steps: 216 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.158 Epsilon: 0.371\n",
      "Episode: 1370 Duration: 0:00:11.862628 Num steps: 270 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 2.129 Epsilon: 0.370\n",
      "Copied model parameters to target network. total_t = 350000, period = 10000\n",
      "Episode: 1371 Duration: 0:00:15.638228 Num steps: 365 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.139 Epsilon: 0.369\n",
      "Episode: 1372 Duration: 0:00:14.842969 Num steps: 344 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.149 Epsilon: 0.369\n",
      "Episode: 1373 Duration: 0:00:15.273353 Num steps: 357 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.149 Epsilon: 0.368\n",
      "Episode: 1374 Duration: 0:00:14.128006 Num steps: 323 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.158 Epsilon: 0.368\n",
      "Episode: 1375 Duration: 0:00:20.285650 Num steps: 474 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 2.208 Epsilon: 0.367\n",
      "Episode: 1376 Duration: 0:00:08.882774 Num steps: 201 Reward: 0.0 Training time per step: 0.036 Avg Reward (Last 100): 2.168 Epsilon: 0.366\n",
      "Episode: 1377 Duration: 0:00:13.875899 Num steps: 322 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.149 Epsilon: 0.366\n",
      "Episode: 1378 Duration: 0:00:12.430745 Num steps: 295 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 2.158 Epsilon: 0.365\n",
      "Episode: 1379 Duration: 0:00:13.956147 Num steps: 318 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.158 Epsilon: 0.365\n",
      "Episode: 1380 Duration: 0:00:11.296284 Num steps: 265 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.149 Epsilon: 0.364\n",
      "Episode: 1381 Duration: 0:00:11.315439 Num steps: 265 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.168 Epsilon: 0.364\n",
      "Episode: 1382 Duration: 0:00:15.857319 Num steps: 366 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.158 Epsilon: 0.363\n",
      "Episode: 1383 Duration: 0:00:08.496882 Num steps: 197 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 2.158 Epsilon: 0.363\n",
      "Episode: 1384 Duration: 0:00:15.128919 Num steps: 355 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.149 Epsilon: 0.362\n",
      "Episode: 1385 Duration: 0:00:11.526099 Num steps: 269 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.149 Epsilon: 0.362\n",
      "Episode: 1386 Duration: 0:00:12.884668 Num steps: 297 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.149 Epsilon: 0.361\n",
      "Episode: 1387 Duration: 0:00:12.605249 Num steps: 292 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.139 Epsilon: 0.361\n",
      "Episode: 1388 Duration: 0:00:13.053455 Num steps: 307 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.139 Epsilon: 0.360\n",
      "Episode: 1389 Duration: 0:00:10.742080 Num steps: 247 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.129 Epsilon: 0.360\n",
      "Episode: 1390 Duration: 0:00:14.985079 Num steps: 353 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.079 Epsilon: 0.359\n",
      "Episode: 1391 Duration: 0:00:18.230141 Num steps: 427 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 2.109 Epsilon: 0.358\n",
      "Episode: 1392 Duration: 0:00:11.426489 Num steps: 268 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.099 Epsilon: 0.358\n",
      "Episode: 1393 Duration: 0:00:14.645560 Num steps: 334 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 2.109 Epsilon: 0.357\n",
      "Episode: 1394 Duration: 0:00:12.299286 Num steps: 289 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.119 Epsilon: 0.357\n",
      "Episode: 1395 Duration: 0:00:14.079470 Num steps: 330 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.129 Epsilon: 0.356\n",
      "Episode: 1396 Duration: 0:00:13.692416 Num steps: 313 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 2.109 Epsilon: 0.355\n",
      "Episode: 1397 Duration: 0:00:12.436497 Num steps: 296 Reward: 2.0 Training time per step: 0.034 Avg Reward (Last 100): 2.109 Epsilon: 0.355\n",
      "Episode: 1398 Duration: 0:00:13.119598 Num steps: 300 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.129 Epsilon: 0.354\n",
      "Episode: 1399 Duration: 0:00:12.267316 Num steps: 283 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.129 Epsilon: 0.354\n",
      "Episode: 1400 Duration: 0:00:23.302428 Num steps: 539 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 2.168 Epsilon: 0.353\n",
      "Episode: 1401 Duration: 0:00:18.929880 Num steps: 443 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 2.188 Epsilon: 0.352\n",
      "Copied model parameters to target network. total_t = 360000, period = 10000\n",
      "Episode: 1402 Duration: 0:00:16.181197 Num steps: 370 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 2.188 Epsilon: 0.351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1403 Duration: 0:00:19.786010 Num steps: 453 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 2.228 Epsilon: 0.351\n",
      "Episode: 1404 Duration: 0:00:16.267388 Num steps: 369 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.228 Epsilon: 0.350\n",
      "Episode: 1405 Duration: 0:00:10.622240 Num steps: 251 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.218 Epsilon: 0.349\n",
      "Episode: 1406 Duration: 0:00:19.208649 Num steps: 437 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 2.228 Epsilon: 0.349\n",
      "Episode: 1407 Duration: 0:00:12.619287 Num steps: 299 Reward: 1.0 Training time per step: 0.034 Avg Reward (Last 100): 2.218 Epsilon: 0.348\n",
      "Episode: 1408 Duration: 0:00:14.018363 Num steps: 318 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.218 Epsilon: 0.348\n",
      "Episode: 1409 Duration: 0:00:16.128643 Num steps: 377 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.228 Epsilon: 0.347\n",
      "Episode: 1410 Duration: 0:00:10.300085 Num steps: 243 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.218 Epsilon: 0.346\n",
      "Episode: 1411 Duration: 0:00:11.786562 Num steps: 276 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.228 Epsilon: 0.346\n",
      "Episode: 1412 Duration: 0:00:12.846752 Num steps: 300 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.198 Epsilon: 0.345\n",
      "Episode: 1413 Duration: 0:00:18.838509 Num steps: 422 Reward: 4.0 Training time per step: 0.037 Avg Reward (Last 100): 2.228 Epsilon: 0.345\n",
      "Episode: 1414 Duration: 0:00:13.437056 Num steps: 314 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.218 Epsilon: 0.344\n",
      "Episode: 1415 Duration: 0:00:10.536883 Num steps: 241 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 2.208 Epsilon: 0.344\n",
      "Episode: 1416 Duration: 0:00:13.324855 Num steps: 308 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.188 Epsilon: 0.343\n",
      "Episode: 1417 Duration: 0:00:13.910709 Num steps: 326 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.149 Epsilon: 0.342\n",
      "Episode: 1418 Duration: 0:00:12.397776 Num steps: 278 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 2.129 Epsilon: 0.342\n",
      "Episode: 1419 Duration: 0:00:19.272599 Num steps: 449 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 2.178 Epsilon: 0.341\n",
      "Episode: 1420 Duration: 0:00:13.985828 Num steps: 315 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 2.178 Epsilon: 0.341\n",
      "Episode: 1421 Duration: 0:00:12.491899 Num steps: 291 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.168 Epsilon: 0.340\n",
      "Episode: 1422 Duration: 0:00:15.094485 Num steps: 345 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.198 Epsilon: 0.339\n",
      "Episode: 1423 Duration: 0:00:13.132152 Num steps: 308 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.198 Epsilon: 0.339\n",
      "Episode: 1424 Duration: 0:00:10.444725 Num steps: 242 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.188 Epsilon: 0.338\n",
      "Episode: 1425 Duration: 0:00:13.623411 Num steps: 305 Reward: 1.0 Training time per step: 0.037 Avg Reward (Last 100): 2.178 Epsilon: 0.338\n",
      "Episode: 1426 Duration: 0:00:16.436283 Num steps: 379 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.188 Epsilon: 0.337\n",
      "Episode: 1427 Duration: 0:00:22.209177 Num steps: 512 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 2.228 Epsilon: 0.336\n",
      "Episode: 1428 Duration: 0:00:20.096902 Num steps: 463 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 2.248 Epsilon: 0.335\n",
      "Episode: 1429 Duration: 0:00:10.197187 Num steps: 238 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.228 Epsilon: 0.335\n",
      "Episode: 1430 Duration: 0:00:09.014823 Num steps: 212 Reward: 0.0 Training time per step: 0.035 Avg Reward (Last 100): 2.188 Epsilon: 0.335\n",
      "Episode: 1431 Duration: 0:00:14.590071 Num steps: 333 Reward: 2.0 Training time per step: 0.036 Avg Reward (Last 100): 2.198 Epsilon: 0.334\n",
      "Copied model parameters to target network. total_t = 370000, period = 10000\n",
      "Episode: 1432 Duration: 0:00:21.992052 Num steps: 512 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 2.208 Epsilon: 0.333\n",
      "Episode: 1433 Duration: 0:00:18.875753 Num steps: 429 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 2.218 Epsilon: 0.332\n",
      "Episode: 1434 Duration: 0:00:15.719761 Num steps: 369 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 2.257 Epsilon: 0.332\n",
      "Episode: 1435 Duration: 0:00:23.100912 Num steps: 532 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 2.307 Epsilon: 0.331\n",
      "Episode: 1436 Duration: 0:00:14.349094 Num steps: 332 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.327 Epsilon: 0.330\n",
      "Episode: 1437 Duration: 0:00:18.921691 Num steps: 439 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 2.347 Epsilon: 0.329\n",
      "Episode: 1438 Duration: 0:00:16.697921 Num steps: 381 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.337 Epsilon: 0.329\n",
      "Episode: 1439 Duration: 0:00:18.952043 Num steps: 442 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 2.356 Epsilon: 0.328\n",
      "Episode: 1440 Duration: 0:00:19.156258 Num steps: 437 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 2.386 Epsilon: 0.327\n",
      "Episode: 1441 Duration: 0:00:19.593151 Num steps: 452 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 2.416 Epsilon: 0.326\n",
      "Episode: 1442 Duration: 0:00:17.647592 Num steps: 408 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.436 Epsilon: 0.326\n",
      "Episode: 1443 Duration: 0:00:20.214267 Num steps: 448 Reward: 5.0 Training time per step: 0.037 Avg Reward (Last 100): 2.475 Epsilon: 0.325\n",
      "Episode: 1444 Duration: 0:00:12.449887 Num steps: 291 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.465 Epsilon: 0.324\n",
      "Episode: 1445 Duration: 0:00:22.822112 Num steps: 527 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 2.495 Epsilon: 0.323\n",
      "Episode: 1446 Duration: 0:00:12.260835 Num steps: 290 Reward: 3.0 Training time per step: 0.034 Avg Reward (Last 100): 2.515 Epsilon: 0.323\n",
      "Episode: 1447 Duration: 0:00:13.416218 Num steps: 307 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.525 Epsilon: 0.322\n",
      "Episode: 1448 Duration: 0:00:11.080169 Num steps: 258 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 2.515 Epsilon: 0.322\n",
      "Episode: 1449 Duration: 0:00:14.685801 Num steps: 343 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 2.535 Epsilon: 0.321\n",
      "Episode: 1450 Duration: 0:00:14.233472 Num steps: 321 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.554 Epsilon: 0.321\n",
      "Episode: 1451 Duration: 0:00:20.235757 Num steps: 464 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 2.604 Epsilon: 0.320\n",
      "Episode: 1452 Duration: 0:00:27.144453 Num steps: 633 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 2.653 Epsilon: 0.319\n",
      "Episode: 1453 Duration: 0:00:20.202525 Num steps: 459 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 2.703 Epsilon: 0.318\n",
      "Episode: 1454 Duration: 0:00:19.921624 Num steps: 463 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 2.743 Epsilon: 0.317\n",
      "Episode: 1455 Duration: 0:00:14.847728 Num steps: 346 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 2.743 Epsilon: 0.316\n",
      "Copied model parameters to target network. total_t = 380000, period = 10000\n",
      "Episode: 1456 Duration: 0:00:22.017921 Num steps: 507 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 2.782 Epsilon: 0.315\n",
      "Episode: 1457 Duration: 0:00:23.458903 Num steps: 545 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 2.832 Epsilon: 0.314\n",
      "Episode: 1458 Duration: 0:00:14.605352 Num steps: 333 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 2.842 Epsilon: 0.314\n",
      "Episode: 1459 Duration: 0:00:19.261347 Num steps: 443 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 2.861 Epsilon: 0.313\n",
      "Episode: 1460 Duration: 0:00:15.328388 Num steps: 355 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.871 Epsilon: 0.312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1461 Duration: 0:00:15.077080 Num steps: 352 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.861 Epsilon: 0.312\n",
      "Episode: 1462 Duration: 0:00:17.924224 Num steps: 414 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 2.891 Epsilon: 0.311\n",
      "Episode: 1463 Duration: 0:00:16.307789 Num steps: 373 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 2.901 Epsilon: 0.310\n",
      "Episode: 1464 Duration: 0:00:18.830862 Num steps: 441 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 2.931 Epsilon: 0.309\n",
      "Episode: 1465 Duration: 0:00:15.846296 Num steps: 365 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 2.950 Epsilon: 0.309\n",
      "Episode: 1466 Duration: 0:00:14.982596 Num steps: 348 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 2.970 Epsilon: 0.308\n",
      "Episode: 1467 Duration: 0:00:25.993847 Num steps: 592 Reward: 8.0 Training time per step: 0.036 Avg Reward (Last 100): 3.040 Epsilon: 0.307\n",
      "Episode: 1468 Duration: 0:00:13.619795 Num steps: 317 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 3.040 Epsilon: 0.307\n",
      "Episode: 1469 Duration: 0:00:18.741799 Num steps: 432 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 3.059 Epsilon: 0.306\n",
      "Episode: 1470 Duration: 0:00:21.656163 Num steps: 501 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 3.089 Epsilon: 0.305\n",
      "Episode: 1471 Duration: 0:00:18.114731 Num steps: 421 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 3.119 Epsilon: 0.304\n",
      "Episode: 1472 Duration: 0:00:15.343675 Num steps: 352 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 3.119 Epsilon: 0.304\n",
      "Episode: 1473 Duration: 0:00:26.241213 Num steps: 610 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 3.168 Epsilon: 0.302\n",
      "Episode: 1474 Duration: 0:00:13.008885 Num steps: 299 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 3.149 Epsilon: 0.302\n",
      "Episode: 1475 Duration: 0:00:18.214109 Num steps: 423 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 3.158 Epsilon: 0.301\n",
      "Episode: 1476 Duration: 0:00:23.623687 Num steps: 541 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 3.158 Epsilon: 0.300\n",
      "Episode: 1477 Duration: 0:00:16.729597 Num steps: 383 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 3.198 Epsilon: 0.299\n",
      "Episode: 1478 Duration: 0:00:10.391891 Num steps: 239 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 3.198 Epsilon: 0.299\n",
      "Episode: 1479 Duration: 0:00:17.829440 Num steps: 394 Reward: 4.0 Training time per step: 0.037 Avg Reward (Last 100): 3.208 Epsilon: 0.298\n",
      "Copied model parameters to target network. total_t = 390000, period = 10000\n",
      "Episode: 1480 Duration: 0:00:18.719137 Num steps: 424 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 3.218 Epsilon: 0.298\n",
      "Episode: 1481 Duration: 0:00:19.368398 Num steps: 445 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 3.257 Epsilon: 0.297\n",
      "Episode: 1482 Duration: 0:00:11.378576 Num steps: 262 Reward: 2.0 Training time per step: 0.035 Avg Reward (Last 100): 3.257 Epsilon: 0.296\n",
      "Episode: 1483 Duration: 0:00:18.014076 Num steps: 415 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 3.267 Epsilon: 0.296\n",
      "Episode: 1484 Duration: 0:00:14.789548 Num steps: 343 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 3.297 Epsilon: 0.295\n",
      "Episode: 1485 Duration: 0:00:26.707881 Num steps: 615 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 3.376 Epsilon: 0.294\n",
      "Episode: 1486 Duration: 0:00:13.143067 Num steps: 300 Reward: 1.0 Training time per step: 0.036 Avg Reward (Last 100): 3.366 Epsilon: 0.293\n",
      "Episode: 1487 Duration: 0:00:16.774015 Num steps: 379 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 3.366 Epsilon: 0.293\n",
      "Episode: 1488 Duration: 0:00:12.775145 Num steps: 293 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 3.386 Epsilon: 0.292\n",
      "Episode: 1489 Duration: 0:00:20.652510 Num steps: 474 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 3.416 Epsilon: 0.291\n",
      "Episode: 1490 Duration: 0:00:19.155874 Num steps: 436 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 3.446 Epsilon: 0.290\n",
      "Episode: 1491 Duration: 0:00:21.685000 Num steps: 478 Reward: 5.0 Training time per step: 0.037 Avg Reward (Last 100): 3.475 Epsilon: 0.290\n",
      "Episode: 1492 Duration: 0:00:16.002752 Num steps: 349 Reward: 3.0 Training time per step: 0.037 Avg Reward (Last 100): 3.465 Epsilon: 0.289\n",
      "Episode: 1493 Duration: 0:00:25.037856 Num steps: 560 Reward: 7.0 Training time per step: 0.037 Avg Reward (Last 100): 3.525 Epsilon: 0.288\n",
      "Episode: 1494 Duration: 0:00:24.215260 Num steps: 561 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 3.564 Epsilon: 0.287\n",
      "Episode: 1495 Duration: 0:00:20.092783 Num steps: 462 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 3.594 Epsilon: 0.286\n",
      "Episode: 1496 Duration: 0:00:14.475421 Num steps: 334 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 3.594 Epsilon: 0.285\n",
      "Episode: 1497 Duration: 0:00:18.135140 Num steps: 417 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 3.624 Epsilon: 0.285\n",
      "Episode: 1498 Duration: 0:00:22.519631 Num steps: 521 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 3.663 Epsilon: 0.284\n",
      "Episode: 1499 Duration: 0:00:27.955262 Num steps: 643 Reward: 8.0 Training time per step: 0.036 Avg Reward (Last 100): 3.713 Epsilon: 0.283\n",
      "Episode: 1500 Duration: 0:00:17.178805 Num steps: 392 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 3.743 Epsilon: 0.282\n",
      "Episode: 1501 Duration: 0:00:13.663009 Num steps: 315 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 3.703 Epsilon: 0.281\n",
      "Episode: 1502 Duration: 0:00:18.341678 Num steps: 420 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 3.703 Epsilon: 0.281\n",
      "Copied model parameters to target network. total_t = 400000, period = 10000\n",
      "Episode: 1503 Duration: 0:00:19.022251 Num steps: 444 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 3.703 Epsilon: 0.280\n",
      "Episode: 1504 Duration: 0:00:16.312197 Num steps: 371 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 3.693 Epsilon: 0.279\n",
      "Episode: 1505 Duration: 0:00:25.374546 Num steps: 583 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 3.733 Epsilon: 0.278\n",
      "Episode: 1506 Duration: 0:00:21.853447 Num steps: 509 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 3.772 Epsilon: 0.277\n",
      "Episode: 1507 Duration: 0:00:21.303654 Num steps: 484 Reward: 8.0 Training time per step: 0.036 Avg Reward (Last 100): 3.812 Epsilon: 0.276\n",
      "Episode: 1508 Duration: 0:00:10.530012 Num steps: 245 Reward: 1.0 Training time per step: 0.035 Avg Reward (Last 100): 3.812 Epsilon: 0.276\n",
      "Episode: 1509 Duration: 0:00:25.848983 Num steps: 574 Reward: 10.0 Training time per step: 0.037 Avg Reward (Last 100): 3.881 Epsilon: 0.275\n",
      "Episode: 1510 Duration: 0:00:21.703286 Num steps: 501 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 3.911 Epsilon: 0.274\n",
      "Episode: 1511 Duration: 0:00:24.955244 Num steps: 574 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 3.970 Epsilon: 0.273\n",
      "Episode: 1512 Duration: 0:00:17.943377 Num steps: 411 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 4.000 Epsilon: 0.272\n",
      "Episode: 1513 Duration: 0:00:31.060925 Num steps: 714 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 4.059 Epsilon: 0.271\n",
      "Episode: 1514 Duration: 0:00:22.688110 Num steps: 515 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 4.069 Epsilon: 0.270\n",
      "Episode: 1515 Duration: 0:00:16.453038 Num steps: 381 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 4.079 Epsilon: 0.269\n",
      "Episode: 1516 Duration: 0:00:20.794684 Num steps: 475 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 4.129 Epsilon: 0.268\n",
      "Episode: 1517 Duration: 0:00:16.303952 Num steps: 368 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 4.139 Epsilon: 0.268\n",
      "Episode: 1518 Duration: 0:00:20.552737 Num steps: 465 Reward: 6.0 Training time per step: 0.036 Avg Reward (Last 100): 4.188 Epsilon: 0.267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1519 Duration: 0:00:25.600406 Num steps: 588 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 4.267 Epsilon: 0.266\n",
      "Episode: 1520 Duration: 0:00:14.348094 Num steps: 330 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 4.238 Epsilon: 0.265\n",
      "Episode: 1521 Duration: 0:00:29.380801 Num steps: 673 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 4.307 Epsilon: 0.264\n",
      "Episode: 1522 Duration: 0:00:22.124776 Num steps: 505 Reward: 6.0 Training time per step: 0.036 Avg Reward (Last 100): 4.356 Epsilon: 0.263\n",
      "Episode: 1523 Duration: 0:00:18.144743 Num steps: 422 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 4.376 Epsilon: 0.262\n",
      "Copied model parameters to target network. total_t = 410000, period = 10000\n",
      "Episode: 1524 Duration: 0:00:16.174979 Num steps: 367 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 4.376 Epsilon: 0.262\n",
      "Episode: 1525 Duration: 0:00:21.673768 Num steps: 495 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 4.416 Epsilon: 0.261\n",
      "Episode: 1526 Duration: 0:00:18.599266 Num steps: 434 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 4.446 Epsilon: 0.260\n",
      "Episode: 1527 Duration: 0:00:20.582645 Num steps: 468 Reward: 6.0 Training time per step: 0.036 Avg Reward (Last 100): 4.475 Epsilon: 0.259\n",
      "Episode: 1528 Duration: 0:00:18.620481 Num steps: 436 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 4.465 Epsilon: 0.258\n",
      "Episode: 1529 Duration: 0:00:26.086463 Num steps: 598 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 4.495 Epsilon: 0.257\n",
      "Episode: 1530 Duration: 0:00:20.336056 Num steps: 468 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 4.525 Epsilon: 0.256\n",
      "Episode: 1531 Duration: 0:00:19.402880 Num steps: 445 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 4.564 Epsilon: 0.256\n",
      "Episode: 1532 Duration: 0:00:13.089694 Num steps: 297 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 4.574 Epsilon: 0.255\n",
      "Episode: 1533 Duration: 0:00:14.000466 Num steps: 322 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 4.545 Epsilon: 0.255\n",
      "Episode: 1534 Duration: 0:00:27.104312 Num steps: 621 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 4.594 Epsilon: 0.253\n",
      "Episode: 1535 Duration: 0:00:26.320113 Num steps: 603 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 4.624 Epsilon: 0.252\n",
      "Episode: 1536 Duration: 0:00:20.134060 Num steps: 471 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 4.614 Epsilon: 0.252\n",
      "Episode: 1537 Duration: 0:00:16.530426 Num steps: 373 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 4.614 Epsilon: 0.251\n",
      "Episode: 1538 Duration: 0:00:22.139676 Num steps: 505 Reward: 7.0 Training time per step: 0.036 Avg Reward (Last 100): 4.644 Epsilon: 0.250\n",
      "Episode: 1539 Duration: 0:00:22.824968 Num steps: 528 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 4.673 Epsilon: 0.249\n",
      "Episode: 1540 Duration: 0:00:24.321192 Num steps: 559 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 4.723 Epsilon: 0.248\n",
      "Episode: 1541 Duration: 0:00:23.349246 Num steps: 533 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 4.733 Epsilon: 0.247\n",
      "Episode: 1542 Duration: 0:00:28.080025 Num steps: 644 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 4.752 Epsilon: 0.246\n",
      "Episode: 1543 Duration: 0:00:21.046473 Num steps: 487 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 4.782 Epsilon: 0.245\n",
      "Copied model parameters to target network. total_t = 420000, period = 10000\n",
      "Episode: 1544 Duration: 0:00:24.306220 Num steps: 553 Reward: 8.0 Training time per step: 0.036 Avg Reward (Last 100): 4.812 Epsilon: 0.244\n",
      "Episode: 1545 Duration: 0:00:22.460780 Num steps: 514 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 4.851 Epsilon: 0.243\n",
      "Episode: 1546 Duration: 0:00:27.973685 Num steps: 642 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 4.871 Epsilon: 0.242\n",
      "Episode: 1547 Duration: 0:00:30.591840 Num steps: 709 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 4.950 Epsilon: 0.241\n",
      "Episode: 1548 Duration: 0:00:18.005849 Num steps: 421 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 5.000 Epsilon: 0.240\n",
      "Episode: 1549 Duration: 0:00:27.289008 Num steps: 617 Reward: 8.0 Training time per step: 0.036 Avg Reward (Last 100): 5.069 Epsilon: 0.239\n",
      "Episode: 1550 Duration: 0:00:15.553181 Num steps: 349 Reward: 3.0 Training time per step: 0.036 Avg Reward (Last 100): 5.079 Epsilon: 0.238\n",
      "Episode: 1551 Duration: 0:00:26.925065 Num steps: 625 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 5.139 Epsilon: 0.237\n",
      "Episode: 1552 Duration: 0:00:30.958376 Num steps: 710 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 5.218 Epsilon: 0.236\n",
      "Episode: 1553 Duration: 0:00:23.292225 Num steps: 530 Reward: 9.0 Training time per step: 0.036 Avg Reward (Last 100): 5.238 Epsilon: 0.235\n",
      "Episode: 1554 Duration: 0:00:30.171797 Num steps: 691 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 5.297 Epsilon: 0.234\n",
      "Episode: 1555 Duration: 0:00:20.301735 Num steps: 459 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 5.297 Epsilon: 0.233\n",
      "Episode: 1556 Duration: 0:00:18.480079 Num steps: 427 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 5.297 Epsilon: 0.232\n",
      "Episode: 1557 Duration: 0:00:25.481869 Num steps: 583 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 5.317 Epsilon: 0.231\n",
      "Episode: 1558 Duration: 0:00:31.696463 Num steps: 727 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 5.356 Epsilon: 0.230\n",
      "Episode: 1559 Duration: 0:00:22.199707 Num steps: 509 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 5.376 Epsilon: 0.229\n",
      "Episode: 1560 Duration: 0:00:23.055651 Num steps: 533 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 5.406 Epsilon: 0.228\n",
      "Episode: 1561 Duration: 0:00:19.248702 Num steps: 441 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 5.416 Epsilon: 0.227\n",
      "Copied model parameters to target network. total_t = 430000, period = 10000\n",
      "Episode: 1562 Duration: 0:00:25.490056 Num steps: 571 Reward: 6.0 Training time per step: 0.036 Avg Reward (Last 100): 5.446 Epsilon: 0.226\n",
      "Episode: 1563 Duration: 0:00:26.938972 Num steps: 612 Reward: 7.0 Training time per step: 0.036 Avg Reward (Last 100): 5.475 Epsilon: 0.225\n",
      "Episode: 1564 Duration: 0:00:19.209873 Num steps: 446 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 5.475 Epsilon: 0.224\n",
      "Episode: 1565 Duration: 0:00:26.079770 Num steps: 600 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 5.495 Epsilon: 0.223\n",
      "Episode: 1566 Duration: 0:00:21.120484 Num steps: 485 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 5.515 Epsilon: 0.222\n",
      "Episode: 1567 Duration: 0:00:23.894697 Num steps: 554 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 5.545 Epsilon: 0.221\n",
      "Episode: 1568 Duration: 0:00:39.142934 Num steps: 891 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 5.584 Epsilon: 0.219\n",
      "Episode: 1569 Duration: 0:00:24.483121 Num steps: 565 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 5.663 Epsilon: 0.218\n",
      "Episode: 1570 Duration: 0:00:20.010112 Num steps: 453 Reward: 8.0 Training time per step: 0.036 Avg Reward (Last 100): 5.703 Epsilon: 0.218\n",
      "Episode: 1571 Duration: 0:00:31.096755 Num steps: 709 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 5.752 Epsilon: 0.216\n",
      "Episode: 1572 Duration: 0:00:16.721417 Num steps: 387 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 5.752 Epsilon: 0.216\n",
      "Episode: 1573 Duration: 0:00:19.732116 Num steps: 449 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 5.772 Epsilon: 0.215\n",
      "Episode: 1574 Duration: 0:00:23.166044 Num steps: 527 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 5.752 Epsilon: 0.214\n",
      "Episode: 1575 Duration: 0:00:21.529051 Num steps: 498 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 5.812 Epsilon: 0.213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1576 Duration: 0:00:23.500780 Num steps: 539 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 5.832 Epsilon: 0.212\n",
      "Episode: 1577 Duration: 0:00:19.046857 Num steps: 437 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 5.822 Epsilon: 0.211\n",
      "Episode: 1578 Duration: 0:00:20.466388 Num steps: 471 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 5.842 Epsilon: 0.210\n",
      "Episode: 1579 Duration: 0:00:27.904420 Num steps: 639 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 5.911 Epsilon: 0.209\n",
      "Episode: 1580 Duration: 0:00:19.336519 Num steps: 441 Reward: 5.0 Training time per step: 0.035 Avg Reward (Last 100): 5.921 Epsilon: 0.208\n",
      "Copied model parameters to target network. total_t = 440000, period = 10000\n",
      "Episode: 1581 Duration: 0:00:32.023954 Num steps: 742 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 6.010 Epsilon: 0.207\n",
      "Episode: 1582 Duration: 0:00:28.789462 Num steps: 656 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 6.040 Epsilon: 0.206\n",
      "Episode: 1583 Duration: 0:00:20.426934 Num steps: 473 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 6.079 Epsilon: 0.205\n",
      "Episode: 1584 Duration: 0:00:21.313887 Num steps: 484 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 6.089 Epsilon: 0.204\n",
      "Episode: 1585 Duration: 0:00:37.878321 Num steps: 870 Reward: 20.0 Training time per step: 0.035 Avg Reward (Last 100): 6.257 Epsilon: 0.203\n",
      "Episode: 1586 Duration: 0:00:21.802487 Num steps: 489 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 6.188 Epsilon: 0.202\n",
      "Episode: 1587 Duration: 0:00:17.652591 Num steps: 408 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 6.208 Epsilon: 0.201\n",
      "Episode: 1588 Duration: 0:00:27.800543 Num steps: 646 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 6.267 Epsilon: 0.200\n",
      "Episode: 1589 Duration: 0:00:18.780850 Num steps: 425 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 6.287 Epsilon: 0.199\n",
      "Episode: 1590 Duration: 0:00:39.944906 Num steps: 914 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 6.347 Epsilon: 0.197\n",
      "Episode: 1591 Duration: 0:00:33.770152 Num steps: 768 Reward: 16.0 Training time per step: 0.036 Avg Reward (Last 100): 6.455 Epsilon: 0.196\n",
      "Episode: 1592 Duration: 0:00:21.259880 Num steps: 481 Reward: 6.0 Training time per step: 0.036 Avg Reward (Last 100): 6.465 Epsilon: 0.195\n",
      "Episode: 1593 Duration: 0:00:17.488448 Num steps: 403 Reward: 3.0 Training time per step: 0.035 Avg Reward (Last 100): 6.465 Epsilon: 0.194\n",
      "Episode: 1594 Duration: 0:00:32.061579 Num steps: 729 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 6.525 Epsilon: 0.193\n",
      "Episode: 1595 Duration: 0:00:24.100491 Num steps: 547 Reward: 7.0 Training time per step: 0.036 Avg Reward (Last 100): 6.535 Epsilon: 0.192\n",
      "Episode: 1596 Duration: 0:00:24.568739 Num steps: 560 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 6.594 Epsilon: 0.191\n",
      "Episode: 1597 Duration: 0:00:27.525030 Num steps: 627 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 6.653 Epsilon: 0.190\n",
      "Copied model parameters to target network. total_t = 450000, period = 10000\n",
      "Episode: 1598 Duration: 0:00:26.796935 Num steps: 604 Reward: 7.0 Training time per step: 0.036 Avg Reward (Last 100): 6.673 Epsilon: 0.189\n",
      "Episode: 1599 Duration: 0:00:37.025524 Num steps: 845 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 6.762 Epsilon: 0.187\n",
      "Episode: 1600 Duration: 0:00:27.011208 Num steps: 614 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 6.772 Epsilon: 0.186\n",
      "Episode: 1601 Duration: 0:00:39.952053 Num steps: 913 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 6.871 Epsilon: 0.185\n",
      "Episode: 1602 Duration: 0:00:24.392217 Num steps: 555 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 6.921 Epsilon: 0.184\n",
      "Episode: 1603 Duration: 0:00:27.243637 Num steps: 625 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 6.970 Epsilon: 0.183\n",
      "Episode: 1604 Duration: 0:00:24.793351 Num steps: 564 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 7.000 Epsilon: 0.182\n",
      "Episode: 1605 Duration: 0:00:28.147931 Num steps: 639 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 7.050 Epsilon: 0.180\n",
      "Episode: 1606 Duration: 0:00:28.757249 Num steps: 651 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 7.069 Epsilon: 0.179\n",
      "Episode: 1607 Duration: 0:00:28.919331 Num steps: 659 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 7.139 Epsilon: 0.178\n",
      "Episode: 1608 Duration: 0:00:30.870603 Num steps: 698 Reward: 10.0 Training time per step: 0.036 Avg Reward (Last 100): 7.158 Epsilon: 0.177\n",
      "Episode: 1609 Duration: 0:00:26.622336 Num steps: 597 Reward: 8.0 Training time per step: 0.036 Avg Reward (Last 100): 7.228 Epsilon: 0.176\n",
      "Episode: 1610 Duration: 0:00:39.214099 Num steps: 892 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 7.248 Epsilon: 0.174\n",
      "Episode: 1611 Duration: 0:00:29.020175 Num steps: 668 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 7.307 Epsilon: 0.173\n",
      "Copied model parameters to target network. total_t = 460000, period = 10000\n",
      "Episode: 1612 Duration: 0:00:22.118285 Num steps: 506 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 7.297 Epsilon: 0.172\n",
      "Episode: 1613 Duration: 0:00:29.107002 Num steps: 659 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 7.337 Epsilon: 0.171\n",
      "Episode: 1614 Duration: 0:00:28.180836 Num steps: 644 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 7.386 Epsilon: 0.170\n",
      "Episode: 1615 Duration: 0:00:20.320697 Num steps: 458 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 7.386 Epsilon: 0.169\n",
      "Episode: 1616 Duration: 0:00:23.996193 Num steps: 549 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 7.416 Epsilon: 0.168\n",
      "Episode: 1617 Duration: 0:00:20.627781 Num steps: 468 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 7.406 Epsilon: 0.167\n",
      "Episode: 1618 Duration: 0:00:36.540807 Num steps: 831 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 7.455 Epsilon: 0.165\n",
      "Episode: 1619 Duration: 0:00:29.087766 Num steps: 658 Reward: 9.0 Training time per step: 0.036 Avg Reward (Last 100): 7.485 Epsilon: 0.164\n",
      "Episode: 1620 Duration: 0:00:14.102399 Num steps: 318 Reward: 4.0 Training time per step: 0.036 Avg Reward (Last 100): 7.426 Epsilon: 0.164\n",
      "Episode: 1621 Duration: 0:00:32.030793 Num steps: 730 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 7.515 Epsilon: 0.162\n",
      "Episode: 1622 Duration: 0:00:23.728664 Num steps: 544 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 7.495 Epsilon: 0.161\n",
      "Episode: 1623 Duration: 0:00:21.568998 Num steps: 487 Reward: 7.0 Training time per step: 0.036 Avg Reward (Last 100): 7.505 Epsilon: 0.161\n",
      "Episode: 1624 Duration: 0:00:29.615111 Num steps: 667 Reward: 10.0 Training time per step: 0.036 Avg Reward (Last 100): 7.554 Epsilon: 0.159\n",
      "Episode: 1625 Duration: 0:00:20.688229 Num steps: 466 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 7.574 Epsilon: 0.158\n",
      "Episode: 1626 Duration: 0:00:41.495396 Num steps: 945 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 7.663 Epsilon: 0.157\n",
      "Episode: 1627 Duration: 0:00:21.819026 Num steps: 502 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 7.693 Epsilon: 0.156\n",
      "Episode: 1628 Duration: 0:00:22.774400 Num steps: 508 Reward: 5.0 Training time per step: 0.036 Avg Reward (Last 100): 7.683 Epsilon: 0.155\n",
      "Copied model parameters to target network. total_t = 470000, period = 10000\n",
      "Episode: 1629 Duration: 0:00:26.938481 Num steps: 609 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 7.703 Epsilon: 0.154\n",
      "Episode: 1630 Duration: 0:00:23.732548 Num steps: 526 Reward: 7.0 Training time per step: 0.036 Avg Reward (Last 100): 7.703 Epsilon: 0.153\n",
      "Episode: 1631 Duration: 0:00:21.838096 Num steps: 497 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 7.762 Epsilon: 0.152\n",
      "Episode: 1632 Duration: 0:00:28.735679 Num steps: 652 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 7.802 Epsilon: 0.151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1633 Duration: 0:00:33.212201 Num steps: 754 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 7.881 Epsilon: 0.150\n",
      "Episode: 1634 Duration: 0:00:38.405817 Num steps: 874 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 7.960 Epsilon: 0.148\n",
      "Episode: 1635 Duration: 0:00:23.832825 Num steps: 541 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 7.941 Epsilon: 0.147\n",
      "Episode: 1636 Duration: 0:00:32.088143 Num steps: 728 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 8.010 Epsilon: 0.146\n",
      "Episode: 1637 Duration: 0:00:16.449366 Num steps: 378 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 8.000 Epsilon: 0.145\n",
      "Episode: 1638 Duration: 0:00:29.079600 Num steps: 659 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 8.069 Epsilon: 0.144\n",
      "Episode: 1639 Duration: 0:00:27.257327 Num steps: 627 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 8.089 Epsilon: 0.143\n",
      "Episode: 1640 Duration: 0:00:31.178339 Num steps: 712 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 8.119 Epsilon: 0.141\n",
      "Episode: 1641 Duration: 0:00:26.251715 Num steps: 586 Reward: 7.0 Training time per step: 0.036 Avg Reward (Last 100): 8.099 Epsilon: 0.140\n",
      "Episode: 1642 Duration: 0:00:20.937538 Num steps: 478 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 8.139 Epsilon: 0.139\n",
      "Episode: 1643 Duration: 0:00:29.153153 Num steps: 655 Reward: 12.0 Training time per step: 0.036 Avg Reward (Last 100): 8.188 Epsilon: 0.138\n",
      "Episode: 1644 Duration: 0:00:24.079129 Num steps: 546 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 8.198 Epsilon: 0.137\n",
      "Copied model parameters to target network. total_t = 480000, period = 10000\n",
      "Episode: 1645 Duration: 0:00:33.860405 Num steps: 766 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 8.267 Epsilon: 0.136\n",
      "Episode: 1646 Duration: 0:00:22.734104 Num steps: 521 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 8.307 Epsilon: 0.135\n",
      "Episode: 1647 Duration: 0:00:39.487920 Num steps: 880 Reward: 13.0 Training time per step: 0.036 Avg Reward (Last 100): 8.366 Epsilon: 0.133\n",
      "Episode: 1648 Duration: 0:00:29.062550 Num steps: 656 Reward: 9.0 Training time per step: 0.036 Avg Reward (Last 100): 8.347 Epsilon: 0.132\n",
      "Episode: 1649 Duration: 0:00:34.284125 Num steps: 779 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 8.376 Epsilon: 0.131\n",
      "Episode: 1650 Duration: 0:00:31.707587 Num steps: 718 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 8.406 Epsilon: 0.130\n",
      "Episode: 1651 Duration: 0:00:26.649965 Num steps: 608 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 8.455 Epsilon: 0.128\n",
      "Episode: 1652 Duration: 0:00:20.792164 Num steps: 451 Reward: 5.0 Training time per step: 0.037 Avg Reward (Last 100): 8.416 Epsilon: 0.128\n",
      "Episode: 1653 Duration: 0:00:33.771648 Num steps: 758 Reward: 9.0 Training time per step: 0.036 Avg Reward (Last 100): 8.376 Epsilon: 0.126\n",
      "Episode: 1654 Duration: 0:00:35.929434 Num steps: 815 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 8.376 Epsilon: 0.125\n",
      "Episode: 1655 Duration: 0:00:39.590128 Num steps: 899 Reward: 18.0 Training time per step: 0.035 Avg Reward (Last 100): 8.446 Epsilon: 0.123\n",
      "Episode: 1656 Duration: 0:00:46.518323 Num steps: 1040 Reward: 20.0 Training time per step: 0.036 Avg Reward (Last 100): 8.594 Epsilon: 0.121\n",
      "Episode: 1657 Duration: 0:00:34.818663 Num steps: 779 Reward: 13.0 Training time per step: 0.036 Avg Reward (Last 100): 8.683 Epsilon: 0.120\n",
      "Episode: 1658 Duration: 0:00:20.893409 Num steps: 474 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 8.673 Epsilon: 0.119\n",
      "Copied model parameters to target network. total_t = 490000, period = 10000\n",
      "Episode: 1659 Duration: 0:00:30.473830 Num steps: 688 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 8.693 Epsilon: 0.118\n",
      "Episode: 1660 Duration: 0:00:25.510449 Num steps: 584 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 8.713 Epsilon: 0.117\n",
      "Episode: 1661 Duration: 0:00:40.337856 Num steps: 905 Reward: 16.0 Training time per step: 0.036 Avg Reward (Last 100): 8.802 Epsilon: 0.115\n",
      "Episode: 1662 Duration: 0:00:26.007994 Num steps: 592 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 8.871 Epsilon: 0.114\n",
      "Episode: 1663 Duration: 0:00:24.061089 Num steps: 542 Reward: 6.0 Training time per step: 0.035 Avg Reward (Last 100): 8.871 Epsilon: 0.113\n",
      "Episode: 1664 Duration: 0:00:26.135824 Num steps: 584 Reward: 8.0 Training time per step: 0.036 Avg Reward (Last 100): 8.881 Epsilon: 0.112\n",
      "Episode: 1665 Duration: 0:00:25.256825 Num steps: 579 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 8.931 Epsilon: 0.111\n",
      "Episode: 1666 Duration: 0:00:43.089259 Num steps: 977 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 9.050 Epsilon: 0.109\n",
      "Episode: 1667 Duration: 0:00:26.239409 Num steps: 590 Reward: 9.0 Training time per step: 0.036 Avg Reward (Last 100): 9.079 Epsilon: 0.108\n",
      "Episode: 1668 Duration: 0:00:35.598131 Num steps: 808 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 9.208 Epsilon: 0.107\n",
      "Episode: 1669 Duration: 0:00:38.582849 Num steps: 872 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 9.238 Epsilon: 0.105\n",
      "Episode: 1670 Duration: 0:00:32.056634 Num steps: 723 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 9.287 Epsilon: 0.104\n",
      "Episode: 1671 Duration: 0:00:24.033635 Num steps: 519 Reward: 8.0 Training time per step: 0.037 Avg Reward (Last 100): 9.287 Epsilon: 0.103\n",
      "Episode: 1672 Duration: 0:00:23.022278 Num steps: 523 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 9.267 Epsilon: 0.102\n",
      "Episode: 1673 Duration: 0:00:26.173183 Num steps: 590 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 9.307 Epsilon: 0.101\n",
      "Copied model parameters to target network. total_t = 500000, period = 10000\n",
      "Episode: 1674 Duration: 0:00:37.723544 Num steps: 845 Reward: 13.0 Training time per step: 0.036 Avg Reward (Last 100): 9.386 Epsilon: 0.100\n",
      "Episode: 1675 Duration: 0:00:32.756794 Num steps: 749 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 9.446 Epsilon: 0.100\n",
      "Episode: 1676 Duration: 0:00:39.766849 Num steps: 893 Reward: 18.0 Training time per step: 0.036 Avg Reward (Last 100): 9.554 Epsilon: 0.100\n",
      "Episode: 1677 Duration: 0:00:34.992573 Num steps: 786 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 9.634 Epsilon: 0.100\n",
      "Episode: 1678 Duration: 0:00:42.613724 Num steps: 967 Reward: 22.0 Training time per step: 0.035 Avg Reward (Last 100): 9.812 Epsilon: 0.100\n",
      "Episode: 1679 Duration: 0:00:34.246950 Num steps: 763 Reward: 13.0 Training time per step: 0.036 Avg Reward (Last 100): 9.881 Epsilon: 0.100\n",
      "Episode: 1680 Duration: 0:00:28.639771 Num steps: 646 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 9.881 Epsilon: 0.100\n",
      "Episode: 1681 Duration: 0:00:41.435764 Num steps: 941 Reward: 17.0 Training time per step: 0.035 Avg Reward (Last 100): 10.000 Epsilon: 0.100\n",
      "Episode: 1682 Duration: 0:00:50.482855 Num steps: 1135 Reward: 22.0 Training time per step: 0.035 Avg Reward (Last 100): 10.089 Epsilon: 0.100\n",
      "Episode: 1683 Duration: 0:00:32.776185 Num steps: 737 Reward: 18.0 Training time per step: 0.036 Avg Reward (Last 100): 10.188 Epsilon: 0.100\n",
      "Episode: 1684 Duration: 0:00:31.253362 Num steps: 715 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 10.257 Epsilon: 0.100\n",
      "Episode: 1685 Duration: 0:00:24.800779 Num steps: 561 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 10.287 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 510000, period = 10000\n",
      "Episode: 1686 Duration: 0:00:35.853323 Num steps: 815 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 10.198 Epsilon: 0.100\n",
      "Episode: 1687 Duration: 0:00:31.954624 Num steps: 720 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 10.267 Epsilon: 0.100\n",
      "Episode: 1688 Duration: 0:00:37.074428 Num steps: 834 Reward: 12.0 Training time per step: 0.036 Avg Reward (Last 100): 10.356 Epsilon: 0.100\n",
      "Episode: 1689 Duration: 0:00:37.717746 Num steps: 849 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 10.426 Epsilon: 0.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1690 Duration: 0:00:34.095542 Num steps: 769 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 10.525 Epsilon: 0.100\n",
      "Episode: 1691 Duration: 0:00:29.107632 Num steps: 656 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 10.525 Epsilon: 0.100\n",
      "Episode: 1692 Duration: 0:00:33.005616 Num steps: 746 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 10.465 Epsilon: 0.100\n",
      "Episode: 1693 Duration: 0:00:34.066159 Num steps: 770 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 10.535 Epsilon: 0.100\n",
      "Episode: 1694 Duration: 0:00:26.108655 Num steps: 589 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 10.634 Epsilon: 0.100\n",
      "Episode: 1695 Duration: 0:00:25.830442 Num steps: 584 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 10.584 Epsilon: 0.100\n",
      "Episode: 1696 Duration: 0:00:35.538731 Num steps: 805 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 10.673 Epsilon: 0.100\n",
      "Episode: 1697 Duration: 0:00:26.935216 Num steps: 611 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 10.653 Epsilon: 0.100\n",
      "Episode: 1698 Duration: 0:00:33.230727 Num steps: 745 Reward: 12.0 Training time per step: 0.036 Avg Reward (Last 100): 10.683 Epsilon: 0.100\n",
      "Episode: 1699 Duration: 0:00:27.653111 Num steps: 615 Reward: 9.0 Training time per step: 0.036 Avg Reward (Last 100): 10.703 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 520000, period = 10000\n",
      "Episode: 1700 Duration: 0:00:37.615311 Num steps: 847 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 10.743 Epsilon: 0.100\n",
      "Episode: 1701 Duration: 0:00:38.187033 Num steps: 862 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 10.792 Epsilon: 0.100\n",
      "Episode: 1702 Duration: 0:00:36.723601 Num steps: 829 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 10.782 Epsilon: 0.100\n",
      "Episode: 1703 Duration: 0:00:31.568635 Num steps: 713 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 10.842 Epsilon: 0.100\n",
      "Episode: 1704 Duration: 0:00:27.423321 Num steps: 622 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 10.842 Epsilon: 0.100\n",
      "Episode: 1705 Duration: 0:00:27.200937 Num steps: 619 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 10.891 Epsilon: 0.100\n",
      "Episode: 1706 Duration: 0:00:25.089206 Num steps: 578 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 10.911 Epsilon: 0.100\n",
      "Episode: 1707 Duration: 0:00:19.135934 Num steps: 428 Reward: 6.0 Training time per step: 0.036 Avg Reward (Last 100): 10.881 Epsilon: 0.100\n",
      "Episode: 1708 Duration: 0:00:16.388663 Num steps: 369 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 10.832 Epsilon: 0.100\n",
      "Episode: 1709 Duration: 0:00:37.402218 Num steps: 845 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 10.891 Epsilon: 0.100\n",
      "Episode: 1710 Duration: 0:00:25.855007 Num steps: 585 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 10.901 Epsilon: 0.100\n",
      "Episode: 1711 Duration: 0:00:29.463563 Num steps: 666 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 10.881 Epsilon: 0.100\n",
      "Episode: 1712 Duration: 0:00:32.553978 Num steps: 737 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 10.842 Epsilon: 0.100\n",
      "Episode: 1713 Duration: 0:00:38.057233 Num steps: 856 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 10.941 Epsilon: 0.100\n",
      "Episode: 1714 Duration: 0:00:28.404777 Num steps: 645 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 10.950 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 530000, period = 10000\n",
      "Episode: 1715 Duration: 0:00:35.746776 Num steps: 809 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 10.960 Epsilon: 0.100\n",
      "Episode: 1716 Duration: 0:00:33.963147 Num steps: 770 Reward: 24.0 Training time per step: 0.035 Avg Reward (Last 100): 11.149 Epsilon: 0.100\n",
      "Episode: 1717 Duration: 0:00:27.603918 Num steps: 617 Reward: 10.0 Training time per step: 0.036 Avg Reward (Last 100): 11.178 Epsilon: 0.100\n",
      "Episode: 1718 Duration: 0:00:36.095790 Num steps: 801 Reward: 16.0 Training time per step: 0.036 Avg Reward (Last 100): 11.287 Epsilon: 0.100\n",
      "Episode: 1719 Duration: 0:00:38.029547 Num steps: 850 Reward: 13.0 Training time per step: 0.036 Avg Reward (Last 100): 11.327 Epsilon: 0.100\n",
      "Episode: 1720 Duration: 0:00:29.254878 Num steps: 659 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 11.337 Epsilon: 0.100\n",
      "Episode: 1721 Duration: 0:00:37.373693 Num steps: 846 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 11.446 Epsilon: 0.100\n",
      "Episode: 1722 Duration: 0:00:23.874757 Num steps: 537 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 11.396 Epsilon: 0.100\n",
      "Episode: 1723 Duration: 0:00:32.676351 Num steps: 733 Reward: 11.0 Training time per step: 0.036 Avg Reward (Last 100): 11.436 Epsilon: 0.100\n",
      "Episode: 1724 Duration: 0:00:30.459888 Num steps: 689 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 11.455 Epsilon: 0.100\n",
      "Episode: 1725 Duration: 0:00:24.899539 Num steps: 562 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 11.436 Epsilon: 0.100\n",
      "Episode: 1726 Duration: 0:00:42.827918 Num steps: 972 Reward: 18.0 Training time per step: 0.035 Avg Reward (Last 100): 11.564 Epsilon: 0.100\n",
      "Episode: 1727 Duration: 0:00:31.751669 Num steps: 720 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 11.535 Epsilon: 0.100\n",
      "Episode: 1728 Duration: 0:00:32.415427 Num steps: 736 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 11.584 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 540000, period = 10000\n",
      "Episode: 1729 Duration: 0:00:32.489555 Num steps: 744 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 11.663 Epsilon: 0.100\n",
      "Episode: 1730 Duration: 0:00:42.547651 Num steps: 970 Reward: 20.0 Training time per step: 0.035 Avg Reward (Last 100): 11.792 Epsilon: 0.100\n",
      "Episode: 1731 Duration: 0:00:25.061896 Num steps: 567 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 11.812 Epsilon: 0.100\n",
      "Episode: 1732 Duration: 0:00:43.658859 Num steps: 980 Reward: 22.0 Training time per step: 0.036 Avg Reward (Last 100): 11.931 Epsilon: 0.100\n",
      "Episode: 1733 Duration: 0:00:34.095143 Num steps: 773 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 11.980 Epsilon: 0.100\n",
      "Episode: 1734 Duration: 0:00:29.303459 Num steps: 664 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 12.010 Epsilon: 0.100\n",
      "Episode: 1735 Duration: 0:00:38.255069 Num steps: 864 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 12.050 Epsilon: 0.100\n",
      "Episode: 1736 Duration: 0:00:22.580881 Num steps: 513 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 12.050 Epsilon: 0.100\n",
      "Episode: 1737 Duration: 0:00:28.555655 Num steps: 654 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 12.010 Epsilon: 0.100\n",
      "Episode: 1738 Duration: 0:00:32.223645 Num steps: 731 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 12.079 Epsilon: 0.100\n",
      "Episode: 1739 Duration: 0:00:25.423814 Num steps: 578 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 12.059 Epsilon: 0.100\n",
      "Episode: 1740 Duration: 0:00:29.773404 Num steps: 672 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 12.059 Epsilon: 0.100\n",
      "Episode: 1741 Duration: 0:00:32.107653 Num steps: 732 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 12.129 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 550000, period = 10000\n",
      "Episode: 1742 Duration: 0:00:37.052131 Num steps: 842 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 12.208 Epsilon: 0.100\n",
      "Episode: 1743 Duration: 0:00:36.581187 Num steps: 827 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 12.248 Epsilon: 0.100\n",
      "Episode: 1744 Duration: 0:00:36.781300 Num steps: 834 Reward: 18.0 Training time per step: 0.035 Avg Reward (Last 100): 12.307 Epsilon: 0.100\n",
      "Episode: 1745 Duration: 0:00:28.140588 Num steps: 634 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 12.317 Epsilon: 0.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1746 Duration: 0:00:30.942846 Num steps: 691 Reward: 10.0 Training time per step: 0.036 Avg Reward (Last 100): 12.267 Epsilon: 0.100\n",
      "Episode: 1747 Duration: 0:00:25.269015 Num steps: 542 Reward: 7.0 Training time per step: 0.037 Avg Reward (Last 100): 12.238 Epsilon: 0.100\n",
      "Episode: 1748 Duration: 0:00:35.037932 Num steps: 794 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 12.238 Epsilon: 0.100\n",
      "Episode: 1749 Duration: 0:00:34.845219 Num steps: 789 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 12.307 Epsilon: 0.100\n",
      "Episode: 1750 Duration: 0:00:31.631103 Num steps: 715 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 12.337 Epsilon: 0.100\n",
      "Episode: 1751 Duration: 0:00:28.073386 Num steps: 632 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 12.317 Epsilon: 0.100\n",
      "Episode: 1752 Duration: 0:00:40.435371 Num steps: 921 Reward: 17.0 Training time per step: 0.035 Avg Reward (Last 100): 12.406 Epsilon: 0.100\n",
      "Episode: 1753 Duration: 0:00:34.104154 Num steps: 772 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 12.475 Epsilon: 0.100\n",
      "Episode: 1754 Duration: 0:00:20.300821 Num steps: 455 Reward: 6.0 Training time per step: 0.036 Avg Reward (Last 100): 12.446 Epsilon: 0.100\n",
      "Episode: 1755 Duration: 0:00:29.814292 Num steps: 660 Reward: 10.0 Training time per step: 0.036 Avg Reward (Last 100): 12.455 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 560000, period = 10000\n",
      "Episode: 1756 Duration: 0:00:29.121289 Num steps: 661 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 12.416 Epsilon: 0.100\n",
      "Episode: 1757 Duration: 0:00:34.343731 Num steps: 778 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 12.337 Epsilon: 0.100\n",
      "Episode: 1758 Duration: 0:00:35.232811 Num steps: 799 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 12.337 Epsilon: 0.100\n",
      "Episode: 1759 Duration: 0:00:32.721047 Num steps: 739 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 12.406 Epsilon: 0.100\n",
      "Episode: 1760 Duration: 0:00:37.481242 Num steps: 849 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 12.475 Epsilon: 0.100\n",
      "Episode: 1761 Duration: 0:00:29.790458 Num steps: 676 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 12.505 Epsilon: 0.100\n",
      "Episode: 1762 Duration: 0:00:23.812848 Num steps: 542 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 12.416 Epsilon: 0.100\n",
      "Episode: 1763 Duration: 0:00:32.959600 Num steps: 750 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 12.446 Epsilon: 0.100\n",
      "Episode: 1764 Duration: 0:00:32.206341 Num steps: 727 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 12.495 Epsilon: 0.100\n",
      "Episode: 1765 Duration: 0:00:28.333026 Num steps: 640 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 12.505 Epsilon: 0.100\n",
      "Episode: 1766 Duration: 0:00:32.539752 Num steps: 735 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 12.535 Epsilon: 0.100\n",
      "Episode: 1767 Duration: 0:00:31.048468 Num steps: 705 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 12.505 Epsilon: 0.100\n",
      "Episode: 1768 Duration: 0:00:15.091030 Num steps: 341 Reward: 4.0 Training time per step: 0.035 Avg Reward (Last 100): 12.455 Epsilon: 0.100\n",
      "Episode: 1769 Duration: 0:00:37.594572 Num steps: 847 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 12.406 Epsilon: 0.100\n",
      "Episode: 1770 Duration: 0:00:29.295681 Num steps: 664 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 12.366 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 570000, period = 10000\n",
      "Episode: 1771 Duration: 0:00:29.852754 Num steps: 675 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 12.327 Epsilon: 0.100\n",
      "Episode: 1772 Duration: 0:00:30.085009 Num steps: 681 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 12.406 Epsilon: 0.100\n",
      "Episode: 1773 Duration: 0:00:26.164591 Num steps: 597 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 12.446 Epsilon: 0.100\n",
      "Episode: 1774 Duration: 0:00:43.004712 Num steps: 964 Reward: 21.0 Training time per step: 0.036 Avg Reward (Last 100): 12.574 Epsilon: 0.100\n",
      "Episode: 1775 Duration: 0:00:30.592293 Num steps: 692 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 12.554 Epsilon: 0.100\n",
      "Episode: 1776 Duration: 0:00:40.807404 Num steps: 920 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 12.584 Epsilon: 0.100\n",
      "Episode: 1777 Duration: 0:00:35.609116 Num steps: 805 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 12.525 Epsilon: 0.100\n",
      "Episode: 1778 Duration: 0:00:38.129609 Num steps: 863 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 12.525 Epsilon: 0.100\n",
      "Episode: 1779 Duration: 0:00:22.704537 Num steps: 514 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 12.376 Epsilon: 0.100\n",
      "Episode: 1780 Duration: 0:00:36.306731 Num steps: 822 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 12.406 Epsilon: 0.100\n",
      "Episode: 1781 Duration: 0:00:39.276115 Num steps: 888 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 12.455 Epsilon: 0.100\n",
      "Episode: 1782 Duration: 0:00:34.356892 Num steps: 774 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 12.475 Epsilon: 0.100\n",
      "Episode: 1783 Duration: 0:00:35.895819 Num steps: 813 Reward: 18.0 Training time per step: 0.035 Avg Reward (Last 100): 12.436 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 580000, period = 10000\n",
      "Episode: 1784 Duration: 0:00:38.294157 Num steps: 867 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 12.406 Epsilon: 0.100\n",
      "Episode: 1785 Duration: 0:00:35.991010 Num steps: 818 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 12.406 Epsilon: 0.100\n",
      "Episode: 1786 Duration: 0:00:40.870537 Num steps: 905 Reward: 17.0 Training time per step: 0.036 Avg Reward (Last 100): 12.495 Epsilon: 0.100\n",
      "Episode: 1787 Duration: 0:00:30.965809 Num steps: 700 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 12.505 Epsilon: 0.100\n",
      "Episode: 1788 Duration: 0:00:32.664702 Num steps: 739 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 12.515 Epsilon: 0.100\n",
      "Episode: 1789 Duration: 0:00:37.138663 Num steps: 842 Reward: 29.0 Training time per step: 0.035 Avg Reward (Last 100): 12.683 Epsilon: 0.100\n",
      "Episode: 1790 Duration: 0:00:36.931491 Num steps: 829 Reward: 16.0 Training time per step: 0.036 Avg Reward (Last 100): 12.683 Epsilon: 0.100\n",
      "Episode: 1791 Duration: 0:00:43.511153 Num steps: 975 Reward: 17.0 Training time per step: 0.036 Avg Reward (Last 100): 12.703 Epsilon: 0.100\n",
      "Episode: 1792 Duration: 0:00:50.708357 Num steps: 1127 Reward: 26.0 Training time per step: 0.036 Avg Reward (Last 100): 12.851 Epsilon: 0.100\n",
      "Episode: 1793 Duration: 0:00:24.054810 Num steps: 548 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 12.861 Epsilon: 0.100\n",
      "Episode: 1794 Duration: 0:00:33.637123 Num steps: 757 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 12.842 Epsilon: 0.100\n",
      "Episode: 1795 Duration: 0:00:39.789002 Num steps: 890 Reward: 15.0 Training time per step: 0.036 Avg Reward (Last 100): 12.861 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 590000, period = 10000\n",
      "Episode: 1796 Duration: 0:00:39.579868 Num steps: 894 Reward: 17.0 Training time per step: 0.035 Avg Reward (Last 100): 12.950 Epsilon: 0.100\n",
      "Episode: 1797 Duration: 0:00:29.043339 Num steps: 658 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 12.901 Epsilon: 0.100\n",
      "Episode: 1798 Duration: 0:00:34.703779 Num steps: 783 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 12.960 Epsilon: 0.100\n",
      "Episode: 1799 Duration: 0:00:37.349807 Num steps: 841 Reward: 18.0 Training time per step: 0.035 Avg Reward (Last 100): 13.020 Epsilon: 0.100\n",
      "Episode: 1800 Duration: 0:00:35.205032 Num steps: 794 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 13.069 Epsilon: 0.100\n",
      "Episode: 1801 Duration: 0:00:30.164486 Num steps: 683 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 12.990 Epsilon: 0.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1802 Duration: 0:00:29.703430 Num steps: 673 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 12.960 Epsilon: 0.100\n",
      "Episode: 1803 Duration: 0:00:25.101425 Num steps: 570 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 12.921 Epsilon: 0.100\n",
      "Episode: 1804 Duration: 0:00:34.775670 Num steps: 789 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 12.901 Epsilon: 0.100\n",
      "Episode: 1805 Duration: 0:00:41.484816 Num steps: 937 Reward: 18.0 Training time per step: 0.035 Avg Reward (Last 100): 12.990 Epsilon: 0.100\n",
      "Episode: 1806 Duration: 0:00:22.990758 Num steps: 520 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 12.941 Epsilon: 0.100\n",
      "Episode: 1807 Duration: 0:00:32.609419 Num steps: 739 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 12.941 Epsilon: 0.100\n",
      "Episode: 1808 Duration: 0:00:25.722140 Num steps: 588 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 12.970 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 600000, period = 10000\n",
      "Episode: 1809 Duration: 0:00:34.201777 Num steps: 771 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 13.050 Epsilon: 0.100\n",
      "Episode: 1810 Duration: 0:00:44.004648 Num steps: 992 Reward: 23.0 Training time per step: 0.035 Avg Reward (Last 100): 13.119 Epsilon: 0.100\n",
      "Episode: 1811 Duration: 0:00:35.526414 Num steps: 803 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 13.188 Epsilon: 0.100\n",
      "Episode: 1812 Duration: 0:00:34.949904 Num steps: 784 Reward: 12.0 Training time per step: 0.036 Avg Reward (Last 100): 13.208 Epsilon: 0.100\n",
      "Episode: 1813 Duration: 0:00:42.631878 Num steps: 964 Reward: 20.0 Training time per step: 0.035 Avg Reward (Last 100): 13.327 Epsilon: 0.100\n",
      "Episode: 1814 Duration: 0:00:41.692152 Num steps: 944 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 13.327 Epsilon: 0.100\n",
      "Episode: 1815 Duration: 0:00:38.555053 Num steps: 868 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 13.386 Epsilon: 0.100\n",
      "Episode: 1816 Duration: 0:00:39.458898 Num steps: 888 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 13.376 Epsilon: 0.100\n",
      "Episode: 1817 Duration: 0:00:30.214003 Num steps: 683 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 13.267 Epsilon: 0.100\n",
      "Episode: 1818 Duration: 0:00:35.862783 Num steps: 809 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 13.307 Epsilon: 0.100\n",
      "Episode: 1819 Duration: 0:00:34.014367 Num steps: 771 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 13.297 Epsilon: 0.100\n",
      "Episode: 1820 Duration: 0:00:35.218403 Num steps: 797 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 13.297 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 610000, period = 10000\n",
      "Episode: 1821 Duration: 0:00:31.158758 Num steps: 696 Reward: 15.0 Training time per step: 0.036 Avg Reward (Last 100): 13.347 Epsilon: 0.100\n",
      "Episode: 1822 Duration: 0:00:29.745752 Num steps: 677 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 13.347 Epsilon: 0.100\n",
      "Episode: 1823 Duration: 0:00:25.315629 Num steps: 566 Reward: 8.0 Training time per step: 0.036 Avg Reward (Last 100): 13.356 Epsilon: 0.100\n",
      "Episode: 1824 Duration: 0:00:36.860488 Num steps: 840 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 13.436 Epsilon: 0.100\n",
      "Episode: 1825 Duration: 0:00:30.095714 Num steps: 678 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 13.446 Epsilon: 0.100\n",
      "Episode: 1826 Duration: 0:00:36.975880 Num steps: 813 Reward: 11.0 Training time per step: 0.036 Avg Reward (Last 100): 13.475 Epsilon: 0.100\n",
      "Episode: 1827 Duration: 0:00:33.194392 Num steps: 748 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 13.485 Epsilon: 0.100\n",
      "Episode: 1828 Duration: 0:00:37.432617 Num steps: 846 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 13.515 Epsilon: 0.100\n",
      "Episode: 1829 Duration: 0:00:37.641907 Num steps: 853 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 13.535 Epsilon: 0.100\n",
      "Episode: 1830 Duration: 0:00:26.057380 Num steps: 591 Reward: 8.0 Training time per step: 0.035 Avg Reward (Last 100): 13.485 Epsilon: 0.100\n",
      "Episode: 1831 Duration: 0:00:34.136748 Num steps: 772 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 13.406 Epsilon: 0.100\n",
      "Episode: 1832 Duration: 0:00:30.592531 Num steps: 698 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 13.465 Epsilon: 0.100\n",
      "Episode: 1833 Duration: 0:00:34.740938 Num steps: 784 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 13.376 Epsilon: 0.100\n",
      "Episode: 1834 Duration: 0:00:26.641356 Num steps: 605 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 13.337 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 620000, period = 10000\n",
      "Episode: 1835 Duration: 0:00:36.487746 Num steps: 824 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 13.317 Epsilon: 0.100\n",
      "Episode: 1836 Duration: 0:00:23.106233 Num steps: 524 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 13.238 Epsilon: 0.100\n",
      "Episode: 1837 Duration: 0:00:33.298389 Num steps: 752 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 13.277 Epsilon: 0.100\n",
      "Episode: 1838 Duration: 0:00:32.540811 Num steps: 739 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 13.327 Epsilon: 0.100\n",
      "Episode: 1839 Duration: 0:00:38.183956 Num steps: 858 Reward: 16.0 Training time per step: 0.036 Avg Reward (Last 100): 13.376 Epsilon: 0.100\n",
      "Episode: 1840 Duration: 0:00:26.234223 Num steps: 598 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 13.406 Epsilon: 0.100\n",
      "Episode: 1841 Duration: 0:00:33.044922 Num steps: 749 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 13.416 Epsilon: 0.100\n",
      "Episode: 1842 Duration: 0:00:33.587992 Num steps: 761 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 13.396 Epsilon: 0.100\n",
      "Episode: 1843 Duration: 0:00:35.142817 Num steps: 794 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 13.406 Epsilon: 0.100\n",
      "Episode: 1844 Duration: 0:00:38.348251 Num steps: 858 Reward: 14.0 Training time per step: 0.036 Avg Reward (Last 100): 13.406 Epsilon: 0.100\n",
      "Episode: 1845 Duration: 0:00:42.384195 Num steps: 956 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 13.416 Epsilon: 0.100\n",
      "Episode: 1846 Duration: 0:00:49.902161 Num steps: 1133 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 13.525 Epsilon: 0.100\n",
      "Episode: 1847 Duration: 0:00:38.918123 Num steps: 875 Reward: 15.0 Training time per step: 0.036 Avg Reward (Last 100): 13.574 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 630000, period = 10000\n",
      "Episode: 1848 Duration: 0:00:30.066752 Num steps: 675 Reward: 9.0 Training time per step: 0.036 Avg Reward (Last 100): 13.594 Epsilon: 0.100\n",
      "Episode: 1849 Duration: 0:00:35.807402 Num steps: 814 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 13.594 Epsilon: 0.100\n",
      "Episode: 1850 Duration: 0:00:30.555760 Num steps: 691 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 13.535 Epsilon: 0.100\n",
      "Episode: 1851 Duration: 0:00:21.376720 Num steps: 487 Reward: 7.0 Training time per step: 0.035 Avg Reward (Last 100): 13.465 Epsilon: 0.100\n",
      "Episode: 1852 Duration: 0:00:31.918553 Num steps: 723 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 13.485 Epsilon: 0.100\n",
      "Episode: 1853 Duration: 0:00:34.996381 Num steps: 790 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 13.436 Epsilon: 0.100\n",
      "Episode: 1854 Duration: 0:00:26.406579 Num steps: 587 Reward: 9.0 Training time per step: 0.036 Avg Reward (Last 100): 13.406 Epsilon: 0.100\n",
      "Episode: 1855 Duration: 0:00:45.543189 Num steps: 1036 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 13.535 Epsilon: 0.100\n",
      "Episode: 1856 Duration: 0:00:41.886741 Num steps: 946 Reward: 23.0 Training time per step: 0.035 Avg Reward (Last 100): 13.663 Epsilon: 0.100\n",
      "Episode: 1857 Duration: 0:00:40.595946 Num steps: 922 Reward: 24.0 Training time per step: 0.035 Avg Reward (Last 100): 13.762 Epsilon: 0.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1858 Duration: 0:00:44.570082 Num steps: 1012 Reward: 24.0 Training time per step: 0.035 Avg Reward (Last 100): 13.881 Epsilon: 0.100\n",
      "Episode: 1859 Duration: 0:00:31.743000 Num steps: 717 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 13.911 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 640000, period = 10000\n",
      "Episode: 1860 Duration: 0:00:51.611088 Num steps: 1157 Reward: 25.0 Training time per step: 0.036 Avg Reward (Last 100): 14.030 Epsilon: 0.100\n",
      "Episode: 1861 Duration: 0:00:41.495537 Num steps: 932 Reward: 16.0 Training time per step: 0.036 Avg Reward (Last 100): 14.000 Epsilon: 0.100\n",
      "Episode: 1862 Duration: 0:00:28.721654 Num steps: 650 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 14.010 Epsilon: 0.100\n",
      "Episode: 1863 Duration: 0:00:39.256770 Num steps: 892 Reward: 21.0 Training time per step: 0.035 Avg Reward (Last 100): 14.149 Epsilon: 0.100\n",
      "Episode: 1864 Duration: 0:00:23.283634 Num steps: 535 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 14.129 Epsilon: 0.100\n",
      "Episode: 1865 Duration: 0:00:39.849277 Num steps: 897 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 14.168 Epsilon: 0.100\n",
      "Episode: 1866 Duration: 0:00:31.037386 Num steps: 704 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 14.198 Epsilon: 0.100\n",
      "Episode: 1867 Duration: 0:00:38.547387 Num steps: 877 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 14.228 Epsilon: 0.100\n",
      "Episode: 1868 Duration: 0:00:50.044552 Num steps: 1133 Reward: 21.0 Training time per step: 0.035 Avg Reward (Last 100): 14.277 Epsilon: 0.100\n",
      "Episode: 1869 Duration: 0:00:31.779348 Num steps: 717 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 14.356 Epsilon: 0.100\n",
      "Episode: 1870 Duration: 0:00:32.901890 Num steps: 742 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 14.337 Epsilon: 0.100\n",
      "Episode: 1871 Duration: 0:00:22.395607 Num steps: 503 Reward: 6.0 Training time per step: 0.036 Avg Reward (Last 100): 14.287 Epsilon: 0.100\n",
      "Episode: 1872 Duration: 0:00:31.585129 Num steps: 713 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 14.297 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 650000, period = 10000\n",
      "Episode: 1873 Duration: 0:00:38.820749 Num steps: 873 Reward: 21.0 Training time per step: 0.036 Avg Reward (Last 100): 14.347 Epsilon: 0.100\n",
      "Episode: 1874 Duration: 0:00:34.522270 Num steps: 774 Reward: 16.0 Training time per step: 0.036 Avg Reward (Last 100): 14.396 Epsilon: 0.100\n",
      "Episode: 1875 Duration: 0:00:26.515948 Num steps: 599 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 14.277 Epsilon: 0.100\n",
      "Episode: 1876 Duration: 0:00:27.397248 Num steps: 615 Reward: 9.0 Training time per step: 0.036 Avg Reward (Last 100): 14.257 Epsilon: 0.100\n",
      "Episode: 1877 Duration: 0:00:46.064554 Num steps: 1036 Reward: 22.0 Training time per step: 0.035 Avg Reward (Last 100): 14.327 Epsilon: 0.100\n",
      "Episode: 1878 Duration: 0:00:37.428229 Num steps: 838 Reward: 18.0 Training time per step: 0.036 Avg Reward (Last 100): 14.386 Epsilon: 0.100\n",
      "Episode: 1879 Duration: 0:00:44.740488 Num steps: 1007 Reward: 20.0 Training time per step: 0.035 Avg Reward (Last 100): 14.446 Epsilon: 0.100\n",
      "Episode: 1880 Duration: 0:00:29.269545 Num steps: 664 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 14.475 Epsilon: 0.100\n",
      "Episode: 1881 Duration: 0:00:38.404106 Num steps: 862 Reward: 13.0 Training time per step: 0.036 Avg Reward (Last 100): 14.446 Epsilon: 0.100\n",
      "Episode: 1882 Duration: 0:00:30.117848 Num steps: 677 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 14.436 Epsilon: 0.100\n",
      "Episode: 1883 Duration: 0:00:33.593051 Num steps: 761 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 14.396 Epsilon: 0.100\n",
      "Episode: 1884 Duration: 0:00:30.859027 Num steps: 701 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 14.317 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 660000, period = 10000\n",
      "Episode: 1885 Duration: 0:00:42.579591 Num steps: 954 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 14.317 Epsilon: 0.100\n",
      "Episode: 1886 Duration: 0:00:38.294515 Num steps: 862 Reward: 17.0 Training time per step: 0.035 Avg Reward (Last 100): 14.356 Epsilon: 0.100\n",
      "Episode: 1887 Duration: 0:00:42.943431 Num steps: 975 Reward: 18.0 Training time per step: 0.035 Avg Reward (Last 100): 14.366 Epsilon: 0.100\n",
      "Episode: 1888 Duration: 0:00:26.147821 Num steps: 585 Reward: 8.0 Training time per step: 0.036 Avg Reward (Last 100): 14.327 Epsilon: 0.100\n",
      "Episode: 1889 Duration: 0:00:39.974081 Num steps: 905 Reward: 21.0 Training time per step: 0.035 Avg Reward (Last 100): 14.416 Epsilon: 0.100\n",
      "Episode: 1890 Duration: 0:00:37.570448 Num steps: 853 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 14.257 Epsilon: 0.100\n",
      "Episode: 1891 Duration: 0:00:37.648765 Num steps: 850 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 14.238 Epsilon: 0.100\n",
      "Episode: 1892 Duration: 0:00:33.947758 Num steps: 767 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 14.228 Epsilon: 0.100\n",
      "Episode: 1893 Duration: 0:00:27.205231 Num steps: 611 Reward: 8.0 Training time per step: 0.036 Avg Reward (Last 100): 14.050 Epsilon: 0.100\n",
      "Episode: 1894 Duration: 0:00:42.587216 Num steps: 941 Reward: 24.0 Training time per step: 0.036 Avg Reward (Last 100): 14.178 Epsilon: 0.100\n",
      "Episode: 1895 Duration: 0:00:21.892312 Num steps: 491 Reward: 7.0 Training time per step: 0.036 Avg Reward (Last 100): 14.139 Epsilon: 0.100\n",
      "Episode: 1896 Duration: 0:00:43.340258 Num steps: 973 Reward: 24.0 Training time per step: 0.036 Avg Reward (Last 100): 14.228 Epsilon: 0.100\n",
      "Episode: 1897 Duration: 0:00:31.614606 Num steps: 712 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 14.168 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 670000, period = 10000\n",
      "Episode: 1898 Duration: 0:00:35.686769 Num steps: 805 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 14.198 Epsilon: 0.100\n",
      "Episode: 1899 Duration: 0:00:30.848888 Num steps: 696 Reward: 11.0 Training time per step: 0.035 Avg Reward (Last 100): 14.158 Epsilon: 0.100\n",
      "Episode: 1900 Duration: 0:00:36.822338 Num steps: 826 Reward: 14.0 Training time per step: 0.036 Avg Reward (Last 100): 14.119 Epsilon: 0.100\n",
      "Episode: 1901 Duration: 0:00:35.512955 Num steps: 805 Reward: 13.0 Training time per step: 0.035 Avg Reward (Last 100): 14.109 Epsilon: 0.100\n",
      "Episode: 1902 Duration: 0:00:49.384123 Num steps: 1116 Reward: 27.0 Training time per step: 0.035 Avg Reward (Last 100): 14.267 Epsilon: 0.100\n",
      "Episode: 1903 Duration: 0:00:46.072107 Num steps: 1037 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 14.347 Epsilon: 0.100\n",
      "Episode: 1904 Duration: 0:00:41.126727 Num steps: 930 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 14.446 Epsilon: 0.100\n",
      "Episode: 1905 Duration: 0:00:28.063957 Num steps: 634 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 14.416 Epsilon: 0.100\n",
      "Episode: 1906 Duration: 0:00:40.904128 Num steps: 929 Reward: 15.0 Training time per step: 0.035 Avg Reward (Last 100): 14.386 Epsilon: 0.100\n",
      "Episode: 1907 Duration: 0:00:38.045283 Num steps: 853 Reward: 15.0 Training time per step: 0.036 Avg Reward (Last 100): 14.465 Epsilon: 0.100\n",
      "Episode: 1908 Duration: 0:00:47.889633 Num steps: 1081 Reward: 18.0 Training time per step: 0.035 Avg Reward (Last 100): 14.535 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 680000, period = 10000\n",
      "Episode: 1909 Duration: 0:00:31.463031 Num steps: 708 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 14.564 Epsilon: 0.100\n",
      "Episode: 1910 Duration: 0:00:42.806241 Num steps: 964 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 14.564 Epsilon: 0.100\n",
      "Episode: 1911 Duration: 0:00:33.592338 Num steps: 759 Reward: 18.0 Training time per step: 0.035 Avg Reward (Last 100): 14.515 Epsilon: 0.100\n",
      "Episode: 1912 Duration: 0:00:42.946556 Num steps: 974 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 14.545 Epsilon: 0.100\n",
      "Episode: 1913 Duration: 0:00:37.941304 Num steps: 862 Reward: 29.0 Training time per step: 0.035 Avg Reward (Last 100): 14.713 Epsilon: 0.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1914 Duration: 0:00:40.438697 Num steps: 912 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 14.703 Epsilon: 0.100\n",
      "Episode: 1915 Duration: 0:00:32.377309 Num steps: 732 Reward: 16.0 Training time per step: 0.035 Avg Reward (Last 100): 14.703 Epsilon: 0.100\n",
      "Episode: 1916 Duration: 0:00:40.178702 Num steps: 922 Reward: 18.0 Training time per step: 0.035 Avg Reward (Last 100): 14.733 Epsilon: 0.100\n",
      "Episode: 1917 Duration: 0:00:29.973401 Num steps: 680 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 14.703 Epsilon: 0.100\n",
      "Episode: 1918 Duration: 0:00:33.722213 Num steps: 757 Reward: 14.0 Training time per step: 0.036 Avg Reward (Last 100): 14.713 Epsilon: 0.100\n",
      "Episode: 1919 Duration: 0:00:47.809766 Num steps: 1081 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 14.762 Epsilon: 0.100\n",
      "Episode: 1920 Duration: 0:00:25.606638 Num steps: 588 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 14.733 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 690000, period = 10000\n",
      "Episode: 1921 Duration: 0:00:35.262628 Num steps: 798 Reward: 14.0 Training time per step: 0.035 Avg Reward (Last 100): 14.743 Epsilon: 0.100\n",
      "Episode: 1922 Duration: 0:00:48.782174 Num steps: 1089 Reward: 28.0 Training time per step: 0.036 Avg Reward (Last 100): 14.871 Epsilon: 0.100\n",
      "Episode: 1923 Duration: 0:00:53.375480 Num steps: 1215 Reward: 25.0 Training time per step: 0.035 Avg Reward (Last 100): 14.970 Epsilon: 0.100\n",
      "Episode: 1924 Duration: 0:00:34.803157 Num steps: 782 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 15.010 Epsilon: 0.100\n",
      "Episode: 1925 Duration: 0:01:03.380150 Num steps: 1415 Reward: 35.0 Training time per step: 0.036 Avg Reward (Last 100): 15.168 Epsilon: 0.100\n",
      "Episode: 1926 Duration: 0:00:45.579983 Num steps: 1025 Reward: 22.0 Training time per step: 0.035 Avg Reward (Last 100): 15.287 Epsilon: 0.100\n",
      "Episode: 1927 Duration: 0:00:43.611285 Num steps: 994 Reward: 21.0 Training time per step: 0.035 Avg Reward (Last 100): 15.386 Epsilon: 0.100\n",
      "Episode: 1928 Duration: 0:00:29.809010 Num steps: 674 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 15.297 Epsilon: 0.100\n",
      "Episode: 1929 Duration: 0:00:43.252642 Num steps: 980 Reward: 20.0 Training time per step: 0.035 Avg Reward (Last 100): 15.356 Epsilon: 0.100\n",
      "Episode: 1930 Duration: 0:00:35.420482 Num steps: 798 Reward: 19.0 Training time per step: 0.035 Avg Reward (Last 100): 15.406 Epsilon: 0.100\n",
      "Copied model parameters to target network. total_t = 700000, period = 10000\n",
      "Episode: 1931 Duration: 0:00:39.229161 Num steps: 885 Reward: 12.0 Training time per step: 0.035 Avg Reward (Last 100): 15.446 Epsilon: 0.100\n",
      "Episode: 1932 Duration: 0:00:26.037277 Num steps: 589 Reward: 10.0 Training time per step: 0.035 Avg Reward (Last 100): 15.426 Epsilon: 0.100\n",
      "Episode: 1933 Duration: 0:00:25.174929 Num steps: 572 Reward: 9.0 Training time per step: 0.035 Avg Reward (Last 100): 15.366 Epsilon: 0.100\n",
      "Episode: 1934 Duration: 0:00:41.191986 Num steps: 925 Reward: 18.0 Training time per step: 0.036 Avg Reward (Last 100): 15.416 Epsilon: 0.100\n",
      "Episode: 1935 Duration: 0:00:41.799346 Num steps: 938 Reward: 17.0 Training time per step: 0.036 Avg Reward (Last 100): 15.495 Epsilon: 0.100\n",
      "Episode: 1936 Duration: 0:00:41.866416 Num steps: 949 Reward: 18.0 Training time per step: 0.035 Avg Reward (Last 100): 15.554 Epsilon: 0.100\n",
      "Episode: 1937 Duration: 0:00:42.032026 Num steps: 819 Reward: 13.0 Training time per step: 0.041 Avg Reward (Last 100): 15.614 Epsilon: 0.100\n",
      "Episode: 1938 Duration: 0:00:33.673545 Num steps: 686 Reward: 11.0 Training time per step: 0.040 Avg Reward (Last 100): 15.614 Epsilon: 0.100\n",
      "Episode: 1939 Duration: 0:00:26.081469 Num steps: 543 Reward: 9.0 Training time per step: 0.038 Avg Reward (Last 100): 15.554 Epsilon: 0.100\n",
      "Episode: 1940 Duration: 0:00:37.503328 Num steps: 794 Reward: 13.0 Training time per step: 0.037 Avg Reward (Last 100): 15.525 Epsilon: 0.100\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "for i in range(num_episodes):\n",
    "    total_t, episode_reward, duration, num_steps_in_episode, time_per_step, epsilon = play_one(\n",
    "        env,\n",
    "        total_t,\n",
    "        rb,\n",
    "        model,\n",
    "        opt,\n",
    "        target_model,\n",
    "        im_transformer,\n",
    "        gamma,\n",
    "        batch_sz,\n",
    "        epsilon,\n",
    "        epsilon_change,\n",
    "        epsilon_min,\n",
    "      )\n",
    "\n",
    "    episode_rewards[i] = episode_reward\n",
    "\n",
    "    last_100_avg = episode_rewards[max(0, i - 100):i + 1].mean()\n",
    "    print(\"Episode:\", i,\n",
    "        \"Duration:\", duration,\n",
    "        \"Num steps:\", num_steps_in_episode,\n",
    "        \"Reward:\", episode_reward,\n",
    "        \"Training time per step:\", \"%.3f\" % time_per_step,\n",
    "        \"Avg Reward (Last 100):\", \"%.3f\" % last_100_avg,\n",
    "        \"Epsilon:\", \"%.3f\" % epsilon\n",
    "      )\n",
    "    sys.stdout.flush()\n",
    "print(\"Total duration:\", datetime.now() - t0)\n",
    "model.save_weights('model.h5')\n",
    "# Plot the smoothed returns\n",
    "y = running_avg(episode_rewards)\n",
    "plt.plot(episode_rewards, label='orig')\n",
    "plt.plot(y, label='smoothed')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Breakout DQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
