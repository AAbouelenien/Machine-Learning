{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start our introduction by some real-world examples of A/B testing, there is no real order to these examples\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Medicine</h3>\n",
    "\n",
    "The first example is in medicine\n",
    "\n",
    "Lets say we are a pharmaceutical company\n",
    "\n",
    "We just made a new drug and we want to know if our drug works or not\n",
    "\n",
    "If our drug works, then we can bring it to the market and make lots of money\n",
    "\n",
    "The question is, how do we know whether or not the drug works ?\n",
    "\n",
    "Well lets say the drug is for reducing blood pressure\n",
    "\n",
    "Then we would like to answer the question, does this drug reduce blood pressure to a significant degree ?\n",
    "\n",
    "To answer the question, we would have to run an experiment\n",
    "\n",
    "In our experiment, we will have two groups of people, typically called the control group and the treatment group\n",
    "\n",
    "The treatment group would receive our drug, and the control group would receive a placebo\n",
    "\n",
    "We can think of that (placebo) as pill that does not actually have and medicinal effects, its like a sugar pill\n",
    "\n",
    "Then, after the people in our experiment have taken their pills, we are going to measure their blood pressure\n",
    "\n",
    "We are going to write down each measurement, and we are going to compare the outcome\n",
    "\n",
    "Now realistically, we might actually want to do something like measure the difference in blood pressure before and after taking the drug, but thats outside the scope of this notebook\n",
    "\n",
    "In fact, their are a lot of details that are outside the scope of this notebook, but this is just meant to be an example, so we just pretend for the sake of this section that they do not exist :)\n",
    "\n",
    "----\n",
    "\n",
    "Its important to remember that everyone is different\n",
    "\n",
    "So for example, one person might have a starting blood pressure of 140, and after they take the drug it goes down to 130\n",
    "\n",
    "But for another person, they might have a starting blood pressure of 130, and they are in the placebo group, so they take a sugar pill and their blood pressure remains at 130\n",
    "\n",
    "In other words, there is hoing to be some overlap between the two groups\n",
    "\n",
    "<img src='extras/50.1.PNG' width='200'></img>\n",
    "\n",
    "So how can we give a definitive answer for whether the drug is working or not ?\n",
    "\n",
    "Well the answer is, we need to do some calculations\n",
    "\n",
    "Of course these calculations, are exactly what this course is about\n",
    "\n",
    "So lets move on to the next example\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Making a Website</h3>\n",
    "\n",
    "Here is another example that we should be able to relate to\n",
    "\n",
    "We have all at some point tried to make a website, perhaps we have ven created a website to try to sell things online and try to get users to sign up for our newsletter\n",
    "\n",
    "In other words, our site has some performance metric, typically we call this a conversion rate\n",
    "\n",
    "So, for example, out of 1000 people who visit our website, one person might buy something, or out of 1000 people who visit our website, maybe 10 people might sign up for our newsletter\n",
    "\n",
    "In these examples, the conversion rates would be 0.1% and 1%, respectively\n",
    "\n",
    "---\n",
    "\n",
    "Now obviously, we want to be successful businessmen, and in order to do that, we must maximise our conversion rate\n",
    "\n",
    "In other words, whats better than one person ot of 1000 buying something ?\n",
    "\n",
    "Well, how about two or three or four \n",
    "\n",
    "So we want as many people as possible to buy our prodcuts\n",
    "\n",
    "Now of course, people buying a product in our website is highly reliant on how attractive and trustworthy our website is\n",
    "\n",
    "If our website does not look good, or our website does not comple people to buy our prodcuts, or our website does not seem trustworthy, then people might avoid buying things from our website\n",
    "\n",
    "Now we, as businessmen, decide that the best option for us is to hire a copywritter to write some to write some content for our sales page\n",
    "\n",
    "Our copywritter gives us several options, and its up to us to decide which one we like best\n",
    "\n",
    "In other words, we want to again, compare these different options by considering which one, gives us the highest conversion rate\n",
    "\n",
    "Now this seems rather straightforward, but in fact, there are some statistical details we want to keep in mind, luckily these are also covered in the following notebooks\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Local Business Flyers</h3>\n",
    "\n",
    "Here is another example, lets say we run a local business and we are handing out flyers to people in our neighborhood\n",
    "\n",
    "We have two or three design for the flyer that our desinger gave us, and we want to find which one is the best\n",
    "\n",
    "Of course, best can be defined by the rate of people that call us back from each flyer\n",
    "\n",
    "Again, the same technique we discussed previously can be used for this task also\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Cloud Storage</h3>\n",
    "\n",
    "Here is another example, lets suppose we run a cloud storage service\n",
    "\n",
    "We need to order some hard drives, but we cant decide between Seagate and Western Digital\n",
    "\n",
    "Of course, we know that with repeated use, our hard drives will eventually fail\n",
    "\n",
    "The question is not if they will fail, but when they will fail\n",
    "\n",
    "Now. obviously, we would like if our hard drives could last as long as possible so we can keep our cost down\n",
    "\n",
    "Luckily, as we might guess, this is a job for A/B Testing!\n",
    "\n",
    "As a side note, this is a very famous example, as the company Backblaze, which specialises in doing online backups, does a periodic report that lists out the stats for each hard drive that they have tested in the recent past\n",
    "\n",
    "---\n",
    "\n",
    "<h3>What is the pattern?</h3>\n",
    "\n",
    "So what does all these examples have in common ?\n",
    "\n",
    "Well hopefully we realise that the pattern was, we needed to compare two or more items\n",
    "\n",
    "<img src='extras/50.2.PNG' width='150'></img>\n",
    "\n",
    "Mathematically, each of these items will produce a number\n",
    "\n",
    "And what we are usually looking for, is one group to have a higher number or a lower number than the other group/groups\n",
    "\n",
    "So for example, if we are measuring how long a hard drive lasts, we would want that number to be higher\n",
    "\n",
    "If we are measuring blood pressure, we might want that number to be lower\n",
    "\n",
    "The question is, how do we detect these differences ?\n",
    "\n",
    "Thats what the techniques from the following notebooks will teach us\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to discuss the question, what is bayesian Mmachine learning ?\n",
    "\n",
    "And we are also going to talk a little bit more about what the following notebooks are about\n",
    "\n",
    "---\n",
    "\n",
    "<h3>What is Bayesian Machine Learning</h3>\n",
    "\n",
    "Common mistakes are made in the definition of Bayesian machine learning\n",
    "\n",
    "We had some exposure to the concept of bais, maybe as early as grade school where we learned about bayes rule\n",
    "\n",
    "Of course, thats not machine learning per say, but rather just basic probability\n",
    "\n",
    "We might think we have encountered bayesian machine learning before in the context of regularisation\n",
    "\n",
    "In models like linear regression, this corresponds to the maximum a posteriori or MAP estimate\n",
    "\n",
    "This is not bayesian machine learning though\n",
    "\n",
    "In college/university, there are actually some different fields in which we may encounter bayes again\n",
    "\n",
    "As we might already know, usually the statistics department is seperate from the computer science department\n",
    "\n",
    "And so we will have the statistics folks in one area of the school, and the machine learning folks in another area of the school\n",
    "\n",
    "Each department has different priorities and different paradigms of thinking\n",
    "\n",
    "They often even use different programming languages\n",
    "\n",
    "Now the reason we mention this is because whil we may have had some exposure to Bayesian ideas in the past, our exposure was probably isolated to one of these two sides\n",
    "\n",
    "So we may have studied bayesian concepts from the perspective of statistics, but we would not know about Bayesian concepts from the perspective of machine learning, which are very different\n",
    "\n",
    "Overall, the lesson of this section is that we have to disregard our preconceived notions about what Bayesian machine learning is and to remember that there are many faces on this vast topic\n",
    "\n",
    "This lecture is going to give us a high level overview of all the different areas we may encounter of our study of Bayesian macine learning in Bayesian statistics\n",
    "\n",
    "---\n",
    "\n",
    "<h3>What we hope to cover</h3>\n",
    "\n",
    "The first area we want to talk about is what this course covers\n",
    "\n",
    "We are going to focus on the specific application of AB testing, but there's also the undercurrent of Bayesian the machine learning\n",
    "\n",
    "Specifically, this can be considered as  an introduction to bayesian machine learning because it introduces us to what it means to be Bayesian :o\n",
    "\n",
    "We do this by comparing traditional frequentist, methods to Bayesian methods\n",
    "\n",
    "This is the same approach we took when we learned about the numpy stack\n",
    "\n",
    "The idea wasn't just to dive into numpy for no reason, the idea was to compare the same operations in both pure Python and in nupmy so we could see the difference and convince ourselves that there's a real reason why we would want to use numpy\n",
    "\n",
    "Similarly for these notebooks, we're going to compare these two approaches and we are going to understand why we should take the Bayesian approach as opposed to the traditional frequentist approach.\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Bayesian Approach</h3>\n",
    "\n",
    "The Bayesian approach generally follows the rule, everything is random\n",
    "\n",
    "To be more precise, everything is a random variable\n",
    "\n",
    "Now, what do we mean by that?\n",
    "\n",
    "Well, let's take a simple distribution like the Gaussian \n",
    "\n",
    "<img src='extras/50.3.PNG' width='300'></img>\n",
    "\n",
    "For the Gaussian distribution, We have two parameters, the mean and the variance.\n",
    "\n",
    "To give ourselves a concrete example, suppose we want to model the height of all the students of our class using a Gaussian.\n",
    "\n",
    "So we want to find the mean and variance of the height of the people in our class \n",
    "\n",
    "In order to find these values, here is what a frequentist would do\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Frequentist Approach</h3>\n",
    "\n",
    "They would say, collect a bunch of data, meaning measure the height of everyone in our class.\n",
    "\n",
    "$$\\text{Data} : X = \\{ x_1,x_2,\\cdots,x_N\\}$$\n",
    "\n",
    "Then, using maximum likelihood estimation, we would find the mean and variance \n",
    "\n",
    "To do this, we first calculate the likelihood function.\n",
    "\n",
    "$$\\text{Likelihood} : L = \\prod^N_{i=1}f(x;\\mu,\\sigma^2)$$\n",
    "\n",
    "Then we maximize the likelihood with respect to the Gaussian mean and variance\n",
    "\n",
    "Pretty simple, and we have done this before in previous notebooks\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Bayesian Approach</h3>\n",
    "\n",
    "The Bayesian treats this problem differently.\n",
    "\n",
    "Remember that we're following the motto, <strong>Everything is a random variable</strong>\n",
    "\n",
    "So with the frequentist paradigm, when we solve for the mean $\\mu$ and variance $\\sigma^2$, these both just give us a number\n",
    "\n",
    "In the Bayesian paradigm, the mean and variance are random variables, and our job is to find their probability distributions $p(\\mu,\\sigma^2\\vert X)$\n",
    "\n",
    "Now, of course, the details of this calculation are what the following notebooks are about\n",
    "\n",
    "So we're not going to actually discuss that right now\n",
    "\n",
    "But suffice it to say, that's the major difference between the frequentist and Bayesian paradigms \n",
    "\n",
    "Instead of finding a number, our job is to find a distribution.\n",
    "\n",
    "And again, in terms of the big picture, this is our introduction to the field of Bayesian machine learning\n",
    "\n",
    "It's a lesson on how to go from non-Bayesian methods to Bayesian methods\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Sampling</h3>\n",
    "\n",
    "Now, of course, our learning doesn't stop here if we go the statistics route that a lot of our attention will be focused on sampling\n",
    "\n",
    "There are many different kinds of sampling, such as \n",
    "<ul>\n",
    "    <li>Importance sampling</li>\n",
    "    <li>Markov Chain Monte Carlo (MCMC)</li>\n",
    "    <li>Gibbs</li>    \n",
    "</ul>\n",
    "\n",
    "The idea behind sampling methods is to do numerical approximation of an integral\n",
    "\n",
    "In fact, when some people talk about Monte Carlo, they talk about it as a technique of numerical integration rather than a technique of statistics\n",
    "\n",
    "So this is interesting for someone who has studied other areas of machine learning, such as deep learning\n",
    "\n",
    "With deep learning, we will find that the main skill we apply from calculus is differentiation, which is often synonymous with the course calculus 1 \n",
    "\n",
    "With Bayesian machine learning, the main skill we apply from calculus is integration, which is often synonymous with the course calculus 2\n",
    "\n",
    "Now it's important to remember that this on its own is not a small topic\n",
    "\n",
    "Sampling by itself could be one or more courses,\n",
    "that's how complex this topic really is\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Machine Learning Models</h3>\n",
    "\n",
    "Now, there is a whole other direction we could go, the direction of machine learning and creating predictive models \n",
    "\n",
    "Lets take a very simple model such as linear regression\n",
    "\n",
    "This is the first model we usually learn about when we learn about machine learning\n",
    "\n",
    "It has a very basic equation\n",
    "\n",
    "$$\\large y = w^Tx$$\n",
    "\n",
    "In other words, our prediction is the DOT products between the weights $W$ and the features $X$ \n",
    "\n",
    "We call $W$ the parameter and our job is to find it given some data set $X$ and $Y$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Machine Learning Models (Bayesian)</h3>\n",
    "\n",
    "And so we should have an idea where we are going with this, we have to recall our Bayesian rule that everything is random \n",
    "\n",
    "In the Bayesian paradigm, remember that the parameters are not just numbers, but rather distributions\n",
    "\n",
    "So in Bayesian linear regression, for example, we're not interested in finding the vector $w$, but the distribution of the random vector $w$ \n",
    "\n",
    "We want to find \n",
    "\n",
    "$$\\large p(w \\vert X,Y)$$\n",
    "\n",
    "Where $X$ and $Y$ is our training data\n",
    "\n",
    "---\n",
    "\n",
    "And of course, the same concept can be applied to many other classic machine learning models we already know and love\n",
    "\n",
    "Some examples are logistic regression, neural networks, Gaussian mixture models, which are like a superpowered version of KMeans clustering, PCA, also known as Principal Components Analysis and Matrix Factorization, which is used for recommender systems\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Bayesian Networks</h3>\n",
    "\n",
    "The concepts of Bayesian machine learning can be generalised further to give us what are called Bayesian networks\n",
    "\n",
    "Whereas the previous examples were specific models, Bayesian networks are a general model \n",
    "\n",
    "Using Bayes Nets we can create custom models with any kinds of dependencies we like\n",
    "\n",
    "And this way we are actually modeling causation\n",
    "\n",
    "This, of course, can be based on our understanding of the system we are trying to model\n",
    "\n",
    "So, for example, suppose we want to predict whether or not a student will pass our exam\n",
    "\n",
    "We could have input features such as\n",
    "\n",
    "<ul>\n",
    "    <li>number of hours spent studying</li>\n",
    "    <li>number of hours spent playing video games</li>\n",
    "    <li>number of hours spent sleeping</li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Non-Bayesian Example</h3>\n",
    "\n",
    "So if you use that non-basic model like a neural network, we might just plug these into the input of the neural network and calculate our output based on a fully connected neural net\n",
    "\n",
    "<img src='extras/50.4.PNG' width='600'></img>\n",
    "\n",
    "In other words, everything is connected to everything, and the neural networks job is to find the right weights\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Bayesian Network</h3>\n",
    "\n",
    "The Bayes net method is to actually model dependencies\n",
    "\n",
    "So we might say, number of hours spent playing video games actually has an effect on the number of hours spent sleeping \n",
    "\n",
    "Then the exam score might depend directly on the number of hours spent studying and the number of hours spent sleeping with only an indirect effect from the number of hours playing video games\n",
    "\n",
    "<img src='extras/50.5.PNG' width='400'></img>\n",
    "\n",
    "\n",
    "So with Bayes Net, we can model these dependencies explicitly, whereas with neural networks we always use the same model and let the neural network figure out what the weight should be\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Latent Dirichlet Allocation (LDA)</h3>\n",
    "\n",
    "Lastly, there are some very famous examples of Bayes nets, such as LDA or Latent Dirichlet allocation which is an algorithm we use for topic modeling\n",
    "\n",
    "<img src='extras/50.6.PNG' width='400'></img>\n",
    "\n",
    "This is a separate example because while it's a specific instance of a Bayesian network, it doesn't have its roots in a non Bayesian model such as logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so in the following lectures, we are going to review some of the basic concepts in probability\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Probability Review</h3>\n",
    "\n",
    "\n",
    "So let's get started with some of the basic kinds of distributions that we can have in probability\n",
    "\n",
    "These are the joint, the marginal and the conditional distributions \n",
    "\n",
    "\n",
    "For this section, we are going to assume that we have two random variables \n",
    "\n",
    "In this subsection, we refer to these random variables as $A$ and $B$ \n",
    "\n",
    "In this context, the \n",
    "\n",
    "$$\\text{Marginal Distributions} : p(A),p(B)$$\n",
    "\n",
    "$$\\text{Joint Distribution} : p(A,B)$$\n",
    "\n",
    "\n",
    "Recall that the $,$ is an abbreviated way of <strong>and</strong>\n",
    "\n",
    "So this means $P(A \\text{ and } B)$ \n",
    "\n",
    "Then we have the \n",
    "\n",
    "$$ \\text{Conditional Distribution} : p(A \\vert B), p(B \\vert A)$$\n",
    "\n",
    "Recall that the $\\vert$ is the symbol that we use for saying <strong>given</strong>\n",
    "\n",
    "So this stands for a $P(A \\text{ given } B)$ and $P(B \\text{ given } A)$ \n",
    "\n",
    "Note that out of these three, the joint distribution is the most general because it is from this distribution that we can calculate everything else\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Marginalisation</h3>\n",
    "\n",
    "Let's see how that works by starting with the marginal \n",
    "\n",
    "\n",
    "if we have the joint $P(A,B)$, we can find $P(A)$ by taking the joint and summing over all possible values of $B$\n",
    "\n",
    "$$p(A) = \\sum_B p(A,B)$$\n",
    "\n",
    "if we want $P(B)$, we can take the joint again and some over all possible values of $A$\n",
    "\n",
    "$$p(B) = \\sum_A p(A,B)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Conditional Distributions</h3>\n",
    "\n",
    "We can also calculate the conditional distribution from the joint\n",
    "\n",
    "Recall that the conditional distribution $P(A \\vert B)$ is defined as \n",
    "\n",
    "$$p(A\\vert B) = \\frac{p(A,B)}{p(B)}$$\n",
    "\n",
    "Since we know that $p(B)$ can be calculated from $P(A,B)$ we know how to calculate the conditional purely from this joint distribution\n",
    "\n",
    "$$p(A\\vert B) = \\frac{p(A,B)}{p(B)} = \\frac{p(A,B)}{\\sum\\limits_A p(A,B)}$$\n",
    "\n",
    "Note that, if we wanted to calculate $p(B \\vert A)$, we could just switch around the $A$s and $B$s\n",
    "\n",
    "$$p(B\\vert A) = \\frac{p(A,B)}{p(A)} = \\frac{p(A,B)}{\\sum\\limits_B p(A,B)}$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Conditional from Conditional</h3>\n",
    "\n",
    "One interesting thing we can ask is, if we don't have the joint, but rather we only have the conditional $p(A\\vert B)$ and the marginal $p(B)$, can we calculate $p(B \\vert A)$\n",
    "\n",
    "$$\\text{Given} : p(A \\vert B), p(B)$$\n",
    "$$\\text{Want} : p(B \\vert A)$$\n",
    "\n",
    "That is the conditional, but with the symbol switched around\n",
    "\n",
    "The answer is yes\n",
    "\n",
    "Note that if we take the rule from conditional probabilities we mentioned earlier, we can just rearrange the symbols to show that\n",
    "\n",
    "$$p(A,B) = p(A \\vert B)p(B)$$\n",
    "\n",
    "That is, we can calculate the joint distribution from the conditional and the marginal\n",
    "\n",
    "Furthermore, we know that \n",
    "\n",
    "$P(B \\vert A) = \\large \\frac{p(A,B)}{p(A)}$\n",
    "\n",
    "But $p(A) = \\sum\\limits_B p(A,B)$, so we get\n",
    "\n",
    "$P(B \\vert A) = \\large \\frac{p(A,B)}{p(A)} = \\frac{p(A,B)}{\\sum\\limits_B p(A,B)}$\n",
    "\n",
    "However, if we want to express this in terms of our original givens, then we can replace $p(A,B)$ with $P(A\\vert B)p(B)$, so we get\n",
    "\n",
    "$P(B \\vert A) = \\large \\frac{p(A,B)}{p(A)} = \\frac{p(A,B)}{\\sum\\limits_B p(A,B)} = \\frac{p(A|B)p(B)}{\\sum\\limits_B p(A|B)p(B}$\n",
    "\n",
    "In fact, we have just arrived at Bayes Rule, which is essentially the centrepiece of this topic\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Discrete vs Continuous Random Variables</h3>\n",
    "\n",
    "Next, let's recognise that in the previous part of this section, we have assumed that $A$ and $B$ were discrete random variables.\n",
    "\n",
    "But what if they were a continuous random variables?\n",
    "\n",
    "Let's call these $x$ and $y$\n",
    "\n",
    "In this case, the lowercase $p$ does not refer to probability, but rather probability density\n",
    "\n",
    "However, all of the rules we just discussed still hold\n",
    "\n",
    "The main difference is that instead of <strong>sum</strong>, we have <strong>integrals</strong>\n",
    "\n",
    "Its still the case that the joint distribution is the most general\n",
    "\n",
    "If we have the joint and we want to find the marginal, say, $p(x)$, then we would integrate the joint over $y$\n",
    "\n",
    "Likewise, if we want $p(y)$, then we would integrate the joint over $x$\n",
    "\n",
    "$$\\text{Joint}: p(x,y)$$\n",
    "\n",
    "$$\\text{Marginal}: p(x) = \\int p(x,y)dy, \\ \\ p(y) = \\int p(x,y)dx$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Conditional Distributions</h3>\n",
    "\n",
    "Note that conditional probability densities, are now the ratio of the joint density to the marginal density\n",
    "\n",
    "$$p(x\\vert y) = \\frac{p(x,y)}{p(y)} = \\frac{p(x,y)}{\\int p(x,y) dx}$$\n",
    "\n",
    "$$p(y\\vert x) = \\frac{p(x,y)}{p(x)} = \\frac{p(x,y)}{\\int p(x,y) dy}$$\n",
    "\n",
    "Again, this is analogous to what we had before\n",
    "\n",
    "It is also the case that we can replace the marginal on the denominator with the integral over the joint since as we just discussed that is how we calculate the marginal\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Bayes' Rule</h3>\n",
    "\n",
    "Furthermore, Bayes rule is also expressed analogously\n",
    "\n",
    "Again, we simply replace the sum, with an integral over the relevant random variable\n",
    "\n",
    "$$p(x,y) = \\frac{p(y \\vert x) p(x)}{\\int p(y \\vert x) p(x) dx}$$\n",
    "\n",
    "$$p(y,x) = \\frac{p(x \\vert y) p(y)}{\\int p(x \\vert y) p(y) dy}$$\n",
    "\n",
    "Now we should be warned that, this is not just some theoretical exercise\n",
    "\n",
    "In fact, most of the random variables will be working with in the following notebooks are continuous\n",
    "\n",
    "So we should not get scared when you see integrals\n",
    "\n",
    "They are essentially everywhere in Bayesian statistics and Bayesian machine learning\n",
    "\n",
    "So if you want to do Bayesian machine learning, then we must get comfortable with seeing and working with integrals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the previous review rection may seem a bit abstract, but luckily these concepts are immediately applicable in the real world\n",
    "\n",
    "These following notebooks are all about large scale business problems\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Example</h3>\n",
    "\n",
    "So let's consider this\n",
    "\n",
    "Say we have a Web site with users from the U.S., Canada and Mexico\n",
    "\n",
    "Suppose that we have a table like the one below where we have counted how many users from each country came to our Web site and either bought something or didn't buy something \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>CA</td>\n",
    "        <td>US</td>\n",
    "        <td>MX</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Buy = True</td>\n",
    "        <td>20</td>\n",
    "        <td>50</td>\n",
    "        <td>10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Buy = False</td>\n",
    "        <td>300</td>\n",
    "        <td>500</td>\n",
    "        <td>200</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Using these counts, which we could grab using a database query, let's suppose we want to calculate the probability that a user buys something from our Web site given what country they are from\n",
    "\n",
    "This is useful information for us as a website owner\n",
    "\n",
    "Why?\n",
    "\n",
    "Well, if we find that our Web site is underperforming in one country, then we should look into why that might be the case\n",
    "\n",
    "Perhaps our Web site goes too slow for those users because our data center is too far away\n",
    "\n",
    "So we might look into how we can store our Web site's content closer to those users\n",
    "\n",
    "Or perhaps the products that we show on our front page are not relevant to those users\n",
    "\n",
    "For example, tools for ice fishing are probably more relevant to Canadians than they are to Mexicans or Americans\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Marginal</h3>\n",
    "\n",
    "So how do we calculate probabilities from counts?\n",
    "\n",
    "Well, we probably already know this intuitively\n",
    "\n",
    "So let's just go ahead and calculate some values\n",
    "\n",
    "Let's say we want $p(\\text{country})$, that is the probability that a user comes from a particular country\n",
    "\n",
    "Intuitively, we know that this is just the count of users who came from that country divided by the number of all users\n",
    "\n",
    "As we know, the frequentist interpretation of probability, says that as the number of users approaches infinity, this estimate will approach the true probability\n",
    "\n",
    "So anyway, what do we get?\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>CA</td>\n",
    "        <td>US</td>\n",
    "        <td>MX</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Buy = True</td>\n",
    "        <td>20</td>\n",
    "        <td>50</td>\n",
    "        <td>10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Buy = False</td>\n",
    "        <td>300</td>\n",
    "        <td>500</td>\n",
    "        <td>200</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Well, we get that \n",
    "\n",
    "$$p(country=MX) = \\frac{210}{210+550+320} = 0.19$$\n",
    "\n",
    "$$p(country=US) = \\frac{550}{210+550+320} = 0.51$$\n",
    "\n",
    "$$p(country=CA) = \\frac{320}{210+550+320} = 0.30$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Joint Probabilities</h3>\n",
    "\n",
    "What about the joint probabilities?\n",
    "\n",
    "Let's first think about which probabilities we will need to calculate \n",
    "\n",
    "how many joint probabilities are ther?\n",
    "\n",
    "In fact, there are six\n",
    "\n",
    "Let's recall an important rule\n",
    "\n",
    "The number of probability values increases exponentially as we add a more and more random variables\n",
    "\n",
    "Suppose we have $N$ random variables $x_1$ up to $x_N$ then the number of probability values in our joint distribution is the cardinality of $x_1$  times the cardinality of $x_2$ times the cardinality of $x_3$, all the way up to the cardinality of $x_N$\n",
    "\n",
    "$$\\text{Volume} = \\vert x_1 \\vert \\times \\vert x_2 \\vert \\times \\vert x_3 \\vert \\times \\ldots \\times \\vert x_N \\vert$$\n",
    "\n",
    "Generally speaking, recall that this is a bad thing because we need exponentially more values to get an accurate estimate of these probabilities as we add more and more random variables\n",
    "\n",
    "This is known as the <strong>curse of dimensionality</strong>\n",
    "\n",
    "---\n",
    "\n",
    "In any case, let's calculate these probabilities\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>CA</td>\n",
    "        <td>US</td>\n",
    "        <td>MX</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Buy = True</td>\n",
    "        <td>20</td>\n",
    "        <td>50</td>\n",
    "        <td>10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Buy = False</td>\n",
    "        <td>300</td>\n",
    "        <td>500</td>\n",
    "        <td>200</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "We get that\n",
    "\n",
    "$$p(\\text{Buy=True, Country=CA}) = \\frac{20}{1080} = 0.019$$\n",
    "\n",
    "$$p(\\text{Buy=False, Country=CA}) = \\frac{300}{1080} = 0.28$$\n",
    "\n",
    "$$p(\\text{Buy=True, Country=US}) = \\frac{50}{1080} = 0.046$$\n",
    "\n",
    "$$p(\\text{Buy=False, Country=US}) = \\frac{500}{1080} = 0.46$$\n",
    "\n",
    "$$p(\\text{Buy=True, Country=MX}) = \\frac{10}{1080} = 0.0093$$\n",
    "\n",
    "$$p(\\text{Buy=False, Country=MX}) = \\frac{200}{1080} = 0.0185$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Why so many zeros?</h3>\n",
    "\n",
    "One thing we should recognize from these joint probabilities is that there are a lot more zeros\n",
    "\n",
    "Why are these values so much smaller than the marginal probabilities?\n",
    "\n",
    "Remember that the sum of all outcomes in a probability distribution must sum to one\n",
    "\n",
    "As we just discussed, the volume of the probability space grows exponentially as we add a more and more random variables\n",
    "\n",
    "Therefore, the actual probabilities must get smaller and smaller exponentially because no matter how large the volume gets, it still must sum to one\n",
    "\n",
    "Note that this is actually a problem computationally\n",
    "\n",
    "What happens when our probabilities get very small, like $10^{-500}$ ? \n",
    "\n",
    "In these cases, usually what happens is those probabilities get rounded down to zero\n",
    "\n",
    "Remember that 32 bit floating point numbers only have 32 binary digits to represent them\n",
    "\n",
    "This is one reason why in statistics we like to work with log probabilities instead of probabilities\n",
    "\n",
    "As an exercise, we should try to figure out what is the smallest number that can be represented in Python\n",
    "\n",
    "```python\n",
    "import sys\n",
    "print(sys.float_info.min) \n",
    "```\n",
    "---\n",
    "\n",
    "<h3>Conditional Probabilities</h3>\n",
    "\n",
    "Next, let's go back to what we were originally trying to do, which was to calculate the conditional probabilities\n",
    "\n",
    "As we recall from our review, the conditional is the joint divided by the marginal, which we just calculated\n",
    "\n",
    "Therefore, calculating the conditionals is just one more step\n",
    "\n",
    "Note that there is some roundoff error in these values\n",
    "\n",
    "Therefore, if we would like as an exercise, we might want to try to save more digits to try and calculate these numbers with higher precision\n",
    "\n",
    "$$p(\\text{Buy=True \\vert Country=CA}) = \\frac{0.19}{0.30} = 0.07$$\n",
    "\n",
    "$$p(\\text{Buy=False \\vert Country=CA}) = \\frac{0.28}{0.30} = 0.93$$\n",
    "\n",
    "$$p(\\text{Buy=True \\vert Country=US}) = \\frac{0.046}{0.51} = 0.09$$\n",
    "\n",
    "$$p(\\text{Buy=False \\vert Country=US}) = \\frac{0.46}{0.51} = 0.91$$\n",
    "\n",
    "$$p(\\text{Buy=True \\vert Country=MX}) = \\frac{0.0093}{0.19} = 0.05$$\n",
    "\n",
    "$$p(\\text{Buy=False \\vert Country=MX}) = \\frac{0.185}{0.19} = 0.97$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Conditional Probabilities - Alternate Calculation</h3>\n",
    "\n",
    "Note that there is another way to calculate these conditional probabilities\n",
    "\n",
    "To use an example, let's suppose that we want to calculate $p(\\text{But=True \\vert Country = US})$ \n",
    "\n",
    "Using our expressions, not the rounded-off values, we get the following\n",
    "\n",
    "$$p(\\text{Buy=True}\\vert \\text{Country=US}) = \\frac{p(\\text{Buy=True,Country=US})}{p(\\text{Country=US})} = \\frac{50/1080}{(50+500)/1080}$$\n",
    "\n",
    "But notice something interesting\n",
    "\n",
    "The $1080$ term appears on both the top and bottom expressions, so it effectively cancels out\n",
    "\n",
    "In other words, the conditional can be calculated as follows\n",
    "\n",
    "$$p(\\text{Buy=True}\\vert \\text{Country=US}) = \\frac{50}{50+550}$$\n",
    "\n",
    "It's just the number of users from the US for which $\\text{Buy = True}$ , divided by just the number of people from the US\n",
    "\n",
    "We do not need to consider users from other countries\n",
    "\n",
    "Put another way,we only need to consider data in our table from the $\\text{Country = US}$ column\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>CA</td>\n",
    "        <td>US</td>\n",
    "        <td>MX</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Buy = True</td>\n",
    "        <td>20</td>\n",
    "        <td>50</td>\n",
    "        <td>10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Buy = False</td>\n",
    "        <td>300</td>\n",
    "        <td>500</td>\n",
    "        <td>200</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Similar but different problem</h3>\n",
    "\n",
    "Next, let's consider a similar but different problem with slightly different numbers \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>CA</td>\n",
    "        <td>US</td>\n",
    "        <td>MX</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Buy = True</td>\n",
    "        <td>20</td>\n",
    "        <td>50</td>\n",
    "        <td>10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Buy = False</td>\n",
    "        <td>180</td>\n",
    "        <td>450</td>\n",
    "        <td>90</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Now that we know how to calculate conditional probabilities, let's calculate the probability of $\\text{Buy = True}$ for all countries separately\n",
    "\n",
    "After doing this calculation, we get the following\n",
    "\n",
    "$$p(\\text{Buy = True | Country=CA}) = \\frac{20}{180} = 0.1$$\n",
    "\n",
    "$$p(\\text{Buy = True | Country=US}) = \\frac{50}{450} = 0.1$$\n",
    "\n",
    "$$p(\\text{Buy = True | Country=MX}) = \\frac{10}{90} = 0.1$$\n",
    "\n",
    "\n",
    "In this case we see that they are all the same\n",
    "\n",
    "What does this mean?\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Independance</h3>\n",
    "\n",
    "This is a concept in probability known as independence\n",
    "\n",
    "What does it mean for two random variables to be independent?\n",
    "\n",
    "Intuitively, it means that knowing the value of one random variable doesn't tell us anything about the other\n",
    "\n",
    "Let's consider an example\n",
    "\n",
    "Suppose random variable $A$ represents the result of a coin toss, where this coin is a coin that someone is going to take out of his pocket and flip right now\n",
    "\n",
    "Suppose that random variable $B$ represents the result of a coin toss, with that coin, is a coin that we will take out of our pocket and flip whenever we read this notebook, wherever we are in the world\n",
    "\n",
    "Clearly, the result of the other person's coin toss has no bearing at all on our coin toss\n",
    "\n",
    "In fact, if We take our coin and flip it twice, the first result is completely independent of the second result and vice versa\n",
    "\n",
    "Mathematically, we can say that two random variables are independent, if and only if there are joint distribution, $p(A,B)$ is equal to the product of their marginal distributions\n",
    "\n",
    "$$A \\perp B \\text{ iff } p(A,B) = p(A)p(B)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Independance</h3>\n",
    "\n",
    "What does independence mean for conditional distributions?\n",
    "\n",
    "Let's suppose that $Buy$ and $Country$ are independent, so we know that\n",
    "\n",
    "$$\\text{Buy} \\perp \\text{Country} \\leftrightarrow p(\\text{Buy,Country}) = p(\\text{Buy})p(\\text{Country})$$\n",
    "\n",
    "If we start with the rule of conditional probability, where the conditional is the joint divided by the marginal\n",
    "\n",
    "$$p(\\text{Buy|Country}) = \\frac{p(Buy,Country)}{p(Country)}$$\n",
    "\n",
    "what do we get?\n",
    "\n",
    "Well, we can replace the joint $p(\\text{Buy,Country})$ with the product $p(\\text{Buy})p(\\text{Country})$, and remember, we can only do this if $\\text{Buy}$ and $\\text{Country}$ are independent\n",
    "\n",
    "So we gey\n",
    "\n",
    "$$p(\\text{Buy|Country}) = \\frac{p(\\text{Buy})p(\\text{Country})}{p(Country)}$$\n",
    "\n",
    "This now becomes a matter of basic algebra\n",
    "\n",
    "$\\require{\\cancel}$\n",
    "\n",
    "$$p(\\text{Buy|Country}) = \\frac{p(\\text{Buy})\\cancel{p(\\text{Country})}}{\\cancel{p(Country)}} = p(\\text{Buy})$$\n",
    "\n",
    "Therefore, we've derived another way of stating independence in terms of probability distributions\n",
    "\n",
    "If $Buy$ and $Country$ are independent, then $p(\\text{Buy|Country}) = p(\\text{Buy})$\n",
    "\n",
    "Obviously if we flip around the symbols, we will get the same thing for $p(\\text{Country|Buy})$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Back to out problem</h3>\n",
    "\n",
    "Going back to our second example, we can see that this is true for our dataset\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>CA</td>\n",
    "        <td>US</td>\n",
    "        <td>MX</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Buy = True</td>\n",
    "        <td>20</td>\n",
    "        <td>50</td>\n",
    "        <td>10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Buy = False</td>\n",
    "        <td>180</td>\n",
    "        <td>450</td>\n",
    "        <td>90</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "If we calculate $p(\\text{Buy})$, which is just the total number of users for which $\\text{Buy=True}$, divided by all users, we again get $0.1$\n",
    "\n",
    "This is equal to all of the distributions conditioned on $\\text{Country}$ as per our definition of independence\n",
    "\n",
    "Therefore, we can conclude that for this particular dataset $\\text{Buy}$ is independent of $\\text{Country}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to do a simple probability exercise to test our understanding of independence\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Simple Probability Exercise</h3>\n",
    "\n",
    "Previously, we mention that coin tosses are independent\n",
    "\n",
    "There's no way that one coin toss will be related to the outcome of another coin toss\n",
    "\n",
    "Well, at least this is the case if what we've learned about physics is true :)\n",
    "\n",
    "So let's assume that coin tosses are indeed independent\n",
    "\n",
    "Let's also assume that they are identically distributed, that is, the probability of heads will not change from one coin toss to the next, that would be quite silly\n",
    "\n",
    "Together, we usually call this iid, which stands for independent and identically distributed\n",
    "\n",
    "That means each outcome is independent and they all came from the same distribution\n",
    "\n",
    "All right, so for this exercise, we want to use actual numbers\n",
    "\n",
    "So let's make the reasonable assumption that our coin is a fair coin\n",
    "\n",
    "That is $P(heads) = P(tails) = 0.5$\n",
    "\n",
    "Hopefully, we can agree that this is reasonable\n",
    "\n",
    "Next, suppose that we plan to toss the coin 200 times in total\n",
    "\n",
    "Now, tossing coins requires some amount of work\n",
    "\n",
    "So let's suppose that we got up to one hundred coin tosses and we've written down our results so far\n",
    "\n",
    "Up until this point, we've gotten 80 heads and 20 tails\n",
    "\n",
    "Note that this adds up to one hundred\n",
    "\n",
    "It seems a bit lopsided, but there's no law stating that this is not possible\n",
    "\n",
    "So, again, so far we've tossed 80 heads and 20 tails\n",
    "\n",
    "Here is the exercise question\n",
    "\n",
    "What is the total number of heads we expect to get by the end of the experiment?\n",
    "\n",
    "That is after performing the next one hundred coin tosses to finish our game\n",
    "\n",
    "How many heads in total do we think we will have gotten?\n",
    "\n",
    "How many tails do we think we will have gotten?\n",
    "\n",
    "To recap, we want to do two hundred coin tosses in total and so far we've completed 100 coin tosses\n",
    "\n",
    "we've logged 80 heads and 20 tails by the end of the experiment\n",
    "\n",
    "By the end of the experiment, how many heads and tails do we expect to count knowing the results so far?\n",
    "\n",
    "---\n",
    "\n",
    "<h3>2 \"Typical\" Answers</h3>\n",
    "\n",
    "Usually there are two options that students go with\n",
    "\n",
    "The first option is knowing that this is a fair coin, we can reasonably expect the total number of heads and tails to be one hundred each\n",
    "\n",
    "That's what fair means, after all, right?\n",
    "\n",
    "So if this is indeed a fair coin, then we should get 100 heads and 100 tails, exactly half of our tosses should go to either outcome\n",
    "\n",
    "The second option is that the past is given and it cannot affect the future outcomes, therefore, since we've already tossed the coin 100 times, we only have 100 coin tosses left \n",
    "\n",
    "Out of these 100, we can expect to get 50 heads and 50 tails\n",
    "\n",
    "So 50 heads plus the 80 heads we already got is equal to 130 heads, 50 tails, plus the 20 tails we already got is equal to 70 tails\n",
    "\n",
    "So which of these answers is correct?\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Correct Answer</h3>\n",
    "\n",
    "The correct answer is, in fact, one hundred thirty heads and seventy tails\n",
    "\n",
    "The key to understanding this is to remember that coin tosses are independent\n",
    "\n",
    "That is the probability of coin toss two given coin toss one, is just equal to the probability of coin toss two, which, as we know, is just equal to $0.5$ for both heads and tails.\n",
    "\n",
    "It does not matter what we rolled in the past\n",
    "\n",
    "The past is the past and cannot be changed \n",
    "\n",
    "Because we've already seen the outcome of those coin tosses, they are no longer considered random\n",
    "\n",
    "Therefore, the only outcomes we can try to predict are the future coin tosses and the expected values for that are 50 heads and 50 tails \n",
    "\n",
    "Added to the 80 heads and 20 tails we collected so far, the total is one hundred thirty heads and seventy tails\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Gambler's Fallacy</h3>\n",
    "\n",
    "If we guessed the incorrect answer, note that this mistake is so common that it actually has a name\n",
    "\n",
    "It's known as the gambler's fallacy\n",
    "\n",
    "This is the false belief that things will balance out in the end\n",
    "\n",
    "It should be pretty clear why this is called the gambler's fallacy\n",
    "\n",
    "Somebody goes into a casino and they lose a bunch of times\n",
    "\n",
    "They think, I've lost so many times that it must now be more probable that I win in the future, even if I only have a one out of one hundred chance of winning\n",
    "\n",
    "That means if I just lost ninety nine times, then my winning in the next round should be guaranteed\n",
    "\n",
    "Of course, this is all incorrect, that's why it's called a fallacy\n",
    "\n",
    "If all games are independent, then it does not matter how many times we have lost in the past, they have no effect on the future\n",
    "\n",
    "To put it bluntly, it doesn't matter how tired we are of losing, our chances of losing the next game are just as bad as they always have been :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to talk about a famous problem in probability that actually comes from a TV show\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Monty Hall Problem</h3>\n",
    "\n",
    "The TV show is called Let's Make a Deal, and its host was named Monty Hall, so we call this the Monty Hall problem\n",
    "\n",
    "The problem goes like this\n",
    "\n",
    "We are on a game show and we have to pick a door\n",
    "\n",
    "There are three doors, behind one of the doors,there is a car, and behind the two other doors, there are goats\n",
    "\n",
    "<img src='extras/50.7.PNG' width='400'></img>\n",
    "\n",
    "We will assume that we'd rather have a car than goat, We may find ourselves in conflict, but for the purpose of this section, let's assume that we wpuld rather have a car\n",
    "\n",
    "---\n",
    "\n",
    "<h3>How does it work?</h3>\n",
    "\n",
    "So how does this game work?\n",
    "\n",
    "Well, it goes like this.\n",
    "\n",
    "We pick a door a but you do not get to see what is behind it \n",
    "\n",
    "<img src='extras/50.8.PNG' width='400'></img>\n",
    "\n",
    "Without loss of generality, let's say you pick door number one\n",
    "\n",
    "<img src='extras/50.9.PNG' width='400'></img>\n",
    "\n",
    "Next, the host, who knows what is behind each of the doors opens one of the doors we did not choose, and he must always reveals one of the goats\n",
    "\n",
    "<img src='extras/50.9.PNG' width='500'></img>\n",
    "\n",
    "\n",
    "Let's say he opens door number two and it's a goat \n",
    "\n",
    "To make this clear, the host of the game show must open one of the doors we did not choose and he must open a door that has a goat behind it\n",
    "\n",
    "Therefore, it will always be the case that whatever door Monty Hall opens, it will be a goat\n",
    "\n",
    "The next question we have to answer is, do we stay with door number one or do we switch to door number three?\n",
    "\n",
    "Our job is to pick a door that does not contain a goat, and so the real question is, does Monty Hall opening some other door to reveal the goat help us in any way?\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Isn't it obvious</h3>\n",
    "\n",
    "Now, this might seem like a silly question at first\n",
    "\n",
    "We might think it doesn't matter if we switch or not, the probability of a car behind being door number one or door number three is still 1/3\n",
    "\n",
    "However, if this is the conclusion we came to, we should think about it a little more in the context of Bayes rule\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Solution</h3>\n",
    "\n",
    "So in this formulation, we are going to assume we chose door number one to begin with, as we stated earlier\n",
    "\n",
    "So that's a given, we can write it out we want, but it would appear in every expression, so we are not going to show it at all\n",
    "\n",
    "Now, let's say the random variable $C$ represents where the car is\n",
    "\n",
    "$C= 1$ means the car is behind door number $1$, and $C= 2$ means the car is behind door number $2$, and $C= 3$ means the car is behind door number $3$\n",
    "\n",
    "$$C : \\text{which door the car is behind} (C=1,2,3)$$\n",
    "\n",
    "Let's let the random variable $H$ represent which door Monty Hall opens\n",
    "\n",
    "Remember that in our setup, Monty Hall opens door number two without loss of generality\n",
    "\n",
    "It doesn't really matter which we use since the problem is symmetric, so let's just assume $2$\n",
    "\n",
    "$$H : \\text{which door Monty Hall opens} (assume H = 2)$$\n",
    "\n",
    "So we can define these conditional probabilities\n",
    "\n",
    "$$p(H=2|C=1) = 0.5$$\n",
    "\n",
    "$$p(H=2|C=2) = 0$$\n",
    "\n",
    "$$p(H=2|C=3) = 1$$\n",
    "\n",
    "Let's look at why these are defined this way\n",
    "\n",
    "Remember that we chose door number one\n",
    "\n",
    "So if the car is actually behind door number one, which corresponds to $C=1$, then Monty Hall will open either of the other doors\n",
    "\n",
    "He can choose either one because he just wants to show us a goat\n",
    "\n",
    "Therefore, it doesn't matter which door Monty Hall chooses either two or three, if the car is behind door number one\n",
    "\n",
    "But if we chose door number one and the car is behind that door number two, then Monty Hall cant open door number two because he doesn't want to show us the car, so the probability of that is zero\n",
    "\n",
    "So if $C=2$, then $H=2$ is not possible according to the rules of this game\n",
    "\n",
    "Similarly, if the car's behind door number three, then Monty Hall has to open door number two, because that's the only door left with a goat\n",
    "\n",
    "That is if $C=3$, then Monty Hall has no choice but to open door number two, so $H=2$ given $C=3$ with probability one hundred percent\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Solution cont.</h3>\n",
    "\n",
    "Next, let's think about what probability we are looking for\n",
    "\n",
    "We want to know if we should stick with door number one or switch to door number three\n",
    "\n",
    "So we want to calculate \n",
    "\n",
    "$$p(C=3 \\vert H=2),p(C=1 \\vert H=2)$$\n",
    "\n",
    "That is the probability of the car being behind door number three, given that Monty Hall opens door number two and the probability of the car being behind the door number one, given that Monty Hall opens door number two\n",
    "\n",
    "And of course, we should recognize that this is just Bayes rule\n",
    "\n",
    "Remember Bayes rule, conceptually speaking, is a way of switching around the givens\n",
    "\n",
    "In the previous subsection, we had $p(H|C)$ and now we want $p(X|H)$\n",
    "\n",
    "Let's start by writing out the expression for the probabilities that we won\n",
    "\n",
    "Considering $p(C=3 \\vert H=2)$ first, we have the following\n",
    "\n",
    "First, we can write this down as the joint over the marginal\n",
    "\n",
    "$$p(C=3 \\vert H=2) = \\frac{p(H=2,C=3)}{p(H=2)}$$\n",
    "\n",
    "However, we don't have either of these quantities\n",
    "\n",
    "What we do have is $p(H \\vert C)$ \n",
    "\n",
    "So let's replace the top and bottom with $p(H \\vert C)p(C)$\n",
    "\n",
    "$$p(C=3 \\vert H=2) = \\frac{p(H=2,C=3)}{p(H=2)} = \\frac{p(H=2 \\vert C=3)p(C=3)}{\\sum\\limits_{c=1}^3 p(H=2 \\vert C=c)p(C=c)}$$\n",
    "\n",
    "As we recall, this is just Bayes rule\n",
    "\n",
    "$$p(C=3 \\vert H=2) = \\frac{p(H=2 \\vert C=3)p(C=3)}{p(H=2 \\vert C=1)p(C=1) + p(H=2 \\vert C=2)p(C=2) + p(H=2 \\vert C=3)p(C=3)}$$\n",
    "\n",
    "Now, we might wonder, how do we know what $p(C)$ should be?\n",
    "\n",
    "Well, this is just the prior probability that a car is behind any one of the doors without any other information\n",
    "\n",
    "We can assume this is $\\frac{1}{3}$, since there is no reason to believe that the producers of the show would be biased towards any particular door \n",
    "\n",
    "After plugging in the relevant values, What do we get?\n",
    "\n",
    "$$p(C=3 \\vert H=2) = \\frac{1(1/3)}{(1/2)(1/3) + 0(1/3)+1(1/3)} = \\frac{2}{3}$$\n",
    "\n",
    "---\n",
    "\n",
    "Not that we can do a similar calculation for $p(C=1 \\vert H=2)$\n",
    "\n",
    "\n",
    "We dont need to derive Bayes rule again, so let's just plug in the numbers\n",
    "\n",
    "$$p(C=1 \\vert H=2) = \\frac{(1/2)(1/3)}{(1/2)(1/3) + 0(1/3) + 1(1/3)} = \\frac{1}{3}$$\n",
    "\n",
    "So we have our answer\n",
    "\n",
    "We should always switch to door number three, because that gives us double the chance of winning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to introduce the problem of maximum likelihood\n",
    "\n",
    "Before we start going into the mathematics, we want to discuss what this problem is trying to solve in the first place\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Maximum Likelihood Estimation</h3>\n",
    "\n",
    "Maximum likelihood estimation is a technique for statistical modeling\n",
    "\n",
    "Imagine that we've collected a bunch of data from an experiment\n",
    "\n",
    "Now, we would like to fit a model to that data\n",
    "\n",
    "Usually such a model comes with parameters\n",
    "\n",
    "Our job is to find the best parameters such that they model the data you collected as closely as possible\n",
    "\n",
    "To give ourselves a modern example of this, take neural networks\n",
    "\n",
    "At a high level, the learning part of deep learning is just the act of finding the weights of the neural network that best explain the data set\n",
    "\n",
    "Hence, learning is just a fancy term for model fitting\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Bernoulli Distribution</h3>\n",
    "\n",
    "OK, so for these notebooks, one of the most fundamental models we will be interested in is the Bernoulli distribution\n",
    "\n",
    "As we recall, the Bernoulli distribution describes a coin toss\n",
    "\n",
    "So, for example, we can say the $p(\\text{heads})=0.6$ and $p(\\text{tails})=0.4$  \n",
    "This is one instance of the Bernoulli distribution\n",
    "\n",
    "---\n",
    "\n",
    "Of course, we have to be more mathematical than this\n",
    "\n",
    "We need to ask ourselves, what is the equation that describes the Bernoulli distribution?\n",
    "\n",
    "As we know, the Bernoulli is a discrete distribution and it describes a discrete, random variable\n",
    "\n",
    "Therefore, the distribution is called a PMF, which stands for probability mass function\n",
    "\n",
    "In general, we can write it as follows\n",
    "\n",
    "$$p(x) = \\theta^x(1-\\theta)^{(1-x)}$$\n",
    "\n",
    "In this case, $x$ can only take on the values $0$ or $1$\n",
    "\n",
    "If we're thinking of a coin toss, then usually we would say zero is tails and one equals heads\n",
    "\n",
    "$\\theta$ is called a parameter and it is the one and only parameter of this distribution.\n",
    "\n",
    "As we may know, it's the probability that $x$ is equal to one\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Example</h3>\n",
    "\n",
    "As an exercise, we can show that $\\theta$ is equal to the $p(x=1)$\n",
    "\n",
    "If we plug in $x=1$, we get \n",
    "\n",
    "$$p(x=1) = \\theta^1(1-\\theta)^{1-1} = \\theta$$\n",
    "\n",
    "Therefore, proving that the probability that $x$ is equal to one is equal to $\\theta$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Excercise</h3>\n",
    "\n",
    "As another exercise, we can check that the probability that $x$ is equal to zero is equal to $1 - \\theta$ \n",
    "\n",
    "And therefore $p(x=0) + p(x=1) = 1$ as it should be\n",
    "\n",
    "$$p(x=0) = \\theta^0(1-\\theta)^{1-0} = 1-\\theta$$\n",
    "\n",
    "$$p(x=0) + p(x=1) = 1 - \\theta + \\theta = 1$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Problem Setup</h3>\n",
    "\n",
    "OK, so hopefully that was just an easy review\n",
    "\n",
    "The next step is to actually do maximum likelihood\n",
    "\n",
    "So let's pretend we've collected a bunch of coin tosses, $x_1$,$x_2$ all the way up to $x_N$\n",
    "\n",
    "$$data = \\{x_1,x_2,\\ldots,x_N\\}$$\n",
    "\n",
    "Now we want to ask, what is the best value of $\\theta$ to describe this data that we collected\n",
    "\n",
    "In maximum likelihood estimation, we start by writing down the likelihood \n",
    "\n",
    "Sometimes we write this as \n",
    "\n",
    "$$L(\\theta) = p(\\text{data} \\vert \\theta)$$\n",
    "\n",
    "note : $L$ is the likelihood\n",
    "\n",
    "Furthermore, note that this is equal to the product of the PMFs evaluated at each $x_i$ for $i=1 \\ldots N$\n",
    "\n",
    "$$L(\\theta) = p(\\text{data} \\vert \\theta) = \\prod^N_{i=1}p(x_i \\vert \\theta)$$\n",
    "\n",
    "\n",
    "Traditionally we write this as $p(x_i \\vert \\theta)$, even though previously we just wrote $p(x)$ \n",
    "\n",
    "In that form, the given $\\theta$ part is implicit \n",
    "\n",
    "In this subsection, we are being explicit about the fact that $\\theta$ is given.\n",
    "\n",
    "We can then simply plug in our expression for the PMF\n",
    "\n",
    "$$L(\\theta) = p(\\text{data} \\vert \\theta) = \\prod^N_{i=1}p(x_i \\vert \\theta) = \\prod^N_{i=1}\\theta^{x_i}(1-\\theta)^{1-x_i}$$\n",
    "\n",
    "So before we move on, let's understand what we've written down\n",
    "\n",
    "This is not just some random math equation, it has meaning\n",
    "\n",
    "Hopefully this meaning is readily understood\n",
    "\n",
    "This is the probability of observing the data that we observed, assuming that each coin toss was iid, independent and identically distributed\n",
    "\n",
    "---\n",
    "\n",
    "<h3>What is the likelihood a function of?</h3>\n",
    "\n",
    "It's important to understand what the likelihood is a function of and what it is not a function of\n",
    "\n",
    "Many times people see $x$s and they automatically think we usually use $x$ as a variable, therefore $x$ is the variable\n",
    "\n",
    "But that's incorrect\n",
    "\n",
    "In this case, $x$ is not a variable, these are the values from our experiment\n",
    "\n",
    "As we recall, for Bernoulli, these are just ones and zeros\n",
    "\n",
    "In actuality, it is $theta$ that is the variable \n",
    "\n",
    "Recall that $theta$ is the value that we are trying to solve for, it's the parameter of our distribution\n",
    "\n",
    "So imagine that we have three $x$s\n",
    "\n",
    "$$\\text{Example}: x_1=1, x_2=0, x_3=1$$\n",
    "\n",
    "Then we would plug them into the equation for the likelihood and we would get what we see here \n",
    "\n",
    "$$L(\\theta) = \\prod^N_{i=1} \\theta^{x_i}(1-\\theta)^{1-x_i} = \\theta^1(1-\\theta)^1\\theta^1$$\n",
    "\n",
    "As an excercise, we might want to go through this slowly and double check to make sure it's correct\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Why is it called 'maximum likelihood'?</h3>\n",
    "\n",
    "OK, so let's remember what we are doing, maximum likelihood, why is it called maximum likelihood?\n",
    "\n",
    "The purpose of establishing the likelihood function is that we now want to find which value of $\\theta$ will maximize it\n",
    "\n",
    "Hopefully that makes sense\n",
    "\n",
    "We are asking, what value of $\\theta$ makes the data that we collect most probable\n",
    "\n",
    "In other words, what value of $\\theta$ maximizes the likelihood?\n",
    "\n",
    "Obviously, if we got 100 heads and 0 tails, then saying the probability of heads is five percent would not make much sense\n",
    "\n",
    "Instead, it's more likely that the probability of heads is closer to one hundred percent\n",
    "\n",
    "By the way, intuitively we know this, or at least We hope we do\n",
    "\n",
    "If we have $N_H$ heads and $N_T$ tails, then we might estimate the probability of heads as \n",
    "\n",
    "$$p(H) \\approx \\frac{N_H}{N_H + N_T}$$\n",
    "\n",
    "So the question is, using the likelihood function, can we maximize it so that we find a value of theta that makes sense\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Maximising a function</h3>\n",
    "\n",
    "OK, so how do we maximize a function?\n",
    "\n",
    "Well, we call in our old friend calculus\n",
    "\n",
    "Specifically, we would like to find the derivative of $L$ wrt $\\theta$\n",
    "\n",
    "We can solve this by finding out which value of $\\theta$ makes this derivative zero\n",
    "\n",
    "$$\\frac{dL}{d\\theta}=0$$\n",
    "\n",
    "Note that we typically call this value $\\hat \\theta$, since $\\hat {}$ that is the usual symbol that we use for statistical \n",
    "\n",
    "So we can say that\n",
    "\n",
    "$$\\hat \\theta = \\arg \\max_\\theta L(\\theta)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Log-Likelihood</h3>\n",
    "\n",
    "Before we do this, we should mention that most of the time, it's better to take the log of the likelihood first\n",
    "\n",
    "This is simply called the log likelihood\n",
    "\n",
    "Usually it leads to a simpler expression for the derivative that's easier to set to zero and solve \n",
    "\n",
    "for the Bernoulli case, it happens that we can solve it even without taking the log, so we're encouraged to do that as an exercise on our own\n",
    "\n",
    "So why does this work?\n",
    "\n",
    "It works because the log is a monotonically increasing function\n",
    "\n",
    "What this means is that if, we find a value of $\\theta$ that maximizes the $\\log L$, then this value of $\\theta$ also maximizes $L$\n",
    "\n",
    "In fact, we can try this ourself, we can pick any two values $A$ and $B$ where $A$ is greater than $B$ \n",
    "\n",
    "Then take the $\\log$ of both $A$ and $B$\n",
    "\n",
    "It should be the case that $\\log A$ is also greater than $\\log B$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Mecahincal Work</h3>\n",
    "\n",
    "OK, so at this point, it's just a matter of mechanical work\n",
    "\n",
    "Let's take the $\\log L$ . note that we usually denote this $l$\n",
    "\n",
    "$$l(\\theta) = log  L(\\theta) = \\log \\prod^N_{i=1} \\theta^{x_i} (1-\\theta)^{1-x_i} \\\\ = \\sum^N_{i=1} \\{x_i \\log \\theta + (1-x_i) \\log (1-\\theta)\\}$$\n",
    "\n",
    "---\n",
    "\n",
    "The next step is to take the derivative of the log likelihood, as we recall, the derivative of $\\log \\theta$ is just $\\frac{1}{\\theta}$\n",
    "\n",
    "$$\\frac{dl}{d\\theta} = \\frac{1}{\\theta} \\sum^M_{i=1} x_i - \\frac{1}{1-\\theta} \\sum^N_{i=1} (1-x_i)$$\n",
    "\n",
    "---\n",
    "\n",
    "The next step is to take the derivative we just found, set it to zero in solve fot $\\theta$\n",
    "\n",
    "$$\\frac{1}{\\theta} \\sum^M_{i=1} x_i - \\frac{1}{1-\\theta} \\sum^N_{i=1} (1-x_i) = 0$$\n",
    "\n",
    "We should arrive at the fact that \n",
    "\n",
    "$$\\theta = \\frac{1}{N} \\sum^N_{i=1}x_i$$\n",
    "\n",
    "OK, so does this make sense?\n",
    "\n",
    "In fact, we claim that this is exactly the same as saying the number of heads divided by N, which as we know, is how we typically estimate the probability of heads\n",
    "\n",
    "Why is this?\n",
    "\n",
    "Well, remember that heads is equal to one tails is equal to zero\n",
    "\n",
    "Therefore, if we sum up all the heads we got, it's equal to the sum of all the $x$s\n",
    "\n",
    "The tails don't contribute because they are zero\n",
    "\n",
    "So the sum of all the $x$s is equal to the number of heads \n",
    "\n",
    "Furthermore, $N$ is equal to the total number of coin tosses, therefore this is indeed equal to the number of heads divided by the total number of coin tosses\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Common mistake</h3>\n",
    "\n",
    "Note that one common mistake with maximum likelihood on the Bernoulli is to confuse the likelihood with the binomial distribution\n",
    "\n",
    "$$p(x=k) = {N \\choose k} \\theta^k(1-\\theta)^{(N-k)}$$\n",
    "\n",
    "The binomial distribution is a related but different probability\n",
    "\n",
    "In particular, the binomial distribution will tell us the probability of getting $k$ heads out of $N$ coin tosses\n",
    "\n",
    "However, there is a coefficient in front of the $\\theta$ terms, where we have ${N \\choose k}$\n",
    "\n",
    "This is because the binomial distribution tells us the probability of getting $k$ heads in any order out of $N$ tosses\n",
    "\n",
    "For example, if we have three tosses and two heads out of those three tosses, then it could have happened HHT, HTH, THH\n",
    "\n",
    "The binomial distribution accounts for all of these\n",
    "\n",
    "The likelihood is not the probability of simply getting $k$ heads out of $N$ tosses in any possible order\n",
    "\n",
    "This will make more sense when we consider the Gaussian distribution, which we will discuss next\n",
    "\n",
    "In that example, we might be working with heights\n",
    " \n",
    "So we would measure that Alice is equal to 5'2\" Bob is equal to 5'8\" and Carol is equal to 5'5\"\n",
    "\n",
    "In this situation, we consider only the scenario where Alice is exactly 5'2\" to Bob is exactly 5'8\" and Carol is exactly 5'5\"\n",
    "\n",
    "We do not consider the situation where these heights could have been shuffled around\n",
    "\n",
    "For example, if Bob is 5'2\" or if Alice is 5'8\", that would simply be very silly\n",
    "\n",
    "So make sure we don't conflate the Bernoulli likelihood with the Binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we assume this is too theoretical, let's remember that this is exactly applicable to areas such as online advertising, Web page analysis and so forth\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Click-Through Rates</h3>\n",
    "\n",
    "Remember that the click through rate can be thought of as the probability that the user clicks on an advertisement or a link or whatever else we would like them to click\n",
    "\n",
    "Similarly, the conversion rate can be thought of as the probability that the user purchases a product, signs up for our newsletter or whatever else we would like them to do\n",
    "\n",
    "As we may recognize, these are binary events\n",
    "\n",
    "Either the user clicks or the user doesn't click\n",
    "\n",
    "Either the user buys or the user doesn't buy\n",
    "\n",
    "These are therefore Bernoulli distributed\n",
    "\n",
    "It's important to note, however, that whereas with a coin we are very used to thinking of the probability of success as around 50 percent, the probability of a click or a conversion is much lower\n",
    "\n",
    "So as someone who works in online advertising or digital media, we should get very comfortable with seeing very tiny numbers\n",
    "\n",
    "On average, we usually expect the click through rates for advertisements to be about 0.2-0.3%\n",
    "\n",
    "Notice how much smaller that is than 50%\n",
    "\n",
    "---\n",
    "\n",
    "<h3>What are these estimating?</h3>\n",
    "\n",
    "Formally, the clickthrough rate is defined as the number of clicks divided by the number of impressions\n",
    "\n",
    "$$\\text{Click Through Rate} (CTR) = \\frac{\\# \\ \\text{ clicks}}{\\# \\text{ impressions}}$$\n",
    "\n",
    "The conversion rate is defined as the number of people who have taken the desired action on our webpage divided by the total number of visits to that webpage\n",
    "\n",
    "$$\\text{Coversion Rate} = \\frac{\\# \\text{ desired action}}{\\# \\text{ page visits}}$$\n",
    "\n",
    "Of course, using what we know about maximum likelihood estimation, we know what these rates are actually estimating\n",
    "\n",
    "They are estimating the probability that users will perform these actions\n",
    "\n",
    "This is very applicable throughout these notebooks because our job will be to think about methods to improve this probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to continue our discussion of maximum likelihood estimation\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Maximum Likelihood Estimation Continued</h3>\n",
    "\n",
    "Previously, we looked at maximum likelihood for the Bernoulli\n",
    "\n",
    "These are for random variables that can only take on the values zero and one\n",
    "\n",
    "But sometimes we have random variables that can take on more values, for example, all real numbers.\n",
    "\n",
    "So the question is, is there a distribution that is appropriate for this context?\n",
    "\n",
    "And what is that distribution?\n",
    "\n",
    "As we can probably tell from our experience in previous notebooks, the next topic will be to use maximum likelihood on the Gaussian or the normal distribution\n",
    "\n",
    "<img src='extras/50.11.PNG' width='250'></img>\n",
    "\n",
    "Recall that these two names refer to the same distribution\n",
    "\n",
    "Statisticians typically call it the normal, while machine learning people tend to call it the Gaussian\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Other Real-Valued Distributions</h3>\n",
    "\n",
    "It's important to recognize that the Gaussian is not the only choice for real valued distribution\n",
    "\n",
    "Other real value distributions include the T-distribution, the exponential distribution, the gamma distribution and many more\n",
    "\n",
    "<img src='extras/50.12.PNG'></img>\n",
    "\n",
    "\n",
    "Of course, you should choose the distribution that you believe best represents our data\n",
    "\n",
    "Note that the process of solving maximum likelihood problems is the same no matter the distribution\n",
    "\n",
    "So if we can understand the previous lecture and this lecture, there's no reason we can't apply the same process to the exponential or the gamma\n",
    "\n",
    "The Gaussian is a good choice because it is ubiquitous in nature\n",
    "\n",
    "This may be due to the Central Limit Theorem, which says that sums of random variables, no matter the distribution, tend to the normal.\n",
    "\n",
    "So if we're measuring something which is the result of the summation of many other random things, then the result will look normally distributed\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Application</h3>\n",
    "\n",
    "Another point worth reminding ourselves about is the application\n",
    "\n",
    "Many times students become so intimidated by the math that they easily forget about what the application is\n",
    "\n",
    "Previously, we noted that the Bernoulli could be applied to binary outcomes\n",
    "\n",
    "This could be when the user clicks or doesn't click\n",
    "\n",
    "It could be when the user buys or doesn't buy\n",
    "\n",
    "We can apply this to any desirable action that the user may or may not do\n",
    "\n",
    "But there are situations where we want to measure real value rewards as well\n",
    "\n",
    "A simple example of this is ratings\n",
    "\n",
    "For example, we might ask users for ratings out of five or out of ten\n",
    "\n",
    "Pretty intuitively, a rating closer to ten is better and a rating closer to zero is worse\n",
    "\n",
    "Obviously, we can use these ratings to recommend the most desirable items, products or pages to our users\n",
    "\n",
    "Another example is the time that the user spends on our webpage\n",
    "\n",
    "We might think of this as a proxy for engagement\n",
    "\n",
    "Obviously, the longer the user spends on our Web page, the better\n",
    "\n",
    "Of course, these may not be Gaussian distributed, but usually the Gaussian is a fine approximation\n",
    "\n",
    "And remember, we can still apply the exact same process to any distribution you choose\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Problem Setup</h3>\n",
    "\n",
    "OK, so let's set up our problem, as we will see, it's pretty much the exact same process as the Bernoulli\n",
    "\n",
    "We start by saying that we've collected a bunch of data points, call them $x_1$ up to $x_N$\n",
    "\n",
    "$$\\text{Data}: X = \\{x_1,x_2,\\ldots,x_N\\}$$\n",
    "\n",
    "These could be something very basic\n",
    "\n",
    "Like we go around your classroom asking everybody how tall they are and we record their answer on a spreadsheet.\n",
    "\n",
    "OK, very simple, we can all do that\n",
    "\n",
    "OK, next step is to write down our likelihood function\n",
    "\n",
    "As we recall, it is the product of PDFs evaluated at each of the $x_i$s\n",
    "\n",
    "$$\\text{Likelihood} : L(\\theta) = \\prod^N_{i=1}p(x_i,\\theta), \\text{ where } \\theta = \\{\\mu,\\sigma^2\\}$$\n",
    "\n",
    "Note that there is one slight difference between this example and the previous Bernoulli example\n",
    "\n",
    "In the previous Bernoulli example, the random variable was discrete\n",
    "\n",
    "Therefore, the likelihood function was made up of the product of PMFs, and it was a true probability\n",
    "\n",
    "In this case, our random variable is continuous\n",
    "\n",
    "Therefore, the likelihood is made up of the product of PDFs, and it is not a probability, it is a probability density\n",
    "\n",
    "---\n",
    "\n",
    "<h3>PDF pf Gaussian</h3>\n",
    "\n",
    "OK, so what is the PDF for the Gaussian?\n",
    "\n",
    "Hopefully we've memorized this and we can easily recall it when necessary, if not, then we need to study harder :)\n",
    "\n",
    "$$ \\large L(\\theta) = \\prod^N_{i=1} \\frac{1}{2 \\pi \\sigma^2} e^{-\\frac{1}{2} \\left(\\frac{x_i-\\mu}{\\sigma} \\right)^2}$$\n",
    "\n",
    "<img src='extras/50.13.PNG' width='400'></img>\n",
    "\n",
    "So what makes up the Gaussian PDF?\n",
    "\n",
    "First, we have the normalizing coefficient\n",
    "\n",
    "That's $\\large \\frac{1}{2 \\pi \\sigma^2}$\n",
    "\n",
    "Next we have the exponent term \n",
    "\n",
    "Inside the exponent\n",
    "\n",
    "We have $\\large -\\frac{1}{2} \\left(\\frac{x_i-\\mu}{\\sigma} \\right)^2$\n",
    "\n",
    "Note that this distribution is symmetric about $\\mu$\n",
    "\n",
    "That's why the Gaussian distribution looks like a bell curve, which is the same on both sides\n",
    "\n",
    "---\n",
    "\n",
    "OK, so what's the next step\n",
    "\n",
    "As we recall, we are doing maximum likelihood, what does that mean again?\n",
    "\n",
    "That means that given the data, we want to find the parameters that best describe that data\n",
    "\n",
    "In other words, what values of $\\mu$ and $\\sigma$ make the most sense given the data that we collected?\n",
    "\n",
    "These values will maximize the likelihood.\n",
    "\n",
    "If you want to plug in some numbers, I think you will find you get a very sensible result.\n",
    "\n",
    "For example, suppose that we collect the heights of a bunch of our friends\n",
    "\n",
    "Suppose they're all around the range Five feet, six feet\n",
    "\n",
    "<img src='extras/50.14.PNG' width='400'></img>\n",
    "\n",
    "Would it make sense for you to have the value, say, one hundred feet?\n",
    "\n",
    "We should find ourselves agreeing that this is not sensible\n",
    "\n",
    "So we're trying to find a value of $\\mu$ that fits the data best\n",
    "\n",
    "Clearly, one hundred feet is not a great choice\n",
    "\n",
    "The value that is a great choice would be the one that maximizes the likelihood\n",
    "\n",
    "And again, if this does not sound right, we can always try this on your calculator\n",
    "\n",
    "We can pick some random heights or ask our friends, then set $\\mu$ to be 100 feet and set some value for the $\\sigma^2$\n",
    "\n",
    "We will see that the likelihood is much smaller when $\\mu$ is one hundred feet compared to when it takes on a much more reasonable value, like somewhere between five and six feet\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Using Calculus</h3>\n",
    "\n",
    "OK, but we know that there's a better way to do this than just guessing what $\\mu$ and $\\sigma$ should be\n",
    "\n",
    "And that is to maximize the likelihood with respect to $\\mu$ and $\\sigma$ using calculus\n",
    "\n",
    "$$\\hat \\mu,\\hat \\sigma^2 = \\arg \\max_{\\mu,\\sigma^2} L(\\mu,\\sigma^2)$$\n",
    "\n",
    "In fact, we can do these one at a time\n",
    "\n",
    "Let's begin by looking at $\\mu$\n",
    "\n",
    "As we know, the process is to take the log before differentiating\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Log-Likelihood</h3>\n",
    "\n",
    "So let's start by taking the log likelihood\n",
    "\n",
    "$$\\large l(\\mu,\\sigma^2) = \\log L(\\mu,\\sigma^2) = \\log \\prod^N_{i=1} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2}\\left(\\frac{x_i - \\mu}{\\sigma}\\right)^2}$$\n",
    "\n",
    "The first step, as we know, is to bring the log inside the product\n",
    "\n",
    "The log of a product is the sum of logs, basic high school algebra\n",
    "\n",
    "$$\\large l(\\mu,\\sigma^2) = \\sum^N_{i=1} \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2} \\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2}$$\n",
    "\n",
    "Note that the PDF of the Gaussian is also a product\n",
    "\n",
    "It's the product of two terms, the one over square root term and the exponential term\n",
    "\n",
    "Therefore, we can turn this log of a product into a sum of logs also\n",
    "\n",
    "$$\\large l(\\mu,\\sigma^2) = \\sum^N_{i=1} \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} + \\log e^{-\\frac{1}{2} \\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2}$$\n",
    "\n",
    "In the next step, we can simplify the log terms\n",
    "\n",
    "As we know, one over the square root $\\left( \\frac{1}{\\boxed{\\sqrt{2 \\pi \\sigma^2}}} \\right)$ is the same as taking the power to minus one half \n",
    "\n",
    "We can bring that minus one half down\n",
    "\n",
    "$$\\large l(\\mu,\\sigma^2) = \\sum^N_{i=1}  \\boxed{-\\frac{1}{2}} \\log {2 \\pi \\sigma^2} + \\log e^{-\\frac{1}{2} \\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2}$$\n",
    "\n",
    "Furthermore, we recall that the log and the exponential are inverse functions $\\left( \\boxed{\\log e}^{-\\frac{1}{2} \\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2} \\right)$\n",
    "\n",
    "Therefore, the log of the exponential of something is just that thing\n",
    "\n",
    "The log and the exponential cancel out\n",
    "\n",
    "$$\\large l(\\mu,\\sigma^2) = \\sum^N_{i=1}  -\\frac{1}{2} \\log {2 \\pi \\sigma^2} -\\frac{1}{2} \\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Just some simple algebra</h3>\n",
    "\n",
    "Finally, note that the first term inside the sum does not depend on the index variable $i$\n",
    "\n",
    "Therefore it can be brought outside the sum \n",
    "\n",
    "$$\\large l(\\mu,\\sigma^2) = -\\frac{1}{2} \\log {2 \\pi \\sigma^2} \\sum^N_{i=1}  1 - \\frac{1}{2} \\sum^N_{i=1}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2$$\n",
    "\n",
    "Since one summed up $N$ times is just $N$, we get \n",
    "\n",
    "$$\\large l(\\mu,\\sigma^2) = -\\frac{N}{2} \\log {2 \\pi \\sigma^2} - \\frac{1}{2} \\sum^N_{i=1}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2$$\n",
    "\n",
    "At this point, the sum is only over the $x_i - \\mu$ term\n",
    "\n",
    "OK, so you can see that by taking the log likelihood, we have greatly simplified our expression\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Take the derivative</h3>\n",
    "\n",
    "In the next step, we'll see why taking the log was so useful\n",
    "\n",
    "As we recall, the next step is to take the derivative of the log likelihood, set it to zero and solve from $\\mu$\n",
    "\n",
    "So we're going to take our expression for the log likelihood that we just found, differentiate it with respect to $\\mu$, set it to zero and solve from $\\mu$\n",
    "\n",
    "Since the log likelihood is a function of two variables, $\\mu$ and $\\sigma^2$, we are going to take the partial derivative with respect to $\\mu$ \n",
    "\n",
    "Note that the first term $-\\frac{N}{2} \\log {2 \\pi \\sigma^2}$ involves only $\\sigma^2$ not $\\mu$\n",
    "\n",
    "Therefore it's constant and it's derivative is zero\n",
    "\n",
    "$\\require{\\cancel}$\n",
    "\n",
    "$$\\large l(\\mu,\\sigma^2) = \\cancel{-\\frac{N}{2} \\log {2 \\pi \\sigma^2}} - \\frac{1}{2} \\sum^N_{i=1}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2$$\n",
    "\n",
    "\n",
    "The second term is just a quadratic, which we know how to differentiate \n",
    "\n",
    "$$\\large \\frac{dl}{d\\mu} = \\sum^N_{i=1} \\left(\\frac{x_i-\\mu}{\\sigma}\\right)\\frac{1}{\\sigma}$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Sample Mean</h3>\n",
    "\n",
    "As we recall, we would like to set the previous expression to zero and solve for $\\mu$\n",
    "\n",
    "$$\\sum^N_{i=1} \\left(\\frac{x_i-\\mu}{\\sigma}\\right)\\frac{1}{\\sigma} = 0$$\n",
    "\n",
    "After doing so, we should end up with the familiar expression\n",
    "\n",
    "$$\\large \\mu = \\frac{1}{N}\\sum^N_{i=1}x_i$$\n",
    " \n",
    "As we recall, this is known as the sample mean\n",
    "\n",
    "This makes perfect sense since $\\mu$ the mean of the Gaussian distribution\n",
    "\n",
    "The next step will be a bit harder, we continue in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to continue our discussion on maximum likelihood estimation for the Gaussian\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Gaussian MLE Continued</h3>\n",
    "\n",
    "As we recall, the Gaussian has two parameters its mean and its variance\n",
    "\n",
    "So far, we've solved for the maximum likelihood estimate of the mean\n",
    "\n",
    "We determine that it's just the sample mean\n",
    "\n",
    "We have yet to solve for the variance\n",
    "\n",
    "So that's what we're going to do in this section\n",
    "\n",
    "$$\\large \\hat \\mu, \\boxed{ \\hat \\sigma^2} = \\arg \\max_{\\mu,\\sigma^2} L(\\mu,\\sigma^2)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Simplifying the log-likelihood</h3>\n",
    "\n",
    "So let's start by expressing the likelihood which we saw in the previous section in a slightly simpler way\n",
    "\n",
    "$$\\large l(\\mu,\\sigma^2) = -\\frac{N}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2}\\sum^N_{i=1} \\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2$$\n",
    "\n",
    "We know that the parameter is $\\sigma^2$, but $\\sigma$ never actually appears by itself, it's always squared\n",
    "\n",
    "This might look a little confusing so we can replace it with a new variable $v$, which we will define to be equal to $\\sigma^2$\n",
    "\n",
    "$$V = \\sigma^2$$\n",
    "\n",
    "So in the first term, $\\sigma^2$ turns into $v$\n",
    "\n",
    "In the second term, $\\sigma^2$ appears in the denominator, which we can replace with $v$ if we get rid of the square\n",
    "\n",
    "$$\\large l(\\mu,v) = -\\frac{N}{2} \\log(2\\pi v) - \\frac{1}{2} \\frac{1}{v} \\sum^N_{i=1} (x_i - \\mu)^2, \\text{ where } v = \\sigma^2$$\n",
    "\n",
    "Notice that $v$ does not depend on the index $i$, so it can be brought outside the summation\n",
    "\n",
    "We can leave it by itself as just $\\frac{1}{v}$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Differentiate</h3>\n",
    "\n",
    "The next step, as we know, is to differentiate the log likelihood with respect to $v$ and solve for $v$ after setting the result to zero\n",
    "\n",
    "Note that this time both terms involve $v$ so we can't drop any terms\n",
    "\n",
    "As we recall, the derivative of $\\log v$ is just $\\frac{1}{v}$, so that's what happens to the first term\n",
    "\n",
    "For the second term, the derivative of $\\frac{1}{v}$ is $-\\frac{1}{v^2}$\n",
    "\n",
    "So that's what happens to the second term\n",
    "\n",
    "$$\\large \\frac{dl}{dv} = - \\frac{N}{2}\\frac{1}{v} - \\frac{1}{2}(-1)\\frac{1}{v^2} \\sum^N_{i=1}(x_i - \\mu)^2$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Set to zero and solve</h3>\n",
    "\n",
    "The final step is to set all on a zero and solve for $v$, we should arrive at\n",
    "\n",
    "$$v = \\frac{1}{N} \\sigma^N_{i=1}(x_i-\\mu)^2$$\n",
    "\n",
    "We get that $v$ is equal to the usual expression for the sample variance\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Success!</h3>\n",
    "\n",
    "OK, so at this point, we've solved the maximum likelihood estimate for both the mean and the variance of the Gaussian\n",
    "\n",
    "We will notice something interesting about our expression for the sample variance\n",
    "\n",
    "If we look closely at the right hand side, we notice that it depends on $\\mu$, the mean of the Gaussian\n",
    "\n",
    "$$\\large \\hat \\sigma^2 = \\frac{1}{N} \\sum^N_{i=1} (x_i - \\boxed{\\mu})^2$$\n",
    "\n",
    "However, we don't actually know this value\n",
    "\n",
    "All we know is the sample mean, which is not the same as the mean\n",
    "\n",
    "One thing we can do is replace the true mean  with our estimate of the true mean, which is the sample mean\n",
    "\n",
    "$$\\large \\hat \\sigma^2 = \\frac{1}{N} \\sum^N_{i=1} (x_i - \\boxed{\\hat \\mu})^2$$\n",
    "\n",
    "If we want to be explicit about our symbols, then we can call the left hand side $\\hat \\sigma^2$ on the right hand side we have $\\hat \\mu$\n",
    "\n",
    "Again, this is called the maximum likelihood estimate of the variance\n",
    "\n",
    "---\n",
    "\n",
    "<h3>But still, there's more work to do</h3>\n",
    "\n",
    "There is one issue, however\n",
    "\n",
    "In order to see why, let's first recognise that functions of random variables are also random variables\n",
    "\n",
    "Let's repeat that, the functions of random variables are also random variables\n",
    "\n",
    "To give an example, consider the sum of $X_1$ and $X_2$\n",
    "\n",
    "Suppose that $X_1$ and $X_2$ are both independent coin flips, so the result can be either zero or one\n",
    "\n",
    "If $Z$ is a new random variable that's the sum of $X_1$ and $X_2$, $Z = X_1 + X_2$, then what values can it take on?\n",
    "\n",
    "Well, it can take on zero one or two\n",
    "\n",
    "It will take on the value zero, if both coin flips gave us zero \n",
    "\n",
    "It will take on the value two, if both coin flips gave us one\n",
    "\n",
    "It will give me the value one, if one of the coin flips gave us one and the other one gave us zero\n",
    "\n",
    "We should agree that if we flip two coins representing $X_1$ and $X_2$, then both $X_1$ and $X_2$ are random, as is the sum of $X_1$ plus $X_2$\n",
    "\n",
    "In fact, we can make up any function of random variables and it will also be a random variable\n",
    "\n",
    "For example, $Z = X_1 + X_2 \\times X_3 + \\sin(X_4)$, this is a random variable, albeit a strange one\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Who cares?</h3>\n",
    "\n",
    "OK, so why did we mention that \n",
    "\n",
    "Well we have to recognize, that both the sample mean and the sample variance are random variables\n",
    "\n",
    "Why?\n",
    "\n",
    "Because they are functions of random variables\n",
    "\n",
    "In particular, they are both functions of the random variables, $x_1,x_2 \\dots x_N$\n",
    "\n",
    "$$\\large \\hat \\mu = \\frac{1}{N} \\sum^{N}_{i=1} x_i = f(x_1,x_2,\\ldots,x_N)$$\n",
    "\n",
    "$$\\large \\hat \\sigma^2 = \\frac{1}{N} \\sum^N_{i=1}(x_i-\\hat \\mu)^2 = g(x_1,x_2,\\ldots,x_N)$$\n",
    "\n",
    "\n",
    "Remember, this is the data that we collected, they were random\n",
    "\n",
    "In fact, they were iid random, independent and identically distributed\n",
    "\n",
    "But since $\\hat \\mu$ and $\\hat \\sigma^2$ are functions of $x_1$ up to $x_N$, they are also random variables\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Still, who cares?</h3>\n",
    "\n",
    "So what's the relevance of this\n",
    "\n",
    "Since $\\hat \\mu$ and $\\hat \\sigma^2$ are random variables, we can do the usual things that we do with random variables\n",
    "\n",
    "For example, we can ask what is their probability distribution and what is their expected value?\n",
    "\n",
    "$$\\large \\mu \\sim ????, \\ E(\\hat \\mu)=???$$\n",
    "\n",
    "$$\\large \\sigma^2 \\sim ????, \\ E(\\hat \\sigma^2)=???$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Interesting Facts</h3>\n",
    "\n",
    "Here are some interesting facts\n",
    "\n",
    "The distribution of $\\hat \\mu$ is a Gaussian \n",
    "\n",
    "$$\\large \\mu = \\text{Normal}(\\mu,\\frac{\\sigma^2}{N})$$\n",
    "\n",
    "The expected value of $\\hat \\mu$ or in other words, its mean, is $\\mu$\n",
    "\n",
    "$$E(\\hat \\mu) = \\mu$$\n",
    "\n",
    "That should make total sense\n",
    "\n",
    "Since $\\hat \\mu$ is our estimate of $\\mu$, we would expect that its expected value is $\\mu$\n",
    "\n",
    "The mean of the sample mean is the mean :o\n",
    "\n",
    "Its variance happens to be $\\frac{\\sigma^2}{N}$\n",
    "\n",
    "We'll discuss this more in a later section, so we just treat this as a random tidbit of information for now\n",
    "\n",
    "As a side note, if we want to try to prove this and we remember your probability theory, then we can use characteristic functions to show that this is true\n",
    "\n",
    "---\n",
    "\n",
    "Here are some more interesting facts\n",
    "\n",
    "The expected value of $\\hat \\sigma^2$ is not $\\sigma^2$\n",
    "\n",
    "$$\\large E(\\hat \\sigma^2) \\neq \\sigma^2$$\n",
    "\n",
    "If we go through the derivation, we can show that in fact\n",
    "\n",
    "$$E(\\hat \\sigma^2) = \\frac{N-1}{N}\\sigma^2$$\n",
    "\n",
    "Although we won't go through the derivation in these notebooks, we can find the full derivation  <a href=\"https://lazyprogrammer.me\">here</a> by searching for \"The Unbiased Estimate of the Covariance Matrix\"\n",
    "\n",
    "So what is the consequence of this\n",
    "\n",
    "Typically statisticians will divide by $N-1$  instead of $N$\n",
    "\n",
    "This will give us the unbiased estimate of the variance instead of the maximum likelihood estimate\n",
    "\n",
    "$$\\large \\text{Unbiased Estimate : } \\hat \\sigma^2 = \\frac{1}{N-1}\\sum^N_{i=1}(x_i - \\hat \\mu)^2$$\n",
    "\n",
    "As we might expect, the expected value of the unbiased estimate is equal to the true variance\n",
    "\n",
    "Note that in machine learning, people tend to care less about this distinction, since in machine learning the data is so large that this doesn't really make a difference\n",
    "\n",
    "As we recall, $\\frac{N-1}{N}$ approaches $1$ as $N$ approaches infinity\n",
    "\n",
    "---\n",
    "\n",
    "Finally, here is one last interesting but optional fact\n",
    "\n",
    "The distribution of $\\hat simga^2$ is the chi-squared distribution\n",
    "\n",
    "$$\\large (N-1) \\frac{\\hat \\sigma^2}{\\sigma^2} \\sim \\chi^2_{N-1}$$\n",
    "\n",
    "After some scaling, we can show that it is chi-squared distributed with $N-1$ degrees of freedom\n",
    "\n",
    "It turns out that if we scale the sample variance by $\\frac{N-1}{\\sigma}$ we will get a $\\chi^2$ distributed random variable with $N-1$ degrees of freedom\n",
    "\n",
    "We will never actually use this fact in the following notebooks, but we will see the $\\chi^2$  distribution pop up once or twice\n",
    "\n",
    "So again, this is just an optional, interesting fact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to continue our review of probability\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Probability Review: CDF</h3>\n",
    "\n",
    "Previously we looked at the PDF and discussed how it could be used in maximum likelihood estimation\n",
    "\n",
    "In this section, we will talk about another important function called the CDF\n",
    "\n",
    "<img src='extras/50.15.PNG' width='600'></img>\n",
    "\n",
    "---\n",
    "\n",
    "<h3>CDF (Cumulative Distribution Function)(Discrete)</h3>\n",
    "\n",
    "So to understand the CDF, it's probably more intuitive to start with the discrete case \n",
    "\n",
    "In this scenario, the CDF is defined as follows\n",
    "\n",
    "We say that\n",
    "\n",
    "$$\\large F(x) = P(X \\le x)$$\n",
    "\n",
    "So, as usual, $X$ is the random variable and $x$ is the argument\n",
    "\n",
    "So for example, $F(3)$ is the probability that the random variable can take on a value less than or equal to three\n",
    "\n",
    "OK,  so how can we find this value?\n",
    "\n",
    "Let's suppose that we have a PMF, which is the probability that the random variable, $X$ can take on exactly the value $x$ \n",
    "\n",
    "In this case, all we have to do to find $F$ is to sum up all the values up to and including $x$\n",
    "\n",
    "$$\\large P(X \\le x) = \\sum^x_{k=-\\infty} p(k), \\text{ where }p(k) = \\text{Prob}(X=k)$$\n",
    "\n",
    "So why does this make sense?\n",
    "\n",
    "Well, let's consider $x=3$ again\n",
    "\n",
    "If we want to know the probability that $X$ can be less than or equal to three, we just sum up all the individual probabilities\n",
    "\n",
    "So the probability that it can be three plus the probability that it can be two, plus the probability that it can be one, plus the probability that it can be zero and so forth\n",
    "\n",
    "Clearly by summing up all these individual probabilities, we will get the total probability that $X$ is less than or equal to three\n",
    "\n",
    "---\n",
    "\n",
    "<h3>CDF (Continuous)</h3>\n",
    "\n",
    "Now, let's think about how the CDF is defined for the continuous case\n",
    "\n",
    "As you know, when working with continuous random variables, sums turned into integrals\n",
    "\n",
    "In this case, the CDF\n",
    "\n",
    "$$\\large F(x) = P(X \\le x) = \\int^x_{-\\infty} f(t) dt$$\n",
    "\n",
    "\n",
    "\n",
    "Note that the $t$ inside the integral is called a dummy variable\n",
    "\n",
    "So it doesn't matter what letter we use here, it can be $t,s,u,v$, whatever\n",
    "\n",
    "This variable disappears after we take the integral, just like how it disappears when we do a summation\n",
    "\n",
    "Recognize that the CDF in the continuous case is a probability\n",
    "\n",
    "This is unlike the PDF, which is the derivative of the CDF, and because of that it has different units and it is not a probability\n",
    "\n",
    "PDF\n",
    "\n",
    "$$\\large f(x) = \\frac{dF(x)}{dx}$$\n",
    "\n",
    "This is different from the discrete case when we do situations, where the PMF and the CDF are both probabilities\n",
    "\n",
    "---\n",
    "\n",
    "<h3>CDF in Pictures (Discrete)</h3>\n",
    "\n",
    "Not that we can also think of the CDF in terms of pictures\n",
    "\n",
    "For the discrete case, we can imagine the distribution as a bunch of bars\n",
    "\n",
    "If we want to know the CDF at some value $x$, we sum up all the bars up to and including the bar at $x$\n",
    "\n",
    "<img src='extras/50.16.PNG' width='600'></img>\n",
    "\n",
    "---\n",
    "\n",
    "<h3>CDF in Pictures(Continuous)</h3>\n",
    "\n",
    "For the continuous case, the distribution is a continuous function\n",
    "\n",
    "The integral is the area under this curve\n",
    "\n",
    "<img src='extras/50.17.PNG' width='600'></img>\n",
    "\n",
    "\n",
    "Therefore, if we want to know the CDF at some value $x$, we find the area under the curve from minus $-\\infty$ up to $x$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>CDF shape</h3>\n",
    "\n",
    "One consequence of how we define the CDF is that the values at the extreme ends are always zero and one \n",
    "\n",
    "The CDF at $-\\infty$ must be zero because it's the probability that the random variable can be less than or equal to $-\\infty$\n",
    "\n",
    "Notice that this doesn't include any values and therefore the probability is zero\n",
    "\n",
    "The CDF at $+\\infty$ must be one because it's the probability that the random variable can be less than or equal to $\\infty$\n",
    "\n",
    "Of course, this includes all possible values of the randim variable, and therefore this probability is one\n",
    "\n",
    "In addition, since the PDF and the PMF are always greater than or equal to zero, the CDF is always non decreasing.\n",
    "\n",
    "It is recommended to check out the CDFs for various distributions on Wikipedia, for example, we can look at the gamma, the beta, the normal and so on\n",
    "\n",
    "<img src='extras/50.18.PNG' width='400'></img>\n",
    "\n",
    "We'll recognize that they all have this sigmoidal shape\n",
    "\n",
    "This is just a consequence of the fact that the CDF always starts at zero, always ends at one end one and  is always non-decreasing\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Inverse CDF</h3>\n",
    "\n",
    "One important function in statistics is the inverse of the CDF\n",
    "\n",
    "This is also known as the percent point function and it's also related to the percentile\n",
    "\n",
    "So what does this mean?\n",
    "\n",
    "Basically, we just go in the opposite direction compared to the CDF\n",
    "\n",
    "Instead of asking what is the probability that the random variable is less than or equal to some value?\n",
    "\n",
    "Let's ask the opposite question\n",
    "\n",
    "Given some probability that the random variable is less than or equal to some value, what is this value? \n",
    "\n",
    "To make this more clear, let's consider more concrete examples\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Example: Exam Grades</h3>\n",
    "\n",
    "One example, which is particularly relevant for a classroom is to consider exam grades\n",
    "\n",
    "As we know, teachers like to create bell curves out of their students exam grades\n",
    "\n",
    "<img src='extras/50.19.PNG' width='300'></img>\n",
    "\n",
    "OK, so let's say we create a Gaussian model for the exam grades for students in this class\n",
    "\n",
    "Since we don't want to confuse our exam scores with probabilities, let's say the maximum exam grade you can get is two hundred\n",
    "\n",
    "So 200/200 hundred is a perfect score.\n",
    "\n",
    "Now, we can ask a question like, what's the probability that the student will achieve a score of 170 or less?\n",
    "\n",
    "That's the CDF, as we recall\n",
    "\n",
    "So we can go in to ```scipy``` and call the CDF function passing in 170, ```norm.cdf(170,mu,sigma)```\n",
    "\n",
    "This will output the probability that any student in the class scored a 170 or below \n",
    "\n",
    "Just to give some real numbers for this example, let's say the answer is 95%\n",
    "\n",
    "By the way, what if we want to know the probability that a student scored above 170?\n",
    "\n",
    "Well, the probability of above 170 is one minus the probability of 170 or below\n",
    "\n",
    "Therefore, we take one minus the CDF of 170\n",
    "\n",
    "So the probability that a student got above 170 out of 200 is five percent\n",
    "\n",
    "That's to be expected, since that is a very high grade\n",
    "\n",
    "---\n",
    "\n",
    "OK, but now let's get back to the inverse CDF\n",
    "\n",
    "Let's say we want to ask, what is the maximum score that the bottom 95% of the class achieved?\n",
    "\n",
    "Of course, the answer is the inverse CDF of 0.95, as one might expect\n",
    "\n",
    "Given the previous examples, the answer to this is 170\n",
    "\n",
    "That is, if we are on the bottom 95% of the students, then we got a score of 170 or below\n",
    "\n",
    "Now, somewhat confusingly, when we say we are in the 95th percentile, we mean that we are doing better than the bottom 95 percent\n",
    "\n",
    "In other words, we've got a score of 170 or above\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Example: Heights</h3>\n",
    "\n",
    "OK, so let's do another example \n",
    "\n",
    "Let's say we've measured all the heights of our friends, and again, we're going to model these heights as a Gaussian distribution \n",
    "\n",
    "<img src='extras/50.20.PNG' width='400'></img>\n",
    "\n",
    "\n",
    "Let's suppose that the average height is 170 cm and the standard deviation is 7 cm\n",
    "\n",
    "OK, now let's say our friend says my height is in the 95th percentile\n",
    "\n",
    "Can we determine how tall your friend is?\n",
    "\n",
    "The answer is yes, simply calculate the inverse CDF of 95%\n",
    "\n",
    "The answer is 181.5 cm\n",
    "\n",
    "Now, let's say we measure another friend's height and they are 160 cm tall, what percentile are they in?\n",
    "\n",
    "You can calculate the answer by using the CDF\n",
    "\n",
    "In this case, the answer is approximately 8%\n",
    "\n",
    "Therefore, our friend is in the 8th percentile\n",
    "\n",
    "In other words, only 8% percent of people are shorter than they are\n",
    "\n",
    "Conversely, 92% of people are taller than they are\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Summary</h3>\n",
    "\n",
    "OK, so to summarize this section, we looked at both the CDF and the inverse CDF as well as the related concept of percentiles\n",
    "\n",
    "We saw that the CDF is defined as a summation or an integral \n",
    "\n",
    "Since we'll be working mostly with continuous distributions, we will be making use of integrals \n",
    "\n",
    "In the next notebook, we'll see that percentiles and inverse CDFs are extremely handy when we discuss confidence intervals and hypothesis testing\n",
    "\n",
    "In the next section, we'll see how we can actually compute CDFs and inverse CDFs in code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set mean and stadnard deviation of heights as in our example\n",
    "mu = 170\n",
    "sd = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples from a gaussian distribution with this mean and standard deviation\n",
    "# there are several ways to do this, here is one way using numpy\n",
    "# in statistics, loccation and scale are defined mathematically\n",
    "# for the gaussian, these are mu and sd\n",
    "# these are different for every distribution\n",
    "x = norm.rvs(loc=mu,scale=sd,size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170.41865610874137"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate maximum likelihood estimate of the mean\n",
    "x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.77550434153163"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate maximum likelihood estimate of variance\n",
    "# recall var = sd^2\n",
    "x.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.77550434153163"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as a sanity check lets calculate the variance\n",
    "((x-x.mean())**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.055175713016057"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maximum likelihood std\n",
    "x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.278287213668314"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again, the .var and .std give us the maximum likelihood variance and standard deviation\n",
    "# recall that this is different from the unbiased versions\n",
    "# the difference is that we divide by N-1 instead of N\n",
    "# so what do we do when we want to divide by N-1?\n",
    "# ddof : delta degrees of freedom\n",
    "# basically the thing we want to subtract from N\n",
    "# we get ~50, so thats a worse estimate than the maximum likelihood variance\n",
    "x.var(ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.278287213668314"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets do another sanity check by manually calculating the unbiased estimate of the variance\n",
    "((x-x.mean())**2).sum()/(len(x)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.090718384879512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unbiased estimate of std\n",
    "x.std(ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181.5139753886603"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# at what height are we in the 95th percentile\n",
    "norm.ppf(0.95,loc=mu,scale=sd)\n",
    "# so we get 181.5\n",
    "# meaning that if our height is 181.5 cm\n",
    "# then 95% of the population are shorter than us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07656372550983476"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we are 160 cm tall, what percentile are we in\n",
    "norm.cdf(160,loc=mu,scale=sd)\n",
    "# small are to the left of bell curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07656372550983481"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we are 180 cm tall\n",
    "# what is the probability that someone is taller than us\n",
    "1 - norm.cdf(180,loc=mu,scale=sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07656372550983476"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1- cdf has a special name\n",
    "# usually used by statisticians and people in medicine\n",
    "# but uncommon in machine learning\n",
    "# this is called the survival function\n",
    "norm.sf(180,loc=mu,scale=sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note something interesting\n",
    "# this is not exactly the same as 1-cdf\n",
    "# the reason for this is that conputers are not 100% accurate\n",
    "# we only have a finite amount of space to store each number\n",
    "# and because of that we would lose some precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, here is a high level review of how the Bayesian approach is different from the Frequentist approach\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Point Esitmates</h3>\n",
    "\n",
    "One important fact that we'll want to recognize from the previous sections, is that when we estimated parameters like the mean, they were point estimates\n",
    "\n",
    "That means, given the data, we always ended up with just a single value\n",
    "\n",
    "There are some important questions we haven't yet considered about this single value\n",
    "\n",
    "For instance, how confident are we in this estimate?\n",
    "\n",
    "Is it a good estimate or is it a bad estimate?\n",
    "\n",
    "This brings us to the concept of confidence intervals, which we will study later\n",
    "\n",
    "Another question we can ask is this, suppose we have two estimates in the following notebooks, that's often what we're doing\n",
    "\n",
    "We're trying to compare the clickthrough rate between advertisement $A$ and advertisement $B$ \n",
    "\n",
    "We want to know whether $A$ is better than $B$ or vice versa\n",
    "\n",
    "Obviously, the one with the highest clickthrough rate wins\n",
    "\n",
    "But how can we be sure that $A$ is better than $B$?\n",
    "\n",
    "This goes back to the idea of confidence\n",
    "\n",
    "If we are not even confident about our estimates, then we are also not confident that $A$ is better than $B$\n",
    "\n",
    "So that's another question we will discuss next\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Looking Ahead: Frequentist to Bayesian</h3>\n",
    "\n",
    "Of course, the whole goal of this course is to raise our understanding to the level of the Bayesian perspective\n",
    "\n",
    "We recognize that point estimates are limited \n",
    "\n",
    "The Bayesian asks, what if instead of a point estimate, we can find a distribution \n",
    "\n",
    "That is instead of just finding $\\hat \\mu$, the maximum likelihood estimate of $\\mu$, what if we can find the distribution of $\\mu$?\n",
    "\n",
    "$$\\text{Frequentist : }\\hat \\mu$$\n",
    "$$\\text{Bayesian : }p(\\mu \\vert \\text{ data})$$\n",
    "\n",
    "This is more powerful since, as we can see, it directly gives us some form of an interval of confidence although in the Bayesian approach they are not called confidence intervals\n",
    "\n",
    "And because we have distributions, we can ask questions like what is the probability that $A$ is better than $B$?\n",
    "\n",
    "So the Bayesian approach is to treat everything as if they were random\n",
    "\n",
    "The classical approach, also called the frequentist approach, is to treat the parameters as if they are fixed but unknown\n",
    "\n",
    "Our point estimates are our best guess \n",
    "\n",
    "In the Bayesian approach, we say that those parameters are not fixed but are also random variables\n",
    "\n",
    "And because they are random, they have distributions which can be reasoned about by using the data we collect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
