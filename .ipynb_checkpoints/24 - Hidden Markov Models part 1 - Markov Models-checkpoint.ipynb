{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following parts we are going to talk about Hidden Markov Models which are used for modeling sequences of data which are everywhere : Stock prices , language , credit scoring , webpage visits\n",
    "\n",
    "consider the following sentence : \"Like and cats dogs I\"\n",
    "\n",
    "surely this sentence makes no sense , and thats what happens when we use a model like bag of words\n",
    "\n",
    "The fact that it becomes very hard to tell what a sentence mean when taking away the time aspect tells us that there a lot of information carried there\n",
    "\n",
    "The original sentence was \"I like cats and dogs\" , which we would have probably decoded ourselves , but we can imagine how this gets progressively harder as the sentence gets longer\n",
    "\n",
    "Lets recall the difference between supervised and unsupervised learning :\n",
    "\n",
    "<ul>\n",
    "    <li>Unsupervised learning : is for when we want to discover a pattern or model a distribution but we dont have any labels</li>\n",
    "    <li>Supervised learnig : is for when we have labels , and want to be able to accurately predict albels</li>\n",
    "</ul>\n",
    "\n",
    "HMMs are for modelling sequences : x(1),x(2),...,x(t),...,x(T) , there are no labels here , which means it is unsupervised\n",
    "\n",
    "But we often see HMMs used for classification as well , for ex , we can train an HMM to model the male voice , then train an HMM to model a female voice , then we can predict given a new voice sample whether or not that voice was male or female , how can we do this ?\n",
    "\n",
    "The key idea is Baues rule , what we actually modeled was p(X|male) , p (X|female)\n",
    "\n",
    "<ul>\n",
    "    <li>p(X|male) collect all male samples , train an HMM</li>\n",
    "    <li>p(X|female) collect all female samples , train another HMM</li>\n",
    "</ul>\n",
    "\n",
    "Bayes rule reverses the conditional\n",
    "\n",
    "<ul>\n",
    "    <li>p(male|X) and p(female|X)</li>\n",
    "</ul>\n",
    "\n",
    "so now we can find the most probable class , and our prediction becomes whatever the most probable class is\n",
    "\n",
    "we know form Bayes rule that :\n",
    "\n",
    "$$posterior = \\frac{likelihood \\times prior}{normalisation \\ constant}$$\n",
    "\n",
    "$$p(male|X) = \\frac{p(X|male)p(male)}{p(X)}$$\n",
    "\n",
    "$$p(female|X) = \\frac{p(X|female)p(female)}{p(X)}$$\n",
    "\n",
    "The real beuaty of Bayes rule is that it does not care what p(X|C) is , just use some kind of model\n",
    "\n",
    "We can use an HMM to model it , but we might as well , as we have seen before , model it using Naive Bayes , this is when we made the independance assumption , meaning that each sample is independant , so in this case we take the probability of each individual feature and multiply them together to get the final p(X|C)\n",
    "\n",
    "$$p(X|C) = p(x(1,1)|c)p(x(1,2)|C) ... p(x(T,D)|C)$$\n",
    "\n",
    "Next we begin by introducing Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to discuss the Markov property , which is a property that makes a markov model what it is\n",
    "\n",
    "Think about weather predictions , suppose we can predict next days weather based on what we saw on the previous days , for ex:\n",
    "\n",
    "\"Monday was sunny , Tuesday was sunny , therefore the probability of being sunny on Thursday is 90%\"\n",
    "\n",
    "we can see how this type of model is widely applicable in many fields , for instance in NLP we could create automatic writing systems by creating probability models , for ex , if we start by the sequence :\n",
    "\n",
    "\"The MacBook was created by ____\"\n",
    "\n",
    "<ul>\n",
    "    <li>Apple</li>\n",
    "    <li>Microsoft</li>\n",
    "    <li>Whole Foods</li>\n",
    "</ul>\n",
    "\n",
    "p(\"apple\"|\"The MacBook was created by\") = high probability\n",
    "\n",
    "we might also use Markov Models for stock predictions , so we feed in the stock price for every day in the previous month and we try to predict tommorrow's stock price\n",
    "\n",
    "<h4>Markov Property</h4>\n",
    "\n",
    "The Markov property is when \"Tommorow's\" weather only depends on \"Today's\" weather but not \"Yesterday's\" weather , \n",
    "\n",
    "$$ p(weather \\ today|weather \\ previous \\ days) = p(weather \\ today | weather \\ yesterday) $$\n",
    "\n",
    "Its when the next word in a sentence depends only on the previous word in a sentence but not on any of the other words \n",
    "\n",
    "$$ p(word[n] | word[n-1],...,word[1]) = p(word[n]|word[n-1]) $$\n",
    "\n",
    "Its when tommorows stock price depends only on todays stock price but not any day before that\n",
    "\n",
    "$$ p(GOOG[t]|GOOG[t-1],...,GOOG[1]) = p(GOOG[t]|GOOG[t-1]) $$\n",
    "\n",
    "The Markov Propery is also called the Markov Assumption , since as we can see its quite the string assumption , we are throwing away all the historic data except for the most recent\n",
    "\n",
    "In more general terms , the rendom variable (weather,stock price,word) is a state , say s(t) = \"the state at time t\" , then the markov assumption is that the current state depends only on the previous state or that the next state depends only on the current state\n",
    "\n",
    "$$\\large p(s_t|s_{t-1},s_{t-2},...,s_{0}) = p(s_t|s_{t-1})$$\n",
    "\n",
    "But why would we want to make the markov assumption ?\n",
    "\n",
    "The goal is to model the joint probability , or in other words the probability of seeing an entire specific sequence , we can use the chain rule of probability to find the distribution :\n",
    "\n",
    "$$ p(s_4,s_3,s_2,s_1) = p(s_4|s_3,s_2,s_1)p(s_3,s_2,s_1) $$\n",
    "\n",
    "$$= p(s_4|s_3,s_2,s_1)p(s_3|s_2,s_1)p(s_2,s_1)$$\n",
    "\n",
    "$$=p(s_4|s_3,s_2,s_1)p(s_3|s_2,s_1)p(s_2,s_1)p(s_1)$$\n",
    "\n",
    "we can see that we get quite a large expression when fully expanded , but if we use the Markov Property , this becomes much simpler :\n",
    "\n",
    "$$p(s_4|s_3)p(s_3|s_2)p(s_2|s_1)p(s_1)$$\n",
    "\n",
    "Think about how often the sequence $s_3,s_2,s_1$ occurs , if it does not happen that often how can we accurately measure $p(s_4|s_3,s_2,s_1)$ , lets take an example\n",
    "\n",
    "Had we not have made the Markov Assumption , it would be very hard to measure the probability distributions , consider a 1000 word wikipidea article , without the markov assumption\n",
    "\n",
    "$$p(w_{1000} | w_{999},...,w_1)$$ \n",
    "\n",
    "But how many articles have the exact sequnce $$w_{999},...,w_1$$ , probably just htis article , hence only 1 possible value for $w_1000$ with probability 100% , probably not that useful\n",
    "\n",
    "conversely , if we are thinking of the begining of the article , we only have one previous word say \"the\" , then we have an enormous number of possible next words\n",
    "\n",
    "$$ p(next word | \"the\") $$\n",
    "\n",
    "we may also condition on multiple previous words :\n",
    "\n",
    "$$p(w_t | w_{t-1},w_{t-2})$$\n",
    "\n",
    "\n",
    "To generalise this concept :\n",
    "\n",
    "First-order Markov:\n",
    "\n",
    "$$p(s_t|s_{t-1},s_{t-2},...,s_0) = p(s_t|s_{t-1})$$\n",
    "\n",
    "Second-order Markov:\n",
    "\n",
    "$$p(s_t|s_{t-1},s_{t-2},...,s_0) = p(s_t|s_{t-1},s_{t-2})$$\n",
    "\n",
    "Third-order Markov:\n",
    "\n",
    "$$p(s_t|s_{t-1},s_{t-2},...,s_0) = p(s_t|s_{t-1},s_{t-2},s_{t-3})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lets continue to talk about markov model , returning to our weather examples\n",
    " \n",
    "suppose we have thee states :\n",
    "\n",
    "<ul>\n",
    "    <li>sun</li>\n",
    "    <li>rain</li>\n",
    "    <li>cloud</li>\n",
    "</ul>\n",
    "\n",
    "we can represent this model as a graph , where each node is a state , and each edge is the probability of going from one state to the next state\n",
    "\n",
    "<img src='extras/24.1.PNG' width = 400></img>\n",
    "we can already see that this is a first-order markov model , since the weights only depend on the current state and it only affects the next state \n",
    "\n",
    "notice that it is possible for the next state to be the same as the current state , for instance we can have two rainy days in a row\n",
    "\n",
    "How many weights (probabilities) do we have for M states ?\n",
    "\n",
    "since we can go from any state to any other state (including itself) , there are <strong>M x M</strong> probabilities\n",
    "\n",
    "we store these in the state transition probability matrix A , where\n",
    "\n",
    "$$ A_{ij} = p(s_t = j | s_{t-1} = i), \\ \\forall \\ i = 1 \\cdots M , j = 1 \\cdots M$$\n",
    "\n",
    "note : assume stationary distribution , same probabilities for all time\n",
    "\n",
    "since whenever we are in state i , we must go to one of the M states and there are no other possibilities , therefore :\n",
    "\n",
    "$$ \\sum^M_{j=1} A_{ij} = 1 , \\forall i = 1 \\cdots M$$\n",
    "\n",
    "<h4>Starting position</h4>\n",
    "\n",
    "Another important concept in markov model is where we start , for ex , we are trying to model the first word of a sentence , this is called the initial state distribution and is represented by an M-dimensional vector $\\pi$ , and we usually treat is as a <strong>1 x M row vector</strong>\n",
    "\n",
    "$$ \\pi_i = p(s_1=i) , \\forall i = 1 \\cdots M$$\n",
    "\n",
    "of course al lthe probabilities must sum up to 1 since it is the probability that we start in each of the M states\n",
    "\n",
    "$$\\sum^M_{i=1} \\pi_i = 1$$\n",
    "\n",
    "<h4>Example</h4>\n",
    "\n",
    "Now lets try to calculate the probability of the following sequence :\n",
    "\n",
    "$$sun \\rightarrow sun \\rightarrow rain \\rightarrow cloud$$\n",
    "\n",
    "$$p(sun \\rightarrow sun \\rightarrow rain \\rightarrow cloud) = p(cloud|rain)p(rain|sun)p(sun|sun)p(sun)$$\n",
    "\n",
    "$= 0.2 \\times 0.05 \\times 0.8 \\times p(sun)$\n",
    "\n",
    "p(sun) is the same as $\\pi(sun)$ , not given in diagram\n",
    "\n",
    "In general the probability of any sequence can be calculated as :\n",
    "\n",
    "$$p(s_1,s_2,...,s_T) = p(s_1)\\prod^T_{t=2} p(s_t|s_{t-1})$$\n",
    "\n",
    "or counting from zero :\n",
    "\n",
    "$$p(s_0,s_2,...,s_T) = p(s_0)\\prod^T_{t=1} p(s_t|s_{t-1})$$\n",
    "\n",
    "<h4>Training</h4>\n",
    "\n",
    "Training a markov model is very intuitive , we just use maximum likelihood\n",
    "\n",
    "consider the dataset\n",
    "\n",
    "<ul>\n",
    "    <li>\"I like dogs\"</li>\n",
    "    <li>\"I like cats\"</li>\n",
    "    <li>\"I love kangaroos\"</li>\n",
    "</ul>\n",
    "\n",
    "we can treat each state as a word , so we have 6 states : I,like,love,dogs,cats,kangaroos , we can give these indexes 0 $\\rightarrow$ 5\n",
    "\n",
    "if we use maximum likelihood , then our initial state distribution is just :\n",
    "\n",
    "$\\pi(I) = 1 , \\pi(w) = 0 \\  for \\ w = \\{like,love,dogs,cats,kangoroos\\}$\n",
    "\n",
    "$\\pi = [1,0,0,0,0,0]$\n",
    "\n",
    "since all sentences start with word \"I\" , no sentences start with any other word\n",
    "\n",
    "next if the current word is \"I\" then there are two possibilities for next word \"like\" or \"love\"\n",
    "\n",
    "$p(like|I) = \\frac{2}{3} , p(love|I) = \\frac{1}{3} $\n",
    "\n",
    "$p(dogs|like) = p (cats|like) = \\frac{1}{2}$\n",
    "\n",
    "$p(kangroos | love ) = 1$\n",
    "\n",
    "all other state transition probabilities are 0\n",
    "\n",
    "<h4>Smoothing</h4>\n",
    "    \n",
    "The English language has over 1 million words , even if we have a large training data as wikipidea , will it have every possible transition bbetween pair of words ?\n",
    "\n",
    "probably not , which is problematic , if a transition never appears in training set , then it will have a probability of 0\n",
    "\n",
    "But in order to calculate the probability of a sequence , we have to multiply the probability of every transition that occurs , therefore if we find a transition in our test set that did not appear in our training set then its probability will be zero , and anything multiplied by zero is zero , so the probability of the entire sequence becomes zero just because a single transition did not occur during training\n",
    "\n",
    "the solution to this problem is smoothing\n",
    "\n",
    "No smoothing :\n",
    "\n",
    "$$A_{ij} = \\frac{count(i \\rightarrow j)}{count(i)}$$\n",
    "\n",
    "Add-one smoothing :\n",
    "\n",
    "$$A_{ij} = \\frac{count(i \\rightarrow j) + 1}{count(i) + V}$$\n",
    "\n",
    "Add-epsilon smoothing (can use cross-validation) :\n",
    "\n",
    "$$A_{ij} = \\frac{count(i \\rightarrow j) + \\epsilon}{count(i) + \\epsilon V}$$\n",
    "\n",
    "we add 1 or $\\epsilon$ so that to prevent the probability from becoming 0 , we add V to the denominator so that the sum of probabilities along each row still sums up to 1\n",
    "\n",
    "V is the vocabulary size , since each word in vocabulary is a state , then V = M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will talk about Markov Chains , which are actually just Markov models , but usually when we study Markov chains we are talking about discrete-time stochastic process (which obivosuly seems more mathematically invloved)\n",
    "\n",
    "So one question we might ask is , what is the probability of a sunny day 5 days from now ?\n",
    "\n",
    "first , consider the probability of it being sunny tomorrow , assuming we dont know the weather today $p(s_1 = sunny)$\n",
    "\n",
    "$$p(s_1 = sunny) = p(s_1 = sunny \\land s_0 = sunny) + \\\\ p(s_1 = sunny \\land s_0 = rainy) + \\\\ p(s_1 = sunny \\land s_0 = cloudy) \\\\ A_{sunny,sunny}\\pi_{sunny} + A_{rainy,sunny}\\pi_{rainy} + A_{cloudy,sunny}\\pi_{cloudy}$$\n",
    "\n",
    "In general :\n",
    "\n",
    "1 state ahead :\n",
    "\n",
    "$$p(s_1) = \\pi A$$\n",
    "\n",
    "2 states ahead:\n",
    "\n",
    "$$p(s_2) = \\pi AA = \\pi A^2$$\n",
    "\n",
    "t states ahead :\n",
    "\n",
    "$$p(s_t) = \\pi A^t$$\n",
    "\n",
    "<h4>Stationary Distributions</h4>\n",
    "\n",
    "now consider we start in a state distribution $p(s)$ and we multiply by A and then we end up with the same state distribution p(s) , this is what we call a stationary distribution , no matter how many times we transition form the state distribution still end up with the same state distribution\n",
    "\n",
    "$$p(s) = p(s)A$$\n",
    "\n",
    "How can we find a stationary distribution ?\n",
    "\n",
    "This is just the eigenvalue problem where eigenvalue is 1\n",
    "\n",
    "note : eigenvectors are not unique - normalise it to sum to 1\n",
    "\n",
    "note : eigen solvers solve $Av = \\lambda v$ with v being a column vector, but we want to satisfy $vA =  \\lambda v$ with v (representing $\\pi$) being a colum vector ,  this is the same as  $A^T v = \\lambda v$ with v being a column vector, so what we need to solve for $A^T$ not A\n",
    "\n",
    "\n",
    "<h4>Limiting Distributions</h4>\n",
    "\n",
    "Another question we may ask is what state do we expect ot end up in , we can think of this as the final state distribution or the state distribution at time $t = \\infty$\n",
    "\n",
    "$$p(s) = \\pi A^{\\infty}$$\n",
    "\n",
    "another way of thinking of that is , no matter how many times we transition after this point , we still expect to have the same state probability distribution \n",
    "\n",
    "$$p(s_\\infty) = \\lim_{t \\rightarrow \\infty} \\pi A^t$$\n",
    "\n",
    "we call this the limiting distribution or equilivruim distribution , since it is the state distribution that we settle in to after a very long time\n",
    "\n",
    "But this means that :\n",
    "\n",
    "$$\\pi A^\\infty = \\pi A^\\infty A \\rightarrow p(s_\\infty) = p(s_\\infty)A$$\n",
    "\n",
    "This implies : limiting distribution $\\rightarrow$ stationary distribution\n",
    "\n",
    "but the opposite is not true , a stationary distribution is not necessarily a limiting distribution\n",
    "\n",
    "so what does the equilibrium distribution tell us ? , \n",
    "\n",
    "suppose we have\n",
    "\n",
    "$p(s_{\\infty}) = \\left[\\frac{5}{10},\\frac{1}{10},\\frac{4}{10} \\right]$ (sunny,rainy,cloudy)\n",
    "\n",
    "Thim means that if we measure 1000 days of weather in the far future , we expect 500 days to be sunny , 100 days to be rainy , 400 days to be cloudy\n",
    "\n",
    "This allows us to gather staitistics/draw samples over time , as if they were coming from the stationary distribution\n",
    "\n",
    "This would not work however with something like stock prices , surely apple stock prices averaged over a year in 1996 would be completely different from apple's stock price when iPhone 6 released"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets take another example which models state of healthy vs having the cold or flu\n",
    "\n",
    "$p(sick|healthy) = 0.005$\n",
    "\n",
    "$p(healthy|healthy) = 0.995$\n",
    "\n",
    "$p(sick|sick) = 0.8$\n",
    "\n",
    "$p(healthy|sick) = 0.2$\n",
    "\n",
    "----------\n",
    "\n",
    "#1 : p(healthy 10 days in a row | healthy at t = 0)\n",
    "\n",
    "$0.995^9 = 95.6\\%$ , why 9 not 10 ?\n",
    "\n",
    "consider p(healthy 2 days in a row | healthy at t = 0) , that just 0.995 (exponent 1)\n",
    "\n",
    "100 days in a row ? : $0.995^99 = 60.9\\%$\n",
    "\n",
    "---------------\n",
    "\n",
    "#2 what is more probable ? p(sick 10 days in a row | sick at t = 0) or p(sick 5 days in a row then healthy 5 days | sichk at t = 0)\n",
    "\n",
    "p(sick 10 days | sick at t = 0) = $0.8^9 = 13.4\\% $\n",
    "\n",
    "p(sick 5 days in a row then healthy 5 days | sichk at t = 0) = $0.8^4 \\times 0.2 \\times 0.995^4 = 8\\%$\n",
    "\n",
    "so the first one is more probable\n",
    "\n",
    "--------------\n",
    "\n",
    "#3 : probability of alternating between sick and healthy for 10 days starting sick\n",
    "\n",
    "p(SHSHSHSHSH | $s_0$ = S) = $0.2^5 \\times 0.5^4 = 2^{-13}$\n",
    "\n",
    "Now for a harder problem\n",
    "\n",
    "----------------\n",
    "\n",
    "#4 : Expected number of days staying in same state (ex : How many days would I expect to stay sick given the model)\n",
    "\n",
    "first lets consider the probability of being in state i then going to state i in next state  $A_{i,i}$\n",
    "\n",
    "$p(s_t=i|s_{t-1}=i) = A_{i,i}$\n",
    "\n",
    "The probability of moving to a state other than i is just $1-A_{i,i}$\n",
    "\n",
    "$p(s_t\\neq i|s_{t-1}=i) = 1-A_{i,i}$\n",
    "\n",
    "we want to calculate the probability that we stay in state i for n transitions then transition to a different state , our joint probability distribution becomes :\n",
    "\n",
    "$$p(s_1 = i , s_2 = i , .... , s_n = i , s_{n+1} \\neq i ) = A_{i,i}^{n-1} \\left( 1 - A_{i,i} \\right)$$\n",
    "\n",
    "now we want to get the expected value for n :\n",
    "\n",
    "$$E(n) = \\sum np(n) \\\\ = \\sum_{n=1}^\\infty n A_{i,i}^{n-1} (1-A_{i,i}) \\\\ \\sum nA_{i,i}^{n-1} - \\sum nA_{i,i}^n$$\n",
    "\n",
    "let $a = A_{i,i}$\n",
    "\n",
    "-----------------------------------------\n",
    "\n",
    "first sum : $\\sum nA_{i,i}^{n-1}$\n",
    "\n",
    "1 : $X = 1 + 2a + 3a^2 + \\cdots$ \n",
    "\n",
    "multiply by 1 by a\n",
    "\n",
    "2 : $ax = 1a + 2a^2 + \\cdots$ \n",
    "\n",
    "now 1-2 :\n",
    "\n",
    "$(1-a)X = 1 + a + a^2 + \\cdots$\n",
    "\n",
    "now using the rule of of infinite sum for geometric series:\n",
    "\n",
    "$(1-a)X = \\frac{1}{1-a}$\n",
    "\n",
    "$X = \\frac{1}{{(1-a)}^2}$\n",
    "\n",
    "------------------------------\n",
    "\n",
    "second sum : $\\sum nA_{i,i}^{n}$\n",
    "\n",
    "1 : $X = 1a + 2a^2 + 3a^3 + \\cdots$ \n",
    "\n",
    "multiply by 1 by  a\n",
    "\n",
    "2 : $ax = 1a^2 + 2a^3 + \\cdots$ \n",
    "\n",
    "now 1-2 :\n",
    "\n",
    "$(1-a)X = a + a^2 + a^3 + \\cdots$\n",
    "\n",
    "\n",
    "$(1-a)X = \\frac{1}{1-a}-1 = \\frac{a}{1-a}$\n",
    "\n",
    "$X = \\frac{a}{{(1-a)}^2}$\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "now lets combine these , we subtracted second sum from first :\n",
    "\n",
    "$$E(n) = \\frac{1}{(1-a)^2} - \\frac{a}{(1-a)^2}$$\n",
    "\n",
    "$$E(n) = \\frac{1}{(1-a)}$$\n",
    "\n",
    "Expected number of sick days ?\n",
    "\n",
    "$$E(n) = \\frac{1}{1-0.8} = \\frac{1}{0.2} = 5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to talk about SEO and Bounce Rate Optimisation\n",
    "\n",
    "we hava a website , and we would like to increase conversions , increase traffic and acoid high bounce rate which may lead to google giving our page a low ranking\n",
    "\n",
    "Lets try to think about how a Markov model could be used \n",
    "\n",
    "---------------------\n",
    "\n",
    "<h4>Arrival</h4>\n",
    "\n",
    "How do people get to our page ?\n",
    "\n",
    "<ul>\n",
    "<li>Home page ?</li>\n",
    "<li>Landing page ?</li>\n",
    "</ul>\n",
    "\n",
    "This is the first page of a sequence of pages , the markov analogy here is the initial state distribution $\\pi$\n",
    "\n",
    "so once we have our Markov model , $\\pi$ will tell us which of our pages users are most likely to start on\n",
    "\n",
    "------------------\n",
    "\n",
    "<h4>Sequences of pages</h4>\n",
    "\n",
    "If we think that people take a certain sequence of actions , for ex :\n",
    "\n",
    "Landing page $\\rightarrow$  buy button $\\rightarrow$ checkout $\\rightarrow$ close browser\n",
    "\n",
    "we can check the probability tp cpnfirm the validity of that sequence\n",
    "\n",
    "of course Longer sequence == more multiplication == smaller number\n",
    "\n",
    "so there are two things that we can do to get a more meaningful answer :\n",
    "\n",
    "<ul>\n",
    "    <li>compare the log probability of two different sequences , so are people getting through entire checkout process or is it more probable that they are just bouncing</li>\n",
    "    <li>consider just the transition probabilities themselves , these are conditional probabilities instead of joint probabilities , so once we have made it to the landing page , what is the probability of hitting buy and once they have hit buy what is the probability of hitting checkout</li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Bounce rate</h4>\n",
    "\n",
    "As far as it concerns us this is hard to measuer ( unless we are google and we own the browser and we have analytics on almost every site on the web\n",
    "\n",
    "This is because once the user has left our site , we can no longer run code on their computer or ttrack what they are doing\n",
    "\n",
    "Lets assume we created the perfect logging system that knows when someone has bounced or they have gotten what they need from our page and left , in other words we know if that user found that page useful or not\n",
    "\n",
    "once we have done this , we can measure which page has the highest bouncing rate , at which point , we can manually analyze that page and ask our marketting people , what is different about this page that people do not find it useful or what about this page that makes people want to leave , we can then fix that problem and then hopefully , later analysis shows that the fixed page no longer has a high bounce rate\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Data</h4>\n",
    "\n",
    "the file is site_data.csv which has 2 columns\n",
    "\n",
    "<ul>\n",
    "    <li>last_page_id (current page)</li>\n",
    "    <li>next_page_id</li>\n",
    "</ul>\n",
    "\n",
    "our site has 10 pages , IDs 0...9\n",
    "\n",
    "start pages have last_page_id = -1 (we will see it shortly in code)\n",
    "\n",
    "the end of a sequence can be represented with two different codes B(bounce) or C(close)\n",
    "\n",
    "so if we see page id then B that means the user saw the page then bounced and if we see the page id then C that means the user saw the page stayed there , may be read some useful information and then closed the window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0 -1  8\n",
       "1  4  8\n",
       "2 -1  2\n",
       "3  1  B\n",
       "4 -1  5"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('datasets/site_data.csv',header = None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give B and C numeric ids , B --> 10 , C --> 11\n",
    "data = data.replace({'B':10},regex=True)\n",
    "data = data.replace({'C':11},regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the first this means 8 is a start page\n",
    "# see the third column , so user visited page 1 then bounced\n",
    "X = data.to_numpy().astype('int32')\n",
    "# get initial pages to calcualte pi\n",
    "initials = X[X[:,0]==-1]\n",
    "# get transitions\n",
    "transitions = X[X[:,0]!=-1]\n",
    "# we have 12 states : 10 pages + B + C\n",
    "M = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets fill in pi , we will use add one smoothing\n",
    "pi = np.ones(M) # this can be M-2 since we cant start in B or C\n",
    "# first we count\n",
    "for _,page in initials:\n",
    "    pi[page]+=1\n",
    "#next we get probabilities\n",
    "pi = pi/(pi.sum())\n",
    "# we can also convert these to log probabilities\n",
    "#pi = np.log(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next lets fill A\n",
    "A = np.ones((M,M))\n",
    "# first are counts\n",
    "for from_,to in transitions:\n",
    "    A[from_][to] += 1\n",
    "# then lets change these to probabilities\n",
    "A = A/A.sum(axis=1,keepdims=True)\n",
    "# log probabilities\n",
    "#A = np.log(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state distribution: \n",
      "[1.02974483e-01 1.03477779e-01 9.50727264e-02 9.51230560e-02\n",
      " 1.02420857e-01 9.77905280e-02 9.79918466e-02 9.97030550e-02\n",
      " 1.01514923e-01 1.03830087e-01 5.03296593e-05 5.03296593e-05]\n",
      "page with highest probability to start at :  9\n"
     ]
    }
   ],
   "source": [
    "# now lets print intiial state distribution\n",
    "print('initial state distribution: ')\n",
    "print(pi) # we cant start at B or C , but they have low probs ince we use add one smoothing\n",
    "print('page with highest probability to start at : ',pi.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounce rate for each page\n",
      "[0.12790123 0.12587671 0.12643106 0.1273679  0.12551261 0.12363405\n",
      " 0.12075984 0.12365591 0.12523295 0.1316888  0.08333333 0.08333333]\n",
      "page with highest bounce rate :  9\n"
     ]
    }
   ],
   "source": [
    "# now lets get bounce rate for each page\n",
    "print('bounce rate for each page')\n",
    "print(A[:,10]) # no bounce rates for B or C , still they get low probs due to add one smoothinh\n",
    "print('page with highest bounce rate : ',A[:,10].argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next example , we will discuss a non-first order markov chain , in this example we will try to create a 2nd-order language model\n",
    "\n",
    "so first we are going to train a model on the data to determine the distribution of a word giving the previous two words and then we will use this model to generate new phrases\n",
    "\n",
    "what we are attempting is Poetry Generation !\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Robert Frost Poems</h4>\n",
    "\n",
    "Text file with all poems concatenated together\n",
    "\n",
    "the first thing we want to do is tokenise each sentence and remove punctuation , to remove punctuation we can use python's translate function\n",
    "\n",
    "we also want to strip the new line and lower case all characters before splitting the sentence into tokens\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Counts</h4>\n",
    "\n",
    "In addition to the second order model counts we need to , there are other counts we need to make :\n",
    "<ul>\n",
    "    <li>Initial distribution of words : the distribution of the first word of a sentence</li>\n",
    "    <li>second word distribution (wont have 2 previous words)</li>\n",
    "    <li>End of line distribution (w(t-2),w(t-1) $\\rightarrow$ END) , so we will use a special token for END</li>\n",
    "</ul>\n",
    "\n",
    "The second order model keeps track of 3 words , if we were to represent this as a matrix it would be of size VxVxV , and it would be a very sparse matrix , so instead we use a dictionary , the key = ($w_{t-2},w_{t-1}$) , the value is a dictionary whose keys are words , values are counts (which later will be changed to probabilities)\n",
    "\n",
    "for ex if we have in our corpus:\n",
    "\n",
    "\"I love cats\"\n",
    "\n",
    "\"I love cats\"\n",
    "\n",
    "\"I love dogs\"\n",
    "\n",
    "then the key = (I,love) value = {dogs:1,cats:2}\n",
    "\n",
    "then we change to probabilities\n",
    "\n",
    "key = (I,love) value = {dogs:1/3,cats:2/3}\n",
    "\n",
    "the second word distribution would also be a VxV matrix and also would be sparse so same concept applies\n",
    "\n",
    "After we finish calculating probabilities we use them to sample the next word\n",
    "\n",
    "one thing we can do is also calculate the log probability of a generated sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(sent):\n",
    "    # remove punctuation\n",
    "    sent = sent.rstrip()\n",
    "    sent = sent.translate(str.maketrans('','',string.punctuation))\n",
    "    tokens = [w for w in sent.lower().split()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "# read poems\n",
    "for line in open('datasets/robert_frost.txt'):\n",
    "    # add <END>token and append sentence\n",
    "    tokens = tokenise(line)\n",
    "    if len(tokens) > 0:\n",
    "        sentences.append( tokens + ['<END>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial character distribution , p(w1) , where w1 is the first word of a sentence\n",
    "pi = {}\n",
    "# second character distribution , p( w2 | w1) # where w2 is the second word of a sentence\n",
    "second = {}\n",
    "# state transition , our 2nd-order model p( w(t) | w(t-2),w(t-1) )\n",
    "A = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets fill in pi\n",
    "for sentence in sentences:\n",
    "    pi[sentence[0]] = pi.get(sentence[0],0) + 1\n",
    "        \n",
    "# now change to probabilities\n",
    "pi_sum = sum(pi.values())\n",
    "\n",
    "for k,v in pi.items():\n",
    "    pi[k] = v/pi_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next lets fill in second\n",
    "\n",
    "for sentence in sentences:\n",
    "    w1,w2 = sentence[0],sentence[1]\n",
    "    if w1 not in second.keys():\n",
    "        second[w1] = {}\n",
    "    second[w1][w2] = second[w1].get(w2,0)+1\n",
    "        \n",
    "# now change to probability\n",
    "\n",
    "for k,word2count in second.items():\n",
    "    sum_counts = sum(word2count.values())\n",
    "    \n",
    "    for word,count in word2count.items():\n",
    "        second[k][word] = count/sum_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now A , this is the same as second with a sequence of three words instead of three\n",
    "\n",
    "for sentence in sentences:\n",
    "    for i in range(len(sentence)-2):\n",
    "        w1,w2,w3 = sentence[i],sentence[i+1],sentence[i+2]\n",
    "        \n",
    "        if (w1,w2) not in A.keys():\n",
    "            A[(w1,w2)] = {}\n",
    "        A[(w1,w2)][w3] = A[(w1,w2)].get(w3,0)+1\n",
    "        \n",
    "# now change to probability\n",
    "\n",
    "for k,word2count in A.items():\n",
    "    sum_counts = sum(word2count.values())\n",
    "    \n",
    "    for word,count in word2count.items():\n",
    "        A[k][word] = count/sum_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets generate a line\n",
    "\n",
    "# we will be sampling words as we will see shortly\n",
    "# we will stop whenever we sample the token <END>\n",
    "# or whenever we reach the max len of a sentence\n",
    "max_len = max(len(sentence) for sentence in sentences) - 1 # since these have <END> token added\n",
    "\n",
    "def sample_word(word2prop):\n",
    "    i = np.random.choice(len(word2prop),p = list(word2prop.values()))\n",
    "    w = list(word2prop.keys())[i]\n",
    "    return w\n",
    "\n",
    "def sample_line():\n",
    "    line = []\n",
    "    # first we use pi probabilities to sample the first word\n",
    "    w1 = sample_word(pi)\n",
    "    line.append(w1)\n",
    "    # next we want to use second to sample the second word given first\n",
    "    w2 = sample_word(second[w1])\n",
    "    \n",
    "    if w2 == '<END>':\n",
    "        return ' '.join(line) \n",
    "    line.append(w2)\n",
    "    \n",
    "    for _ in range(max_len-2):\n",
    "        # sample a word given previous 2 words\n",
    "        w1,w2 = line[-2],line[-1]\n",
    "        # now lets sample w3 from A\n",
    "        w3 = sample_word(A[(w1,w2)])\n",
    "        if w3 == '<END>':\n",
    "            return ' '.join(line)\n",
    "        line.append(w3)\n",
    "\n",
    "    return ' '.join(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_stanza(n=4):\n",
    "    for _ in range(n):\n",
    "        print(sample_line())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looked up toward heaven and there a tent in grove and\n",
      "two winds would meet\n",
      "thats no way for a home for me in the mountain\n",
      "i shall be telling this with a hornyhanded kindness\n"
     ]
    }
   ],
   "source": [
    "sample_stanza()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    while True:\n",
    "        sample_stanza()\n",
    "        print('-----------------------------------------------')\n",
    "        r = input('one more [Y/n] ')\n",
    "        print('-----------------------------------------------')\n",
    "        if r.lower() == 'n':\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but i dont trust your eyes youve said enough\n",
      "old davis owned a solid mica mountain\n",
      "life long in that grave word\n",
      "hes got hay down thats been there for hawks since chickentime\n",
      "-----------------------------------------------\n",
      "one more [Y/n] Y\n",
      "-----------------------------------------------\n",
      "like a bird silent in flight\n",
      "yet does perhaps for all three\n",
      "what bones the cellar to the cellar\n",
      "to find yourself\n",
      "-----------------------------------------------\n",
      "one more [Y/n] n\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not done Yet !\n",
    "\n",
    "Markov models are still being used , Google's PageRank algorithm is based on Markov Models\n",
    "\n",
    "<h4>PageRank</h4>\n",
    "\n",
    "We have M webpages that link to each other , and we would like to assign importance scores $x_1,...,x_M \\geq 0$ , so we want to assign a PageRank to each page\n",
    "\n",
    "we can think as web pages visits as a sequence and the page as the state\n",
    "\n",
    "The rank comes from the limiting distribution , that is in the long run , what is the proportion of visits that will be to this page ?\n",
    "\n",
    "But how can we do this ?\n",
    "\n",
    "How do we train the markov model , what values do we assign to the state transition matrix and how do we guarantee that the limiting distribution exists and is unique\n",
    "\n",
    "The main idea is that a link to a page is like a vote for its importance\n",
    "\n",
    "so we begin by counting , $A_{i,j}$ means page i has a link to page j , after we finish we divide each row by its sum to get a valid probability distribution\n",
    "\n",
    "---\n",
    "This approach already solves a couple of problems\n",
    "\n",
    "A spammer may try to sell 1000 links on their page , But we know that the transition matrix has to remain a valid probability matrix , so the rows must sum to 1 , which means that each of the spammer's links has strength 1/1000\n",
    "\n",
    "Haha , Spammer then tries to make 1000 pages , each with 1 link and he thinks maybe he fooled google !\n",
    "\n",
    "Unfortunately since nobody knows about these 1000 pages we just created nobody is going to link to them which means they are impossible to get to so in the limiting distribution those states will have 0 probability because we cant even get to them so their out going links are worthless\n",
    "\n",
    "Remember a markov chain's limiting distribution will model the long run proportion of visits to a state so if you never visit that state its probability will be zero\n",
    "\n",
    "---\n",
    "\n",
    "Now does the limiting distribution exists , and if so is it unique ?\n",
    "\n",
    "<h4>Perron-Frobenuis Theorem</h4>\n",
    "\n",
    "States that if we have a Markov Matrix (all rows sum to 1) , and all values are strictly positive (none are 0) , then the stationary distribution exists and is unique\n",
    "\n",
    "in fact we can start in any initial state , and as time approaches infinity , we will always end up with the same stationary distribution , therefore this is also the limiting distribution\n",
    "\n",
    "---\n",
    "\n",
    "How can we satisfy the PF criterion ?\n",
    "\n",
    "Smoothing : give things that are 0 a small value , so there is a small possibility that we can still get to that state (good news for the spammer)\n",
    "\n",
    "so create a uniform distribution , U = a/M (MxM matrix)\n",
    "\n",
    "The PageRank solution :\n",
    "\n",
    "G = 0.85A + 0.15U\n",
    "\n",
    "G : PageRank matrix\n",
    "\n",
    "A : our original matrix (which may not satisfy PF criterion )\n",
    "\n",
    "U : our uniformly distributed matrix\n",
    "\n",
    "now all elements in G are strictly positive , and all rows sum to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take a quick example on the PF criterion\n",
    "M = 20\n",
    "# create a random matrix , sample from unigorm distribution [0,1]\n",
    "A = np.random.rand(M,M)\n",
    "# just make sure A is strictly positive\n",
    "A = A + 1e-8\n",
    "# now normalise so each row sum up to 1\n",
    "A = A/A.sum(axis=1,keepdims=True)\n",
    "# now A satisfies the PF criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 : get limiting distribution iteratively , we know this converges as time approaches infinity\n",
    "v1 = np.random.rand(M)\n",
    "# normalise so its a valid probability distribution\n",
    "v1 = v1/v1.sum()\n",
    "\n",
    "for i in range(1000):\n",
    "    # or v1@A if v is a 1xM vector these are the same \n",
    "    # but this for looks more like the eigenvalue problem\n",
    "    # explais why we will feed in A.T not A\n",
    "    v1 = A.T@v1 \n",
    "    # make sure it remains a valid probability distribution\n",
    "v1 = v1/v1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 : get statinoary distribution using eigens as we have shown before\n",
    "evals,evecs = np.linalg.eig(A.T)\n",
    "\n",
    "# get index where eigen value = 1\n",
    "i = np.argmax(np.isclose(evals,1))\n",
    "# get corresponding eigen value\n",
    "v2 = evecs[:,i]\n",
    "v2 = np.real(v2)\n",
    "# normalise\n",
    "v2 = v2/v2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.isclose(A.T@v2,v2).all())\n",
    "print(np.isclose(A.T@v1,v1).all())\n",
    "print(np.isclose(v1,v2).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03886189 0.05215062 0.04046588 0.05911771 0.04666012 0.04141172\n",
      " 0.05218089 0.05025517 0.05706079 0.04281098 0.05228438 0.05308884\n",
      " 0.04950908 0.05474549 0.05838592 0.04162408 0.05790279 0.04501665\n",
      " 0.05539791 0.05106911]\n",
      "---------------\n",
      "[0.03886189 0.05215062 0.04046588 0.05911771 0.04666012 0.04141172\n",
      " 0.05218089 0.05025517 0.05706079 0.04281098 0.05228438 0.05308884\n",
      " 0.04950908 0.05474549 0.05838592 0.04162408 0.05790279 0.04501665\n",
      " 0.05539791 0.05106911]\n"
     ]
    }
   ],
   "source": [
    "print(v1)\n",
    "print('---------------')\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we get our unique stationary distribution which is also the limiting distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
