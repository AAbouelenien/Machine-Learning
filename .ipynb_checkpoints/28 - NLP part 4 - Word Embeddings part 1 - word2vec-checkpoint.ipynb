{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we start by looking at the word2vec algorithm\n",
    "\n",
    "recall what we discussed previously (notebook 27) , we discussed how we can create language models using bigrams , that a probability of the next word given the current word , crucially we use a neural network to accomplish this \n",
    "\n",
    "Bigram model :\n",
    "\n",
    "$$p(x_{t+1}|x_t) = softmax \\left( W^{(2)T} f \\left( W^{(1)T}x_t\\right) \\right)$$\n",
    "\n",
    "<img src='extras/28.1.PNG' width = '400'></img>\n",
    "\n",
    "we were also introduced to word embeddings , and we looked at how to work with word vectors , amongst this we stressed on a single fact , a word embedding matrix is just a matrix of size VxD , but wait a minute !\n",
    "\n",
    "if we look at our bigram neural network model what do we see ,  what we see is exactly a matrix that could be used as a word embedding , in fact this neural netowk has 2 matricies of size (VxD) because the input is of size V and the output is of size V\n",
    "\n",
    "So what we can conclude is that , if we use a meural network to find word embeddings , look no further than the bigram model , we already know how to implement this so the next question would be : Does it give good embeddings ?\n",
    "\n",
    "The answer is no , but anyways this is our job now , word2vec is an extension of the bigram model that would yeild better results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se start by looking at one extension on the simple bigram that makes it into a word2vec algorithm\n",
    "\n",
    "This is called CBOW - continuous bag of words\n",
    "\n",
    "---\n",
    "\n",
    "<h4>CBOW</h4>\n",
    "\n",
    "The basic idea is this , instead of having just one input word to try to predict hte next word , instead we are goinf to have a wjole list of words trying to predict a word in the middle\n",
    "\n",
    "lets take an example , consider the following sentence\n",
    "\n",
    "<p>The quick <span style=\"color:blue\">brown fox <span style=\"color:green\">jumps</span> over the</span> lazy dog</p>\n",
    "\n",
    "Then an example of a training sample for CBOW would be input : [brown,fox,over,the] , output : [jumps]\n",
    "\n",
    "<img src='extras/28.2.PNG' width='400'></img>\n",
    "\n",
    "In this example , we would say that the context size is 2 , since we have gone 2 words to the left and 2 words to the right (depending on the author , context size may also refer to the total number of surrounding words which in this case is 4)\n",
    "\n",
    "In practice , context size is usually set from 5-10 (on either side) , of course this is a hyperparameter that may require some tuning\n",
    "\n",
    "Another thing that might not be clear is that the input weight $W^{(1)}$ is the same shared weight for all the input words , this is just the input weight in the bigram repeared however many times we have a context word\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Why does it make sense ?</h4>\n",
    "\n",
    "Lets try to get some intuition on why this kind of model might make sense\n",
    "\n",
    "recall that this is not the first time we have seen this in 'article spinning'\n",
    "\n",
    "\n",
    "for a simple article spinner , consider the following two sentences:\n",
    "\n",
    "\"I walked to the store\"\n",
    "\n",
    "\"I went to the store\"\n",
    "\n",
    "these more or less have the same meaning , so if we replace one setence with another in a story that we are writing it would essentialy have no effect on how this story is interpreted which is what we want\n",
    "\n",
    "so one simple way of making these modifications is to come up with a language model where the current word is not only conditioned on the previous word (as in a bigram) , but on the next word as well\n",
    "\n",
    "so we want to find the probabilities : \n",
    "\n",
    "$$p(w_t \\vert w_{t-1},w_{t+1})$$\n",
    "\n",
    "Then sample from them to get words that can replace original\n",
    "\n",
    "If we think about it , this is exactly like our CBOW model with a context size of 1\n",
    "\n",
    "As usual , these probabilities can be found by counting or by the use of a neural network (which is what we are doing in this case)\n",
    "\n",
    "---\n",
    "\n",
    "<h4>The mechanics of CBOW</h4>\n",
    "\n",
    "lets now dig in to the model itself and the details of how it really works so that we have some idea of how we would implement it\n",
    "\n",
    "note that we wont be implementing the CBOW variant of word2vec since other methods have been shown to work better , but its a good start since it is the most intuitive\n",
    "\n",
    "This picture should give us a good idea of what is going on\n",
    "\n",
    "<img src='extras/28.3.PNG' width='300'></img>\n",
    "\n",
    "for each word in the context we grab its word embedding , we call that $W^{(1)}_c$ , that is the cth row of $W^{(1)}$\n",
    "\n",
    "Then (probably not clear in picture) we take the mean of all these vectors \n",
    "\n",
    "$$h = \\frac{1}{\\vert C \\vert} \\sum_{c \\in C} W^{(1)} c$$\n",
    "\n",
    "now we can see why we call this bag of words since it looks like the bag of words classifier we looked at previously but in this case we are not yet done\n",
    "\n",
    "This only gives us the value at the hidden layer , we still need to predict the middle word , in order to do that\n",
    "\n",
    "$$p(y|C) = softmax \\left( W^{(2)T} h\\right)$$\n",
    "\n",
    "note that there is no hidden activation function\n",
    "\n",
    "normally in deep learning we do use a hidden activation in order to make the mode lnon-linear\n",
    "\n",
    "but in all of the word2vec algorithms non-linear hidden activation functions are not used , interestingly both word2vec and GloVe are linear models , they are part of the \"Deep Learning Family\" but there is actually nothing deep about the models thaey are based on  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is another method to extend the bigram called the skipgram\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Skipgram</h4>\n",
    "\n",
    "what we will see that the skip gram is kind of the opposite of what we learned about which waas CBOW\n",
    "\n",
    "back to our example:\n",
    "\n",
    "<p>The quick <span style=\"color:blue\">brown fox <span style=\"color:green\">jumps</span> over the</span> lazy dog</p>\n",
    "\n",
    "recall that for CBOW , input : [brown,fox,over,the] , and we try to predict [jumps]\n",
    "\n",
    "with skipgram we do the opposite , we instead take the word jumps as input and we try to predict all 4 words [brown,fox,over,the]\n",
    "\n",
    "<img src='extras/28.4.PNG' width='400'></img>\n",
    "\n",
    "<img src='extras/28.5.PNG' width='300'></img>\n",
    "\n",
    "lets try to explain this in terms we know about\n",
    "\n",
    "It is helpful to think in terms of bigram , one training sample for this model would be input : jumps , target : over , this makes sense since we do softmax at the end and we try to predict one word\n",
    "\n",
    "with the skipgram we can do the same thing except we can pretend that now there are three more training samples for a total of 4 training samples , so here are all our samples\n",
    "\n",
    "<ul>\n",
    "    <li>jumps $\\rightarrow$ over</li>\n",
    "    <li>jumps $\\rightarrow$ brown</li>\n",
    "    <li>jumps $\\rightarrow$ fox</li>\n",
    "    <li>jumps $\\rightarrow$ the</li>\n",
    "</ul>\n",
    "\n",
    "again thats just like our bigram neural network with more training samples added to it\n",
    "\n",
    "in particular , instead of having just bigrams we are also adding pair of words which come from the context , it is like a bigram but we are skipping a few words in between , hence the term skipgrams\n",
    "\n",
    "---\n",
    "\n",
    "<h4>1 problem - 2 perspectives</h4>\n",
    "\n",
    "we have 2 different ways of thinking of the same problem\n",
    "\n",
    "The first way is to think of one sample as one word with four targets \"4-headed dragon\" , that means we have a shared output weight $W^{(2)}$ that repeats 4 times , we have 4 softmaxes , 4 predictions and 4 errors summed to get total error\n",
    "\n",
    "The other way (probably much simpler) , is to think of it as 4 different samples each with one input word and one target word (each has 1 softmax and 1 prediction) , in the end we still end up summing the individual errors since we are doing batch gradient descent , so mathematically these two are equivalent\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Good Accuracy ?</h4>\n",
    "\n",
    "normally when we build a neural network classifier we are trying to get good accuracy\n",
    "\n",
    "in other words for a certain input , we are always trying to predict a certain target\n",
    "\n",
    "but in this case that actually doesnot make any sense , we have a single input and we are actually trying to make it hit multiple targets\n",
    "\n",
    "so a good reminder is that with language modelling the notion of accuracy is meaningless\n",
    "\n",
    "There is no such thing as good accuracy because a single word can be pointing to any number of other words , and we can see that directly now even with just one sample\n",
    "\n",
    "we will be using skipgram , but there are still some more modifications to make in order for it to work better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is yet another modification we need to make in order to get our final word2vec model , this is called Hierarchical Softmax\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Hierarchical Softmax</h4>\n",
    "\n",
    "first lets talk about the motivation behind Hierarchical Softmax\n",
    "\n",
    "In any language model when we are trying to predict a word then our output , which we are going to do a softmax over , is going to have a very large dimensionality , the number of outputs have to be of size V (size of vocabulary)\n",
    "\n",
    "V can be extremely large , pretrained GloVe - V = 400,000 , pretrained word2vec , V = 3 million , we wont have to worry about these large numbers since we ourselves will limit our vocabulary to be much smaller for educational purposes\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Large number of output classes</h4>\n",
    "\n",
    "first we know emperically (based on experimental evidence) that this negatively affects accuracy\n",
    "\n",
    "Mathematically , if we have 3 million classes , we have to pick the correct answer out of 3 million possible answers !\n",
    "\n",
    "Thats basically saying that 1 is right and 2,999,999 are wrong\n",
    "\n",
    "Now suppose that each word is equally represented at the targets , that means that 99.99997% of the time , any given word is not the target\n",
    "\n",
    "The proportion of time that a word spends being the target is just 1 out of 3 million\n",
    "\n",
    "in other words , we spend the majority of the time pushing the answer away from words than we do trying to get it towards the right asnwer (we will return to this point shortly just skip if not clear)\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Softmax</h4>\n",
    "\n",
    "consider the formula for Softmax\n",
    "\n",
    "$$p(y=j|x) = \\frac{\\exp(w_j^Tx)}{\\boxed{\\sum\\limits^V_{k=1}}\\exp(w_k^Tx)}$$\n",
    "\n",
    "is says that whenever we want to compute the output probability we need to first exponentiate each output node and then some over all the values\n",
    "\n",
    "Taking this sum is $O(V)$ , and so if V is large , then thats going to make thing s extremely slow\n",
    "\n",
    "looking at the equation makes it easier to understand the previous problem , to compute the probability for $p(y=j|x)$ the softmax requires all the vectors in $W^{(2)}$ (to calculate the denominator) not just $w_j$ this means that when we backpropagate , 1 out of 3M of our computation will be to get closer to the correct word and 2,999,999 out of 3,000,000 will be to get away from the wrong words\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Solutions</h4>\n",
    "\n",
    "There are two solutions to this , but we will spend the majority of our efforts focusing on the method that has been shown to work best\n",
    "\n",
    "In either of these solutions what we want to do is get rid of the softmax entirely\n",
    "\n",
    "The first solution (the one we wont focus on) is Hierarchical softmax\n",
    "\n",
    "for hierarchical softmax the basic idea is this , instead of trying to predict whether or not the output word belongs to 1 of the 3 million classes all at the same time , lets instead break it down into a binary tree\n",
    "\n",
    "<img src='extras/28.6.PNG' width='300'></img>\n",
    "\n",
    "so for example , one side may be responsible for 1M words and the other side may be responsible for the other 2M , then this 2M could be further split into 2M at the left and 1M at the right and at every node we keep splitting into 2 until eventually we get into a single word , so every leaf node in this tree represents a word\n",
    "\n",
    "This also means that every path dont go down doesnot need to be considered\n",
    "\n",
    "---\n",
    "\n",
    "<h4>more details</h4>\n",
    "\n",
    "1) since at every node we have to make a binary decision we can use the sigmoid for this , this tells us the probability that we should go right or left\n",
    "\n",
    "$$p(go \\ right) = \\sigma(v_{node}^Th)$$\n",
    "\n",
    "typically we can say that the sigmoid outputs the probability of going right so then the probability of going left is just $1 - p(go \\ right)$\n",
    "\n",
    "2) in order to get the final probability of a single word represented by a leaf all we need to do is mulyiply all the individual probability along the path of the tree from the root to that node\n",
    "\n",
    "<img src='extras/28.7.PNG' width='300'></img>\n",
    "\n",
    "$$p(w|input) = \\prod_{node \\in path \\ to \\ w} \\sigma(v_{node}^T h_{input})$$\n",
    "\n",
    "since this is a probability tree , we know that all the probabilities at the leaf nodes will sum up to 1\n",
    "\n",
    "Here are some examples :\n",
    "\n",
    "<ul>\n",
    "    <li>$p(cat) = 0.4$</li>\n",
    "    <li>$p(racoon) = 0.6 \\times 0.7 = 0.42$</li>\n",
    "    <li>$p(dog) = 0.6 \\times 0.3 \\times 0,1 = 0.018$</li>\n",
    "    <li>$p(apple) = 0.6 \\times 0.3 \\times 0.9 = 0.162$</li>\n",
    "</ul>\n",
    "\n",
    "sum = 1 , just like a softmax should , so that why we say that the hierarchical softmax is an approximation to the softmax\n",
    "\n",
    "3) since we have a binary tree , there is no need for a DxV output matrix , this huge matrix also leads to slowness in the naive model \n",
    "\n",
    "recall that we have 2 matricies , input to hidden size VxD , and output size DxV\n",
    "\n",
    "The input to hidden matrix is not a problem , since when we have an input word , all we do is use that as an index \n",
    "\n",
    "But for the output matrix , the naive approach requires us to do a full matrix multiply because h and $W^{(2)}$ are both dense\n",
    "\n",
    "Now , at each node all we need to do is compute a scalar , so we just need a single vector of size D\n",
    "\n",
    "now because the binary tree has $O(V)$ leaf nodes , its also going to have $O(V)$ weight vectors of size V , so every split of the tree has its own $V$ size vector $v_{node}$\n",
    "\n",
    "The trick is most of these we never need to multiply with because we dont go down that path in the tree , in fact the number of operations we know need to do is $O(logV)$ since now this is a binary search\n",
    "\n",
    "---\n",
    "\n",
    "<h4>How to make this tree ? </h4>\n",
    "\n",
    "The way this tree is formed has a big impact on performance so its quite important ,quite a bit of research has been done here are the links of some papers <a href = 'http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf\n",
    "' >Hierarchical Softmax</a> , <a href = 'http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf\n",
    "' >More about Hierarchical Softmax</a>\n",
    "\n",
    "One trick to consider in regards to the hierarchical softmax is one good way of finding this tree\n",
    "\n",
    "we know that words like \"the\",\"a\",\"in\" show up a lot in our data (infact \"the\" is the most frequent word in english language)\n",
    "\n",
    "but words as \"Microprocessor\",\"programming\",\"onomatopoeia\" will be very infrequent\n",
    "\n",
    "so what do we want ? \n",
    "\n",
    "we would like\n",
    "\n",
    "<ul>\n",
    "    <li>frequent words to be closer to the top , so we dont go that far down into the tree</li>\n",
    "    <li>infrequent words to be closer to the bottom , since we dont mind going that far into the tree because thats just not going to happen very often</li>\n",
    "</ul>\n",
    "\n",
    "And so we can use a technique called \"Huffman coding\" which accomplished this optimally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to the second method which in comparison to Hierarchical Clustering is much simpler , This technique is called Negative Sampling\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Negative Sampling</h4>\n",
    "\n",
    "The basic idea is this , we discussed before how when using softmax , basically every word is going to be a non-target over 99% of the time\n",
    "\n",
    "with softmax what we are saying is every word thats not the target is going to be considered wrong\n",
    "\n",
    "<img src='extras/28.8.PNG' width='150'></img>\n",
    "\n",
    "what negative sampling does is , instead of saying everyother word is wrong , we simply take a sample of these other words and say that some of them are wrong\n",
    "\n",
    "<img src='extras/28.9.PNG' width='150'></img>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Multiclass and Binary Classification</h4>\n",
    "\n",
    "lets look exactly at how this is implemented so we get a clear picture\n",
    "\n",
    "first we need to consider the difference between multiclass and binary classification\n",
    "\n",
    "\n",
    "<strong>Multiclass :</strong>\n",
    "\n",
    "we normally use the softmax to get predictions\n",
    "\n",
    "$$p(y_{out}|x_{in}) = \\frac{\\exp(W^{(2)^{\\ \\ T}}_{\\ \\ out}W^{(1)}_{\\ \\ in})}{\\sum\\limits^V_{j=1}\\exp(W^{(2)^T}_{\\ \\ j} W^{(1)}_{\\ \\ in})}$$\n",
    "\n",
    "Then we use the multiclass cross entropy as our objective\n",
    "\n",
    "$$J = t_{out} \\log p(y_{out}|x_in)$$\n",
    "\n",
    "note : the above is the positive form , so this is just the log likelihood\n",
    "\n",
    "\n",
    "<strong>Binary :</strong>\n",
    "\n",
    "we use the sigmoid to get our predictions\n",
    "\n",
    "$$p(y_{out} = 1 | x_{in}) = \\sigma(W^{(2)^{\\ \\ T}}_{\\ \\ out} W^{(1)}_{in})$$\n",
    "\n",
    "then we use the binary cross entropy as our cost\n",
    "\n",
    "$$J = t_{out} logp(y_{out}=1 \\vert x_{in}) + (1-t_{out}) log (1-p(y_{out} = 1 \\vert x_{in}))$$\n",
    "\n",
    "note : again thats the likelihood\n",
    "\n",
    "we have seen all these equations before , but now it is in the context of a bigram neural network model instead of just a general input feature x\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Negative Sampling</h4>\n",
    "\n",
    "so the idea behind negative sampling is , instead of doing softmax and doing the full softmax cross-entropy just forget about softmax :)\n",
    "\n",
    "lets instead do a bunch of little binary cross-entropies\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Example</h4>\n",
    "\n",
    "again our example :\n",
    "\n",
    "<p>The quick <span style=\"color:blue\">brown fox <span style=\"color:green\">jumps</span> over the</span> lazy dog</p>\n",
    "\n",
    "Input word : <span style=\"color:green\">jumps</span></span>\n",
    "\n",
    "Target words : <span style=\"color:blue\">brown,fox,over,the</span></span>\n",
    "\n",
    "Negative samples : <span style=\"color:red\">apple,orange,boat,tokyo</span></span>\n",
    "\n",
    "note : the negative samples are just randomly sampled words in our vocabulary , so if we were to write these out explicitly , we would first calculate the following outputs\n",
    "\n",
    "$$p(brown|jumps) = \\sigma(W^{(2)^T}_{brown}W^{(1)}_{jumps})$$\n",
    "\n",
    "$$p(fox|jumps) = \\sigma(W^{(2)^T}_{fox}W^{(1)}_{jumps})$$\n",
    "\n",
    "$$p(over|jumps) = \\sigma(W^{(2)^T}_{over}W^{(1)}_{jumps})$$\n",
    "\n",
    "$$p(the|jumps) = \\sigma(W^{(2)^T}_{the}W^{(1)}_{jumps})$$\n",
    "\n",
    "$$p(apple|jumps) = \\sigma(W^{(2)^T}_{apple}W^{(1)}_{jumps})$$\n",
    "\n",
    "$$p(orange|jumps) = \\sigma(W^{(2)^T}_{orange}W^{(1)}_{jumps})$$\n",
    "\n",
    "$$p(boat|jumps) = \\sigma(W^{(2)^T}_{boat}W^{(1)}_{jumps})$$\n",
    "\n",
    "$$p(tokyo|jumps) = \\sigma(W^{(2)^T}_{tokyo}W^{(1)}_{jumps})$$\n",
    "\n",
    "ok , so now we have these 8 posterior probabilities , what do we do with them ?\n",
    "\n",
    "well of course now we can calculate the binary cross entropy\n",
    "\n",
    "so we know that brown,fox,over, are all positive samples , and we know that apple,orange,boat,tokyo are all negative samples\n",
    "\n",
    "so the full expression for the binary cross entropy becomes :\n",
    "\n",
    "$$J = \\log p(brown \\vert jumps) + \\log p(fox \\vert jumps) + \\log p(over \\vert jumps) + \\log p(the \\vert jumps) + \\log [1-p(apple \\vert jumps)] + \\log[1 - p(orange \\vert jumps)] + \\log[1-p(boat \\vert jumps)] + \\log[1-p(tokyo \\vert jumps)]$$\n",
    "\n",
    "note : this is what we want to maximise\n",
    "\n",
    "notice how when we already know the labels of each word we dont need to specify the $t$s or the $1-t$s since we know this are going to evaluate to either 1 or 0\n",
    "\n",
    "we can see how the problem with softmax is solved now , since $p(y_{out} = 1 | x_{in}) = \\sigma(W^{(2)^{\\ \\ T}}_{\\ \\ out} W^{(1)}_{in})$ , the probability depends now only on vectors $W^{(2)}_{\\ \\ out} \\ and \\ W^{(1)}_{in}$ so when we backpropagate we need only to update these two , unlike the softmax the required us to update the entirety of $W^{(2)}$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "we can generalise this equation to make it a little more compact\n",
    "\n",
    "if we let C = set of context words and N = set of negative samples , then we can write the cost as \n",
    "\n",
    "$$J = \\sum_{c \\in C} \\log \\sigma \\left(W^{(2)^T}_{\\ \\ c} W^{(1)}_{\\ \\ in}\\right) + \\sum_{n \\in N} \\log \\left[1 - \\sigma \\left( W^{(2)^{\\ T}}_n W^{(1)}_{\\ in} \\right)  \\right]$$\n",
    "\n",
    "\n",
    "now if we recall one identity we have when it comes to the sigmoid is that if we negate the logit , we get the opposite probability\n",
    "\n",
    "so we write :\n",
    "\n",
    "$$p(y=1 \\vert x) = \\sigma(logit) , p(y=0 \\vert x) = \\sigma(-logit)$$\n",
    "\n",
    "$$\\sigma(logit) + \\sigma(-logit) = 1$$\n",
    "\n",
    "so we can write the cost even more compactly by replacing the 1 - $\\sigma(...)$ term\n",
    "\n",
    "$$J = \\sum_{c \\in C} \\log \\sigma \\left( W^{(2)^{\\ \\ T}}_{\\ \\ c} W^{(1)}_{\\ \\ in}\\right) + \\sum_{n \\in N} \\log \\sigma \\left( - W^{(2)^{\\ T}}_{n} W^{(1)}_{\\ \\ in}\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Negative Sampling Gradient</h4>\n",
    "\n",
    "since we will be implementing negative sampling , we want to derive the gradient , luckily this is just binary classification so this should be easy\n",
    "\n",
    "lets return to the raw form of our loss since it is easier to work with :\n",
    "\n",
    "$$J = -\\sum^N_{n=1} t_{n} log p_{n} + (1-t_{n}) \\log (1-p_n) $$\n",
    "\n",
    "note : p refers to $p(y=1|x)$ , where y can refer to any output word\n",
    "\n",
    "note : n now refers to sample index and N refers to toal number of samples in batch\n",
    "\n",
    "recall that derivative of the cost wrt to $W^{(2)}$ was :\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W^{(2)}} = H^T(P-T)$$\n",
    "\n",
    "note : this is the same derivative we get when we do binary logistic regression (for full derivation refer to notebook 5)\n",
    "\n",
    "note : here H just refers to the value at the hidden layer\n",
    "\n",
    "note : as usual we use capital letters to reger to the full array of things , P : a vector of all of the N output probabilities whereas $p_n$ refers to the output probability at the nth sample\n",
    "\n",
    "H refers to the word vector of the input word , and in fact , unlike the general case where we have and NxD matrix H is just a D size vector because it is just the word vector for the single input word\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Problem!</h4>\n",
    "\n",
    "This might sound conufing at first lets see why\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W^{(2)}} = H^T (P-T)$$\n",
    "\n",
    "$H = W^{(1)}_{input} $ (a vector of length D)\n",
    "\n",
    "P - T (vector of length N)\n",
    "\n",
    "so it doesnot seem like we can multiply these since they are two vectors of different sizes\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Fixing the problem</h4>\n",
    "\n",
    "it helps to think about just a single sample to see what we really want to do\n",
    "\n",
    "suppose we have 1 sample , so N = 1 , then what we want is multiply $W^{(1)}_{input}$ , which is a D-size vector ,  by P-T , which is now a scalar , a scalar multiplied by a vector is allowed\n",
    "\n",
    "now whats the update we want to do here \n",
    "\n",
    "well , we dont want to update the entirety of $W^{(2)}$ because the entirety of $W^{(2)}$ doesnot influence this output , in fact only $W^{(2)}_{output}$ affects the current word , this applies to both positive and negative samples since that is handeled by T\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W^{(2)}_{output}} = W^{(1)}_{input}(P-T)$$\n",
    "\n",
    "\n",
    "<img src='extras/28.10.PNG' width='200'></img>\n",
    "\n",
    "so here only the blue line needs to be updated , all other output weights do not contribute to that output\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Fixing multiple samples</h4>\n",
    "\n",
    "now what happens if we have N samples , well we dont need to think so abstractically , say we have 3 samples , N = 3\n",
    "\n",
    "Then we just have 3 output vectors to update which are all slices of $W^{(2)}$\n",
    "\n",
    "so we cab see gere that we have 3 vector equations\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W^{(2)}_{brown}} = W^{(1)}_{jumps} \\left(p_{brown} - t_{brown} \\right)$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W^{(2)}_{fox}} = W^{(1)}_{jumps} \\left(p_{fox} - t_{fox} \\right)$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W^{(2)}_{over}} = W^{(1)}_{jumps} \\left(p_{over} - t_{over} \\right)$$\n",
    "\n",
    "Theoretically we could just use a python for loop to calculate these updates but we dont like python for loops\n",
    "\n",
    "so how can we further vectorise this operation ?\n",
    "\n",
    "well this is just the outer product of $W^{(1)}_{input}$ and $P-T$ , so we can write :\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W^{(2)}_c} = np.outer\\left( W^{(1)}_{jumps} , \\left(P_c - T_c \\right) \\right) , C = \\{brown,fox,over\\}$$\n",
    "\n",
    "note that the slice of $W^{(2)}$ that needs updating is of size <u>DxN</u> , since we have selected N columns out of the full $W^{(2)}$ matrix which is of size VxD , these N columns correspond to either the context words or the negative samples\n",
    "\n",
    "As a revision of how outer product works , for vectors a and b indexed by i and j respectively , the outer product C = np.outer(a,b) is defined as $C(i,j) = a(i)b(j)$\n",
    "\n",
    "The outer product of $W^{(1)}_{jumps}$ (size D) and P-T (size N) is also <u>DxN</u> since we have N output words and each is represented by a vector of length D\n",
    "\n",
    "---\n",
    "\n",
    "<h4>updating the input weight</h4>\n",
    "\n",
    "Now that we have the update for $W^{(2)}$ what about $W^{(1)}$ ?\n",
    "\n",
    "This is easier since there is only 1 row of $W^{(1)}$ to update , that is the row which belongs to the input word\n",
    "\n",
    "Yet it is harder since we need to apply the chain rule , the further back we go in a neural network the more terms we need to consider\n",
    "\n",
    "The important thing to note is that there is no hidden activation function , this is equivalent to the activation function being identity which means its derivative is 1\n",
    "\n",
    "ok lets do it from scratch :)\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial a_n} = - \\frac{\\partial}{\\partial a_n} \\left[t_n log \\sigma(a_n) + (1-t_n) log \\{ 1 - \\sigma(a_n)\\} \\right]$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial a_n} = - \\frac{t_n}{\\sigma(a_n)} \\frac{\\partial \\sigma (a_n)}{\\partial a_n} + \\frac{1-t_n}{1-\\sigma(a_n)}\\frac{\\partial \\sigma(a_n)}{\\partial a_n}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial a_n} = \\left[ - \\frac{t_n}{\\sigma(a_n)} \\frac{1-t_n}{1-\\sigma(a_n)}\\right]\\sigma(a_n) \\left[ 1 - \\sigma(a_n) \\right]$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial a_n} = -t_n \\left[ 1 - \\sigma(a_n) \\right] + (1-t_n)\\sigma(a_n)$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial a_n} = \\sigma(a_n) - t_n = p_n - t_n$$\n",
    "\n",
    "now we also know that :\n",
    "\n",
    "$$a_n = W^{(2)^{\\ \\ T}}_{\\ \\ out(n)} W^{(1)}_{\\ \\ in}$$\n",
    "\n",
    "so we can apply the chain rule to get the rest of the expression\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W^{(1)}_{\\ \\ in}} = \\sum^N_{n=1} \\frac{\\partial J}{\\partial a_n}\\frac{\\partial a_n}{\\partial W^{(1)}_{\\ \\ in}}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W^{(1)}_{\\ \\ in}} = \\sum^N_{n=1} \\left( p_n - t_n\\right) \\frac{\\partial W^{(2)^{\\ \\ T}}_{\\ out(n)} W^{(1)}_{\\ in}}{\\partial W^{(1)}_{\\ in}}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W^{(1)}_{\\ in }} = \\sum^N_{n=1} \\left(p_n-t_n\\right) W^{(2)}_{out(n)}$$\n",
    "\n",
    "notice how this makes a lot of sense , the gradient for $W^{(1)}_{ \\ in}$ is dependant on the error at all N output nodes and the weight going to those output nodes\n",
    "\n",
    "<img src='extras/28.11.PNG' width='200'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to consider some little details about Negative sampling , these will be needed for the implementation\n",
    "\n",
    "---\n",
    "\n",
    "The first obvious question is , how many negative samples should you chose ?\n",
    "\n",
    "The answer is normally a number like 5 or 10 is chosen or maybe as ahigh as 25 , this is a hyperparameter and thus should be always optimised to our particular dataset\n",
    "\n",
    "---\n",
    "\n",
    "The second question is how do we actually choose the negative samples ?\n",
    "\n",
    "One may assume a uniform distribution but thats not the case , instead we sample from a modified unigram distribution\n",
    "\n",
    "A unigram distribution is just the probability of a single word occuring - lets call that $p(w)$ \n",
    "\n",
    "How can we estimate $p(w)$ ?\n",
    "\n",
    "As usual we can estimate it by counting \n",
    "\n",
    "$$p(w) = \\frac{count(w)}{\\sum\\limits_{w'}count(w)}$$\n",
    "\n",
    "But we are not done yet , the problem with this approach is that infrequent words are too infrequent and so they are very unlikely to ever be sampled\n",
    "\n",
    "we can smooth out this distribution by raising all of our counts by 0.75 \n",
    "\n",
    "0.75 is the typical value to be used , but it seems like we can change this value drastically and still get pretty reasonable results\n",
    "\n",
    "$$\\tilde p(w) = \\frac{{count(w)}^{0.75}}{\\sum\\limits_{w'} {count(w')}^{0.75}}$$\n",
    "\n",
    "the fact that this is not the true maximum likelihood estimate of the unigram distribution is why we call it the modified unigram distribution\n",
    "\n",
    "---\n",
    "\n",
    "consider our running example : \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "input : jumps\n",
    "\n",
    "context : brown,fox,over,the\n",
    "\n",
    "But what if during our negative sampling procedure we accidently sample one of the context words ?\n",
    "\n",
    "for example the is pretty frequent , so we might sample 'the' again in our negative samples\n",
    "\n",
    "in word2vec this generally doesnot matter and so if we do end up considering one of the context words a positive sample and a negative sample at the same time , so be it\n",
    "\n",
    "It would actually take a lot of work to renormalise the negative sampling distribution for each context , so we simply dont bother\n",
    "\n",
    "---\n",
    "\n",
    "we discussed before that with negative sampling we take a middle word , some context words which count as positive samples and then we pick some negative samples randomly from our vocabulary\n",
    "\n",
    "BUT in the actual implementation , this is not what we are going to do , instead we are going to do something a little simpler\n",
    "\n",
    "In particular we still have our middle word and our context so for our example :\n",
    "\n",
    "<p>The quick <span style=\"color:blue\">brown fox <span style=\"color:green\">jumps</span> over the</span> lazy dog</p>\n",
    "\n",
    "positive samples : \n",
    "\n",
    "<ul>\n",
    "    <li>$jumps \\rightarrow brown$</li>\n",
    "    <li>$jumps \\rightarrow fox$</li>\n",
    "    <li>$jumps \\rightarrow over$</li>\n",
    "    <li>$jumps \\rightarrow the$</li>\n",
    "</ul>\n",
    "\n",
    "but instead of coming up with negative samples for our context , we instead take one negative sample for the middle word\n",
    "\n",
    "so what we would normally epect is to have the following as the negative samples\n",
    "\n",
    "negative samples :\n",
    "\n",
    "<ul>\n",
    "    <li>$jumps \\rightarrow boat$</li>\n",
    "    <li>$jumps \\rightarrow tokyo$</li>\n",
    "    <li>$jumps \\rightarrow phone$</li>\n",
    "    <li>$jumps \\rightarrow sea$</li>\n",
    "</ul>\n",
    "\n",
    "but what we actually do is sample a single random word , for ex: lighthouse , so now we will 4 new negative samples using this random word as the middle word but with the existing context\n",
    "\n",
    "<p>The quick <span style=\"color:blue\">brown fox <span style=\"color:red\">lighthouse</span> over the</span> lazy dog</p>\n",
    "\n",
    "and so our negative samples are (this is what we will use in our implementation ):\n",
    "\n",
    "negative samples :\n",
    "\n",
    "<ul>\n",
    "    <li>$lighthouse \\rightarrow brown$</li>\n",
    "    <li>$lighthouse \\rightarrow fox$</li>\n",
    "    <li>$lighthouse \\rightarrow over$</li>\n",
    "    <li>$lighthouse \\rightarrow the$</li>\n",
    "</ul>\n",
    "\n",
    "so just to reiterate this idea , usually when people talk about negative sampling they talk about it in terms of sampling context words with a fixed middle word\n",
    "\n",
    "but during implementation we are going to fix the context words and insert an incorrect middle word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we jsut think back to the basic idea behind the bigram nerual network and the word2vec algorithm , but thinking back to the original idea , all we want to do is find a VxD matrix that has good word vectors\n",
    "\n",
    "---\n",
    "\n",
    "<h4>2 possible word embeddings</h4>\n",
    "\n",
    "if we look at the architecture of a bigram neural network , we realise it has two of these , the first is VxD and the second is DxV (of course if we just transpose this we get a VxD matrix)\n",
    "\n",
    "<img src = 'extras/28.1.PNG' width='400'></img>\n",
    "\n",
    "so which one is the one we are interested in ?\n",
    "\n",
    "---\n",
    "\n",
    "<h4>which one do we choose ?</h4>\n",
    "\n",
    "in reality either of these can serve as our final word vectors , but here are some things we can do :\n",
    "\n",
    "<ul>\n",
    "    <li>$W_{e} = W^{1}$<ul>\n",
    "    <li>Just forget about the second weight matrix entirely , and just make the word embedding equal to the input to hidden weight</li>\n",
    "    </ul></li>\n",
    "    <li>$W_{e} = concat([W^{(1)},W^{(2)^T}]) \\rightarrow shape \\ is \\ V\\times2D$<ul>\n",
    "    <li>The second way is to concatenate the two weights together</li>\n",
    "    </ul></li>\n",
    "    <li>$W_{e} = (W^{(1)}+W{(2)^T})/{2}$<ul>\n",
    "    <li>just take the average of the two weights</li>\n",
    "    </ul></li>\n",
    "</ul>\n",
    "\n",
    "in all three of these cases , sometimes people normalise all the vectors so that they have unit length , this also makes it so that they all lie on the unit sphere , of course if we are using cosine distance to find analogies this actually would not make a difference\n",
    "\n",
    "$$\\hat v = \\frac{v}{\\vert v \\vert}$$\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we want to look at some implementation tricks that can be useful when we implement word2vec\n",
    "\n",
    "---\n",
    "\n",
    "<h4>How to choose which words to train on</h4>\n",
    "\n",
    "In the english language common words are very common and uncommon words are very common\n",
    "\n",
    "if we plot a histogram of how often words are used we will get a very fat-tailed distribution\n",
    "\n",
    "<img src='extras/28.12.PNG' width='300'></img>\n",
    "\n",
    "that means we would spend an overwhelming majority of the time updating word vectors for very common words if we just ran through every sentence\n",
    "\n",
    "so what we do with word2vec is we just drop a bunch of words\n",
    "\n",
    "and we do so randomly , so each time we encounter a sentence we randomly drop some words according to some probability distribution , which is a function of the modified unigram distribution\n",
    "\n",
    "$$p_{drop}(w) = 1 - \\sqrt{\\frac{threshold}{\\tilde p (w)}}$$\n",
    "\n",
    "a typical threshold will be set to something very low , ex : $10^{-5}$ , so what is the effect of this ?\n",
    "\n",
    "say a word is very frequent , say $\\tilde p(w) = 0.1$ , then $p_{drop}(w) = 1-10^{-2} = 0.99$ , so a 99% chance is that we should drop that word\n",
    "\n",
    "now say $\\tilde p(w) = 10^{-5}$ then we get 1-1 = 0 so we would never drop this word\n",
    "\n",
    "in fact if the unigram probability is even lower than this (lower than the threshold) we get a negative number , meaning we also wont drop that word\n",
    "\n",
    "---\n",
    "\n",
    "Now this may not be obvious , but after we drop the words from the sentence we treat the sentence with dropped words as the final sentence we are going to train on\n",
    " \n",
    "for ex if our sentence is :\n",
    "\n",
    "\"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "suppose we drop the words 'the' and 'over'\n",
    "\n",
    "then the effective sentence becomes :\n",
    "\n",
    "'quick brown fox jumps lazy dog'\n",
    "\n",
    "so now if our middle word is jumps then our context words becomes [brown,fox,lazy,dog]\n",
    "\n",
    "note that before dropping any words , the words [lazy,dog] would not be part of this context \n",
    "\n",
    "so doing things this way technically has t he effect of widening the context window\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Learning rate scheduling</h4>\n",
    "\n",
    "The next implementation detail to talk about is learning rate scheduling , in word2vec we normally pick a maximum learning rate and a minimum learnign rate and then we reduce the learning rate linearly from max to min\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Interesting (but not going to try)</h4>\n",
    "\n",
    "we can greatly speed up training by working on multiple threads or cores (each responsible for a different subset of sentences)\n",
    "\n",
    "ex: 1 thread takes 4 hours $\\rightarrow$ 4 threads takes 1 hour\n",
    "\n",
    "Use C/Cython : a lot of the work in word2vec is happening in python which is a slow language , we one thing we can try is writing some functions in Cython or just calling C functions , in fact the original implementation of word2vec is entirely in C\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Built-in Softmax Alternatives</h4>\n",
    "\n",
    "Tensorflow comes in with built-in softmax approximators suchs as the sampled siftmax loss ```tf.nn.sampled_softmax_loss``` and nce loss ```tf.nn.nce_loss```\n",
    "\n",
    "while we wont study these, its good for us to know about them since they essentially fill the same role as the hierarchical softmax and negative sampling do\n",
    "\n",
    "what's nice is that, since they are built in functions, they work a little faster\n",
    "\n",
    "but what we found experimentally was that they did not work as well as negative sampling, at least for the small amount of data we will be using (instructor's comment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this happened to be too slow\n",
    "# so we will be using multiprocessing !\n",
    "# jupyter notebook doesnot seem to run processes from multiprocessing\n",
    "# so we can just copy the following code to a .py file and run it\n",
    "# the Boss reads the data splits the sentences into subsets\n",
    "# it also computes the unigram and p_drop and sends data to each of the workers\n",
    "# the workers work on the subset then they update the global weights\n",
    "# locks are handeled automatically by multiprocessing library\n",
    "# we can load the weights and continue training / try other analogies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using W1 only : \n",
      "euclidean  distance : king - man = queen - woman\n",
      "cosine  distance : king - man = queen - woman\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : king - man = queen - woman\n",
      "cosine  distance : king - man = throne - woman\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : king - man = queen - woman\n",
      "cosine  distance : king - man = throne - woman\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : king - emperor = kingdom - empire\n",
      "cosine  distance : king - emperor = kingdom - empire\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : king - emperor = kings - empire\n",
      "cosine  distance : king - emperor = kingdom - empire\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : king - emperor = kingdom - empire\n",
      "cosine  distance : king - emperor = kingdom - empire\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : north - south = east - west\n",
      "cosine  distance : north - south = east - west\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : north - south = east - west\n",
      "cosine  distance : north - south = east - west\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : north - south = east - west\n",
      "cosine  distance : north - south = east - west\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : mosque - church = islam - christianity\n",
      "cosine  distance : mosque - church = islam - christianity\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : mosque - church = islam - christianity\n",
      "cosine  distance : mosque - church = islam - christianity\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : mosque - church = muslim - christianity\n",
      "cosine  distance : mosque - church = islam - christianity\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : six - five = four - three\n",
      "cosine  distance : six - five = four - three\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : six - five = four - three\n",
      "cosine  distance : six - five = four - three\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : six - five = four - three\n",
      "cosine  distance : six - five = four - three\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : summer - hot = winter - cold\n",
      "cosine  distance : summer - hot = autumn - cold\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : summer - hot = winter - cold\n",
      "cosine  distance : summer - hot = winter - cold\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : summer - hot = winter - cold\n",
      "cosine  distance : summer - hot = winter - cold\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : india - indian = egypt - egyptian\n",
      "cosine  distance : india - indian = egypt - egyptian\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : india - indian = syria - egyptian\n",
      "cosine  distance : india - indian = egypt - egyptian\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : india - indian = egypt - egyptian\n",
      "cosine  distance : india - indian = egyptians - egyptian\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : fire - water = north - south\n",
      "cosine  distance : fire - water = north - south\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : fire - water = north - south\n",
      "cosine  distance : fire - water = north - south\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : fire - water = north - south\n",
      "cosine  distance : fire - water = east - south\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : water - fire = true - false\n",
      "cosine  distance : water - fire = true - false\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : water - fire = true - false\n",
      "cosine  distance : water - fire = true - false\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : water - fire = true - false\n",
      "cosine  distance : water - fire = true - false\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : christianity - bible = muslims - quran\n",
      "cosine  distance : christianity - bible = islam - quran\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : christianity - bible = islam - quran\n",
      "cosine  distance : christianity - bible = islam - quran\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : christianity - bible = islam - quran\n",
      "cosine  distance : christianity - bible = islam - quran\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : sun - moon = morning - night\n",
      "cosine  distance : sun - moon = afternoon - night\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : sun - moon = morning - night\n",
      "cosine  distance : sun - moon = afternoon - night\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : sun - moon = morning - night\n",
      "cosine  distance : sun - moon = morning - night\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : arabic - arab = english - american\n",
      "cosine  distance : arabic - arab = latin - american\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : arabic - arab = english - american\n",
      "cosine  distance : arabic - arab = english - american\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : arabic - arab = english - american\n",
      "cosine  distance : arabic - arab = english - american\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : forward - backward = up - down\n",
      "cosine  distance : forward - backward = up - down\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : forward - backward = up - down\n",
      "cosine  distance : forward - backward = blew - down\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : forward - backward = up - down\n",
      "cosine  distance : forward - backward = up - down\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : fly - flying = walk - walking\n",
      "cosine  distance : fly - flying = walk - walking\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : fly - flying = walk - walking\n",
      "cosine  distance : fly - flying = legs - walking\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : fly - flying = go - walking\n",
      "cosine  distance : fly - flying = walk - walking\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : fusion - fission = gather - disperse\n",
      "cosine  distance : fusion - fission = gather - disperse\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using averaged weight matricies : \n",
      "euclidean  distance : fusion - fission = gather - disperse\n",
      "cosine  distance : fusion - fission = gather - disperse\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : fusion - fission = to - disperse\n",
      "cosine  distance : fusion - fission = gather - disperse\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : proton - electron = positive - negative\n",
      "cosine  distance : proton - electron = positive - negative\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : proton - electron = positive - negative\n",
      "cosine  distance : proton - electron = positive - negative\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : proton - electron = that - negative\n",
      "cosine  distance : proton - electron = positive - negative\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : palestine - jerusalem = japanese - tokyo\n",
      "cosine  distance : palestine - jerusalem = japan - tokyo\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : palestine - jerusalem = japan - tokyo\n",
      "cosine  distance : palestine - jerusalem = japan - tokyo\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : palestine - jerusalem = japans - tokyo\n",
      "cosine  distance : palestine - jerusalem = japan - tokyo\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : xbox - microsoft = playstation - sony\n",
      "cosine  distance : xbox - microsoft = playstation - sony\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : xbox - microsoft = playstation - sony\n",
      "cosine  distance : xbox - microsoft = playstation - sony\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : xbox - microsoft = playstation - sony\n",
      "cosine  distance : xbox - microsoft = playstation - sony\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : dc - superman = washington - spiderman\n",
      "cosine  distance : dc - superman = marvel - spiderman\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : dc - superman = washington - spiderman\n",
      "cosine  distance : dc - superman = marvel - spiderman\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : dc - superman = washington - spiderman\n",
      "cosine  distance : dc - superman = marvel - spiderman\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : long - short = large - small\n",
      "cosine  distance : long - short = larger - small\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : long - short = large - small\n",
      "cosine  distance : long - short = large - small\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : long - short = large - small\n",
      "cosine  distance : long - short = large - small\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : peace - water = personally - fire\n",
      "cosine  distance : peace - water = war - fire\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : peace - water = conflict - fire\n",
      "cosine  distance : peace - water = war - fire\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : peace - water = war - fire\n",
      "cosine  distance : peace - water = war - fire\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : relativity - einstein = gravitational - newton\n",
      "cosine  distance : relativity - einstein = gravitation - newton\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : relativity - einstein = gravity - newton\n",
      "cosine  distance : relativity - einstein = newtons - newton\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : relativity - einstein = newtons - newton\n",
      "cosine  distance : relativity - einstein = newtons - newton\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : relativity - einstein = electrical - tesla\n",
      "cosine  distance : relativity - einstein = electromagnetic - tesla\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : relativity - einstein = electric - tesla\n",
      "cosine  distance : relativity - einstein = electric - tesla\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : relativity - einstein = electrical - tesla\n",
      "cosine  distance : relativity - einstein = electromagnetism - tesla\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : king - prince = queen - princess\n",
      "cosine  distance : king - prince = queen - princess\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : king - prince = queen - princess\n",
      "cosine  distance : king - prince = daughter - princess\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : king - prince = queen - princess\n",
      "cosine  distance : king - prince = queen - princess\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : man - woman = he - she\n",
      "cosine  distance : man - woman = he - she\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : man - woman = his - she\n",
      "cosine  distance : man - woman = he - she\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : man - woman = he - she\n",
      "cosine  distance : man - woman = he - she\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : february - january = november - october\n",
      "cosine  distance : february - january = november - october\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : february - january = november - october\n",
      "cosine  distance : february - january = november - october\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : february - january = november - october\n",
      "cosine  distance : february - january = june - october\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : heir - heiress = prince - princess\n",
      "cosine  distance : heir - heiress = throne - princess\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : heir - heiress = prince - princess\n",
      "cosine  distance : heir - heiress = throne - princess\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : heir - heiress = prince - princess\n",
      "cosine  distance : heir - heiress = throne - princess\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : france - paris = japanese - tokyo\n",
      "cosine  distance : france - paris = japan - tokyo\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : france - paris = japan - tokyo\n",
      "cosine  distance : france - paris = japan - tokyo\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : france - paris = japanese - tokyo\n",
      "cosine  distance : france - paris = japanese - tokyo\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : france - paris = china - beijing\n",
      "cosine  distance : france - paris = china - beijing\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean  distance : france - paris = korea - beijing\n",
      "cosine  distance : france - paris = china - beijing\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : france - paris = china - beijing\n",
      "cosine  distance : france - paris = china - beijing\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : france - paris = italy - rome\n",
      "cosine  distance : france - paris = papal - rome\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : france - paris = italy - rome\n",
      "cosine  distance : france - paris = italy - rome\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : france - paris = italy - rome\n",
      "cosine  distance : france - paris = papacy - rome\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : france - paris = germany - berlin\n",
      "cosine  distance : france - paris = germany - berlin\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : france - paris = german - berlin\n",
      "cosine  distance : france - paris = germany - berlin\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : france - paris = german - berlin\n",
      "cosine  distance : france - paris = germany - berlin\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : miami - florida = dallas - texas\n",
      "cosine  distance : miami - florida = dallas - texas\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : miami - florida = dallas - texas\n",
      "cosine  distance : miami - florida = dallas - texas\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : miami - florida = dallas - texas\n",
      "cosine  distance : miami - florida = dallas - texas\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : france - french = england - english\n",
      "cosine  distance : france - french = england - english\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : france - french = england - english\n",
      "cosine  distance : france - french = england - english\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : france - french = england - english\n",
      "cosine  distance : france - french = england - english\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : japan - japanese = china - chinese\n",
      "cosine  distance : japan - japanese = china - chinese\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : japan - japanese = china - chinese\n",
      "cosine  distance : japan - japanese = korea - chinese\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : japan - japanese = china - chinese\n",
      "cosine  distance : japan - japanese = china - chinese\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : china - chinese = united - american\n",
      "cosine  distance : china - chinese = united - american\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : china - chinese = states - american\n",
      "cosine  distance : china - chinese = united - american\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : china - chinese = united - american\n",
      "cosine  distance : china - chinese = united - american\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : japan - japanese = italy - italian\n",
      "cosine  distance : japan - japanese = italy - italian\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : japan - japanese = italy - italian\n",
      "cosine  distance : japan - japanese = italy - italian\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : japan - japanese = italy - italian\n",
      "cosine  distance : japan - japanese = italy - italian\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : japan - japanese = australia - australian\n",
      "cosine  distance : japan - japanese = australia - australian\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : japan - japanese = australia - australian\n",
      "cosine  distance : japan - japanese = zealand - australian\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : japan - japanese = australia - australian\n",
      "cosine  distance : japan - japanese = australia - australian\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : man - woman = son - mother\n",
      "cosine  distance : man - woman = son - mother\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : man - woman = father - mother\n",
      "cosine  distance : man - woman = father - mother\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : man - woman = father - mother\n",
      "cosine  distance : man - woman = father - mother\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : old - young = brother - son\n",
      "cosine  distance : old - young = eldest - son\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : old - young = father - son\n",
      "cosine  distance : old - young = grandson - son\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : old - young = father - son\n",
      "cosine  distance : old - young = father - son\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : walk - walking = pool - swimming\n",
      "cosine  distance : walk - walking = swim - swimming\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : walk - walking = swim - swimming\n",
      "cosine  distance : walk - walking = swim - swimming\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : walk - walking = pool - swimming\n",
      "cosine  distance : walk - walking = swimmers - swimming\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : man - woman = father - aunt\n",
      "cosine  distance : man - woman = father - aunt\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : man - woman = father - aunt\n",
      "cosine  distance : man - woman = uncle - aunt\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : man - woman = uncle - aunt\n",
      "cosine  distance : man - woman = uncle - aunt\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : nephew - niece = brother - aunt\n",
      "cosine  distance : nephew - niece = uncle - aunt\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : nephew - niece = brother - aunt\n",
      "cosine  distance : nephew - niece = uncle - aunt\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : nephew - niece = brother - aunt\n",
      "cosine  distance : nephew - niece = uncle - aunt\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : man - woman = father - sister\n",
      "cosine  distance : man - woman = brother - sister\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : man - woman = brother - sister\n",
      "cosine  distance : man - woman = son - sister\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : man - woman = father - sister\n",
      "cosine  distance : man - woman = father - sister\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : man - woman = son - wife\n",
      "cosine  distance : man - woman = son - wife\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : man - woman = himself - wife\n",
      "cosine  distance : man - woman = brother - wife\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : man - woman = father - wife\n",
      "cosine  distance : man - woman = father - wife\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : man - woman = actor - actress\n",
      "cosine  distance : man - woman = starred - actress\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : man - woman = starred - actress\n",
      "cosine  distance : man - woman = starred - actress\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : man - woman = actor - actress\n",
      "cosine  distance : man - woman = bafta - actress\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "using W1 only : \n",
      "euclidean  distance : king - kingdom = egyptian - egypt\n",
      "cosine  distance : king - kingdom = egyptian - egypt\n",
      "------------------------------------------\n",
      "using averaged weight matricies : \n",
      "euclidean  distance : king - kingdom = egyptian - egypt\n",
      "cosine  distance : king - kingdom = pharaoh - egypt\n",
      "------------------------------------------\n",
      "using concatenated weight matricies : \n",
      "euclidean  distance : king - kingdom = egyptian - egypt\n",
      "cosine  distance : king - kingdom = pharaoh - egypt\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# first we code word2vec from scratch\n",
    "# then we will be using tensorflow\n",
    "\n",
    "# this model provides several functionalities\n",
    "# since training time is too long we can abort training and continue at any time by passing resume_training = True to fit function \n",
    "# this loads every thing and continues from last finished epoch\n",
    "# we can also call load_weights to skip retraining and test some analogies\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import string\n",
    "from sklearn.utils import shuffle \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import threading\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "from multiprocessing import Process,Array,Value\n",
    "\n",
    "# where we save our weights\n",
    "path = 'outputs/word2vec/numpy/'\n",
    "\n",
    "class Worker:\n",
    "\n",
    "    # at the end of each epoch the last worker to finish should save the weights and cost\n",
    "    def save_weights(self,start_epoch):\n",
    "        costs = np.zeros(len(self.costs))\n",
    "        costs[:start_epoch] = self.costs[:start_epoch]\n",
    "        np.savez('weights.npz' , self.W1, self.W2,costs,start_epoch)\n",
    "        print(\"woker: \",self.worker_id,\" created a checkpoint for epoch : \",start_epoch)\n",
    "        print('----------------------------------------------------------------------')\n",
    "        print('epoch: ',start_epoch,' cost: ',costs[start_epoch-1])\n",
    "        print('----------------------------------------------------------------------')\n",
    "\n",
    "    \n",
    "    def __init__(self,worker_id,W1,W2,costs,V,D,counter,num_workers):\n",
    "        self.worker_id = worker_id\n",
    "        self.W1 = np.frombuffer(W1.get_obj()).reshape(V,D)\n",
    "        self.W2 = np.frombuffer(W2.get_obj()).reshape(D,V)\n",
    "        self.costs = np.frombuffer(costs.get_obj())\n",
    "        self.V,self.D = V,D\n",
    "        self.counter = np.frombuffer(counter.get_obj())\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "\n",
    "    # takes a sentence , position of middle word , and context size\n",
    "    # return context words\n",
    "    \n",
    "    def get_context(self,sent,middle,context_size):    \n",
    "        context = []\n",
    "        start = max(0,middle-context_size)\n",
    "        # we will be using range so range(start,end) takes values start --> end-1\n",
    "        # thats why we have middle+context+1\n",
    "        end = min(middle+context_size+1,len(sent)) \n",
    "        for j in range(start,end): # j is the position of the context word in sentence\n",
    "            if j == middle: # dont include middle word itself as target\n",
    "                continue\n",
    "            context.append(sent[j])    \n",
    "        return context\n",
    "    \n",
    "    \n",
    "    def sigmoid(self,a):\n",
    "        return 1/(1+np.exp(-a))\n",
    "    \n",
    "    def cost(self,T,P,eps=1e-14):\n",
    "        return np.sum(T*np.log(P+eps) + (1-T)*np.log(1-P+eps))\n",
    "        \n",
    "    def train(self,word,context,lr,T,eps):\n",
    "\n",
    "        P = self.sigmoid(self.W1[word]@self.W2[:,context])\n",
    "\n",
    "        gW2 = np.outer(self.W1[word],(P-T))         \n",
    "        gW1 = np.sum((P-T)*self.W2[:,context],axis=1)\n",
    "        \n",
    "        self.W2[:,context] -= lr*gW2\n",
    "        self.W1[word] -= lr*gW1\n",
    "        \n",
    "        return self.cost(P,T,eps)\n",
    "    \n",
    "    def fit(self,sentences,positive_context,negative_context,initial_lr,final_lr,epochs,smoothing,threshold,epsilon,unigram,p_drop,start_epoch):\n",
    "        # decrease learning rate linearly\n",
    "    \n",
    "        delta = (initial_lr-final_lr)/(epochs) \n",
    "        # now let the training begin \n",
    "        for epoch in range(start_epoch,epochs):\n",
    "        \n",
    "            cost = 0\n",
    "            t0 = datetime.now()  \n",
    "            lr = initial_lr - epoch*delta\n",
    "            # shuffle sentences \n",
    "            sentences = shuffle(sentences)\n",
    "            for i,sentence in enumerate(sentences):\n",
    "        \n",
    "\n",
    "                # first we need to drop words according to p drop\n",
    "                sent = [w for w in sentence if np.random.random()<1-p_drop[w]]\n",
    "\n",
    "                if len(sent) < 2: # no conetext\n",
    "                    continue\n",
    "                \n",
    "                N = len(sent)\n",
    "                # we will also shuffle the order of middle words\n",
    "                # so we dont see samples of the same sentence in same order each time\n",
    "                middle = np.random.choice(N,N,replace=False)\n",
    "                \n",
    "                for m in middle:\n",
    "                    pos_word = sent[m]\n",
    "                    pos_context = self.get_context(sent,m,positive_context)\n",
    "                    # sample a word for negative sampling\n",
    "                    neg_word = np.random.choice(self.V,p=unigram)\n",
    "                    neg_context = self.get_context(sent,m,negative_context)\n",
    "                    \n",
    "                    # now we do gradient descent twice\n",
    "                    # once for positive samples and once for negative\n",
    "                    # recall that SGD is a must now since our update for W1 requires is for a single word\n",
    "                    \n",
    "                    c = self.train(pos_word,pos_context,lr,T=1,eps=epsilon)\n",
    "                    cost += c\n",
    "                    c = self.train(neg_word,neg_context,lr,T=0,eps=epsilon)\n",
    "                    cost += c\n",
    "                    \n",
    "                if (i+1)%1000 == 0:\n",
    "                    print(\"worker: \",self.worker_id,' epoch: ',epoch+1,'/',epochs,' finished training on: ',i+1,'/',len(sentences),' sentences')\n",
    "            self.costs[epoch] += cost\n",
    "            print(\"worker: \",self.worker_id,' epoch: ',epoch+1,' cost: ',cost,' time: ',datetime.now()-t0)\n",
    "            # now each worker increments counter\n",
    "            self.counter[epoch] += 1\n",
    "            if (self.counter[epoch]%self.num_workers) == 0:\n",
    "                self.save_weights(epoch+1)\n",
    "\n",
    "\n",
    "class Boss:\n",
    "    # save model parameters\n",
    "    def save_params(self,word2idx,n_files,positive_context,negative_context,initial_lr,final_lr,epochs,smoothing,threshold,epsilon,unigram,p_drop,start_epoch):\n",
    "        params = {'word2idx':word2idx,'n_files':n_files,'positive_context':positive_context,'negative_context':negative_context,'initial_lr':initial_lr,'final_lr':final_lr,'epochs':epochs,'smoothing':smoothing,'threshold':threshold,'epsilon':epsilon,'start_epoch':start_epoch}\n",
    "        with open(path+'params.json' , 'w') as f:\n",
    "            json.dump(params, f)\n",
    "        np.savez('grams.npz' , unigram)\n",
    "\n",
    "        # save model parameters\n",
    "    def load_params(self):\n",
    "        with open(path+'params.json') as f:\n",
    "                params = json.load(f)\n",
    "\n",
    "        n_files = params['n_files']\n",
    "        positive_context = params['positive_context']\n",
    "        negative_context = params['negative_context']\n",
    "        initial_lr = params['initial_lr']\n",
    "        final_lr = params['final_lr']\n",
    "        epochs = params['epochs']\n",
    "        smoothing = params['smoothing']\n",
    "        threshold = params['threshold']\n",
    "        epsilon = params['epsilon']\n",
    "        npz = np.load(path+'grams.npz')\n",
    "        unigram = npz['arr_0']\n",
    "        p_drop = 1 - np.sqrt(threshold/unigram)\n",
    "\n",
    "\n",
    "        return n_files,positive_context,negative_context,initial_lr,final_lr,epochs,smoothing,threshold,epsilon,unigram,p_drop\n",
    "        \n",
    "        \n",
    "    # laod the weights\n",
    "    # the workers does the saving\n",
    "    def load_weights(self,get_w2i=True,get_start_epoch=False,plot_costs=False):\n",
    "        if get_w2i:\n",
    "            with open(path+'params.json') as f:\n",
    "                self.word2idx = json.load(f)['word2idx']\n",
    "        npz = np.load(path+'weights.npz')\n",
    "        W1 = npz['arr_0']\n",
    "        W2 = npz['arr_1']\n",
    "        C = npz['arr_2']\n",
    "        start_epoch = npz['arr_3']\n",
    "        V,D = W1.shape\n",
    "        self.V,self.D = V,D\n",
    "\n",
    "        self.W1 = Array('d', V*D)\n",
    "        W1_np = np.frombuffer(self.W1.get_obj()).reshape(V,D)\n",
    "        np.copyto(W1_np,W1)\n",
    "\n",
    "        self.W2 = Array('d', D*V)\n",
    "        W2_np = np.frombuffer(self.W2.get_obj()).reshape(D,V)\n",
    "        np.copyto(W2_np,W2)\n",
    "\n",
    "        self.costs = Array('d', len(C))\n",
    "        C_np = np.frombuffer(self.costs.get_obj())\n",
    "        np.copyto(C_np,C)\n",
    "\n",
    "        if plot_costs is True:\n",
    "            plt.plot(C_np[:start_epoch])\n",
    "            plt.show()\n",
    "        if get_start_epoch is True:\n",
    "            return start_epoch\n",
    "\n",
    "\n",
    "    def get_analogy(self,w1,w2,w4):\n",
    "        W1 = np.frombuffer(self.W1.get_obj()).reshape(self.V,self.D)\n",
    "        W2 = np.frombuffer(self.W2.get_obj()).reshape(self.D,self.V)\n",
    "        E = W1\n",
    "        print('using W1 only : ')\n",
    "        self.analogy(E,self.word2idx,w1,w2,w4)\n",
    "        print('------------------------------------------')\n",
    "        E = (W1+W2.T)/2\n",
    "        print('using averaged weight matricies : ')\n",
    "        self.analogy(E,self.word2idx,w1,w2,w4)\n",
    "        print('------------------------------------------')\n",
    "        E = np.concatenate((W1,W2.T),axis=1)\n",
    "        print('using concatenated weight matricies : ')\n",
    "        self.analogy(E,self.word2idx,w1,w2,w4)\n",
    "        print('------------------------------------------')\n",
    "        print('------------------------------------------')\n",
    "        \n",
    "        \n",
    "    \n",
    "    # king - man = ? - woman\n",
    "    def analogy(self,E,word2idx,w1,w2,w4):\n",
    "        # first lets get our vector\n",
    "        D = E.shape[1]\n",
    "        king = E[word2idx[w1]]\n",
    "        man = E[word2idx[w2]]\n",
    "        woman = E[word2idx[w4]]\n",
    "        queen = king - man + woman\n",
    "\n",
    "        # next we calculate the distance between our vector and all other vectors\n",
    "        # once using euclidean distance then again using cosine\n",
    "        idx2word = {v:k for k,v in word2idx.items()}\n",
    "        metrics = ['euclidean','cosine',]\n",
    "\n",
    "        for metric in metrics:\n",
    "            distances = pairwise_distances(queen.reshape(1,D),E,metric=metric)\n",
    "            # now we need to consider the 4 closest neighbours to that point\n",
    "            # not to return a word in [w1,w2,w4]\n",
    "            closest =  np.argpartition(distances[0], 4)[:4]\n",
    "            closest = [idx2word[i] for i in closest]\n",
    "\n",
    "            for word in closest:\n",
    "                if word not in [w1,w2,w4]:\n",
    "                    print(metric,' distance :',w1,\"-\",w2,'=',word,'-',w4)\n",
    "                    break\n",
    "\n",
    "    # a method that takes sentences and return tokens\n",
    "    # should also remove punctuation\n",
    "    def tokenise(self,sent):\n",
    "        sent = sent.lower()\n",
    "        sent = sent.translate(str.maketrans('','',string.punctuation))\n",
    "        tokens = sent.split()\n",
    "        return tokens\n",
    "    \n",
    "    # we want to implement a method that reads data from the wiki files\n",
    "    # this should tokenise the sentences and return the tokenised sentences + word2idx\n",
    "    # also we need to limit vocab size\n",
    "    \n",
    "    def get_sentences(self,path='datasets/wiki/',V=20000,n_files=None):\n",
    "        files = glob(path+'*.txt')\n",
    "        files = files[:n_files]\n",
    "        # first we need to get word2count to identify our top words\n",
    "        # we will make word2idx once we filter out the top words\n",
    "        word2count = {}\n",
    "        \n",
    "        # this is a list of lists , each inner list is a sentence of indexes\n",
    "        sentences = []\n",
    "        \n",
    "        # we need to limit covabulary\n",
    "        # first we get word2count \n",
    "        print('counting words')\n",
    "\n",
    "        for i,f in enumerate(files):\n",
    "            for line in open(f,encoding = \"utf8\"):\n",
    "                line = line.rstrip()\n",
    "                if line and line[0] not in ('[', '*', '-', '|', '=', '{', '}'):\n",
    "                    tokens = self.tokenise(line)\n",
    "                    if len(tokens) < 2:\n",
    "                        continue\n",
    "                    for token in tokens:\n",
    "                        word2count[token] = word2count.get(token,0) + 1\n",
    "\n",
    "            print('finished counting : ',i+1,'/',len(files),' files')\n",
    "\n",
    "        print('finisehd counting')\n",
    "        print('processing files')\n",
    "\n",
    "        # now we use word2count to identify most frequent words\n",
    "        # we need the special <none> token to replace words that wont make it to our vocabulary\n",
    "        words  = ['<none>'] + [w for (w,c) in sorted(word2count.items() , reverse=True, key=lambda x: x[1])[:V-1]] \n",
    "        word2idx = {w:i for w,i in zip(words,range(V))}\n",
    "        none = word2idx['<none>']\n",
    "        for i,f in enumerate(files):\n",
    "            # in the wiki files each line is a paragraph , we will be taking each paragraph as a sentence\n",
    "            # we also want to remove header lines\n",
    "            for line in open(f,encoding = \"utf8\"):\n",
    "                line = line.rstrip()\n",
    "                # skip headers , ...\n",
    "                if line and line[0] not in ('[', '*', '-', '|', '=', '{', '}'):\n",
    "                    tokens = self.tokenise(line)\n",
    "                    if len(tokens) < 2:\n",
    "                        continue\n",
    "                    # now we update word2idx and word2count\n",
    "                    # and we append tokenised line to our sentences\n",
    "                    sentence = [word2idx.get(token,none) for token in tokens]\n",
    "                    sentences.append(sentence)\n",
    "\n",
    "            print('finished processing : ',i+1,'/',len(files),' files')\n",
    "        \n",
    "        print('finished processing data')\n",
    "        return sentences,word2idx\n",
    "    \n",
    "    def fit(self,sentences=None,word2idx=None,V = 20000 ,D = 300 ,n_files = None ,positive_context = 5 ,initial_lr = 0.025 ,final_lr = 0.0001 ,negative_context = 5 ,epochs= 20,smoothing = 0.75 ,threshold = 1e-5,epsilon=1e-14,load_weights=False,resume_training = False):\n",
    "        if resume_training == False : # initialise everything\n",
    "            if sentences is None :\n",
    "                sentences,word2idx = self.get_sentences(V=V,n_files = n_files)\n",
    "            \n",
    "            start_epoch = 0\n",
    "            V = V\n",
    "            D = D\n",
    "            self.V = V\n",
    "            self.D = D\n",
    "\n",
    "            if load_weights is False: # initialise new weights\n",
    "                # this is the way we read and wruite to a shared array\n",
    "                W1 = np.random.randn(V,D)\n",
    "                self.W1 = Array('d', V*D)\n",
    "                W1_np = np.frombuffer(self.W1.get_obj()).reshape(V,D)\n",
    "                np.copyto(W1_np, W1)\n",
    "\n",
    "                # same agin for W2\n",
    "                W2 = np.random.randn(D,V)\n",
    "                self.W2 = Array('d', D*V)\n",
    "                W2_np = np.frombuffer(self.W2.get_obj()).reshape(D,V)\n",
    "                np.copyto(W2_np, W2)\n",
    "\n",
    "            else:\n",
    "                self.load_weights(get_w2i=False)\n",
    "\n",
    "            costs = np.zeros(epochs)\n",
    "            self.costs = Array('d', epochs)\n",
    "            costs_np = np.frombuffer(self.costs.get_obj())\n",
    "            np.copyto(costs_np, costs)\n",
    "            # first we want to get the unigram and p_drop\n",
    "            unigram = np.zeros(V)\n",
    "            for sentence in sentences:\n",
    "                unigram[sentence] += 1\n",
    "\n",
    "            unigram = unigram**smoothing\n",
    "            unigram /= unigram.sum()\n",
    "\n",
    "            p_drop = 1 - np.sqrt(threshold/unigram)\n",
    "\n",
    "            # we will save the parameters of fit , so that we can resume training later if we want\n",
    "            self.word2idx = word2idx\n",
    "            self.save_params(word2idx,n_files,positive_context,negative_context,initial_lr,final_lr,epochs,smoothing,threshold,epsilon,unigram,p_drop,start_epoch)\n",
    "\n",
    "        else: # lets load everything\n",
    "            start_epoch = model.load_weights(get_start_epoch=True) # this load weights and costs also word2idx , return start epoch \n",
    "            V,D = self.V,self.D\n",
    "            n_files,positive_context,negative_context,initial_lr,final_lr,epochs,smoothing,threshold,epsilon,unigram,p_drop = model.load_params()\n",
    "            if sentences is None :\n",
    "                sentences,word2idx = self.get_sentences(V=V,n_files = n_files)\n",
    "            self.word2idx = word2idx\n",
    "\n",
    "\n",
    "        # shared counter to know which worker finished training last for each epoch\n",
    "        counter = Array('d', epochs)\n",
    "\n",
    "        # first we get the number of available CPUs\n",
    "        num_workers = multiprocessing.cpu_count() \n",
    "  \n",
    "        subset_size = np.ceil(len(sentences)/num_workers).astype(int)\n",
    "        # shuffle sentences before distributing to workers \n",
    "        sentences = shuffle(sentences)\n",
    "        workers = []\n",
    "        for worker_id in range(num_workers):\n",
    "            worker = Worker(worker_id,self.W1,self.W2,self.costs,V,D,counter,num_workers)\n",
    "            workers.append(worker)\n",
    "    \n",
    "        t0 = datetime.now()  \n",
    "        # then we start the processes\n",
    "        worker_processes = []\n",
    "        for worker in workers:\n",
    "            w_id = worker.worker_id\n",
    "            sentences_subset = sentences[w_id*subset_size:(w_id+1)*subset_size]\n",
    "            p = Process(target=worker.fit,args=(sentences_subset,positive_context,negative_context,initial_lr,final_lr,epochs,smoothing,threshold,epsilon,unigram,p_drop,start_epoch))\n",
    "            p.start()\n",
    "            worker_processes.append(p)\n",
    "        \n",
    "        # join the processes\n",
    "        for p in worker_processes:\n",
    "            p.join()\n",
    "\n",
    "        print('----------------------------------------------------------------')\n",
    "        print('Finished Training')\n",
    "        print('Training Time : ',datetime.now()-t0)\n",
    "        # now lets get all costs and make a plot\n",
    "\n",
    "        plt.plot(self.costs)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# now create the workers\n",
    "if __name__ == '__main__':\n",
    "    model = Boss()\n",
    "    # train model \n",
    "    #model.fit()\n",
    "    # resume training if training was aborted\n",
    "    #model.fit(resume_training=True)\n",
    "    # just load the weights to test some analogies\n",
    "    model.load_weights(plot_costs=True)\n",
    "\n",
    "    model.get_analogy('king', 'man', 'woman')\n",
    "    model.get_analogy('king', 'emperor',  'empire')\n",
    "    model.get_analogy('north', 'south',  'west')\n",
    "    model.get_analogy('mosque', 'church',  'christianity')\n",
    "    model.get_analogy('six', 'five',  'three')\n",
    "    model.get_analogy('summer', 'hot',  'cold')\n",
    "    model.get_analogy('india', 'indian',  'egyptian')\n",
    "    model.get_analogy('fire', 'water',  'south')\n",
    "    model.get_analogy('water', 'fire',  'false')\n",
    "    model.get_analogy('christianity', 'bible',  'quran')\n",
    "    model.get_analogy('sun', 'moon',  'night')\n",
    "    model.get_analogy('arabic', 'arab',  'american')\n",
    "    model.get_analogy('forward', 'backward',  'down')\n",
    "    model.get_analogy('fly', 'flying',  'walking')\n",
    "    model.get_analogy('fusion', 'fission',  'disperse')\n",
    "    model.get_analogy('proton', 'electron',  'negative')\n",
    "    model.get_analogy('palestine', 'jerusalem',  'tokyo')\n",
    "    model.get_analogy('xbox', 'microsoft',  'sony')\n",
    "    model.get_analogy('dc', 'superman',  'spiderman')\n",
    "    model.get_analogy('long', 'short',  'small')\n",
    "    model.get_analogy('peace', 'water',  'fire')\n",
    "    model.get_analogy('relativity', 'einstein',  'newton')\n",
    "    model.get_analogy('relativity', 'einstein',  'tesla')\n",
    "    model.get_analogy('king', 'prince', 'princess')\n",
    "    model.get_analogy('man', 'woman', 'she')\n",
    "    model.get_analogy('february', 'january',  'october')\n",
    "    model.get_analogy('heir', 'heiress',  'princess')\n",
    "    model.get_analogy('france', 'paris',  'tokyo')\n",
    "    model.get_analogy('france', 'paris',  'beijing')\n",
    "    model.get_analogy('france', 'paris',  'rome')\n",
    "    model.get_analogy('france', 'paris',  'berlin')\n",
    "    model.get_analogy('miami', 'florida', 'texas')\n",
    "    model.get_analogy('france', 'french',  'english')\n",
    "    model.get_analogy('japan', 'japanese',  'chinese')\n",
    "    model.get_analogy('china', 'chinese',  'american')\n",
    "    model.get_analogy('japan', 'japanese',  'italian')\n",
    "    model.get_analogy('japan', 'japanese',  'australian')\n",
    "    model.get_analogy('man', 'woman',  'mother')\n",
    "    model.get_analogy('old', 'young',  'son')\n",
    "    model.get_analogy('walk', 'walking',  'swimming')\n",
    "    model.get_analogy('man', 'woman',  'aunt')\n",
    "    model.get_analogy('nephew', 'niece',  'aunt')\n",
    "    model.get_analogy('man', 'woman', 'sister')\n",
    "    model.get_analogy('man', 'woman', 'wife')\n",
    "    model.get_analogy('man', 'woman',  'actress')\n",
    "    model.get_analogy('king', 'kingdom',  'egypt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try again with our BoW classifier\n",
    "class BoW:\n",
    "\n",
    "    def fit(self,option='concat'):\n",
    "        self.word2idx = model.word2idx\n",
    "        V,D = model.V,model.D\n",
    "        W1 =  np.frombuffer(model.W1.get_obj()).reshape(V,D)\n",
    "        W2 = np.frombuffer(model.W2.get_obj()).reshape(D,V)\n",
    "        self.V = V\n",
    "        if option == 'concat':\n",
    "            self.E = np.concatenate((W1,W2.T),axis=1)\n",
    "            self.D = 2*D\n",
    "        elif option == 'average':\n",
    "            self.E = (W1+W2.T)/2\n",
    "            self.D = D\n",
    "        elif option == 'W1':\n",
    "            self.E = W1\n",
    "            self.D = D\n",
    "            \n",
    "\n",
    "    def transform(self,sentences):\n",
    "        N = len(sentences)\n",
    "        X = np.zeros((N,self.D))\n",
    "        for i,sentence in enumerate(sentences):\n",
    "            idxs = []\n",
    "            words = sentence.lower().split()\n",
    "            for word in words:\n",
    "                idx = self.word2idx.get(word,None)\n",
    "                if idx is not None:\n",
    "                    idxs.append(idx)\n",
    "            if len(idxs)>0:\n",
    "                X[i] = self.E[idxs].mean(axis=0)  \n",
    "        return X\n",
    "\n",
    "    def fit_transform(self,sentences):\n",
    "        return self.transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = BoW()\n",
    "vectoriser.fit('concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets read in the input and output sentences\n",
    "Xtrain = []\n",
    "Ytrain = []\n",
    "\n",
    "Xtest = []\n",
    "Ytest = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for line in open(\"datasets/r8/r8-train-all-terms.txt\"):\n",
    "    y,x = line.rstrip().split('\\t')\n",
    "    Ytrain.append(y)\n",
    "    Xtrain.append(x)\n",
    "\n",
    "for line in open(\"datasets/r8/r8-test-all-terms.txt\"):\n",
    "    y,x = line.rstrip().split('\\t')\n",
    "    Ytest.append(y)\n",
    "    Xtest.append(x)\n",
    "\n",
    "# next lets give each class in Y a numerical value\n",
    "# also lets vectorise X\n",
    "\n",
    "Xtrain = vectoriser.transform(Xtrain)\n",
    "Xtest = vectoriser.transform(Xtest)\n",
    "\n",
    "label2idx = {k:v for k,v in zip(set(Ytrain),range(len(set(Ytrain))))}\n",
    "\n",
    "Ytrain = [label2idx[label] for label in Ytrain]\n",
    "Ytest = [label2idx[label] for label in Ytest]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9992707383773929\n",
      "test score: 0.9337597076290544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model1 = RandomForestClassifier(n_estimators=200)\n",
    "model1.fit(Xtrain, Ytrain)\n",
    "print(\"train score:\", model1.score(Xtrain, Ytrain))\n",
    "print(\"test score:\", model1.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we can see we get almost the same accuracy as with the pretrained word2vec \n",
    "# refer to BoW example in notebook 27 for results \n",
    "# (actually thats the same exact train accuracy and a 0.05 difference in test accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a simple model like this , numpy + multiprocessing seems to be superior to tensorflow (gpu)\n",
    "# seeems tensorflow does 'shine' with more complex models\n",
    "# but there is a very important thing to notice here \n",
    "# which is due to the special nature of the problem\n",
    "# in the tensorflow code , we abandoned the N-headed dragon approach in favour of the N-samples\n",
    "# this allowed us to train on batches , if batch size is large then the code should be faster\n",
    "# wrong !\n",
    "# in numpy , we only updated the vectors we used to make the predictions\n",
    "# so if we have 1 input vector of size 300 , and 5 output vectors of size 300\n",
    "# then we update 1x300 + 5x300 = 1800 parameters (1 sample in np , equivalently 5 samples in tf)\n",
    "# but what tensorflow does is that it returns the gradients for the entire weights\n",
    "# thats VxD for W1 and W2 , V = 20,000 , D = 300 , VxD = 20,000 x 300 = 6,000,000 , x 2 (W1,W2)\n",
    "# 12,000,000 parameters !\n",
    "# so this takes much more time even if we have a large batch size\n",
    "# in this sense its amazing that tf + GPU was not too far from np + mp in training time\n",
    "# but to do that we had to increase batch size a lot (1024) which lead to worse results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
