{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be discussing Recursice Neural Networks\n",
    "\n",
    "To motivate this discussion , lets look at how we have progressed in terms of how we have been able to process sentences in machine learning models\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Recursive Neural Networks Intro</h4>\n",
    "\n",
    "The first and simplest way that we always learn about in machine learning is Bag of Words\n",
    "\n",
    "That means that there is no order associated with the feature vector that represents a sentence\n",
    "\n",
    "consider the phrases \"Toy dog\" and \"Dog toy\"\n",
    "\n",
    "<img src='extras/33.1.PNG' width ='300'></img>\n",
    "\n",
    "immediately we can see that these two phrases have totally different meanings and yet a bag of words model treats them exactly the same \n",
    "\n",
    "<img src='extras/33.2.PNG' width ='500'></img>\n",
    "\n",
    "so a machine learning model wouldnot be able to tell the difference between these two , the data itself is fine the information is there , its just that the feature representation is limited\n",
    "\n",
    "---\n",
    "\n",
    "Next we looked at Recurrent Neural Networks\n",
    "\n",
    "We know that these are more powerful because they let us model sentences as sequences of words and so we have the context of all the words in the sentence that came before the current word\n",
    "\n",
    "<img src='extras/33.3.PNG' width='400'></img>\n",
    "\n",
    "This is powerful , but is it the most powerful ?\n",
    "\n",
    "We know that learning long term dependencies is a challenge for RNNs even with modern recurrent units such as the GRU or LSTM\n",
    "\n",
    "This makes sense since , even for us humans , reading a long sentence takes alot more effort than reading a short sentence\n",
    "\n",
    "---\n",
    "\n",
    "but consider how we as humans have learned to parse really long sentences , we dont just read it from start to end do we ?\n",
    "\n",
    "instead we may read a sentence multiple times , we start to compartmentalise parts of the sentence turning it into a sort of nested structure\n",
    "\n",
    "In other words a sentence becomes hierarchical or in other words a tree\n",
    "\n",
    "<img src='extras/33.4.PNG' ></img>\n",
    "\n",
    "And so it makes sense then , that in order to understand language , we should build a neural network to have the same structure as the sentence it is trying to model\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Outline #1</h4>\n",
    "\n",
    "so lets outline what we will be looking at\n",
    "\n",
    "<ul>\n",
    "    <li>How do we represent sentences as trees ?</li>\n",
    "    <li>what does tje data look like ?</li>\n",
    "    <li>How do we convert that into a Tree data structure in code ?</li>\n",
    "    <li>No neural networks yer :|</li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Outline #2</h4>\n",
    "\n",
    "Next , we are going to look at two different models and two different ways to implement them\n",
    "\n",
    "So the two different models we are going to look at are the :\n",
    "\n",
    "<ul>\n",
    "    <li>Plain recursive net</li>\n",
    "    <li>RNTN (recursice neural tensor network)</li>\n",
    "</ul>\n",
    "\n",
    "Basically the difference between these two is that the plain recursive net uses a linear transformation :\n",
    "\n",
    "$$Linear : h^{\\prime} = f\\left(W^Th+b\\right)$$\n",
    "\n",
    "and the RNTN uses a quadratic transformation\n",
    "\n",
    "$$Quadratic : h^{\\prime} = f \\left(h^TAh + W^Th + b\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Outline #3</h4>\n",
    "\n",
    "We will also look at two different ways to implement these models\n",
    "\n",
    "The first way is a very naive approach , it involves building a seperate symbolic graph for each sentence in our data\n",
    "\n",
    "This is of course required because each sentence has a different tree structure\n",
    "\n",
    "That means each neural network used to represent those sentences must have a different structure , and that means if we have N different datapoints , then we will need N different neural networks (but these neural networks have shared weights)\n",
    "\n",
    "And of course this is going to totally kill our memory\n",
    "\n",
    "<img src='extras/33.5.PNG'></img>\n",
    "\n",
    "---\n",
    "\n",
    "so the second method we will look at is implementing a recursive neural network as a custom-built recurrent network\n",
    "\n",
    "How can this be possible ?\n",
    "\n",
    "We know from our Algorithm studies that we can uniquely convert a tree into a sequence using different tree traversal algorithms\n",
    "\n",
    "<img src='extras/33.6.PNG' width='300'></img>\n",
    "\n",
    "\n",
    "this gives us a one-to-one correspondance between each tree and each sequence\n",
    "\n",
    "of course once we have a sequence we know that RNNs will work , and with RNNs we dont need a unique network to represent each sentence\n",
    "\n",
    "so we are converting recurssive nets into recurrent nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to look at a very simple example of how and why we can represent a sentence as a tree\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Sentences as Trees</h4>\n",
    "\n",
    "so the first thing we need to know is what a Parts-of-speech tag is (we already covered this multiple times , nothing complicated)\n",
    "\n",
    "<ul>\n",
    "    <li>Noun - a person,place,or a thing</li>\n",
    "    <li>Verb - action word (walk,run,jump)</li>\n",
    "    <li>Determiner - think of it as a word that describes a relationship with a noun (the,a,my)<ul><li>so for example if we say 'the dog' , that has a different meaning than 'my dog'</li><li>saying 'my dog' means the dog belongs to me , but saying 'the dog' implies that the dog could belong to anyone</li></ul></li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Parse tree</h4>\n",
    "\n",
    "ok so why do we need to know Parts-of-speech tags ?\n",
    "\n",
    "Because thats generally how senetences are split up into trees\n",
    "\n",
    "These are best demonstrated by an example\n",
    "\n",
    "so here is the sentence \n",
    "\n",
    "$$\\text{John hit the ball}$$\n",
    "\n",
    "and here is its parse tree\n",
    "\n",
    "<img src='extras/33.7.PNG' width='400'></img>\n",
    "\n",
    "```\n",
    "N = Noun\n",
    "V = Verb\n",
    "D = Determiner\n",
    "VP = Verb Phrase\n",
    "NP = Noun Phrase\n",
    "```\n",
    "\n",
    "So , first we can see the Parts-of-speech tag for each individual word\n",
    "\n",
    "'John' is a Noun , 'hit' is a Verb , 'the' is a Determiner and 'ball' is another Noun\n",
    "\n",
    "We can see that 'the ball' goes together to form a Noun Phrase\n",
    "\n",
    "We can see that 'hit' goes with 'the ball' to make up the Verb Phrase\n",
    "\n",
    "And finally we can see that 'John' goes with 'hit the ball' to make a Sentence\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Sentences are hierarchical</h4>\n",
    "\n",
    "So we can see that language is hierarchical\n",
    "\n",
    "If we look at very long sentences we can start to see patterns formed\n",
    "\n",
    "In particular we can see that a Noun Phrase is often followed by a Verb Phrase , and a Verb Phrase is replaceable by a Verb (so we can sort of think of them as the same)\n",
    "\n",
    "In Computer Science we talk about class hierarchies , so for example we might have a Shape class and from that we can have derived classes like Circle or Square , and we can always trust that if we have a Circle object , it is always also an instance of Shape\n",
    "\n",
    "<img src='extras/33.8.PNG'></img>\n",
    "\n",
    "So its kind of like that , so here we have the sentence \n",
    "\n",
    "$$\\text{the angry bear chased the frightened little squarrel}$$\n",
    "\n",
    "but if we replace the Verb Phrase with a simple Verb , we would still get a valid sentence , for example $$\\text{the angry bear }\\underline{ran}$$\n",
    "\n",
    "We can also replace the Noun Phrase with a simple Noun and that still gives us a Valid sentence , for example $$\\underline{Bob}\\text{ chased the frightened little squarrel}$$\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Sentiment Analysis</h4>\n",
    "\n",
    "Importantly , for this notebook , we are doing a task called Sentiment Analysis\n",
    "\n",
    "This means that we want to build a model that tells us how positive or negative something is\n",
    "\n",
    "for example : \n",
    "\n",
    "<ul>\n",
    "    <li>This movie sucks $\\rightarrow$ negative sentiment</li>\n",
    "    <li>This movie is great : positive sentiment</li>\n",
    "</ul>\n",
    "\n",
    "Not only will these sentences be parsed as a tree , but each node in the tree will also have a label to tell the sentiment\n",
    "\n",
    "<img src='extras/33.9.PNG' width='600'></img>\n",
    "\n",
    "The reason we want to be able to do this is to handle the problem of negation , which has traditionally , been very difficult for existing NLP models\n",
    "\n",
    "But with a tree we can see that even if an entire phrase is negative/positive combining it with another phrase can reverse its meaning\n",
    "\n",
    "for example if the review (in the picture above) was :\n",
    "\n",
    "$$\\text{There are slow and repetitive parts}$$\n",
    "\n",
    "of course we would just consider that a negative review\n",
    "\n",
    "But we need to consider the other phrase in the sentence\n",
    "\n",
    "$$\\text{it has just enough spice to keep it interesting}$$\n",
    "\n",
    "which we could regard as somewhat positive\n",
    "\n",
    "so that second positive phrase sort of takes over the meaning of the whole sentence , and that probably thank to the word $but$ , which we can see here has neural sentiment\n",
    "\n",
    "Typically when we use the word but , its in the form of a sentence like \"Here is what I thought memontarily BUT this is what I think overall\"\n",
    "\n",
    "so when we see the word but , its usually the second part of the sentence that matters\n",
    "\n",
    "And representing the sentence as a tree allows us to model that relationship\n",
    "\n",
    "---\n",
    "\n",
    "Here is another example :\n",
    "\n",
    "<img src='extras/33.10.PNG' width='550'></img>\n",
    "\n",
    "we can see that the word 'humor' has very postitve sentiment , but its also very far down the tree , so the overall sentence has the potential to change its meaning\n",
    "\n",
    "And indeed we can see that the sentence starts with \n",
    "\n",
    "$$\\text{This film }\\underline{doesn't}$$\n",
    "\n",
    "which negates the positive part of the sentence\n",
    "\n",
    "Also notice that , when we are doing sentiment analysis we dont actually care where the parts-of-speech are\n",
    "\n",
    "That only helped us to parse the tree itself , but once the tree has been parsed we dont need that information any longer\n",
    "\n",
    "Next , we want to look at how these parse trees are actually represented in our data files , and how we may parse them in code , which will bring us to how we can create recursice neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be taking a closer look at the data we will be working with\n",
    " \n",
    "---\n",
    "\n",
    "<h4>Recursive Neural Network Data</h4>\n",
    "\n",
    "so far we have seen two ways of representing sentences\n",
    "\n",
    "<ol>\n",
    "    <li>Bag of words</li>\n",
    "    <li>Sequence of words</li>\n",
    "</ol>\n",
    "\n",
    "But the thirds and most powerful way is to think of sentences as trees\n",
    "\n",
    "Sentences have a dependancy structure , for example think of a very simple sentence , just a (Noun,Verb) \n",
    "\n",
    "$$\\text{Johny walks}$$\n",
    "\n",
    "so here 'Johny' is the Noun and 'walks' is the verb\n",
    "\n",
    "<img src='extras/33.11.PNG' width='250'></img>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Parse Trees</h4>\n",
    "\n",
    "But we can make longer sentences with the same structure , which make up a Noun Phrase and a Verb Phrase\n",
    "\n",
    "<img src='extras/33.12.PNG' width='400'></img>\n",
    "\n",
    "so for example\n",
    "\n",
    "$$\\text{Big John walks quickly}$$\n",
    "\n",
    "so here 'Big John' is the Noun Phrase and 'walks quickly' is the verb phrase\n",
    "\n",
    "'Big John' can the be split up into its constituent parts , same for 'walks quickly'\n",
    "\n",
    "Now we have a tree :)\n",
    "\n",
    "---\n",
    "\n",
    "Now just as a sidenote , how are these trees built ?\n",
    "\n",
    "Unsurprisingly , this is also a machine learning problem\n",
    "\n",
    "we wont discuss how to do it ourselves , we will just use the results\n",
    "\n",
    "As a more philosphical thought , can we imagine an entire system of neural networks all performing different tasks such that the end result is something actionable in the real world\n",
    "\n",
    "so for example , if we are building a brain , the language processing system would be just a small part \n",
    "\n",
    "But we might have a NN to do parts-of-speech tagging another NN to do named-entity recognition and another NN to parse sentences into trees , and thats just a small part of an entire brain\n",
    "\n",
    "An even more challenging problem might be to figure out , what is a general architecture we can use so that we dont even have to tell the system to learn these things\n",
    "\n",
    "In other words , a network of neural networks where each NN can figure out what it should do on its own and be useful for the overall system in a global manner\n",
    "\n",
    "---\n",
    "\n",
    "There are a few ways to represent sentences as parse trees\n",
    "\n",
    "In general , a node in a tree can have any number of children such as this one\n",
    "\n",
    "<img src='extras/33.13.PNG' ></img>\n",
    "\n",
    "for us we will be working with binary trees , that means each node will have up to 2 children\n",
    "\n",
    "the binary trees we will be using are such that , if a node has children it always has two children\n",
    "\n",
    "so there will be no situations where a node only has 1 child\n",
    "\n",
    "---\n",
    "\n",
    "<h4>The task</h4>\n",
    "\n",
    "One of the tasks we can do using RNNs is next word prediction , so we are able to build a language model by maximising $p(x(t)|x(t-1),\\ldots,x(0))$\n",
    "\n",
    "now that we have trees , is there such a thing as next ?\n",
    "\n",
    "The answer is NO , because trees are not a sequence , they are hierarchical\n",
    "\n",
    "Instead we are going to do Sentiment Analysis\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Sentiment Analysis</h4>\n",
    "\n",
    "Recall that we tried to do sentiment analysis using bag of words as input , the disadvantage to this is that it cant handle negation\n",
    "\n",
    "if we have one-hot-encoded words adding a 'not' or 'isnt' to the input vector will give us another vector very close to the original vector , so its very hard for any ML model that uses BoW to handle negation\n",
    "\n",
    "Whats interesting is that Recursive Neural Networks finally solve this problem\n",
    "\n",
    "Research on sentiment Analysis hot about 80% accuracy at best , until recursive neural networks came along , now we can acheive about 85-90% accuracy\n",
    "\n",
    "so why do recursive neural networks work so well ?\n",
    "\n",
    "well , if we have negation then its very easy to just reverse whatever is in the phrase that its negating \n",
    "\n",
    "because the parse tree is structured to identify these dependancies , the NN can use them to do negation\n",
    "\n",
    "<img src='extras/33.14.PNG' width='200'></img>\n",
    "\n",
    "---\n",
    "\n",
    "Now what does this suggest ?\n",
    "\n",
    "It means that we are going to have a label for every node of the tree\n",
    "\n",
    "so lets go back to the sentence tree which we talked about a little bit\n",
    "\n",
    "All we know so far is that it is a binary tree and each node has 0 or 2 children , so lets add to this\n",
    "\n",
    "To be able to understand negation we will have labels at each node\n",
    "\n",
    "one more fact we need to know , that we can figure out easily ourselves just by looking at the data , is that only the leaves of the tree represent words\n",
    "\n",
    "Any inner node represent a phrase made up of words that are its descendants\n",
    "\n",
    "---\n",
    "\n",
    "So now that we know everything about the structure of the tree , what does the data actually look like ?\n",
    "\n",
    "first we can get the data from here <a href='https://nlp.stanford.edu/sentiment/'>here</a>\n",
    "\n",
    "we want to download the file : trainDevTestTrees_PTB.zip , its just a bunch of movie reviews\n",
    "\n",
    "we might be wondering , how did the researchers actually get all this labeled data ?\n",
    "\n",
    "well they used the service called Amazon Mechanical Turk , where people do random tasks for money\n",
    "\n",
    "so they where all given very short phrases , down to the word level , and asked to rate them from 1-5 (5 being highest sentiment , 1 being lowest sentiment and 3 being neutral)\n",
    "\n",
    "In our data of course , these are labeled 0-4\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Data format</h4>\n",
    "\n",
    "because the data needs to be represented as text ,the researchers used perenthesis ,$()$, to seperate each node\n",
    "\n",
    "so for example : $\\text{Great movie}$\n",
    "\n",
    "would be represented by this text : $\\text{(5(5 Great)(3 movie))}$\n",
    "\n",
    "and it would be a height 1 binary tree that represents the sentence $\\text{Great movie}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next , we are going to talk about the architecture of Recursive Neural Networks\n",
    "\n",
    "We are going to talk about how we can create a Recursice Neural Network both for binary trees , and for trees with any number of children\n",
    "\n",
    "sometimes people refer to Recursive Neural Networks as RNNs , but the nit becomes to easy to confuse with Recurrent Neural Networks \n",
    "\n",
    "An unfortunate coincidence , however we will see later why it might be appropriate :)\n",
    "\n",
    "We are going to sometimes refer to them as \"Tree Neural Networks\" , TNNs because thats what they are\n",
    "\n",
    "---\n",
    "\n",
    "<h4>TNNs</h4>\n",
    "\n",
    "The easiest way to understand them is to think about the parse tree of a sentence\n",
    "\n",
    "because we are using binary trees in our data , we will discuss binary trees first\n",
    "\n",
    "our Recursive Neural Network will essentially have the very same structure as the parse tree\n",
    "\n",
    "<img src='extras/33.15.PNG' width = '400'></img>\n",
    "\n",
    "The question is , where do the weights go ? how do we compute the output ?\n",
    "\n",
    "Because the structure is recursive , we can only define a node's value by its children\n",
    "\n",
    "when we look at its children , those are also nodes , so those nodes can only be defined by their own children and so on\n",
    "\n",
    "because of this we only need two weights , one to the left to tell us how the left child connects to the current node , and one for the right to tell us how the right child connects to the current node\n",
    "\n",
    "notice how there is only one bias term (of course having two bias terms is redundant)\n",
    "\n",
    "so here is how we calculate the value at the hidden node $h_1$\n",
    "\n",
    "$$x_1 = w_1 = \\text{word embedding for }w_1$$\n",
    "\n",
    "$$h_1 = f\\left(W_{\\text{left}}x_1 + W_{\\text{right}}x_2 + b\\right)$$\n",
    "\n",
    "Now lets calculate the vakue at the root\n",
    "\n",
    "$$h_{\\text{root}} = f\\left(W_{\\text{left}}h_1 + W_{\\text{right}}x_3 + b\\right)$$\n",
    "\n",
    "we can see that it depends on $h_1$ on its left and $w_3$ on its right\n",
    "\n",
    "the question we might have next is , what is the size of these weights\n",
    "\n",
    "well remember that the structure is recursive , so that at any node its children might be words or its children might be other nodes\n",
    "\n",
    "But the weights have to fit with both , therefore the weights must all be the same size and its input size must match its output size\n",
    "\n",
    "in other words , if our embedding is of size D , then all the $W$s must be of size DxD and all the biases must be of size $D$\n",
    "\n",
    "note that the activation function $f$ can be any of the activation functions we use $relu,tanh,etc...$\n",
    "\n",
    "---\n",
    "\n",
    "so whats next\n",
    "\n",
    "<img src='extras/33.16.PNG' width='400'></img>\n",
    "\n",
    "well , if we are doing a task like sentiment analysis and we have 5 classes (0-4) or 2 classes (+ve/-ve) depending on how we frame the problem , how do we get an output of size 5 or size 2 ?\n",
    "\n",
    "remember that any node can have a label , so we must be able to calculate $p(y \\vert h)$ for any $h$ as an inner node , root node or a single word\n",
    "\n",
    "to do that, all we need to do is multiply that node's value by the output weight $W_o$ add the output bias $b_o$ and softmax it as usual\n",
    "\n",
    "$$p(y \\vert h) = softmax(W_oh+b_o)$$\n",
    "\n",
    "by doing softmax , we can have any number of classes , 2 or 5 would be treated exactly the same\n",
    "\n",
    "note that even though the inner nodes are labeled , we dont necessarily have to use them during training\n",
    "\n",
    "in fact , we will write code for both and then we will see if there is a difference in the results\n",
    "\n",
    "---\n",
    "\n",
    "Now that we fully defined what a binary recursive neural network would look like , what about a recursive neural network with any number of children ?\n",
    "\n",
    "<img src='extras/33.17.PNG' width='250'></img>\n",
    "\n",
    "we can see that by summing over each child we naturally extend the definition if the binary relationship\n",
    "\n",
    "$$h = f \\left(\\sum_i W\\left(rel\\left(h,i\\right)\\right)x_i + b\\right)$$\n",
    "\n",
    "we might wonder how do we even get a tree that is not binary in the first place ?\n",
    "\n",
    "well , stanford NLP tools actually give us a way to generate our own parse trees from arbitary text , the disadvantage of doing that however is that we wont have a labeled dataset\n",
    "\n",
    "now whats interesting is that , they dont have a tool which is as easy to use to get binary trees (which is why we are very lucky that they did the work of generating the data for us)\n",
    "\n",
    "note : they have written some code (in java) to produce binary trees , but still the data will be unlabeled\n",
    "\n",
    "In any case the number of parent-child relationships is obviously limited , so lets call that limit $R$\n",
    "\n",
    "so there are $R$ possible different dependency relations\n",
    "\n",
    "because of this we can make a big $W$ matrix that contains all the R different DxD matricies and multiply the input by the appropriate relation weight to get the appropriate value\n",
    "\n",
    "we can think of the binary tree as a simple case of this , where there are only two types of relation left and right , R = 2\n",
    "\n",
    "note : so here W is of size RxDxD\n",
    "\n",
    "note : rel(h,i) returns a number from 1 ... R (0 to R-1 in code) telling us the type of relationship , these are given by Stanford NLP parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets remember our options\n",
    "# we have two models , Plain Recursive nets (linear) and RNTNs (quadratic)\n",
    "# and we have two ways to implement each , Recursion (trees) or custom-built RNN (sequences)\n",
    "# so out of the 4 option we will implement the following\n",
    "# Plain Recursive Nets + Recursion (what we now so far)\n",
    "# RNTNs + sequences (will look at later)\n",
    "# lets start coding Plain Recursive Nets + Recursion\n",
    "# note that since tensorflow 2.0 supports eager execution\n",
    "# we dont have to worry about graphs being compiled and problems that follow (code becoming slow)\n",
    "# and our models will be running quite fast\n",
    "# but we will still be looking at the sequences solution to see if its worth it\n",
    "# otherwise the RNTN will be just a small change to this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define a class of tree nodes\n",
    "class node:\n",
    "    def __init__(self):\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.label = None \n",
    "        self.word = ''\n",
    "        # eases parsing\n",
    "        self.parent = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next lets load in the data\n",
    "train_trees_text = []\n",
    "for line in open('datasets/trees/train.txt'):\n",
    "    train_trees_text.append(line.rstrip())\n",
    "    \n",
    "test_trees_text = []\n",
    "for line in open('datasets/trees/test.txt'):\n",
    "    test_trees_text.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a function to parse teext to a tree\n",
    "def create_tree(text):    \n",
    "    # first create root node\n",
    "    root = node()\n",
    "    # we also want to keep track of the words we saw\n",
    "    words = []\n",
    "    for i,w in enumerate(text):\n",
    "        if w == ' ':\n",
    "            continue\n",
    "\n",
    "        if w == '(':\n",
    "            # so we need to create a new node\n",
    "            child = node()\n",
    "            if root.left is None:\n",
    "                root.left = child\n",
    "            else:\n",
    "                root.right = child\n",
    "            child.parent = root\n",
    "            # then let l_child continue\n",
    "            root = child\n",
    "\n",
    "        elif w != ')':\n",
    "            # we have a label or a word\n",
    "            # lets peak ahead to see if the next symbol is a character or a ' '\n",
    "            if text[i+1] == ' ': \n",
    "                # so we have a label\n",
    "                root.label = int(w)\n",
    "            else:\n",
    "                # so its a character of the word\n",
    "                root.word += w.lower()\n",
    "        else : # w is )\n",
    "            if root.word != '':\n",
    "                words.append(root.word)\n",
    "            root = root.parent\n",
    "\n",
    "    # our root is actually the left child of the root\n",
    "    root = root.left\n",
    "    return root,words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets convert text to trees\n",
    "vocab = []\n",
    "train_trees = []\n",
    "for text in train_trees_text:\n",
    "    tree,words = create_tree(text)\n",
    "    train_trees.append(tree)\n",
    "    vocab += words\n",
    "\n",
    "test_trees = []\n",
    "# now again for test data\n",
    "for text in test_trees_text:\n",
    "    tree,words = create_tree(text)\n",
    "    test_trees.append(tree)\n",
    "    vocab += words\n",
    "    \n",
    "vocab = list(set(vocab))\n",
    "V = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets also create word2idx dict\n",
    "word2idx = {k:v for k,v in zip(vocab,range(V))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "D = 10\n",
    "is_binary = True # whether or not to do binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will change our problem to binary classification\n",
    "# so rating 0,1 become negative sentiment : 0\n",
    "# 2 is neutral sentiment : -1 , these will be filtered out \n",
    "# 3,4 become positive sentiment : 1\n",
    "\n",
    "def to_binary(tree):\n",
    "    if tree == None:\n",
    "        return\n",
    "    if tree.label<2:\n",
    "        tree.label = 0\n",
    "    elif tree.label == 2:\n",
    "        tree.label = -1\n",
    "    else:\n",
    "        tree.label = 1\n",
    "    to_binary(tree.left)\n",
    "    to_binary(tree.right)\n",
    "    return tree\n",
    "    \n",
    "if is_binary:\n",
    "    K = 2\n",
    "    for tree in train_trees:\n",
    "        to_binary(tree)\n",
    "    for tree in test_trees:\n",
    "        to_binary(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out trees whose label (that of root) is neutral (2 --> -1)\n",
    "\n",
    "train_trees = [tree for tree in train_trees if tree.label != -1]\n",
    "test_trees = [tree for tree in test_trees if tree.label != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are ready to make our model\n",
    "# we will implement two models\n",
    "# one that takes advantage of the fact that each node knows its parent\n",
    "# thus we get an iterative solution\n",
    "# the other is the recurssive solution\n",
    "class TNNI(Model): # TNN Iterative\n",
    "    def __init__(self,V,D,K):\n",
    "        super(TNNI, self).__init__()\n",
    "        self.V = V\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "        We = np.random.randn(V, D)/np.sqrt(V+D)\n",
    "        W1 = np.random.randn(D, D)/np.sqrt(D+D)\n",
    "        W2 = np.random.randn(D, D)/np.sqrt(D+D)\n",
    "        Wo = np.random.randn(D, K)/np.sqrt(D+K)\n",
    "        bo = np.zeros(K)\n",
    "        bh = np.zeros(D)\n",
    "\n",
    "        self.We = tf.Variable(We.astype(np.float32))\n",
    "        self.W1 = tf.Variable(W1.astype(np.float32))\n",
    "        self.W2 = tf.Variable(W2.astype(np.float32))\n",
    "        self.Wo = tf.Variable(Wo.astype(np.float32))\n",
    "        self.bo = tf.Variable(bo.astype(np.float32))\n",
    "        self.bh = tf.Variable(bh.astype(np.float32))\n",
    "    \n",
    "    # takes in lost of roots\n",
    "    # return accuracy\n",
    "    def score(self,trees):\n",
    "        correct = 0\n",
    "        for tree in trees:\n",
    "            t,p = self(tree)\n",
    "            if t==p:\n",
    "                correct+=1\n",
    "        return correct/len(trees)\n",
    "\n",
    "    # takes root , returns logits and targets\n",
    "    def call(self, root,pred=True):\n",
    "        # instead of taking the recursive approach\n",
    "        # we can make use of the fact that each node knows its parent\n",
    "        # and traverse the tree bottom up\n",
    "        # first we get all the leaf nodes\n",
    "        children = []\n",
    "        parents = []\n",
    "        children_h = []\n",
    "        parents_h = []\n",
    "\n",
    "        # so this gets all the leaf nodes in children in postorder\n",
    "        self.postorder(root,children)\n",
    "\n",
    "        # next we traverse the tree bottom up\n",
    "        # we consider nodes in pair\n",
    "        # if they have the same parent , thats great !\n",
    "        # if not , then the node in to the left makes its way to the parents\n",
    "        # and the node to the right will be checked next iteration to the node to its right\n",
    "        # that until we reach the root\n",
    "        # we do so while getting the logits\n",
    "        # this simulates a Reverse Breadth first traversal\n",
    "        # first get word vectors of leaves\n",
    "        children_h = [tf.nn.embedding_lookup(self.We,word2idx[node.word]) for node in children]\n",
    "        # next we get the logits\n",
    "        logits = [tf.matmul(tf.experimental.numpy.atleast_2d(leaf_vec),self.Wo)+self.bo for leaf_vec in children_h]\n",
    "        targets = [leaf.label for leaf in children]\n",
    "        while len(children) !=1 : # so we loop till we reach one node, that is the root\n",
    "            i=0\n",
    "            while i<len(children):\n",
    "\n",
    "                left = children[i]\n",
    "                if i == len(children)-1: # no right , left is the last child\n",
    "                    parents.append(left)\n",
    "                    left_h = children_h[i]\n",
    "                    parents_h.append(left_h)\n",
    "                    i+=1\n",
    "                    continue\n",
    "\n",
    "                right = children[i+1]\n",
    "                if left.parent != right.parent: # so left is alone\n",
    "                    parents.append(left)\n",
    "                    left_h = children_h[i]\n",
    "                    parents_h.append(left_h)\n",
    "                    i+=1\n",
    "                    continue\n",
    "                parent = left.parent\n",
    "                parents.append(parent)\n",
    "                left_h = children_h[i]\n",
    "                right_h = children_h[i+1]\n",
    "\n",
    "                left_h = tf.experimental.numpy.atleast_2d(left_h)\n",
    "                right_h = tf.experimental.numpy.atleast_2d(right_h)\n",
    "                i+=2\n",
    "\n",
    "                h = self.f(tf.matmul(left_h,self.W1) + tf.matmul(right_h,self.W2) + self.bh)\n",
    "                parents_h.append(h)\n",
    "                logit = tf.matmul(h,self.Wo) + self.bo\n",
    "                logits.append(logit)\n",
    "                t = parent.label\n",
    "                targets.append(t)\n",
    "\n",
    "            children = parents\n",
    "            children_h = parents_h\n",
    "            parents = []\n",
    "            parents_h = []\n",
    "        if pred == True:\n",
    "            return targets[-1],tf.argmax(tf.squeeze(logits[-1])).numpy()\n",
    "        logits = [tf.squeeze(logit) for logit in logits]\n",
    "        return targets,logits\n",
    "                        \n",
    "\n",
    "    def loss(self,T,Y,reg):\n",
    "        \n",
    "        if self.train_inner_nodes == True:\n",
    "            # filter out nodes where sentiment is neural\n",
    "            T = tf.constant(T,dtype='int32')\n",
    "            valid = tf.where(T != -1)\n",
    "            T = tf.gather(T,valid)\n",
    "            Y = tf.gather(Y,valid)\n",
    "        else: # we only care about root\n",
    "            T = T[-1]\n",
    "            Y = Y[-1]\n",
    "        cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(T,Y))\n",
    "        l2 = reg*sum(tf.nn.l2_loss(w) for w in self.weights)\n",
    "        cost += l2\n",
    "        return cost\n",
    "    \n",
    "    def postorder(self,root,leaves):\n",
    "        if root.word != '':\n",
    "            leaves.append(root)\n",
    "            return\n",
    "        self.postorder(root.left,leaves)\n",
    "        self.postorder(root.right,leaves)\n",
    "    \n",
    "    # training\n",
    "    def fit(self,trees,epochs = 5,lr=1e-3,reg = 1e-3,f = tf.nn.tanh,train_inner_nodes=True):\n",
    "        opt = Adam(lr) \n",
    "        self.train_inner_nodes = train_inner_nodes\n",
    "        self.f = f\n",
    "        costs = []\n",
    "        for epoch in range(epochs):\n",
    "            t0=datetime.now()\n",
    "            epoch_cost = 0\n",
    "            correct = 0\n",
    "            trees = shuffle(trees)\n",
    "            for i,tree in enumerate(trees):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    T,logits = self(tree,False)\n",
    "                    cost = self.loss(T,logits,reg)\n",
    "                grads = tape.gradient(cost, self.trainable_weights)\n",
    "                opt.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "                # keep track of epoch cost\n",
    "                # and number of correct predictions\n",
    "                epoch_cost += cost.numpy()\n",
    "                # we only care about root label for accuracy\n",
    "                correct += T[-1] == np.argmax(logits[-1])\n",
    "                if (i+1)%100 == 0:\n",
    "                    print('Epoch: ',epoch+1,'/',epochs,' finished: ',i+1,'/',len(trees))\n",
    "            print('Finished Epoch:',epoch+1,'/',epochs,' train accuracy: ',correct/len(trees),'epoch cost: ',epoch_cost,' epoch time: ',datetime.now()-t0)\n",
    "            costs.append(epoch_cost)\n",
    "        plt.plot(costs)\n",
    "        plt.title('cost')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 5  finished:  100 / 6920\n",
      "Epoch:  1 / 5  finished:  200 / 6920\n",
      "Epoch:  1 / 5  finished:  300 / 6920\n",
      "Epoch:  1 / 5  finished:  400 / 6920\n",
      "Epoch:  1 / 5  finished:  500 / 6920\n",
      "Epoch:  1 / 5  finished:  600 / 6920\n",
      "Epoch:  1 / 5  finished:  700 / 6920\n",
      "Epoch:  1 / 5  finished:  800 / 6920\n",
      "Epoch:  1 / 5  finished:  900 / 6920\n",
      "Epoch:  1 / 5  finished:  1000 / 6920\n",
      "Epoch:  1 / 5  finished:  1100 / 6920\n",
      "Epoch:  1 / 5  finished:  1200 / 6920\n",
      "Epoch:  1 / 5  finished:  1300 / 6920\n",
      "Epoch:  1 / 5  finished:  1400 / 6920\n",
      "Epoch:  1 / 5  finished:  1500 / 6920\n",
      "Epoch:  1 / 5  finished:  1600 / 6920\n",
      "Epoch:  1 / 5  finished:  1700 / 6920\n",
      "Epoch:  1 / 5  finished:  1800 / 6920\n",
      "Epoch:  1 / 5  finished:  1900 / 6920\n",
      "Epoch:  1 / 5  finished:  2000 / 6920\n",
      "Epoch:  1 / 5  finished:  2100 / 6920\n",
      "Epoch:  1 / 5  finished:  2200 / 6920\n",
      "Epoch:  1 / 5  finished:  2300 / 6920\n",
      "Epoch:  1 / 5  finished:  2400 / 6920\n",
      "Epoch:  1 / 5  finished:  2500 / 6920\n",
      "Epoch:  1 / 5  finished:  2600 / 6920\n",
      "Epoch:  1 / 5  finished:  2700 / 6920\n",
      "Epoch:  1 / 5  finished:  2800 / 6920\n",
      "Epoch:  1 / 5  finished:  2900 / 6920\n",
      "Epoch:  1 / 5  finished:  3000 / 6920\n",
      "Epoch:  1 / 5  finished:  3100 / 6920\n",
      "Epoch:  1 / 5  finished:  3200 / 6920\n",
      "Epoch:  1 / 5  finished:  3300 / 6920\n",
      "Epoch:  1 / 5  finished:  3400 / 6920\n",
      "Epoch:  1 / 5  finished:  3500 / 6920\n",
      "Epoch:  1 / 5  finished:  3600 / 6920\n",
      "Epoch:  1 / 5  finished:  3700 / 6920\n",
      "Epoch:  1 / 5  finished:  3800 / 6920\n",
      "Epoch:  1 / 5  finished:  3900 / 6920\n",
      "Epoch:  1 / 5  finished:  4000 / 6920\n",
      "Epoch:  1 / 5  finished:  4100 / 6920\n",
      "Epoch:  1 / 5  finished:  4200 / 6920\n",
      "Epoch:  1 / 5  finished:  4300 / 6920\n",
      "Epoch:  1 / 5  finished:  4400 / 6920\n",
      "Epoch:  1 / 5  finished:  4500 / 6920\n",
      "Epoch:  1 / 5  finished:  4600 / 6920\n",
      "Epoch:  1 / 5  finished:  4700 / 6920\n",
      "Epoch:  1 / 5  finished:  4800 / 6920\n",
      "Epoch:  1 / 5  finished:  4900 / 6920\n",
      "Epoch:  1 / 5  finished:  5000 / 6920\n",
      "Epoch:  1 / 5  finished:  5100 / 6920\n",
      "Epoch:  1 / 5  finished:  5200 / 6920\n",
      "Epoch:  1 / 5  finished:  5300 / 6920\n",
      "Epoch:  1 / 5  finished:  5400 / 6920\n",
      "Epoch:  1 / 5  finished:  5500 / 6920\n",
      "Epoch:  1 / 5  finished:  5600 / 6920\n",
      "Epoch:  1 / 5  finished:  5700 / 6920\n",
      "Epoch:  1 / 5  finished:  5800 / 6920\n",
      "Epoch:  1 / 5  finished:  5900 / 6920\n",
      "Epoch:  1 / 5  finished:  6000 / 6920\n",
      "Epoch:  1 / 5  finished:  6100 / 6920\n",
      "Epoch:  1 / 5  finished:  6200 / 6920\n",
      "Epoch:  1 / 5  finished:  6300 / 6920\n",
      "Epoch:  1 / 5  finished:  6400 / 6920\n",
      "Epoch:  1 / 5  finished:  6500 / 6920\n",
      "Epoch:  1 / 5  finished:  6600 / 6920\n",
      "Epoch:  1 / 5  finished:  6700 / 6920\n",
      "Epoch:  1 / 5  finished:  6800 / 6920\n",
      "Epoch:  1 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 1 / 5  train accuracy:  0.725 epoch cost:  3812.9916745349765  epoch time:  0:08:13.628078\n",
      "Epoch:  2 / 5  finished:  100 / 6920\n",
      "Epoch:  2 / 5  finished:  200 / 6920\n",
      "Epoch:  2 / 5  finished:  300 / 6920\n",
      "Epoch:  2 / 5  finished:  400 / 6920\n",
      "Epoch:  2 / 5  finished:  500 / 6920\n",
      "Epoch:  2 / 5  finished:  600 / 6920\n",
      "Epoch:  2 / 5  finished:  700 / 6920\n",
      "Epoch:  2 / 5  finished:  800 / 6920\n",
      "Epoch:  2 / 5  finished:  900 / 6920\n",
      "Epoch:  2 / 5  finished:  1000 / 6920\n",
      "Epoch:  2 / 5  finished:  1100 / 6920\n",
      "Epoch:  2 / 5  finished:  1200 / 6920\n",
      "Epoch:  2 / 5  finished:  1300 / 6920\n",
      "Epoch:  2 / 5  finished:  1400 / 6920\n",
      "Epoch:  2 / 5  finished:  1500 / 6920\n",
      "Epoch:  2 / 5  finished:  1600 / 6920\n",
      "Epoch:  2 / 5  finished:  1700 / 6920\n",
      "Epoch:  2 / 5  finished:  1800 / 6920\n",
      "Epoch:  2 / 5  finished:  1900 / 6920\n",
      "Epoch:  2 / 5  finished:  2000 / 6920\n",
      "Epoch:  2 / 5  finished:  2100 / 6920\n",
      "Epoch:  2 / 5  finished:  2200 / 6920\n",
      "Epoch:  2 / 5  finished:  2300 / 6920\n",
      "Epoch:  2 / 5  finished:  2400 / 6920\n",
      "Epoch:  2 / 5  finished:  2500 / 6920\n",
      "Epoch:  2 / 5  finished:  2600 / 6920\n",
      "Epoch:  2 / 5  finished:  2700 / 6920\n",
      "Epoch:  2 / 5  finished:  2800 / 6920\n",
      "Epoch:  2 / 5  finished:  2900 / 6920\n",
      "Epoch:  2 / 5  finished:  3000 / 6920\n",
      "Epoch:  2 / 5  finished:  3100 / 6920\n",
      "Epoch:  2 / 5  finished:  3200 / 6920\n",
      "Epoch:  2 / 5  finished:  3300 / 6920\n",
      "Epoch:  2 / 5  finished:  3400 / 6920\n",
      "Epoch:  2 / 5  finished:  3500 / 6920\n",
      "Epoch:  2 / 5  finished:  3600 / 6920\n",
      "Epoch:  2 / 5  finished:  3700 / 6920\n",
      "Epoch:  2 / 5  finished:  3800 / 6920\n",
      "Epoch:  2 / 5  finished:  3900 / 6920\n",
      "Epoch:  2 / 5  finished:  4000 / 6920\n",
      "Epoch:  2 / 5  finished:  4100 / 6920\n",
      "Epoch:  2 / 5  finished:  4200 / 6920\n",
      "Epoch:  2 / 5  finished:  4300 / 6920\n",
      "Epoch:  2 / 5  finished:  4400 / 6920\n",
      "Epoch:  2 / 5  finished:  4500 / 6920\n",
      "Epoch:  2 / 5  finished:  4600 / 6920\n",
      "Epoch:  2 / 5  finished:  4700 / 6920\n",
      "Epoch:  2 / 5  finished:  4800 / 6920\n",
      "Epoch:  2 / 5  finished:  4900 / 6920\n",
      "Epoch:  2 / 5  finished:  5000 / 6920\n",
      "Epoch:  2 / 5  finished:  5100 / 6920\n",
      "Epoch:  2 / 5  finished:  5200 / 6920\n",
      "Epoch:  2 / 5  finished:  5300 / 6920\n",
      "Epoch:  2 / 5  finished:  5400 / 6920\n",
      "Epoch:  2 / 5  finished:  5500 / 6920\n",
      "Epoch:  2 / 5  finished:  5600 / 6920\n",
      "Epoch:  2 / 5  finished:  5700 / 6920\n",
      "Epoch:  2 / 5  finished:  5800 / 6920\n",
      "Epoch:  2 / 5  finished:  5900 / 6920\n",
      "Epoch:  2 / 5  finished:  6000 / 6920\n",
      "Epoch:  2 / 5  finished:  6100 / 6920\n",
      "Epoch:  2 / 5  finished:  6200 / 6920\n",
      "Epoch:  2 / 5  finished:  6300 / 6920\n",
      "Epoch:  2 / 5  finished:  6400 / 6920\n",
      "Epoch:  2 / 5  finished:  6500 / 6920\n",
      "Epoch:  2 / 5  finished:  6600 / 6920\n",
      "Epoch:  2 / 5  finished:  6700 / 6920\n",
      "Epoch:  2 / 5  finished:  6800 / 6920\n",
      "Epoch:  2 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 2 / 5  train accuracy:  0.8088150289017341 epoch cost:  3096.3742098659277  epoch time:  0:07:56.996989\n",
      "Epoch:  3 / 5  finished:  100 / 6920\n",
      "Epoch:  3 / 5  finished:  200 / 6920\n",
      "Epoch:  3 / 5  finished:  300 / 6920\n",
      "Epoch:  3 / 5  finished:  400 / 6920\n",
      "Epoch:  3 / 5  finished:  500 / 6920\n",
      "Epoch:  3 / 5  finished:  600 / 6920\n",
      "Epoch:  3 / 5  finished:  700 / 6920\n",
      "Epoch:  3 / 5  finished:  800 / 6920\n",
      "Epoch:  3 / 5  finished:  900 / 6920\n",
      "Epoch:  3 / 5  finished:  1000 / 6920\n",
      "Epoch:  3 / 5  finished:  1100 / 6920\n",
      "Epoch:  3 / 5  finished:  1200 / 6920\n",
      "Epoch:  3 / 5  finished:  1300 / 6920\n",
      "Epoch:  3 / 5  finished:  1400 / 6920\n",
      "Epoch:  3 / 5  finished:  1500 / 6920\n",
      "Epoch:  3 / 5  finished:  1600 / 6920\n",
      "Epoch:  3 / 5  finished:  1700 / 6920\n",
      "Epoch:  3 / 5  finished:  1800 / 6920\n",
      "Epoch:  3 / 5  finished:  1900 / 6920\n",
      "Epoch:  3 / 5  finished:  2000 / 6920\n",
      "Epoch:  3 / 5  finished:  2100 / 6920\n",
      "Epoch:  3 / 5  finished:  2200 / 6920\n",
      "Epoch:  3 / 5  finished:  2300 / 6920\n",
      "Epoch:  3 / 5  finished:  2400 / 6920\n",
      "Epoch:  3 / 5  finished:  2500 / 6920\n",
      "Epoch:  3 / 5  finished:  2600 / 6920\n",
      "Epoch:  3 / 5  finished:  2700 / 6920\n",
      "Epoch:  3 / 5  finished:  2800 / 6920\n",
      "Epoch:  3 / 5  finished:  2900 / 6920\n",
      "Epoch:  3 / 5  finished:  3000 / 6920\n",
      "Epoch:  3 / 5  finished:  3100 / 6920\n",
      "Epoch:  3 / 5  finished:  3200 / 6920\n",
      "Epoch:  3 / 5  finished:  3300 / 6920\n",
      "Epoch:  3 / 5  finished:  3400 / 6920\n",
      "Epoch:  3 / 5  finished:  3500 / 6920\n",
      "Epoch:  3 / 5  finished:  3600 / 6920\n",
      "Epoch:  3 / 5  finished:  3700 / 6920\n",
      "Epoch:  3 / 5  finished:  3800 / 6920\n",
      "Epoch:  3 / 5  finished:  3900 / 6920\n",
      "Epoch:  3 / 5  finished:  4000 / 6920\n",
      "Epoch:  3 / 5  finished:  4100 / 6920\n",
      "Epoch:  3 / 5  finished:  4200 / 6920\n",
      "Epoch:  3 / 5  finished:  4300 / 6920\n",
      "Epoch:  3 / 5  finished:  4400 / 6920\n",
      "Epoch:  3 / 5  finished:  4500 / 6920\n",
      "Epoch:  3 / 5  finished:  4600 / 6920\n",
      "Epoch:  3 / 5  finished:  4700 / 6920\n",
      "Epoch:  3 / 5  finished:  4800 / 6920\n",
      "Epoch:  3 / 5  finished:  4900 / 6920\n",
      "Epoch:  3 / 5  finished:  5000 / 6920\n",
      "Epoch:  3 / 5  finished:  5100 / 6920\n",
      "Epoch:  3 / 5  finished:  5200 / 6920\n",
      "Epoch:  3 / 5  finished:  5300 / 6920\n",
      "Epoch:  3 / 5  finished:  5400 / 6920\n",
      "Epoch:  3 / 5  finished:  5500 / 6920\n",
      "Epoch:  3 / 5  finished:  5600 / 6920\n",
      "Epoch:  3 / 5  finished:  5700 / 6920\n",
      "Epoch:  3 / 5  finished:  5800 / 6920\n",
      "Epoch:  3 / 5  finished:  5900 / 6920\n",
      "Epoch:  3 / 5  finished:  6000 / 6920\n",
      "Epoch:  3 / 5  finished:  6100 / 6920\n",
      "Epoch:  3 / 5  finished:  6200 / 6920\n",
      "Epoch:  3 / 5  finished:  6300 / 6920\n",
      "Epoch:  3 / 5  finished:  6400 / 6920\n",
      "Epoch:  3 / 5  finished:  6500 / 6920\n",
      "Epoch:  3 / 5  finished:  6600 / 6920\n",
      "Epoch:  3 / 5  finished:  6700 / 6920\n",
      "Epoch:  3 / 5  finished:  6800 / 6920\n",
      "Epoch:  3 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 3 / 5  train accuracy:  0.825 epoch cost:  2933.880561083555  epoch time:  0:08:00.924883\n",
      "Epoch:  4 / 5  finished:  100 / 6920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4 / 5  finished:  200 / 6920\n",
      "Epoch:  4 / 5  finished:  300 / 6920\n",
      "Epoch:  4 / 5  finished:  400 / 6920\n",
      "Epoch:  4 / 5  finished:  500 / 6920\n",
      "Epoch:  4 / 5  finished:  600 / 6920\n",
      "Epoch:  4 / 5  finished:  700 / 6920\n",
      "Epoch:  4 / 5  finished:  800 / 6920\n",
      "Epoch:  4 / 5  finished:  900 / 6920\n",
      "Epoch:  4 / 5  finished:  1000 / 6920\n",
      "Epoch:  4 / 5  finished:  1100 / 6920\n",
      "Epoch:  4 / 5  finished:  1200 / 6920\n",
      "Epoch:  4 / 5  finished:  1300 / 6920\n",
      "Epoch:  4 / 5  finished:  1400 / 6920\n",
      "Epoch:  4 / 5  finished:  1500 / 6920\n",
      "Epoch:  4 / 5  finished:  1600 / 6920\n",
      "Epoch:  4 / 5  finished:  1700 / 6920\n",
      "Epoch:  4 / 5  finished:  1800 / 6920\n",
      "Epoch:  4 / 5  finished:  1900 / 6920\n",
      "Epoch:  4 / 5  finished:  2000 / 6920\n",
      "Epoch:  4 / 5  finished:  2100 / 6920\n",
      "Epoch:  4 / 5  finished:  2200 / 6920\n",
      "Epoch:  4 / 5  finished:  2300 / 6920\n",
      "Epoch:  4 / 5  finished:  2400 / 6920\n",
      "Epoch:  4 / 5  finished:  2500 / 6920\n",
      "Epoch:  4 / 5  finished:  2600 / 6920\n",
      "Epoch:  4 / 5  finished:  2700 / 6920\n",
      "Epoch:  4 / 5  finished:  2800 / 6920\n",
      "Epoch:  4 / 5  finished:  2900 / 6920\n",
      "Epoch:  4 / 5  finished:  3000 / 6920\n",
      "Epoch:  4 / 5  finished:  3100 / 6920\n",
      "Epoch:  4 / 5  finished:  3200 / 6920\n",
      "Epoch:  4 / 5  finished:  3300 / 6920\n",
      "Epoch:  4 / 5  finished:  3400 / 6920\n",
      "Epoch:  4 / 5  finished:  3500 / 6920\n",
      "Epoch:  4 / 5  finished:  3600 / 6920\n",
      "Epoch:  4 / 5  finished:  3700 / 6920\n",
      "Epoch:  4 / 5  finished:  3800 / 6920\n",
      "Epoch:  4 / 5  finished:  3900 / 6920\n",
      "Epoch:  4 / 5  finished:  4000 / 6920\n",
      "Epoch:  4 / 5  finished:  4100 / 6920\n",
      "Epoch:  4 / 5  finished:  4200 / 6920\n",
      "Epoch:  4 / 5  finished:  4300 / 6920\n",
      "Epoch:  4 / 5  finished:  4400 / 6920\n",
      "Epoch:  4 / 5  finished:  4500 / 6920\n",
      "Epoch:  4 / 5  finished:  4600 / 6920\n",
      "Epoch:  4 / 5  finished:  4700 / 6920\n",
      "Epoch:  4 / 5  finished:  4800 / 6920\n",
      "Epoch:  4 / 5  finished:  4900 / 6920\n",
      "Epoch:  4 / 5  finished:  5000 / 6920\n",
      "Epoch:  4 / 5  finished:  5100 / 6920\n",
      "Epoch:  4 / 5  finished:  5200 / 6920\n",
      "Epoch:  4 / 5  finished:  5300 / 6920\n",
      "Epoch:  4 / 5  finished:  5400 / 6920\n",
      "Epoch:  4 / 5  finished:  5500 / 6920\n",
      "Epoch:  4 / 5  finished:  5600 / 6920\n",
      "Epoch:  4 / 5  finished:  5700 / 6920\n",
      "Epoch:  4 / 5  finished:  5800 / 6920\n",
      "Epoch:  4 / 5  finished:  5900 / 6920\n",
      "Epoch:  4 / 5  finished:  6000 / 6920\n",
      "Epoch:  4 / 5  finished:  6100 / 6920\n",
      "Epoch:  4 / 5  finished:  6200 / 6920\n",
      "Epoch:  4 / 5  finished:  6300 / 6920\n",
      "Epoch:  4 / 5  finished:  6400 / 6920\n",
      "Epoch:  4 / 5  finished:  6500 / 6920\n",
      "Epoch:  4 / 5  finished:  6600 / 6920\n",
      "Epoch:  4 / 5  finished:  6700 / 6920\n",
      "Epoch:  4 / 5  finished:  6800 / 6920\n",
      "Epoch:  4 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 4 / 5  train accuracy:  0.8472543352601156 epoch cost:  2795.88372271508  epoch time:  0:08:39.511706\n",
      "Epoch:  5 / 5  finished:  100 / 6920\n",
      "Epoch:  5 / 5  finished:  200 / 6920\n",
      "Epoch:  5 / 5  finished:  300 / 6920\n",
      "Epoch:  5 / 5  finished:  400 / 6920\n",
      "Epoch:  5 / 5  finished:  500 / 6920\n",
      "Epoch:  5 / 5  finished:  600 / 6920\n",
      "Epoch:  5 / 5  finished:  700 / 6920\n",
      "Epoch:  5 / 5  finished:  800 / 6920\n",
      "Epoch:  5 / 5  finished:  900 / 6920\n",
      "Epoch:  5 / 5  finished:  1000 / 6920\n",
      "Epoch:  5 / 5  finished:  1100 / 6920\n",
      "Epoch:  5 / 5  finished:  1200 / 6920\n",
      "Epoch:  5 / 5  finished:  1300 / 6920\n",
      "Epoch:  5 / 5  finished:  1400 / 6920\n",
      "Epoch:  5 / 5  finished:  1500 / 6920\n",
      "Epoch:  5 / 5  finished:  1600 / 6920\n",
      "Epoch:  5 / 5  finished:  1700 / 6920\n",
      "Epoch:  5 / 5  finished:  1800 / 6920\n",
      "Epoch:  5 / 5  finished:  1900 / 6920\n",
      "Epoch:  5 / 5  finished:  2000 / 6920\n",
      "Epoch:  5 / 5  finished:  2100 / 6920\n",
      "Epoch:  5 / 5  finished:  2200 / 6920\n",
      "Epoch:  5 / 5  finished:  2300 / 6920\n",
      "Epoch:  5 / 5  finished:  2400 / 6920\n",
      "Epoch:  5 / 5  finished:  2500 / 6920\n",
      "Epoch:  5 / 5  finished:  2600 / 6920\n",
      "Epoch:  5 / 5  finished:  2700 / 6920\n",
      "Epoch:  5 / 5  finished:  2800 / 6920\n",
      "Epoch:  5 / 5  finished:  2900 / 6920\n",
      "Epoch:  5 / 5  finished:  3000 / 6920\n",
      "Epoch:  5 / 5  finished:  3100 / 6920\n",
      "Epoch:  5 / 5  finished:  3200 / 6920\n",
      "Epoch:  5 / 5  finished:  3300 / 6920\n",
      "Epoch:  5 / 5  finished:  3400 / 6920\n",
      "Epoch:  5 / 5  finished:  3500 / 6920\n",
      "Epoch:  5 / 5  finished:  3600 / 6920\n",
      "Epoch:  5 / 5  finished:  3700 / 6920\n",
      "Epoch:  5 / 5  finished:  3800 / 6920\n",
      "Epoch:  5 / 5  finished:  3900 / 6920\n",
      "Epoch:  5 / 5  finished:  4000 / 6920\n",
      "Epoch:  5 / 5  finished:  4100 / 6920\n",
      "Epoch:  5 / 5  finished:  4200 / 6920\n",
      "Epoch:  5 / 5  finished:  4300 / 6920\n",
      "Epoch:  5 / 5  finished:  4400 / 6920\n",
      "Epoch:  5 / 5  finished:  4500 / 6920\n",
      "Epoch:  5 / 5  finished:  4600 / 6920\n",
      "Epoch:  5 / 5  finished:  4700 / 6920\n",
      "Epoch:  5 / 5  finished:  4800 / 6920\n",
      "Epoch:  5 / 5  finished:  4900 / 6920\n",
      "Epoch:  5 / 5  finished:  5000 / 6920\n",
      "Epoch:  5 / 5  finished:  5100 / 6920\n",
      "Epoch:  5 / 5  finished:  5200 / 6920\n",
      "Epoch:  5 / 5  finished:  5300 / 6920\n",
      "Epoch:  5 / 5  finished:  5400 / 6920\n",
      "Epoch:  5 / 5  finished:  5500 / 6920\n",
      "Epoch:  5 / 5  finished:  5600 / 6920\n",
      "Epoch:  5 / 5  finished:  5700 / 6920\n",
      "Epoch:  5 / 5  finished:  5800 / 6920\n",
      "Epoch:  5 / 5  finished:  5900 / 6920\n",
      "Epoch:  5 / 5  finished:  6000 / 6920\n",
      "Epoch:  5 / 5  finished:  6100 / 6920\n",
      "Epoch:  5 / 5  finished:  6200 / 6920\n",
      "Epoch:  5 / 5  finished:  6300 / 6920\n",
      "Epoch:  5 / 5  finished:  6400 / 6920\n",
      "Epoch:  5 / 5  finished:  6500 / 6920\n",
      "Epoch:  5 / 5  finished:  6600 / 6920\n",
      "Epoch:  5 / 5  finished:  6700 / 6920\n",
      "Epoch:  5 / 5  finished:  6800 / 6920\n",
      "Epoch:  5 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 5 / 5  train accuracy:  0.8486994219653179 epoch cost:  2743.7157348170877  epoch time:  0:09:13.272094\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = TNNI(V,D,K)\n",
    "model.fit(train_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.827567270730368"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test_trees)\n",
    "# we get a good test accuracy\n",
    "# probably should have increased the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next lets try again recusrsively\n",
    "# we only need to change the call method to act recusrsively\n",
    "# performing postorder traversal\n",
    "\n",
    "# now we are ready to make our model\n",
    "# we will implement two models\n",
    "# one that takes advantage of the fact that each node knows its parent\n",
    "# thus we get an iterative solution\n",
    "# the other is the recurssive solution\n",
    "class TNNR(Model): # TNN Recursive\n",
    "    def __init__(self,V,D,K):\n",
    "        super(TNNR, self).__init__()\n",
    "        self.V = V\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "        We = np.random.randn(V, D)/np.sqrt(V+D)\n",
    "        W1 = np.random.randn(D, D)/np.sqrt(D+D)\n",
    "        W2 = np.random.randn(D, D)/np.sqrt(D+D)\n",
    "        Wo = np.random.randn(D, K)/np.sqrt(D+K)\n",
    "        bo = np.zeros(K)\n",
    "        bh = np.zeros(D)\n",
    "\n",
    "        self.We = tf.Variable(We.astype(np.float32))\n",
    "        self.W1 = tf.Variable(W1.astype(np.float32))\n",
    "        self.W2 = tf.Variable(W2.astype(np.float32))\n",
    "        self.Wo = tf.Variable(Wo.astype(np.float32))\n",
    "        self.bo = tf.Variable(bo.astype(np.float32))\n",
    "        self.bh = tf.Variable(bh.astype(np.float32))\n",
    "    \n",
    "    # takes in lost of roots\n",
    "    # return accuracy\n",
    "    def score(self,trees):\n",
    "        correct = 0\n",
    "        for tree in trees:\n",
    "            T = []\n",
    "            P = []\n",
    "            self(tree,T,P)\n",
    "            if T[-1]==np.argmax(P[-1]):\n",
    "                correct+=1\n",
    "        return correct/len(trees)\n",
    "\n",
    "    # takes root and calculate fills logits and targets\n",
    "    # this is simply postorder traversal\n",
    "    def call(self,root,targets,logits):\n",
    "        if root.word != '': # thus a leaf\n",
    "            h = tf.nn.embedding_lookup(self.We,word2idx[root.word])\n",
    "        else:\n",
    "            left_h = self(root.left,targets,logits)\n",
    "            right_h = self(root.right,targets,logits)\n",
    "            left_h = tf.experimental.numpy.atleast_2d(left_h)\n",
    "            right_h = tf.experimental.numpy.atleast_2d(right_h)\n",
    "            h = self.f(tf.matmul(left_h,self.W1) + tf.matmul(right_h,self.W2) + self.bh)\n",
    "        \n",
    "        h = tf.experimental.numpy.atleast_2d(h)\n",
    "        # get prediction\n",
    "        logit = tf.matmul(h,self.Wo) + self.bo\n",
    "        # append to logits and targets\n",
    "        logits.append(tf.squeeze(logit))\n",
    "        targets.append(root.label)\n",
    "        # return h to parent\n",
    "        return h                        \n",
    "\n",
    "    def loss(self,T,Y,reg):\n",
    "        \n",
    "        if self.train_inner_nodes == True:\n",
    "            # filter out nodes where sentiment is neural\n",
    "            T = tf.constant(T,dtype='int32')\n",
    "            valid = tf.where(T != -1)\n",
    "            T = tf.gather(T,valid)\n",
    "            Y = tf.gather(Y,valid)\n",
    "        else: # we only care about root\n",
    "            T = T[-1]\n",
    "            Y = Y[-1]\n",
    "        cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(T,Y))\n",
    "        l2 = reg*sum(tf.nn.l2_loss(w) for w in self.weights)\n",
    "        cost += l2\n",
    "        return cost\n",
    "    \n",
    "    def postorder(self,root,leaves):\n",
    "        if root.word != '':\n",
    "            leaves.append(root)\n",
    "            return\n",
    "        self.postorder(root.left,leaves)\n",
    "        self.postorder(root.right,leaves)\n",
    "    \n",
    "    # training\n",
    "    def fit(self,trees,epochs = 5,lr=1e-3,reg = 1e-3,f = tf.nn.tanh,train_inner_nodes=True):\n",
    "        opt = Adam(lr) \n",
    "        self.train_inner_nodes = train_inner_nodes\n",
    "        self.f = f\n",
    "        costs = []\n",
    "        for epoch in range(epochs):\n",
    "            t0=datetime.now()\n",
    "            epoch_cost = 0\n",
    "            correct = 0\n",
    "            trees = shuffle(trees)\n",
    "            for i,tree in enumerate(trees):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    T = []\n",
    "                    logits = []\n",
    "                    self(tree,T,logits)\n",
    "                    cost = self.loss(T,logits,reg)\n",
    "                grads = tape.gradient(cost, self.trainable_weights)\n",
    "                opt.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "                # keep track of epoch cost\n",
    "                # and number of correct predictions\n",
    "                epoch_cost += cost.numpy()\n",
    "                # we only care about root label for accuracy\n",
    "                correct += T[-1] == np.argmax(logits[-1])\n",
    "                if (i+1)%100 == 0:\n",
    "                    print('Epoch: ',epoch+1,'/',epochs,' finished: ',i+1,'/',len(trees))\n",
    "            print('Finished Epoch:',epoch+1,'/',epochs,' train accuracy: ',correct/len(trees),'epoch cost: ',epoch_cost,' epoch time: ',datetime.now()-t0)\n",
    "            costs.append(epoch_cost)\n",
    "        plt.plot(costs)\n",
    "        plt.title('cost')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 5  finished:  100 / 6920\n",
      "Epoch:  1 / 5  finished:  200 / 6920\n",
      "Epoch:  1 / 5  finished:  300 / 6920\n",
      "Epoch:  1 / 5  finished:  400 / 6920\n",
      "Epoch:  1 / 5  finished:  500 / 6920\n",
      "Epoch:  1 / 5  finished:  600 / 6920\n",
      "Epoch:  1 / 5  finished:  700 / 6920\n",
      "Epoch:  1 / 5  finished:  800 / 6920\n",
      "Epoch:  1 / 5  finished:  900 / 6920\n",
      "Epoch:  1 / 5  finished:  1000 / 6920\n",
      "Epoch:  1 / 5  finished:  1100 / 6920\n",
      "Epoch:  1 / 5  finished:  1200 / 6920\n",
      "Epoch:  1 / 5  finished:  1300 / 6920\n",
      "Epoch:  1 / 5  finished:  1400 / 6920\n",
      "Epoch:  1 / 5  finished:  1500 / 6920\n",
      "Epoch:  1 / 5  finished:  1600 / 6920\n",
      "Epoch:  1 / 5  finished:  1700 / 6920\n",
      "Epoch:  1 / 5  finished:  1800 / 6920\n",
      "Epoch:  1 / 5  finished:  1900 / 6920\n",
      "Epoch:  1 / 5  finished:  2000 / 6920\n",
      "Epoch:  1 / 5  finished:  2100 / 6920\n",
      "Epoch:  1 / 5  finished:  2200 / 6920\n",
      "Epoch:  1 / 5  finished:  2300 / 6920\n",
      "Epoch:  1 / 5  finished:  2400 / 6920\n",
      "Epoch:  1 / 5  finished:  2500 / 6920\n",
      "Epoch:  1 / 5  finished:  2600 / 6920\n",
      "Epoch:  1 / 5  finished:  2700 / 6920\n",
      "Epoch:  1 / 5  finished:  2800 / 6920\n",
      "Epoch:  1 / 5  finished:  2900 / 6920\n",
      "Epoch:  1 / 5  finished:  3000 / 6920\n",
      "Epoch:  1 / 5  finished:  3100 / 6920\n",
      "Epoch:  1 / 5  finished:  3200 / 6920\n",
      "Epoch:  1 / 5  finished:  3300 / 6920\n",
      "Epoch:  1 / 5  finished:  3400 / 6920\n",
      "Epoch:  1 / 5  finished:  3500 / 6920\n",
      "Epoch:  1 / 5  finished:  3600 / 6920\n",
      "Epoch:  1 / 5  finished:  3700 / 6920\n",
      "Epoch:  1 / 5  finished:  3800 / 6920\n",
      "Epoch:  1 / 5  finished:  3900 / 6920\n",
      "Epoch:  1 / 5  finished:  4000 / 6920\n",
      "Epoch:  1 / 5  finished:  4100 / 6920\n",
      "Epoch:  1 / 5  finished:  4200 / 6920\n",
      "Epoch:  1 / 5  finished:  4300 / 6920\n",
      "Epoch:  1 / 5  finished:  4400 / 6920\n",
      "Epoch:  1 / 5  finished:  4500 / 6920\n",
      "Epoch:  1 / 5  finished:  4600 / 6920\n",
      "Epoch:  1 / 5  finished:  4700 / 6920\n",
      "Epoch:  1 / 5  finished:  4800 / 6920\n",
      "Epoch:  1 / 5  finished:  4900 / 6920\n",
      "Epoch:  1 / 5  finished:  5000 / 6920\n",
      "Epoch:  1 / 5  finished:  5100 / 6920\n",
      "Epoch:  1 / 5  finished:  5200 / 6920\n",
      "Epoch:  1 / 5  finished:  5300 / 6920\n",
      "Epoch:  1 / 5  finished:  5400 / 6920\n",
      "Epoch:  1 / 5  finished:  5500 / 6920\n",
      "Epoch:  1 / 5  finished:  5600 / 6920\n",
      "Epoch:  1 / 5  finished:  5700 / 6920\n",
      "Epoch:  1 / 5  finished:  5800 / 6920\n",
      "Epoch:  1 / 5  finished:  5900 / 6920\n",
      "Epoch:  1 / 5  finished:  6000 / 6920\n",
      "Epoch:  1 / 5  finished:  6100 / 6920\n",
      "Epoch:  1 / 5  finished:  6200 / 6920\n",
      "Epoch:  1 / 5  finished:  6300 / 6920\n",
      "Epoch:  1 / 5  finished:  6400 / 6920\n",
      "Epoch:  1 / 5  finished:  6500 / 6920\n",
      "Epoch:  1 / 5  finished:  6600 / 6920\n",
      "Epoch:  1 / 5  finished:  6700 / 6920\n",
      "Epoch:  1 / 5  finished:  6800 / 6920\n",
      "Epoch:  1 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 1 / 5  train accuracy:  0.7002890173410404 epoch cost:  3841.14413420856  epoch time:  0:08:43.398867\n",
      "Epoch:  2 / 5  finished:  100 / 6920\n",
      "Epoch:  2 / 5  finished:  200 / 6920\n",
      "Epoch:  2 / 5  finished:  300 / 6920\n",
      "Epoch:  2 / 5  finished:  400 / 6920\n",
      "Epoch:  2 / 5  finished:  500 / 6920\n",
      "Epoch:  2 / 5  finished:  600 / 6920\n",
      "Epoch:  2 / 5  finished:  700 / 6920\n",
      "Epoch:  2 / 5  finished:  800 / 6920\n",
      "Epoch:  2 / 5  finished:  900 / 6920\n",
      "Epoch:  2 / 5  finished:  1000 / 6920\n",
      "Epoch:  2 / 5  finished:  1100 / 6920\n",
      "Epoch:  2 / 5  finished:  1200 / 6920\n",
      "Epoch:  2 / 5  finished:  1300 / 6920\n",
      "Epoch:  2 / 5  finished:  1400 / 6920\n",
      "Epoch:  2 / 5  finished:  1500 / 6920\n",
      "Epoch:  2 / 5  finished:  1600 / 6920\n",
      "Epoch:  2 / 5  finished:  1700 / 6920\n",
      "Epoch:  2 / 5  finished:  1800 / 6920\n",
      "Epoch:  2 / 5  finished:  1900 / 6920\n",
      "Epoch:  2 / 5  finished:  2000 / 6920\n",
      "Epoch:  2 / 5  finished:  2100 / 6920\n",
      "Epoch:  2 / 5  finished:  2200 / 6920\n",
      "Epoch:  2 / 5  finished:  2300 / 6920\n",
      "Epoch:  2 / 5  finished:  2400 / 6920\n",
      "Epoch:  2 / 5  finished:  2500 / 6920\n",
      "Epoch:  2 / 5  finished:  2600 / 6920\n",
      "Epoch:  2 / 5  finished:  2700 / 6920\n",
      "Epoch:  2 / 5  finished:  2800 / 6920\n",
      "Epoch:  2 / 5  finished:  2900 / 6920\n",
      "Epoch:  2 / 5  finished:  3000 / 6920\n",
      "Epoch:  2 / 5  finished:  3100 / 6920\n",
      "Epoch:  2 / 5  finished:  3200 / 6920\n",
      "Epoch:  2 / 5  finished:  3300 / 6920\n",
      "Epoch:  2 / 5  finished:  3400 / 6920\n",
      "Epoch:  2 / 5  finished:  3500 / 6920\n",
      "Epoch:  2 / 5  finished:  3600 / 6920\n",
      "Epoch:  2 / 5  finished:  3700 / 6920\n",
      "Epoch:  2 / 5  finished:  3800 / 6920\n",
      "Epoch:  2 / 5  finished:  3900 / 6920\n",
      "Epoch:  2 / 5  finished:  4000 / 6920\n",
      "Epoch:  2 / 5  finished:  4100 / 6920\n",
      "Epoch:  2 / 5  finished:  4200 / 6920\n",
      "Epoch:  2 / 5  finished:  4300 / 6920\n",
      "Epoch:  2 / 5  finished:  4400 / 6920\n",
      "Epoch:  2 / 5  finished:  4500 / 6920\n",
      "Epoch:  2 / 5  finished:  4600 / 6920\n",
      "Epoch:  2 / 5  finished:  4700 / 6920\n",
      "Epoch:  2 / 5  finished:  4800 / 6920\n",
      "Epoch:  2 / 5  finished:  4900 / 6920\n",
      "Epoch:  2 / 5  finished:  5000 / 6920\n",
      "Epoch:  2 / 5  finished:  5100 / 6920\n",
      "Epoch:  2 / 5  finished:  5200 / 6920\n",
      "Epoch:  2 / 5  finished:  5300 / 6920\n",
      "Epoch:  2 / 5  finished:  5400 / 6920\n",
      "Epoch:  2 / 5  finished:  5500 / 6920\n",
      "Epoch:  2 / 5  finished:  5600 / 6920\n",
      "Epoch:  2 / 5  finished:  5700 / 6920\n",
      "Epoch:  2 / 5  finished:  5800 / 6920\n",
      "Epoch:  2 / 5  finished:  5900 / 6920\n",
      "Epoch:  2 / 5  finished:  6000 / 6920\n",
      "Epoch:  2 / 5  finished:  6100 / 6920\n",
      "Epoch:  2 / 5  finished:  6200 / 6920\n",
      "Epoch:  2 / 5  finished:  6300 / 6920\n",
      "Epoch:  2 / 5  finished:  6400 / 6920\n",
      "Epoch:  2 / 5  finished:  6500 / 6920\n",
      "Epoch:  2 / 5  finished:  6600 / 6920\n",
      "Epoch:  2 / 5  finished:  6700 / 6920\n",
      "Epoch:  2 / 5  finished:  6800 / 6920\n",
      "Epoch:  2 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 2 / 5  train accuracy:  0.8104046242774566 epoch cost:  3073.7230316102505  epoch time:  0:07:39.353026\n",
      "Epoch:  3 / 5  finished:  100 / 6920\n",
      "Epoch:  3 / 5  finished:  200 / 6920\n",
      "Epoch:  3 / 5  finished:  300 / 6920\n",
      "Epoch:  3 / 5  finished:  400 / 6920\n",
      "Epoch:  3 / 5  finished:  500 / 6920\n",
      "Epoch:  3 / 5  finished:  600 / 6920\n",
      "Epoch:  3 / 5  finished:  700 / 6920\n",
      "Epoch:  3 / 5  finished:  800 / 6920\n",
      "Epoch:  3 / 5  finished:  900 / 6920\n",
      "Epoch:  3 / 5  finished:  1000 / 6920\n",
      "Epoch:  3 / 5  finished:  1100 / 6920\n",
      "Epoch:  3 / 5  finished:  1200 / 6920\n",
      "Epoch:  3 / 5  finished:  1300 / 6920\n",
      "Epoch:  3 / 5  finished:  1400 / 6920\n",
      "Epoch:  3 / 5  finished:  1500 / 6920\n",
      "Epoch:  3 / 5  finished:  1600 / 6920\n",
      "Epoch:  3 / 5  finished:  1700 / 6920\n",
      "Epoch:  3 / 5  finished:  1800 / 6920\n",
      "Epoch:  3 / 5  finished:  1900 / 6920\n",
      "Epoch:  3 / 5  finished:  2000 / 6920\n",
      "Epoch:  3 / 5  finished:  2100 / 6920\n",
      "Epoch:  3 / 5  finished:  2200 / 6920\n",
      "Epoch:  3 / 5  finished:  2300 / 6920\n",
      "Epoch:  3 / 5  finished:  2400 / 6920\n",
      "Epoch:  3 / 5  finished:  2500 / 6920\n",
      "Epoch:  3 / 5  finished:  2600 / 6920\n",
      "Epoch:  3 / 5  finished:  2700 / 6920\n",
      "Epoch:  3 / 5  finished:  2800 / 6920\n",
      "Epoch:  3 / 5  finished:  2900 / 6920\n",
      "Epoch:  3 / 5  finished:  3000 / 6920\n",
      "Epoch:  3 / 5  finished:  3100 / 6920\n",
      "Epoch:  3 / 5  finished:  3200 / 6920\n",
      "Epoch:  3 / 5  finished:  3300 / 6920\n",
      "Epoch:  3 / 5  finished:  3400 / 6920\n",
      "Epoch:  3 / 5  finished:  3500 / 6920\n",
      "Epoch:  3 / 5  finished:  3600 / 6920\n",
      "Epoch:  3 / 5  finished:  3700 / 6920\n",
      "Epoch:  3 / 5  finished:  3800 / 6920\n",
      "Epoch:  3 / 5  finished:  3900 / 6920\n",
      "Epoch:  3 / 5  finished:  4000 / 6920\n",
      "Epoch:  3 / 5  finished:  4100 / 6920\n",
      "Epoch:  3 / 5  finished:  4200 / 6920\n",
      "Epoch:  3 / 5  finished:  4300 / 6920\n",
      "Epoch:  3 / 5  finished:  4400 / 6920\n",
      "Epoch:  3 / 5  finished:  4500 / 6920\n",
      "Epoch:  3 / 5  finished:  4600 / 6920\n",
      "Epoch:  3 / 5  finished:  4700 / 6920\n",
      "Epoch:  3 / 5  finished:  4800 / 6920\n",
      "Epoch:  3 / 5  finished:  4900 / 6920\n",
      "Epoch:  3 / 5  finished:  5000 / 6920\n",
      "Epoch:  3 / 5  finished:  5100 / 6920\n",
      "Epoch:  3 / 5  finished:  5200 / 6920\n",
      "Epoch:  3 / 5  finished:  5300 / 6920\n",
      "Epoch:  3 / 5  finished:  5400 / 6920\n",
      "Epoch:  3 / 5  finished:  5500 / 6920\n",
      "Epoch:  3 / 5  finished:  5600 / 6920\n",
      "Epoch:  3 / 5  finished:  5700 / 6920\n",
      "Epoch:  3 / 5  finished:  5800 / 6920\n",
      "Epoch:  3 / 5  finished:  5900 / 6920\n",
      "Epoch:  3 / 5  finished:  6000 / 6920\n",
      "Epoch:  3 / 5  finished:  6100 / 6920\n",
      "Epoch:  3 / 5  finished:  6200 / 6920\n",
      "Epoch:  3 / 5  finished:  6300 / 6920\n",
      "Epoch:  3 / 5  finished:  6400 / 6920\n",
      "Epoch:  3 / 5  finished:  6500 / 6920\n",
      "Epoch:  3 / 5  finished:  6600 / 6920\n",
      "Epoch:  3 / 5  finished:  6700 / 6920\n",
      "Epoch:  3 / 5  finished:  6800 / 6920\n",
      "Epoch:  3 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 3 / 5  train accuracy:  0.8367052023121387 epoch cost:  2848.6641719192266  epoch time:  0:07:42.131529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4 / 5  finished:  100 / 6920\n",
      "Epoch:  4 / 5  finished:  200 / 6920\n",
      "Epoch:  4 / 5  finished:  300 / 6920\n",
      "Epoch:  4 / 5  finished:  400 / 6920\n",
      "Epoch:  4 / 5  finished:  500 / 6920\n",
      "Epoch:  4 / 5  finished:  600 / 6920\n",
      "Epoch:  4 / 5  finished:  700 / 6920\n",
      "Epoch:  4 / 5  finished:  800 / 6920\n",
      "Epoch:  4 / 5  finished:  900 / 6920\n",
      "Epoch:  4 / 5  finished:  1000 / 6920\n",
      "Epoch:  4 / 5  finished:  1100 / 6920\n",
      "Epoch:  4 / 5  finished:  1200 / 6920\n",
      "Epoch:  4 / 5  finished:  1300 / 6920\n",
      "Epoch:  4 / 5  finished:  1400 / 6920\n",
      "Epoch:  4 / 5  finished:  1500 / 6920\n",
      "Epoch:  4 / 5  finished:  1600 / 6920\n",
      "Epoch:  4 / 5  finished:  1700 / 6920\n",
      "Epoch:  4 / 5  finished:  1800 / 6920\n",
      "Epoch:  4 / 5  finished:  1900 / 6920\n",
      "Epoch:  4 / 5  finished:  2000 / 6920\n",
      "Epoch:  4 / 5  finished:  2100 / 6920\n",
      "Epoch:  4 / 5  finished:  2200 / 6920\n",
      "Epoch:  4 / 5  finished:  2300 / 6920\n",
      "Epoch:  4 / 5  finished:  2400 / 6920\n",
      "Epoch:  4 / 5  finished:  2500 / 6920\n",
      "Epoch:  4 / 5  finished:  2600 / 6920\n",
      "Epoch:  4 / 5  finished:  2700 / 6920\n",
      "Epoch:  4 / 5  finished:  2800 / 6920\n",
      "Epoch:  4 / 5  finished:  2900 / 6920\n",
      "Epoch:  4 / 5  finished:  3000 / 6920\n",
      "Epoch:  4 / 5  finished:  3100 / 6920\n",
      "Epoch:  4 / 5  finished:  3200 / 6920\n",
      "Epoch:  4 / 5  finished:  3300 / 6920\n",
      "Epoch:  4 / 5  finished:  3400 / 6920\n",
      "Epoch:  4 / 5  finished:  3500 / 6920\n",
      "Epoch:  4 / 5  finished:  3600 / 6920\n",
      "Epoch:  4 / 5  finished:  3700 / 6920\n",
      "Epoch:  4 / 5  finished:  3800 / 6920\n",
      "Epoch:  4 / 5  finished:  3900 / 6920\n",
      "Epoch:  4 / 5  finished:  4000 / 6920\n",
      "Epoch:  4 / 5  finished:  4100 / 6920\n",
      "Epoch:  4 / 5  finished:  4200 / 6920\n",
      "Epoch:  4 / 5  finished:  4300 / 6920\n",
      "Epoch:  4 / 5  finished:  4400 / 6920\n",
      "Epoch:  4 / 5  finished:  4500 / 6920\n",
      "Epoch:  4 / 5  finished:  4600 / 6920\n",
      "Epoch:  4 / 5  finished:  4700 / 6920\n",
      "Epoch:  4 / 5  finished:  4800 / 6920\n",
      "Epoch:  4 / 5  finished:  4900 / 6920\n",
      "Epoch:  4 / 5  finished:  5000 / 6920\n",
      "Epoch:  4 / 5  finished:  5100 / 6920\n",
      "Epoch:  4 / 5  finished:  5200 / 6920\n",
      "Epoch:  4 / 5  finished:  5300 / 6920\n",
      "Epoch:  4 / 5  finished:  5400 / 6920\n",
      "Epoch:  4 / 5  finished:  5500 / 6920\n",
      "Epoch:  4 / 5  finished:  5600 / 6920\n",
      "Epoch:  4 / 5  finished:  5700 / 6920\n",
      "Epoch:  4 / 5  finished:  5800 / 6920\n",
      "Epoch:  4 / 5  finished:  5900 / 6920\n",
      "Epoch:  4 / 5  finished:  6000 / 6920\n",
      "Epoch:  4 / 5  finished:  6100 / 6920\n",
      "Epoch:  4 / 5  finished:  6200 / 6920\n",
      "Epoch:  4 / 5  finished:  6300 / 6920\n",
      "Epoch:  4 / 5  finished:  6400 / 6920\n",
      "Epoch:  4 / 5  finished:  6500 / 6920\n",
      "Epoch:  4 / 5  finished:  6600 / 6920\n",
      "Epoch:  4 / 5  finished:  6700 / 6920\n",
      "Epoch:  4 / 5  finished:  6800 / 6920\n",
      "Epoch:  4 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 4 / 5  train accuracy:  0.8342485549132947 epoch cost:  2772.112666413188  epoch time:  0:07:42.267564\n",
      "Epoch:  5 / 5  finished:  100 / 6920\n",
      "Epoch:  5 / 5  finished:  200 / 6920\n",
      "Epoch:  5 / 5  finished:  300 / 6920\n",
      "Epoch:  5 / 5  finished:  400 / 6920\n",
      "Epoch:  5 / 5  finished:  500 / 6920\n",
      "Epoch:  5 / 5  finished:  600 / 6920\n",
      "Epoch:  5 / 5  finished:  700 / 6920\n",
      "Epoch:  5 / 5  finished:  800 / 6920\n",
      "Epoch:  5 / 5  finished:  900 / 6920\n",
      "Epoch:  5 / 5  finished:  1000 / 6920\n",
      "Epoch:  5 / 5  finished:  1100 / 6920\n",
      "Epoch:  5 / 5  finished:  1200 / 6920\n",
      "Epoch:  5 / 5  finished:  1300 / 6920\n",
      "Epoch:  5 / 5  finished:  1400 / 6920\n",
      "Epoch:  5 / 5  finished:  1500 / 6920\n",
      "Epoch:  5 / 5  finished:  1600 / 6920\n",
      "Epoch:  5 / 5  finished:  1700 / 6920\n",
      "Epoch:  5 / 5  finished:  1800 / 6920\n",
      "Epoch:  5 / 5  finished:  1900 / 6920\n",
      "Epoch:  5 / 5  finished:  2000 / 6920\n",
      "Epoch:  5 / 5  finished:  2100 / 6920\n",
      "Epoch:  5 / 5  finished:  2200 / 6920\n",
      "Epoch:  5 / 5  finished:  2300 / 6920\n",
      "Epoch:  5 / 5  finished:  2400 / 6920\n",
      "Epoch:  5 / 5  finished:  2500 / 6920\n",
      "Epoch:  5 / 5  finished:  2600 / 6920\n",
      "Epoch:  5 / 5  finished:  2700 / 6920\n",
      "Epoch:  5 / 5  finished:  2800 / 6920\n",
      "Epoch:  5 / 5  finished:  2900 / 6920\n",
      "Epoch:  5 / 5  finished:  3000 / 6920\n",
      "Epoch:  5 / 5  finished:  3100 / 6920\n",
      "Epoch:  5 / 5  finished:  3200 / 6920\n",
      "Epoch:  5 / 5  finished:  3300 / 6920\n",
      "Epoch:  5 / 5  finished:  3400 / 6920\n",
      "Epoch:  5 / 5  finished:  3500 / 6920\n",
      "Epoch:  5 / 5  finished:  3600 / 6920\n",
      "Epoch:  5 / 5  finished:  3700 / 6920\n",
      "Epoch:  5 / 5  finished:  3800 / 6920\n",
      "Epoch:  5 / 5  finished:  3900 / 6920\n",
      "Epoch:  5 / 5  finished:  4000 / 6920\n",
      "Epoch:  5 / 5  finished:  4100 / 6920\n",
      "Epoch:  5 / 5  finished:  4200 / 6920\n",
      "Epoch:  5 / 5  finished:  4300 / 6920\n",
      "Epoch:  5 / 5  finished:  4400 / 6920\n",
      "Epoch:  5 / 5  finished:  4500 / 6920\n",
      "Epoch:  5 / 5  finished:  4600 / 6920\n",
      "Epoch:  5 / 5  finished:  4700 / 6920\n",
      "Epoch:  5 / 5  finished:  4800 / 6920\n",
      "Epoch:  5 / 5  finished:  4900 / 6920\n",
      "Epoch:  5 / 5  finished:  5000 / 6920\n",
      "Epoch:  5 / 5  finished:  5100 / 6920\n",
      "Epoch:  5 / 5  finished:  5200 / 6920\n",
      "Epoch:  5 / 5  finished:  5300 / 6920\n",
      "Epoch:  5 / 5  finished:  5400 / 6920\n",
      "Epoch:  5 / 5  finished:  5500 / 6920\n",
      "Epoch:  5 / 5  finished:  5600 / 6920\n",
      "Epoch:  5 / 5  finished:  5700 / 6920\n",
      "Epoch:  5 / 5  finished:  5800 / 6920\n",
      "Epoch:  5 / 5  finished:  5900 / 6920\n",
      "Epoch:  5 / 5  finished:  6000 / 6920\n",
      "Epoch:  5 / 5  finished:  6100 / 6920\n",
      "Epoch:  5 / 5  finished:  6200 / 6920\n",
      "Epoch:  5 / 5  finished:  6300 / 6920\n",
      "Epoch:  5 / 5  finished:  6400 / 6920\n",
      "Epoch:  5 / 5  finished:  6500 / 6920\n",
      "Epoch:  5 / 5  finished:  6600 / 6920\n",
      "Epoch:  5 / 5  finished:  6700 / 6920\n",
      "Epoch:  5 / 5  finished:  6800 / 6920\n",
      "Epoch:  5 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 5 / 5  train accuracy:  0.8459537572254335 epoch cost:  2718.575970761478  epoch time:  0:08:14.649533\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = TNNR(V,D,K)\n",
    "model.fit(train_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7984623833058759"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test_trees)\n",
    "# so we get about the same test accuracy\n",
    "# as we can see time is on par with iterative solution\n",
    "# thanks to eager execution , also our trees are not that big\n",
    "# a quick note is that the model appears to be overfitting\n",
    "# further training causes train accuracy to increase while test accuracy drops/stays the same\n",
    "# we can see how the model here scores 2% less than the above model when they are the same\n",
    "# but these results are satisfactory (actually I am just being lazy :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to look at how we can transform our Recursive neural networks into a Recurrent neural networks\n",
    "\n",
    "---\n",
    "\n",
    "<h3>ecursive NNs to Recurrent NNs</h3>\n",
    "\n",
    "lets start by thinking what are TNNs for and what are RNNs for\n",
    "\n",
    "well , TNNs are for trees and RNNs are for sequences\n",
    "\n",
    "so the way that we are going to convert our problem from a TNN to an RNN is to convert our parse trees into sequences\n",
    "\n",
    "How can we do this ?\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Algorithms</h3>\n",
    "\n",
    "One simple way of turning a tree into an array is to arrange the array elements in such a way that all the children appear to the left of its parent and the value of each element can be its parent index \n",
    "\n",
    "we will call this the parents array\n",
    "\n",
    "notice that we have used the NULL value -1 for nodes that dont have parents\n",
    "\n",
    "only the root should not have a parent\n",
    "\n",
    "<img src='extras/33.18.PNG' width='600'></img>\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Post-Order Traversal</h3>\n",
    "\n",
    "So what is the algorithm that allows us to construct trees in this way ?\n",
    "\n",
    "We know that a parent has to come after its children , in algorithms terminology, this is called post-order traversal\n",
    "\n",
    "It means we visit the left node first, then the right node then the current node , and we do this recusrsively\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Relations</h3>\n",
    "\n",
    "For our specific dataset, we want to know how a child is related to its parent \n",
    "\n",
    "so it can have a relation defined by the parse tree, or the relation can be a simple left/right that we have in the binary tree case\n",
    "\n",
    "<img src='extras/33.19.PNG' width='550'></img>\n",
    "\n",
    "So we can see that the structure will work for binary trees, or trees with any number of children\n",
    "\n",
    "notice that again we used the null value of -1 in the case that there are no relations\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Word indexes</h3>\n",
    "\n",
    "Finally, we want to know what word stored at a node if any\n",
    "\n",
    "<img src='extras/33.20.PNG' width='500'></img>\n",
    "\n",
    "so we need to stor anothe list, the same length as the others that stores either the word index, if its a leaf node, or -1 if its not a leaf word\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Summary</h3>\n",
    "\n",
    "So to summarise then we need 3 seperate arrays to store a tree\n",
    "\n",
    "<ul>\n",
    "    <li>The parents array, which tells us where to find the parent of a node</li>\n",
    "    <li>The relations array, which tells use how a child is related to its parent</li>\n",
    "    <li>The words array, which tells us if the node has a word associated with it and what the word index is</li>\n",
    "</ul>\n",
    "\n",
    "To create these arrays, we need to make sure that a parent always comes after its children, which requires us to use post-order traversal\n",
    "\n",
    "---\n",
    "\n",
    "<h3>How to write a better TNN</h3>\n",
    "\n",
    "Now that we know how to turn a tree into a set of lists, lets discuss how we would runn this through a recurrent neural network\n",
    "\n",
    "this is slightly complicated than what we did with RNNs since now we have 3 arrays instead of 1, so what do we do with them ?\n",
    "\n",
    "The idea is we want to keep a running list of all the tree nodes values and then update these values as we go through the arrays\n",
    "\n",
    "We can go through each element of each array 1 by 1\n",
    "\n",
    "so for example, we will look at parents[0],relations[0],words[0] at the same time, then we will look at parents[1],relations[1],words[1] at the same time\n",
    "\n",
    "Why is this ok ?\n",
    "\n",
    "Because everything we need to determine a node's value depends only on its children\n",
    "\n",
    "Because its children are allways to the left of its parent in the array we are certain that we have calcualted these values first\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Recurrence</h3>\n",
    "\n",
    "So what should we do in the recurrence function\n",
    "\n",
    "two things\n",
    "\n",
    "the first thing is lets calculated the value at the current node n\n",
    "\n",
    "if its a word, then its jsut the word vector of that word\n",
    "\n",
    "if its not a word, then its just f of its current value \n",
    "\n",
    "but why does node n have all the right stuff it needs by the time we reach it ? (so we only apply activation)\n",
    "\n",
    "well, its because the second thing we do is feed the current value into its parent\n",
    "\n",
    "so whether the current node is a word or an inner node it doesnot matter, the way it feeds its value into its parent is the same\n",
    "\n",
    "its just its own times $W_h$ for its child-parent relation added to whatever value is currently at the parent\n",
    "\n",
    "if the node is the root, it doesnot have a parent so we do nothing here\n",
    "\n",
    "in case thats not clear, we now have a 4th list that stores the value needed by each node, each node supplies its parent by the needed value, when we reach the parent we have everyting we need\n",
    "\n",
    "if the node is lead, then it gets the word embedding , does the multiplication and adds this to parent index\n",
    "\n",
    "when we reach a parent , we have everything we need, the parent gets its value by applying the activation (then we can get the label)\n",
    "\n",
    "the node does two things with its own value,\n",
    "\n",
    "multiply by $W_o$ to get its output prediction \n",
    "\n",
    "multiply by $W_h$ (according to parent-child relation) and supply its parent with this value\n",
    "\n",
    "and this continues till we reach the root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to extend the Recursive Neural Network to get an even more expressive Recursive Neural Tensor Network\n",
    "\n",
    "A very fancy name, but as we will see its a very simple concept\n",
    "\n",
    "note that everything that follows withing this section assumes we are working with binary trees\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Recursice Neural Network</h3>\n",
    "\n",
    "So we already know how a Recursive NN is built\n",
    "\n",
    "let look at an alternative view of the TNN, since that was how it was represented in the paper that first introduced this model\n",
    "\n",
    "to get the inner node value, we concatenate the values from the left and right child, call this x\n",
    "\n",
    "$$\\text{x = concat}(x_{\\text{left}},x_{\\text{right}})$$\n",
    "\n",
    "then the value at this node is just\n",
    "\n",
    "$$h = f(W^Tx+b)$$\n",
    "\n",
    "note that this requires $W$ to be of size 2DxD, because all the inner node values must be of size D and after concatenation $x$ is of size 2D\n",
    "\n",
    "we still have the same number of parameters as before, because before we had $W_\\text{left}$ and $W_\\text{right}$ which were both of size DxD\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Extend this model</h3>\n",
    "\n",
    "So whats a natural way to extend this model ?\n",
    "\n",
    "well we can add a quadratic term\n",
    "\n",
    "$$h_j = f(x^TA_jx + W_j^Tx + b_j)$$\n",
    "\n",
    "So $A$ is the quadratic parameter\n",
    "\n",
    "Now what should the size of $A$ be ?\n",
    "\n",
    "notice we have onyl shown one component of $h$ here\n",
    "\n",
    "its written such that each term is a scalar since its difficult to represent tensor multiplication in terms of matricies\n",
    "\n",
    "in order for the quadratic term to be a valid multiplication, $A_j$ needs to be a matrix of size 2Dx2D since $x$ is of size 2D\n",
    "\n",
    "Since there are D components of $h$, so that $j$ goes from 1 to $D$, that means $A$ is of size $D \\times 2D \\times 2D$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Quadratic Discriminant Analysis</h3>\n",
    "\n",
    "The functional form should remind us a lot of something we have seen in the past\n",
    "\n",
    "In particular, when studying logistic regression, we looked at the closed form solution for logistic regression which we could find if both classes had a gaussian distribution using Bayes rule\n",
    "\n",
    "remember that if both gaussian distribution had the same covariance, then the seperating hypersurface is linear\n",
    "\n",
    "this is also called linear discriminant analysis\n",
    "\n",
    "$$h(x) = W^T x + b$$\n",
    "\n",
    "if they had a different covariance, then the hypersurface was quadratic, since the covariance terms did not cancel out\n",
    "\n",
    "$$h(x) = x^TAx + B^Tx + c$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Another perspective</h3>\n",
    "\n",
    "What another way to think about this ?\n",
    "\n",
    "well, if we break down the individual components of $x$, and lets say $D=2$,then we have\n",
    "\n",
    "<ul>\n",
    "    <li>$x_{left_1}$</li>\n",
    "    <li>$x_{left_2}$</li>\n",
    "    <li>$x_{right_1}$</li>\n",
    "    <li>$x_{right_2}$</li>\n",
    "</ul>\n",
    "\n",
    "well the result is , we are now getting temrs like\n",
    "\n",
    "<ul>\n",
    "    <li>$param \\times x_{left_1} \\times x_{right_2}$</li>\n",
    "    <li>$param \\times x_{left_1} \\times x_{left_2}$</li>\n",
    "</ul>\n",
    "\n",
    "we call these interaction terms\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Implementation</h3>\n",
    "\n",
    "Now how do we implement this\n",
    "\n",
    "Rather than concatenate the left child value and the right child value we are going to leave them seperate and implement an equivalent formulation\n",
    "\n",
    "Hopefully this helps us more easily picture what is happening\n",
    "\n",
    "so lets start again with the plain Recursive neural network\n",
    "\n",
    "$$h(x_L,x_R) = f(W_L^TX_L + W_R^Tx_R + b)$$\n",
    "\n",
    "We can extend this by adding 3 quadratic terms\n",
    "\n",
    "$$h(x_L,x_R) = f(x_L^T A_{LL}x_L + x_L^TA_{LR}x_R + x_R^TA_{RR}x_R + W_L^Tx_L + W_R^Tx_R + b)$$\n",
    "\n",
    "of course, each $A$ must be of size $D \\times D \\times D$\n",
    "\n",
    "how many weights is that in total ?\n",
    "\n",
    "$3 D^3$, which is not the same as the original formulation which had $D \\times 2D \\times 2D = 4D^3$ terms\n",
    "\n",
    "The question we want to ask ourselves is are we missing any interaction terms here ?\n",
    "\n",
    "Which ones are redundant ?\n",
    "\n",
    "(of course the redundant is $A_{RL}$ since $A_{LR}$ already captures the interaction between left and right children)\n",
    "\n",
    "---\n",
    "\n",
    "<h3>RNTN Iplementation</h3>\n",
    "\n",
    "So lets talk about how we are going to implement this\n",
    "\n",
    "The only part that is different is that we are going to represent our trees as a different set of lists\n",
    "\n",
    "in particular, we will get rid of the notion of relations and parents\n",
    "\n",
    "instead we will have 3 lists called : words,left_children,right_children\n",
    "\n",
    "as the name suggest, the word list will contain the word index if that word is a node. otherwwise it will be -1 as usual\n",
    "\n",
    "left_children will contain the index of the left child, if the node has a left child, and the same goes for the right\n",
    "\n",
    "note that any node with a left child will also have a right child since all nodes have either 0 or 2 children\n",
    "\n",
    "now why are we doing this ?\n",
    "\n",
    "recall that in our previous formulation, when we calculated the value at the current node, we would feed its value into the parent\n",
    "\n",
    "but now, we need to not only multiply the current node by a weight but we need to multiply it by its sibiling node values as well, which we currently dont have access to\n",
    "\n",
    "so using lists for left children and right children makes this easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will attempt converting the TNN to an RNN + apply RNTN (quadratic)\n",
    "# the advantage of the RNN (sequence) approach would be that we will be able to train in batches\n",
    "# also, we can apply padding, in this way the input can be a tensor of constant size\n",
    "# and so we will be able to use @tf.function\n",
    "# hopefully all this speeds up the training significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load the data as in the previous script\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define a class of tree nodes\n",
    "class node:\n",
    "    def __init__(self):\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.label = None \n",
    "        self.word = ''\n",
    "        # eases parsing\n",
    "        self.parent = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next lets load in the data\n",
    "train_trees_text = []\n",
    "for line in open('datasets/trees/train.txt'):\n",
    "    train_trees_text.append(line.rstrip())\n",
    "    \n",
    "test_trees_text = []\n",
    "for line in open('datasets/trees/test.txt'):\n",
    "    test_trees_text.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a function to parse teext to a tree\n",
    "def create_tree(text):    \n",
    "    # first create root node\n",
    "    root = node()\n",
    "    # we also want to keep track of the words we saw\n",
    "    words = []\n",
    "    for i,w in enumerate(text):\n",
    "        if w == ' ':\n",
    "            continue\n",
    "\n",
    "        if w == '(':\n",
    "            # so we need to create a new node\n",
    "            child = node()\n",
    "            if root.left is None:\n",
    "                root.left = child\n",
    "            else:\n",
    "                root.right = child\n",
    "            child.parent = root\n",
    "            # then let l_child continue\n",
    "            root = child\n",
    "\n",
    "        elif w != ')':\n",
    "            # we have a label or a word\n",
    "            # lets peak ahead to see if the next symbol is a character or a ' '\n",
    "            if text[i+1] == ' ': \n",
    "                # so we have a label\n",
    "                root.label = int(w)\n",
    "            else:\n",
    "                # so its a character of the word\n",
    "                root.word += w.lower()\n",
    "        else : # w is )\n",
    "            if root.word != '':\n",
    "                words.append(root.word)\n",
    "            root = root.parent\n",
    "\n",
    "    # our root is actually the left child of the root\n",
    "    root = root.left\n",
    "    return root,words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets convert text to trees\n",
    "vocab = []\n",
    "train_trees = []\n",
    "for text in train_trees_text:\n",
    "    tree,words = create_tree(text)\n",
    "    train_trees.append(tree)\n",
    "    vocab += words\n",
    "\n",
    "test_trees = []\n",
    "# now again for test data\n",
    "for text in test_trees_text:\n",
    "    tree,words = create_tree(text)\n",
    "    test_trees.append(tree)\n",
    "    vocab += words\n",
    "    \n",
    "vocab = list(set(vocab))\n",
    "V = len(vocab)\n",
    "\n",
    "# lets also create word2idx dict\n",
    "word2idx = {k:v for k,v in zip(vocab,range(V))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "D = 10\n",
    "is_binary = True # whether or not to do binary classification\n",
    "\n",
    "# we will change our problem to binary classification\n",
    "# so rating 0,1 become negative sentiment : 0\n",
    "# 2 is neutral sentiment : -1 , these will be filtered out \n",
    "# 3,4 become positive sentiment : 1\n",
    "\n",
    "def to_binary(tree):\n",
    "    if tree == None:\n",
    "        return\n",
    "    if tree.label<2:\n",
    "        tree.label = 0\n",
    "    elif tree.label == 2:\n",
    "        tree.label = -1\n",
    "    else:\n",
    "        tree.label = 1\n",
    "    to_binary(tree.left)\n",
    "    to_binary(tree.right)\n",
    "    return tree\n",
    "    \n",
    "if is_binary:\n",
    "    K = 2\n",
    "    for tree in train_trees:\n",
    "        to_binary(tree)\n",
    "    for tree in test_trees:\n",
    "        to_binary(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out trees whose label (that of root) is neutral (2 --> -1)\n",
    "\n",
    "train_trees = [tree for tree in train_trees if tree.label != -1]\n",
    "test_trees = [tree for tree in test_trees if tree.label != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so far so good, everything is as before\n",
    "# now what will be the format of our data ?\n",
    "# our data will be of size NxTxB\n",
    "# N is the number of our trees\n",
    "# T is the number of nodes is a tree \n",
    "# of course since not all trees are of same length, we apply padding\n",
    "# also nodes are arranged in an order such that a parent always is to the right of its children\n",
    "# B is the branching factor + 1, that is the number of children +1\n",
    "# so each node t in T knows its two children\n",
    "# and if a node is a word we need to know the index of that node , so thats where the one comes from\n",
    "# if a node has no children , then thats -1\n",
    "# if a node is not a word thats also -1\n",
    "# traversing the tree in post order is essential here\n",
    "# So X is of shape NxTxD\n",
    "# Y will be of shape NxT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(tree):\n",
    "    X = []\n",
    "    Y = []\n",
    "    def tree_to_list(tree,i,X,Y):\n",
    "        # so each node needs three things\n",
    "        # [word_idx,left,right]\n",
    "        if tree == None:\n",
    "            return -1\n",
    "        word_i = None\n",
    "        left_i = None\n",
    "        right_i = None\n",
    "        if tree.word != '':\n",
    "            word_i = word2idx[tree.word]\n",
    "        else: # not a word so -1\n",
    "            word_i = -1\n",
    "\n",
    "        # now post order traversal\n",
    "        left_i = tree_to_list(tree.left,i,X,Y)\n",
    "        right_i = tree_to_list(tree.right,left_i,X,Y)\n",
    "        \n",
    "        if right_i == -1:\n",
    "            my_i = i+1\n",
    "        else:\n",
    "            my_i = right_i + 1\n",
    "\n",
    "        me = [word_i,left_i,right_i]\n",
    "        X.append(me)\n",
    "        Y.append(tree.label)\n",
    "        return my_i\n",
    "    \n",
    "    tree_to_list(tree,-1,X,Y)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we need to pad our sequences so we can form a matrix\n",
    "# for padding we use [-2,-2,-2]\n",
    "# we also want to r\n",
    "def generate_data(trees):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for tree in trees:\n",
    "        x,y = process(tree)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    max_seq_length = max(len(y) for y in Y)\n",
    "\n",
    "    for x in X:\n",
    "        padd = max_seq_length - len(x)\n",
    "        for i in range(padd):\n",
    "            x.append([-2,0,0])\n",
    "\n",
    "    for y in Y:\n",
    "        padd = max_seq_length - len(y)\n",
    "        for i in range(padd):\n",
    "            y.append(-2)\n",
    "    X = np.stack(X)\n",
    "    Y = np.stack(Y)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Ytrain = generate_data(train_trees)\n",
    "Xtest,Ytest = generate_data(test_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain:  (6920, 103, 3)  Ytrain:  (6920, 103)\n",
      "Xtest:  (1821, 111, 3)  Ytest:  (1821, 111)\n"
     ]
    }
   ],
   "source": [
    "# lets check the shapes\n",
    "print('Xtrain: ',Xtrain.shape,' Ytrain: ',Ytrain.shape)\n",
    "print('Xtest: ',Xtest.shape,' Ytest: ',Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so our data is in the correct shape\n",
    "# now we are ready to make our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing batch size had a negative effect on training\n",
    "# so we train stochastically\n",
    "# but we still can make use of the @tf.function\n",
    "class RNTN(Model): \n",
    "    def __init__(self,V,D,K):\n",
    "        super(RNTN, self).__init__()\n",
    "        self.V = V\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "        We = np.random.randn(V, D)/np.sqrt(V+D)\n",
    "        A_LL = np.random.randn(D, D, D)/np.sqrt(3*D)\n",
    "        A_LR = np.random.randn(D, D, D)/np.sqrt(3*D)\n",
    "        A_RR = np.random.randn(D, D, D)/np.sqrt(3*D)\n",
    "        W_L = np.random.randn(D, D)/np.sqrt(2*D)\n",
    "        W_R = np.random.randn(D, D)/np.sqrt(2*D)\n",
    "        Wo = np.random.randn(D, K)/np.sqrt(D+K)\n",
    "        bo = np.zeros(K)\n",
    "        bh = np.zeros(D)\n",
    "\n",
    "        self.We = tf.Variable(We.astype(np.float32))\n",
    "        self.A_LL = tf.Variable(A_LL.astype(np.float32))\n",
    "        self.A_LR = tf.Variable(A_LR.astype(np.float32))\n",
    "        self.A_RR = tf.Variable(A_RR.astype(np.float32))\n",
    "        self.W_L = tf.Variable(W_L.astype(np.float32))\n",
    "        self.W_R = tf.Variable(W_R.astype(np.float32))\n",
    "        self.Wo = tf.Variable(Wo.astype(np.float32))\n",
    "        self.bo = tf.Variable(bo.astype(np.float32))\n",
    "        self.bh = tf.Variable(bh.astype(np.float32))\n",
    "    \n",
    "    # takes in lost of roots\n",
    "    # return accuracy\n",
    "    def score(self,X,Y):\n",
    "        correct = 0\n",
    "        _,T = Y.shape\n",
    "        for x,y in zip(X,Y):\n",
    "            logits = self(x)\n",
    "            idx = np.where(y == -2)[0]\n",
    "            if idx.size == 0:\n",
    "                root_idx = T-1\n",
    "            else:\n",
    "                root_idx = int(idx[0]-1)\n",
    "            correct += y[root_idx] == np.argmax(logits[root_idx])\n",
    "            \n",
    "        return correct/len(X)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, tree):\n",
    "        T = tree.shape[0]\n",
    "        hiddens = tf.TensorArray(tf.float32, size = T)\n",
    "        logits = tf.TensorArray(tf.float32, size = T) \n",
    "        \n",
    "        \n",
    "        for i in tf.range(T):\n",
    "            node = tree[i]\n",
    "            if node[0] == -2 : # padding\n",
    "                break\n",
    "            \n",
    "            if node[0] >= 0 : # leaf\n",
    "                wordvec = self.We[node[0]] # D\n",
    "                wordvec = tf.expand_dims(wordvec,axis=0) # 1xD\n",
    "                # add two hiddens\n",
    "                hiddens = hiddens.write(i,wordvec)\n",
    "                # get logits\n",
    "                logit = tf.matmul(wordvec,self.Wo) + self.bo\n",
    "                logits = logits.write(i,tf.squeeze(logit))\n",
    "\n",
    "                \n",
    "            if node[0] == -1 : # parent (inner node)\n",
    "                left_h = hiddens.read(node[1])\n",
    "                right_h = hiddens.read(node[2])\n",
    "                \n",
    "                # get hidden vector\n",
    "                \n",
    "                LL = tf.matmul(tf.matmul(left_h,self.A_LL),tf.transpose(left_h))\n",
    "                LL = tf.reshape(LL,(1,D))\n",
    "                \n",
    "                LR = tf.matmul(tf.matmul(left_h,self.A_LR),tf.transpose(right_h))\n",
    "                LR = tf.reshape(LR,(1,D))\n",
    "\n",
    "                RR = tf.matmul(tf.matmul(right_h,self.A_RR),tf.transpose(right_h))\n",
    "                RR = tf.reshape(RR,(1,D))\n",
    "                \n",
    "                L = tf.matmul(left_h,self.W_L)\n",
    "                \n",
    "                R = tf.matmul(right_h,self.W_R)\n",
    "                \n",
    "                h = self.f(LL + LR + RR + L + R + self.bh)\n",
    "                \n",
    "                hiddens = hiddens.write(i,h)\n",
    "                \n",
    "                # logits\n",
    "                logit = tf.matmul(h,self.Wo) + self.bo\n",
    "                logits = logits.write(i,tf.squeeze(logit))\n",
    "\n",
    "        return logits.stack()\n",
    "            \n",
    "    def loss(self,T,Y,reg):    \n",
    "        T = tf.cast(T,dtype='int32')\n",
    "        if self.train_inner_nodes == True:\n",
    "            # filter out nodes where sentiment is neural\n",
    "            # so we remove -1 : which are neutral and -2 which are paddings\n",
    "            valid = tf.where(T >= 0)\n",
    "            T = tf.gather(T,valid)\n",
    "            Y = tf.gather(Y,valid)\n",
    "        else: # we only care about root\n",
    "            \n",
    "            idx = tf.squeeze(tf.where(T == -2))\n",
    "            if len(idx) == 0:\n",
    "                root_idx = len(T)-1\n",
    "            else:\n",
    "                root_idx = int(idx[0]-1)\n",
    "            \n",
    "            T = T[root_idx]\n",
    "            Y = Y[root_idx]\n",
    "        cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(T,Y))\n",
    "        l2 = reg*sum(tf.nn.l2_loss(w) for w in self.weights)\n",
    "        cost += l2\n",
    "        return cost\n",
    "        \n",
    "    # training\n",
    "    def fit(self,X,Y,epochs = 5,lr=1e-3,reg = 1e-3,f = tf.nn.tanh,train_inner_nodes=True):\n",
    "        opt = Adam(lr) \n",
    "        self.train_inner_nodes = train_inner_nodes\n",
    "        self.f = f\n",
    "        costs = []\n",
    "        T = X.shape[1]\n",
    "        for epoch in range(epochs):\n",
    "            t0=datetime.now()\n",
    "            epoch_cost = 0\n",
    "            correct = 0\n",
    "            X,Y = shuffle(X,Y)\n",
    "            for i,(x,y) in enumerate(zip(X,Y)):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    logits = self(x)\n",
    "                    cost = self.loss(y,logits,reg)\n",
    "                grads = tape.gradient(cost, self.trainable_weights)\n",
    "                opt.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "                # keep track of epoch cost\n",
    "                # and number of correct predictions\n",
    "                epoch_cost += cost.numpy()\n",
    "                # we only care about root label for accuracy\n",
    "                idx = np.where(y == -2)[0]\n",
    "                if idx.size == 0:\n",
    "                    root_idx = T-1\n",
    "                else:\n",
    "                    root_idx = int(idx[0]-1)\n",
    "                correct += y[root_idx] == np.argmax(logits[root_idx])\n",
    "                if (i+1)%100 == 0:\n",
    "                    print('Epoch: ',epoch+1,'/',epochs,' finished: ',i+1,'/',len(X))\n",
    "            print('Finished Epoch:',epoch+1,'/',epochs,' train accuracy: ',correct/len(X),'epoch cost: ',epoch_cost,' epoch time: ',datetime.now()-t0)\n",
    "            costs.append(epoch_cost)\n",
    "        plt.plot(costs)\n",
    "        plt.title('cost')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNTN(V,D,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 5  finished:  100 / 6920\n",
      "Epoch:  1 / 5  finished:  200 / 6920\n",
      "Epoch:  1 / 5  finished:  300 / 6920\n",
      "Epoch:  1 / 5  finished:  400 / 6920\n",
      "Epoch:  1 / 5  finished:  500 / 6920\n",
      "Epoch:  1 / 5  finished:  600 / 6920\n",
      "Epoch:  1 / 5  finished:  700 / 6920\n",
      "Epoch:  1 / 5  finished:  800 / 6920\n",
      "Epoch:  1 / 5  finished:  900 / 6920\n",
      "Epoch:  1 / 5  finished:  1000 / 6920\n",
      "Epoch:  1 / 5  finished:  1100 / 6920\n",
      "Epoch:  1 / 5  finished:  1200 / 6920\n",
      "Epoch:  1 / 5  finished:  1300 / 6920\n",
      "Epoch:  1 / 5  finished:  1400 / 6920\n",
      "Epoch:  1 / 5  finished:  1500 / 6920\n",
      "Epoch:  1 / 5  finished:  1600 / 6920\n",
      "Epoch:  1 / 5  finished:  1700 / 6920\n",
      "Epoch:  1 / 5  finished:  1800 / 6920\n",
      "Epoch:  1 / 5  finished:  1900 / 6920\n",
      "Epoch:  1 / 5  finished:  2000 / 6920\n",
      "Epoch:  1 / 5  finished:  2100 / 6920\n",
      "Epoch:  1 / 5  finished:  2200 / 6920\n",
      "Epoch:  1 / 5  finished:  2300 / 6920\n",
      "Epoch:  1 / 5  finished:  2400 / 6920\n",
      "Epoch:  1 / 5  finished:  2500 / 6920\n",
      "Epoch:  1 / 5  finished:  2600 / 6920\n",
      "Epoch:  1 / 5  finished:  2700 / 6920\n",
      "Epoch:  1 / 5  finished:  2800 / 6920\n",
      "Epoch:  1 / 5  finished:  2900 / 6920\n",
      "Epoch:  1 / 5  finished:  3000 / 6920\n",
      "Epoch:  1 / 5  finished:  3100 / 6920\n",
      "Epoch:  1 / 5  finished:  3200 / 6920\n",
      "Epoch:  1 / 5  finished:  3300 / 6920\n",
      "Epoch:  1 / 5  finished:  3400 / 6920\n",
      "Epoch:  1 / 5  finished:  3500 / 6920\n",
      "Epoch:  1 / 5  finished:  3600 / 6920\n",
      "Epoch:  1 / 5  finished:  3700 / 6920\n",
      "Epoch:  1 / 5  finished:  3800 / 6920\n",
      "Epoch:  1 / 5  finished:  3900 / 6920\n",
      "Epoch:  1 / 5  finished:  4000 / 6920\n",
      "Epoch:  1 / 5  finished:  4100 / 6920\n",
      "Epoch:  1 / 5  finished:  4200 / 6920\n",
      "Epoch:  1 / 5  finished:  4300 / 6920\n",
      "Epoch:  1 / 5  finished:  4400 / 6920\n",
      "Epoch:  1 / 5  finished:  4500 / 6920\n",
      "Epoch:  1 / 5  finished:  4600 / 6920\n",
      "Epoch:  1 / 5  finished:  4700 / 6920\n",
      "Epoch:  1 / 5  finished:  4800 / 6920\n",
      "Epoch:  1 / 5  finished:  4900 / 6920\n",
      "Epoch:  1 / 5  finished:  5000 / 6920\n",
      "Epoch:  1 / 5  finished:  5100 / 6920\n",
      "Epoch:  1 / 5  finished:  5200 / 6920\n",
      "Epoch:  1 / 5  finished:  5300 / 6920\n",
      "Epoch:  1 / 5  finished:  5400 / 6920\n",
      "Epoch:  1 / 5  finished:  5500 / 6920\n",
      "Epoch:  1 / 5  finished:  5600 / 6920\n",
      "Epoch:  1 / 5  finished:  5700 / 6920\n",
      "Epoch:  1 / 5  finished:  5800 / 6920\n",
      "Epoch:  1 / 5  finished:  5900 / 6920\n",
      "Epoch:  1 / 5  finished:  6000 / 6920\n",
      "Epoch:  1 / 5  finished:  6100 / 6920\n",
      "Epoch:  1 / 5  finished:  6200 / 6920\n",
      "Epoch:  1 / 5  finished:  6300 / 6920\n",
      "Epoch:  1 / 5  finished:  6400 / 6920\n",
      "Epoch:  1 / 5  finished:  6500 / 6920\n",
      "Epoch:  1 / 5  finished:  6600 / 6920\n",
      "Epoch:  1 / 5  finished:  6700 / 6920\n",
      "Epoch:  1 / 5  finished:  6800 / 6920\n",
      "Epoch:  1 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 1 / 5  train accuracy:  0.7192196531791908 epoch cost:  3842.3688281476498  epoch time:  0:04:25.610575\n",
      "Epoch:  2 / 5  finished:  100 / 6920\n",
      "Epoch:  2 / 5  finished:  200 / 6920\n",
      "Epoch:  2 / 5  finished:  300 / 6920\n",
      "Epoch:  2 / 5  finished:  400 / 6920\n",
      "Epoch:  2 / 5  finished:  500 / 6920\n",
      "Epoch:  2 / 5  finished:  600 / 6920\n",
      "Epoch:  2 / 5  finished:  700 / 6920\n",
      "Epoch:  2 / 5  finished:  800 / 6920\n",
      "Epoch:  2 / 5  finished:  900 / 6920\n",
      "Epoch:  2 / 5  finished:  1000 / 6920\n",
      "Epoch:  2 / 5  finished:  1100 / 6920\n",
      "Epoch:  2 / 5  finished:  1200 / 6920\n",
      "Epoch:  2 / 5  finished:  1300 / 6920\n",
      "Epoch:  2 / 5  finished:  1400 / 6920\n",
      "Epoch:  2 / 5  finished:  1500 / 6920\n",
      "Epoch:  2 / 5  finished:  1600 / 6920\n",
      "Epoch:  2 / 5  finished:  1700 / 6920\n",
      "Epoch:  2 / 5  finished:  1800 / 6920\n",
      "Epoch:  2 / 5  finished:  1900 / 6920\n",
      "Epoch:  2 / 5  finished:  2000 / 6920\n",
      "Epoch:  2 / 5  finished:  2100 / 6920\n",
      "Epoch:  2 / 5  finished:  2200 / 6920\n",
      "Epoch:  2 / 5  finished:  2300 / 6920\n",
      "Epoch:  2 / 5  finished:  2400 / 6920\n",
      "Epoch:  2 / 5  finished:  2500 / 6920\n",
      "Epoch:  2 / 5  finished:  2600 / 6920\n",
      "Epoch:  2 / 5  finished:  2700 / 6920\n",
      "Epoch:  2 / 5  finished:  2800 / 6920\n",
      "Epoch:  2 / 5  finished:  2900 / 6920\n",
      "Epoch:  2 / 5  finished:  3000 / 6920\n",
      "Epoch:  2 / 5  finished:  3100 / 6920\n",
      "Epoch:  2 / 5  finished:  3200 / 6920\n",
      "Epoch:  2 / 5  finished:  3300 / 6920\n",
      "Epoch:  2 / 5  finished:  3400 / 6920\n",
      "Epoch:  2 / 5  finished:  3500 / 6920\n",
      "Epoch:  2 / 5  finished:  3600 / 6920\n",
      "Epoch:  2 / 5  finished:  3700 / 6920\n",
      "Epoch:  2 / 5  finished:  3800 / 6920\n",
      "Epoch:  2 / 5  finished:  3900 / 6920\n",
      "Epoch:  2 / 5  finished:  4000 / 6920\n",
      "Epoch:  2 / 5  finished:  4100 / 6920\n",
      "Epoch:  2 / 5  finished:  4200 / 6920\n",
      "Epoch:  2 / 5  finished:  4300 / 6920\n",
      "Epoch:  2 / 5  finished:  4400 / 6920\n",
      "Epoch:  2 / 5  finished:  4500 / 6920\n",
      "Epoch:  2 / 5  finished:  4600 / 6920\n",
      "Epoch:  2 / 5  finished:  4700 / 6920\n",
      "Epoch:  2 / 5  finished:  4800 / 6920\n",
      "Epoch:  2 / 5  finished:  4900 / 6920\n",
      "Epoch:  2 / 5  finished:  5000 / 6920\n",
      "Epoch:  2 / 5  finished:  5100 / 6920\n",
      "Epoch:  2 / 5  finished:  5200 / 6920\n",
      "Epoch:  2 / 5  finished:  5300 / 6920\n",
      "Epoch:  2 / 5  finished:  5400 / 6920\n",
      "Epoch:  2 / 5  finished:  5500 / 6920\n",
      "Epoch:  2 / 5  finished:  5600 / 6920\n",
      "Epoch:  2 / 5  finished:  5700 / 6920\n",
      "Epoch:  2 / 5  finished:  5800 / 6920\n",
      "Epoch:  2 / 5  finished:  5900 / 6920\n",
      "Epoch:  2 / 5  finished:  6000 / 6920\n",
      "Epoch:  2 / 5  finished:  6100 / 6920\n",
      "Epoch:  2 / 5  finished:  6200 / 6920\n",
      "Epoch:  2 / 5  finished:  6300 / 6920\n",
      "Epoch:  2 / 5  finished:  6400 / 6920\n",
      "Epoch:  2 / 5  finished:  6500 / 6920\n",
      "Epoch:  2 / 5  finished:  6600 / 6920\n",
      "Epoch:  2 / 5  finished:  6700 / 6920\n",
      "Epoch:  2 / 5  finished:  6800 / 6920\n",
      "Epoch:  2 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 2 / 5  train accuracy:  0.8232658959537572 epoch cost:  3005.240827254951  epoch time:  0:04:22.748003\n",
      "Epoch:  3 / 5  finished:  100 / 6920\n",
      "Epoch:  3 / 5  finished:  200 / 6920\n",
      "Epoch:  3 / 5  finished:  300 / 6920\n",
      "Epoch:  3 / 5  finished:  400 / 6920\n",
      "Epoch:  3 / 5  finished:  500 / 6920\n",
      "Epoch:  3 / 5  finished:  600 / 6920\n",
      "Epoch:  3 / 5  finished:  700 / 6920\n",
      "Epoch:  3 / 5  finished:  800 / 6920\n",
      "Epoch:  3 / 5  finished:  900 / 6920\n",
      "Epoch:  3 / 5  finished:  1000 / 6920\n",
      "Epoch:  3 / 5  finished:  1100 / 6920\n",
      "Epoch:  3 / 5  finished:  1200 / 6920\n",
      "Epoch:  3 / 5  finished:  1300 / 6920\n",
      "Epoch:  3 / 5  finished:  1400 / 6920\n",
      "Epoch:  3 / 5  finished:  1500 / 6920\n",
      "Epoch:  3 / 5  finished:  1600 / 6920\n",
      "Epoch:  3 / 5  finished:  1700 / 6920\n",
      "Epoch:  3 / 5  finished:  1800 / 6920\n",
      "Epoch:  3 / 5  finished:  1900 / 6920\n",
      "Epoch:  3 / 5  finished:  2000 / 6920\n",
      "Epoch:  3 / 5  finished:  2100 / 6920\n",
      "Epoch:  3 / 5  finished:  2200 / 6920\n",
      "Epoch:  3 / 5  finished:  2300 / 6920\n",
      "Epoch:  3 / 5  finished:  2400 / 6920\n",
      "Epoch:  3 / 5  finished:  2500 / 6920\n",
      "Epoch:  3 / 5  finished:  2600 / 6920\n",
      "Epoch:  3 / 5  finished:  2700 / 6920\n",
      "Epoch:  3 / 5  finished:  2800 / 6920\n",
      "Epoch:  3 / 5  finished:  2900 / 6920\n",
      "Epoch:  3 / 5  finished:  3000 / 6920\n",
      "Epoch:  3 / 5  finished:  3100 / 6920\n",
      "Epoch:  3 / 5  finished:  3200 / 6920\n",
      "Epoch:  3 / 5  finished:  3300 / 6920\n",
      "Epoch:  3 / 5  finished:  3400 / 6920\n",
      "Epoch:  3 / 5  finished:  3500 / 6920\n",
      "Epoch:  3 / 5  finished:  3600 / 6920\n",
      "Epoch:  3 / 5  finished:  3700 / 6920\n",
      "Epoch:  3 / 5  finished:  3800 / 6920\n",
      "Epoch:  3 / 5  finished:  3900 / 6920\n",
      "Epoch:  3 / 5  finished:  4000 / 6920\n",
      "Epoch:  3 / 5  finished:  4100 / 6920\n",
      "Epoch:  3 / 5  finished:  4200 / 6920\n",
      "Epoch:  3 / 5  finished:  4300 / 6920\n",
      "Epoch:  3 / 5  finished:  4400 / 6920\n",
      "Epoch:  3 / 5  finished:  4500 / 6920\n",
      "Epoch:  3 / 5  finished:  4600 / 6920\n",
      "Epoch:  3 / 5  finished:  4700 / 6920\n",
      "Epoch:  3 / 5  finished:  4800 / 6920\n",
      "Epoch:  3 / 5  finished:  4900 / 6920\n",
      "Epoch:  3 / 5  finished:  5000 / 6920\n",
      "Epoch:  3 / 5  finished:  5100 / 6920\n",
      "Epoch:  3 / 5  finished:  5200 / 6920\n",
      "Epoch:  3 / 5  finished:  5300 / 6920\n",
      "Epoch:  3 / 5  finished:  5400 / 6920\n",
      "Epoch:  3 / 5  finished:  5500 / 6920\n",
      "Epoch:  3 / 5  finished:  5600 / 6920\n",
      "Epoch:  3 / 5  finished:  5700 / 6920\n",
      "Epoch:  3 / 5  finished:  5800 / 6920\n",
      "Epoch:  3 / 5  finished:  5900 / 6920\n",
      "Epoch:  3 / 5  finished:  6000 / 6920\n",
      "Epoch:  3 / 5  finished:  6100 / 6920\n",
      "Epoch:  3 / 5  finished:  6200 / 6920\n",
      "Epoch:  3 / 5  finished:  6300 / 6920\n",
      "Epoch:  3 / 5  finished:  6400 / 6920\n",
      "Epoch:  3 / 5  finished:  6500 / 6920\n",
      "Epoch:  3 / 5  finished:  6600 / 6920\n",
      "Epoch:  3 / 5  finished:  6700 / 6920\n",
      "Epoch:  3 / 5  finished:  6800 / 6920\n",
      "Epoch:  3 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 3 / 5  train accuracy:  0.8439306358381503 epoch cost:  2812.117636501789  epoch time:  0:04:16.186011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4 / 5  finished:  100 / 6920\n",
      "Epoch:  4 / 5  finished:  200 / 6920\n",
      "Epoch:  4 / 5  finished:  300 / 6920\n",
      "Epoch:  4 / 5  finished:  400 / 6920\n",
      "Epoch:  4 / 5  finished:  500 / 6920\n",
      "Epoch:  4 / 5  finished:  600 / 6920\n",
      "Epoch:  4 / 5  finished:  700 / 6920\n",
      "Epoch:  4 / 5  finished:  800 / 6920\n",
      "Epoch:  4 / 5  finished:  900 / 6920\n",
      "Epoch:  4 / 5  finished:  1000 / 6920\n",
      "Epoch:  4 / 5  finished:  1100 / 6920\n",
      "Epoch:  4 / 5  finished:  1200 / 6920\n",
      "Epoch:  4 / 5  finished:  1300 / 6920\n",
      "Epoch:  4 / 5  finished:  1400 / 6920\n",
      "Epoch:  4 / 5  finished:  1500 / 6920\n",
      "Epoch:  4 / 5  finished:  1600 / 6920\n",
      "Epoch:  4 / 5  finished:  1700 / 6920\n",
      "Epoch:  4 / 5  finished:  1800 / 6920\n",
      "Epoch:  4 / 5  finished:  1900 / 6920\n",
      "Epoch:  4 / 5  finished:  2000 / 6920\n",
      "Epoch:  4 / 5  finished:  2100 / 6920\n",
      "Epoch:  4 / 5  finished:  2200 / 6920\n",
      "Epoch:  4 / 5  finished:  2300 / 6920\n",
      "Epoch:  4 / 5  finished:  2400 / 6920\n",
      "Epoch:  4 / 5  finished:  2500 / 6920\n",
      "Epoch:  4 / 5  finished:  2600 / 6920\n",
      "Epoch:  4 / 5  finished:  2700 / 6920\n",
      "Epoch:  4 / 5  finished:  2800 / 6920\n",
      "Epoch:  4 / 5  finished:  2900 / 6920\n",
      "Epoch:  4 / 5  finished:  3000 / 6920\n",
      "Epoch:  4 / 5  finished:  3100 / 6920\n",
      "Epoch:  4 / 5  finished:  3200 / 6920\n",
      "Epoch:  4 / 5  finished:  3300 / 6920\n",
      "Epoch:  4 / 5  finished:  3400 / 6920\n",
      "Epoch:  4 / 5  finished:  3500 / 6920\n",
      "Epoch:  4 / 5  finished:  3600 / 6920\n",
      "Epoch:  4 / 5  finished:  3700 / 6920\n",
      "Epoch:  4 / 5  finished:  3800 / 6920\n",
      "Epoch:  4 / 5  finished:  3900 / 6920\n",
      "Epoch:  4 / 5  finished:  4000 / 6920\n",
      "Epoch:  4 / 5  finished:  4100 / 6920\n",
      "Epoch:  4 / 5  finished:  4200 / 6920\n",
      "Epoch:  4 / 5  finished:  4300 / 6920\n",
      "Epoch:  4 / 5  finished:  4400 / 6920\n",
      "Epoch:  4 / 5  finished:  4500 / 6920\n",
      "Epoch:  4 / 5  finished:  4600 / 6920\n",
      "Epoch:  4 / 5  finished:  4700 / 6920\n",
      "Epoch:  4 / 5  finished:  4800 / 6920\n",
      "Epoch:  4 / 5  finished:  4900 / 6920\n",
      "Epoch:  4 / 5  finished:  5000 / 6920\n",
      "Epoch:  4 / 5  finished:  5100 / 6920\n",
      "Epoch:  4 / 5  finished:  5200 / 6920\n",
      "Epoch:  4 / 5  finished:  5300 / 6920\n",
      "Epoch:  4 / 5  finished:  5400 / 6920\n",
      "Epoch:  4 / 5  finished:  5500 / 6920\n",
      "Epoch:  4 / 5  finished:  5600 / 6920\n",
      "Epoch:  4 / 5  finished:  5700 / 6920\n",
      "Epoch:  4 / 5  finished:  5800 / 6920\n",
      "Epoch:  4 / 5  finished:  5900 / 6920\n",
      "Epoch:  4 / 5  finished:  6000 / 6920\n",
      "Epoch:  4 / 5  finished:  6100 / 6920\n",
      "Epoch:  4 / 5  finished:  6200 / 6920\n",
      "Epoch:  4 / 5  finished:  6300 / 6920\n",
      "Epoch:  4 / 5  finished:  6400 / 6920\n",
      "Epoch:  4 / 5  finished:  6500 / 6920\n",
      "Epoch:  4 / 5  finished:  6600 / 6920\n",
      "Epoch:  4 / 5  finished:  6700 / 6920\n",
      "Epoch:  4 / 5  finished:  6800 / 6920\n",
      "Epoch:  4 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 4 / 5  train accuracy:  0.8569364161849711 epoch cost:  2706.1151754409075  epoch time:  0:04:28.307218\n",
      "Epoch:  5 / 5  finished:  100 / 6920\n",
      "Epoch:  5 / 5  finished:  200 / 6920\n",
      "Epoch:  5 / 5  finished:  300 / 6920\n",
      "Epoch:  5 / 5  finished:  400 / 6920\n",
      "Epoch:  5 / 5  finished:  500 / 6920\n",
      "Epoch:  5 / 5  finished:  600 / 6920\n",
      "Epoch:  5 / 5  finished:  700 / 6920\n",
      "Epoch:  5 / 5  finished:  800 / 6920\n",
      "Epoch:  5 / 5  finished:  900 / 6920\n",
      "Epoch:  5 / 5  finished:  1000 / 6920\n",
      "Epoch:  5 / 5  finished:  1100 / 6920\n",
      "Epoch:  5 / 5  finished:  1200 / 6920\n",
      "Epoch:  5 / 5  finished:  1300 / 6920\n",
      "Epoch:  5 / 5  finished:  1400 / 6920\n",
      "Epoch:  5 / 5  finished:  1500 / 6920\n",
      "Epoch:  5 / 5  finished:  1600 / 6920\n",
      "Epoch:  5 / 5  finished:  1700 / 6920\n",
      "Epoch:  5 / 5  finished:  1800 / 6920\n",
      "Epoch:  5 / 5  finished:  1900 / 6920\n",
      "Epoch:  5 / 5  finished:  2000 / 6920\n",
      "Epoch:  5 / 5  finished:  2100 / 6920\n",
      "Epoch:  5 / 5  finished:  2200 / 6920\n",
      "Epoch:  5 / 5  finished:  2300 / 6920\n",
      "Epoch:  5 / 5  finished:  2400 / 6920\n",
      "Epoch:  5 / 5  finished:  2500 / 6920\n",
      "Epoch:  5 / 5  finished:  2600 / 6920\n",
      "Epoch:  5 / 5  finished:  2700 / 6920\n",
      "Epoch:  5 / 5  finished:  2800 / 6920\n",
      "Epoch:  5 / 5  finished:  2900 / 6920\n",
      "Epoch:  5 / 5  finished:  3000 / 6920\n",
      "Epoch:  5 / 5  finished:  3100 / 6920\n",
      "Epoch:  5 / 5  finished:  3200 / 6920\n",
      "Epoch:  5 / 5  finished:  3300 / 6920\n",
      "Epoch:  5 / 5  finished:  3400 / 6920\n",
      "Epoch:  5 / 5  finished:  3500 / 6920\n",
      "Epoch:  5 / 5  finished:  3600 / 6920\n",
      "Epoch:  5 / 5  finished:  3700 / 6920\n",
      "Epoch:  5 / 5  finished:  3800 / 6920\n",
      "Epoch:  5 / 5  finished:  3900 / 6920\n",
      "Epoch:  5 / 5  finished:  4000 / 6920\n",
      "Epoch:  5 / 5  finished:  4100 / 6920\n",
      "Epoch:  5 / 5  finished:  4200 / 6920\n",
      "Epoch:  5 / 5  finished:  4300 / 6920\n",
      "Epoch:  5 / 5  finished:  4400 / 6920\n",
      "Epoch:  5 / 5  finished:  4500 / 6920\n",
      "Epoch:  5 / 5  finished:  4600 / 6920\n",
      "Epoch:  5 / 5  finished:  4700 / 6920\n",
      "Epoch:  5 / 5  finished:  4800 / 6920\n",
      "Epoch:  5 / 5  finished:  4900 / 6920\n",
      "Epoch:  5 / 5  finished:  5000 / 6920\n",
      "Epoch:  5 / 5  finished:  5100 / 6920\n",
      "Epoch:  5 / 5  finished:  5200 / 6920\n",
      "Epoch:  5 / 5  finished:  5300 / 6920\n",
      "Epoch:  5 / 5  finished:  5400 / 6920\n",
      "Epoch:  5 / 5  finished:  5500 / 6920\n",
      "Epoch:  5 / 5  finished:  5600 / 6920\n",
      "Epoch:  5 / 5  finished:  5700 / 6920\n",
      "Epoch:  5 / 5  finished:  5800 / 6920\n",
      "Epoch:  5 / 5  finished:  5900 / 6920\n",
      "Epoch:  5 / 5  finished:  6000 / 6920\n",
      "Epoch:  5 / 5  finished:  6100 / 6920\n",
      "Epoch:  5 / 5  finished:  6200 / 6920\n",
      "Epoch:  5 / 5  finished:  6300 / 6920\n",
      "Epoch:  5 / 5  finished:  6400 / 6920\n",
      "Epoch:  5 / 5  finished:  6500 / 6920\n",
      "Epoch:  5 / 5  finished:  6600 / 6920\n",
      "Epoch:  5 / 5  finished:  6700 / 6920\n",
      "Epoch:  5 / 5  finished:  6800 / 6920\n",
      "Epoch:  5 / 5  finished:  6900 / 6920\n",
      "Finished Epoch: 5 / 5  train accuracy:  0.8645953757225433 epoch cost:  2654.374640509486  epoch time:  0:04:50.272755\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.828665568369028"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see now that with tf.function\n",
    "# the quadratic model runs in half the time of the previous linear model !\n",
    "# notes for future me : more epochs + lr scheduling \n",
    "# future me : done , 5 epochs at 1e-3 , next 5 at 1e-4\n",
    "# high train accuracy (93.5% ) , model still overfits (83.6% test)\n",
    "# future future me : fix ocerfitting + try with train_inner_nodes = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
