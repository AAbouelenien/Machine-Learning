{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to discuss a new topic, memory networks\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Memory Networks</h3>\n",
    "\n",
    "At this point, ints interesting to look at what we have done so far in the few previous notebooks (32,34-36) at a high level\n",
    "\n",
    "So in the review (32) and Bidirectional RNN (34) notebooks, we mainly focused on many-to-one tasks\n",
    "\n",
    "This is an abstraction that encapsulates quite a few different tasks such as spam detection, sentiment analysis and other classification tasks where the label depends on looking at the entire sequence\n",
    "\n",
    "We also briefly discussed simple many-to-many tasks, where the number of output predictions is equal to the sequence length, in other words, for each input token we make a predicition\n",
    "\n",
    "This also encapsulates a few common taks, such as parts-of-speech tagging, named entity recognition and next word prediction, also known as language modelling\n",
    "\n",
    "We looked at language modelling in the form of learning to generate poetry\n",
    "\n",
    "When the input and output sequence lnegths are equal in size, this is really easy to model because we can just use one LSTM and have it feed directly from the input to the output\n",
    "\n",
    "After that we looked at a new kind of many to many task, where the input sequence length doesnot have to equal the output sequence length\n",
    "\n",
    "We call these Seq2Seq models\n",
    "\n",
    "These also encapsulate a few practical apllications, such as machine translation, question answering and chatbots\n",
    "\n",
    "Note that the previous two notenooks both focused on the Seq2Seq model, so attention is still Seq2Seq and still applies to the same tasks, although we can make the principle of attention work elsewhere\n",
    "\n",
    "In this notebook, we are going to look at something almost completely different\n",
    "\n",
    "We will be surprised to know it does not even involve RNNs !\n",
    "\n",
    "We will get into the exact model next section, but it does involve principles we have learned about before, such as making use of word embeddings, using the softmax to weight different features and so on\n",
    "\n",
    "Now RNNs have been applied to memory networks, and we will talk briefly about that works, but the original memory networks did not make use of RNNs and so thats what we will be looking at\n",
    "\n",
    "---\n",
    "\n",
    "So what is the purpose of Memory Networks ?\n",
    "\n",
    "As we did earlier, we are going to start this section by stating a problem\n",
    "\n",
    "The problem is this, suppose we ahve a story that contains several facts, then we are asked a question about the story\n",
    "\n",
    "If we can give the correct answer, then that shows that we comprehended the story\n",
    "\n",
    "In general, this task can be thought of as question answering\n",
    "\n",
    "but it uses a very different model from what we have seen in previous notebooks\n",
    "\n",
    "---\n",
    "\n",
    "<h3>bAbI dataset</h3>\n",
    "\n",
    "Whats interesting about this task, is that its still something very early in development\n",
    "\n",
    "So a lot of the work here was done at the Facebook Research\n",
    "\n",
    "The main dataset for the models is called the bAbI dataset\n",
    "\n",
    "This is a little different from the datasets we have used so far, because each sample consists of a lot more than one or few phrases\n",
    "\n",
    "In particular each sample contains three parts\n",
    "\n",
    "<ul>\n",
    "    <li>Story</li>\n",
    "    <li>Question</li>\n",
    "    <li>Answer</li> \n",
    "</ul>\n",
    "\n",
    "The story contains multiple sentences, so thats something we are going to have to deal with in the model\n",
    "\n",
    "Our model is going to learn to take the story and question as input, and output the answer\n",
    "\n",
    "---\n",
    "\n",
    "So here is an example of one sample\n",
    "\n",
    "Story:\n",
    "```\n",
    "Marry moved to the bathroom.\n",
    "John went to the hallway\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "Where is Mary?\n",
    "```\n",
    "\n",
    "Answer\n",
    "```\n",
    "bathroom\n",
    "```\n",
    "\n",
    "This story is jsut two lines, then there is a question and an answer\n",
    "\n",
    "---\n",
    "\n",
    "Now here is a full story from what the actual dataset looks like\n",
    "\n",
    "<img src='extras/37.1.PNG' width='400' style=\"float:left\"></img>\n",
    "\n",
    "notice how for each line we have a line number, and then each line is going to be one of two things \n",
    "\n",
    "Either is part of the story, or its a line that contains both a question and an answer\n",
    "\n",
    "So for example lines ```1``` and ```2``` are part of the story but line ```3``` contains a question and answer\n",
    "\n",
    "And we also notice how the data tells us which line is relevant to answer the question\n",
    "\n",
    "So line number ```1``` is relevant for the first question on line ```3``` because that tells us where mary went to, she went to the bathroom\n",
    "\n",
    "now we are not actually going to make use of that information, because we want to let the mode learn how to read and understand the stories\n",
    "\n",
    "Also notice how the story continues after the first question\n",
    "\n",
    "So lines ```4``` and ```5``` are ```Daniel went back to the hallway``` and ```Sandra moved to the garden```\n",
    "\n",
    "Then the next question is ```Where is Daniel```, the answer is ```hallway``` which refers back to line ```4```\n",
    "\n",
    "Then the story continues with line ```7``` and ```8``` which are ```John moved to the office``` and ```Snadra moved to the bathroom```\n",
    "\n",
    "But, the next question on line ```9``` is again, ```Where is Daniel?```\n",
    "\n",
    "And the answer is still ```hallway``` because Daniel hasnt actually moved from the hallway which is line ```4```\n",
    "\n",
    "So we just want to remember that, its a continuing story even after a question is asked\n",
    "\n",
    "And when a new story starts, the line nuber goes to $1$ again\n",
    "\n",
    "Now this might seem a little cumbersome, but knowing it now is better than getting surprised when we see it later\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Single Supporting Fact</h3>\n",
    "\n",
    "So the story we just looked at is called a single supporting fact story, because to answer the question, we only need to refer to one lone in the story\n",
    "\n",
    "Now there is an interesting connection to attention here\n",
    "\n",
    "Remember that attention was useful because it told us which hidden state in the sequence to pay attention to\n",
    "\n",
    "This is very similar because we have a bunch of input sequences and we have to decide which sentence is the one we should pay attention to\n",
    "\n",
    "We will look at this more in the next section, but we jsut rememebr the principle of attention should give us some clues as to what we should do\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Two Supporting Fact</h3>\n",
    "\n",
    "The other type of story we are going to look at is called the two supporting fact story\n",
    "\n",
    "This is where instead of needing one sentence to determine the answer, now we need two\n",
    "\n",
    "So there needs to be some logic that the model has to follow to produce the answer\n",
    "\n",
    "An example of this, where we have removed the irrelevant lines, is\n",
    "\n",
    "```\n",
    "7 John went to the hallway\n",
    "9 John put down the football\n",
    "11 Where is the football?  hallway   9 7\n",
    "```\n",
    "As we can imagine, this is a tougher task and is going to require a more complex model\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Why are these tasks so silly?</h3>\n",
    "\n",
    "But in general, something interesting we should notice is that, these are very very simple tasks compared to human reading comprehension\n",
    "\n",
    "In fact, they seem kind of trivial\n",
    "\n",
    "So we use the example of reading Albert Einsein's Wikipidea page and then being able to answer questions about Albert Einstein\n",
    "\n",
    "But realise that, that requires human level reading ability, and so in comparison, memory networks are still a very new idea and we are still trying to make sure they work on simple datasets\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Where to get the data</h3>\n",
    "\n",
    "Luckily to get the data, we dont have to do anywork since its going to be download directly form a URL in the code, so no need to download it ourselves\n",
    "\n",
    "If we want to, we can also go to Facebook's page that talks about the dataset a little more from <a href='https://research.fb.com/downloads/babi/'>here</a>\n",
    "\n",
    "now one interesting thing is, if we look at the data, there are a lot of stuff in there that we are not going to have a chance to look at\n",
    "\n",
    "So there is something called the ```Yes/No``` task, where we are told a story and then we are asked a Yes/No question about the story\n",
    "\n",
    "An example of this is \n",
    "\n",
    "```\n",
    "2 Sanda journeyed to the bedroom.\n",
    "3 Is Sandra in the hallway?  no    2\n",
    "```\n",
    "\n",
    "There is another task called the counting test where, we are told a story, and we try to determine how many obhects a character in the story is carrying\n",
    "\n",
    "Notice how this is a very toy task, they always use the word ```carrying``` in the question and the answer is always ```1``` or ```none```\n",
    "\n",
    "So an example is \n",
    "\n",
    "```\n",
    "6 Sandra put down the milk\n",
    "7 How many objects is Sandra carrying?     none 5 6\n",
    "```\n",
    "\n",
    "Another example is the negation task\n",
    "\n",
    "This is similar to the Yes/No task because the answer is always going to be Yes/No\n",
    "\n",
    "So an example is\n",
    "\n",
    "```\n",
    "1 Mary is no longer in the bedroom\n",
    "3 Is Mary in the bedroom?  no    1\n",
    "```\n",
    "\n",
    "```\n",
    "2 John went to the hallway\n",
    "3 Is john in the bathroom?  no    2\n",
    "```\n",
    "\n",
    "This also adds an eleemnt of difficulty, because negation is typically difficult in NLP\n",
    "\n",
    "All we are doing usually is adding up word vectors, so there is no specific negation operation that can flip the meaning of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to take a deep dive into how memory networks work\n",
    "\n",
    "Specifically, we are going to focus on both one supporting fact and two supporting facts stories, since thats also what we are going to be lookign at in the code\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Memory networks</h3>\n",
    "\n",
    "One cool thing about memory networks, is that they are not even deep so they train very fast which is nice\n",
    "\n",
    "There are types of memory networks that definitely do incorporate deep architectures, but what we are going to find is that the simple models in this notebook actually work extremely well\n",
    "\n",
    "So we recall that we wanted to think about the principle of attention for this\n",
    "\n",
    "Its not exactly the same, but we will see why we made the comparison\n",
    "\n",
    "So suppose we are given a list of sentences, these sentences represent our story\n",
    "\n",
    "Each sentence, of course, contains a sequence of words, and each word, of course, can be converted into a word vector\n",
    "\n",
    "So what do we do ?\n",
    "\n",
    "Well we do the simple as possible thing, we actually totally ignore  the order of the words and just grab all the word vectors and sum them up\n",
    "\n",
    "This gives us one single vector to represent each sentence\n",
    "\n",
    "So for example, ```John went to the hallway```, we just get those five word vectors and we add them all up\n",
    "\n",
    "<img src='extras/37.2.PNG' width='600'></img>\n",
    "\n",
    "This sounds almost too simple to work, but it does\n",
    "\n",
    "---\n",
    "\n",
    "So now we have a very similar situation as we did with attention\n",
    "\n",
    "We have a vector for each sentence, now we want to figure out which sentence should we pay attention to, to determine the answer to the question\n",
    "\n",
    "And of course we are going to do that using weights\n",
    "\n",
    "How are we going to find these weights?, using the softmax of course\n",
    "\n",
    "\n",
    "<img src='extras/37.3.PNG' width='600'></img>\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Sentence weights</h3>\n",
    "\n",
    "So how do we find the weights for each sentence ?\n",
    "\n",
    "Well for memory networks, the process is actually much simpler than with attention\n",
    "\n",
    "The first step is to turn the question, which is also a sequence of words, into a sequence of vectors and then sum those vectors into one vector\n",
    "\n",
    "So its the same process we did for the story, but importantly the question is going to have its own embedding\n",
    "\n",
    "The next step is to just dot the question vector with the sentence vector and then take the softmax over all those outputs\n",
    "\n",
    "<img src='extras/37.4.PNG' width='600'></img>\n",
    "\n",
    "\n",
    "This is basically as simple as we can make it, dot and softmax and that gives us the score for each sentence\n",
    "\n",
    "---\n",
    "\n",
    "Now we might ask, why does that even make sense\n",
    "\n",
    "We recall that we often talk about dot product as a distance finder\n",
    "\n",
    "Importantly, its not a proper distance metric, as we define metrics pretty rigrously, but in works in general for telling us how similar two things are\n",
    "\n",
    "So if we recall the dot porduct is closely related to the cosine similarity, and these are exactly the same if the length of each of the vectors is 1\n",
    "\n",
    "<img src='extras/37.5.PNG' width='250'></img>\n",
    "\n",
    "And by the way, one important thing to note is that, the embeddings in memory networks are going to be initialised randomly and trained from scratch, since they need to have this special relationship to one another\n",
    "\n",
    "So we can think of it as, the question embedding is going to learn to be close to the sentence embedding if that sentence is relevant to the question\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Memory Network</h3>\n",
    "\n",
    "Ok, so at this point wat do we have\n",
    "\n",
    "<img src='extras/37.6.PNG' width='600'></img>\n",
    "\n",
    "we have turned each sentence in the story into a vector and now we have a weight for each of those vectors\n",
    "\n",
    "These weights depend on the question, so they are going to tell us, how relevant each sentence is for answering the question\n",
    "\n",
    "Now that we have these weights, we can apply them to the sentence vectors and get out a new vector, which we can more or less think of as a vector representing the relevant sentence\n",
    "\n",
    "So whats next ?\n",
    "\n",
    "Well as promised, this model is not going to be deep, so all we are going to do is pass this vector through a logistic regression, or in other words a single dense layer and apply a final softmax\n",
    "\n",
    "The size of the output is just the vocab size, and the answer is always going to be a single word\n",
    "\n",
    "So thats one disadvantage of memory networks, they cant respond to us in sentences\n",
    "\n",
    "So these neural networks are not going to be conversational agents or anything like that\n",
    "\n",
    "Its just purely reading comprehension\n",
    "\n",
    "But now that we have the output, we know exactly what to do\n",
    "\n",
    "Build the model object, compile it and train it, very simple\n",
    "\n",
    "---\n",
    "\n",
    "<h3>2 Supporting Fact</h3>\n",
    "\n",
    "The next thing we are going to talk about is the two supporting fact model\n",
    "\n",
    "The first question we want to ask is, why doesnot our existing model work for this problem?\n",
    "\n",
    "Well the problem is that the softmax is still only designed to pick one thing\n",
    "\n",
    "So we have two relevant sentences, and we have to make sure that we consider them in the right order\n",
    "\n",
    "For example, if we have\n",
    "\n",
    "```\n",
    "John goes to the kitchen\n",
    "John goes to the hallway\n",
    "Where is john?\n",
    "```\n",
    "\n",
    "The answer most certainly depends on which happened first and which happened after\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Strategy</h3>\n",
    "\n",
    "So whats our strategy going to be when we have two supporting facts ?\n",
    "\n",
    "Well in this case, since we know that there are two supporting facts, the answer is quite simple\n",
    "\n",
    "We just take the thing we did before for one supporting fact, and we do it twice\n",
    "\n",
    "<img src='extras/37.7.PNG' width='700'></img>\n",
    "\n",
    "So we see that these two blocks have the exact same structure\n",
    "\n",
    "We pass in the story, which as we recall is jsut a list of embeddings, one for each sentence, and then we weight them so we can answer which is the sentence  I should pay attention to\n",
    "\n",
    "Now of course we have to ask, how do we prevent both of these blocks from simply returning the same answer in both cases\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Strategy</h3>\n",
    "\n",
    "And this is where the memory network starts to look sort of recurrent\n",
    "\n",
    "<img src='extras/37.8.PNG' width='700'></img>\n",
    "\n",
    "For the first block we just pass in the question embedding to determine the weights\n",
    "\n",
    "But for the second block, we pass in the answer from the first block\n",
    "\n",
    "And by the way, in memory networks, we usually call these blocks <strong>hops</strong>\n",
    "\n",
    "So we have the first hop and the second hop\n",
    "\n",
    "And remember that all the vectors we have are the same size as the embedding dimension, so they can all be added, dotted, and so on\n",
    "\n",
    "So what we hope is going to happen here, is that the first block is going to pick the first relevant fact, that going to get passed into the second block, and the second block is going to pick the second relevant fact\n",
    "\n",
    "Again we add a final dense to the end to map the answer back to our vocabulary, and again the answer is just one word\n",
    "\n",
    "Once we have the output, we can instantiate our model object, compile it and train it as usual\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Small details</h3>\n",
    "\n",
    "So their are some small details that help the model perform much better in the two supporting facts scenario\n",
    "\n",
    "First is, we know that for each hop, we actually have two isntances where we are using the story embeddings\n",
    "\n",
    "The first is for creating the weights for each story line (to be dotted with question embedding)\n",
    "\n",
    "The second is for having vectors to actually represent each story line (what we multiply weights by later)\n",
    "\n",
    "So what helps is to actually use two different embeddings for these two different uses\n",
    "\n",
    "Now interestingly, what we can then do, is pass the second story embedding from the first hop to be used for creating the weights in the second hop\n",
    "\n",
    "So we dont need to double the number of embeddings for the number of hops\n",
    "\n",
    "This kind of makes sense because whatever representation we use for the story in the first hop, will probably be useful for determining which sentence to pay attention to in the second hop\n",
    "\n",
    "note that we could have done that for the single supporting fact model as well, but we already do pretty well on that task without it\n",
    "\n",
    "Another trick is to add a dense layer after getting a weighted sum of the story vectors\n",
    "\n",
    "So even though the vector stays the same size, which is just the embedding dimension, this seems to help by transforming the data into perhaps, something more useful for the next stage\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Model interpretation</h3>\n",
    "\n",
    "One interesting thing we can do, similar to attention, is ti print out the score for each sentence in the story\n",
    "\n",
    "This is going to tell us which sentence the model found to be most important\n",
    "\n",
    "So this can give us some confidence that the model is working correctly\n",
    "\n",
    "The nice thing about memory networks, is that because they are so shallow, training is extremely fast, even on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we begin by the single supporting fact story\n",
    "# then we go for the two supporting facts\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, Lambda, Reshape, add, dot, Activation\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import string\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first download the data from \n",
    "# https://research.fb.com/downloads/babi/\n",
    "# we are interested in bAbI tasks Data 1-20 (download links to the right of page)\n",
    "# then extract the file\n",
    "# the files we want are the single/two supporting fact in en-10K files\n",
    "\n",
    "single_train = [line.rstrip() for line in open('datasets\\\\bAbI\\\\tasks_1-20_v1-2\\\\en-10k\\\\qa1_single-supporting-fact_train.txt')]\n",
    "single_test = [line.rstrip() for line in open('datasets\\\\bAbI\\\\tasks_1-20_v1-2\\\\en-10k\\\\qa1_single-supporting-fact_test.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = sent.translate(str.maketrans('','',string.punctuation))\n",
    "    tokens = sent.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we define a function to get data\n",
    "# this returns a list data = [(story,question,answer)]\n",
    "\n",
    "def gen_story_q_a(lines):\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line_no,line = line.split(' ',1) \n",
    "        if line_no == '1': # number line is 1 so thats a new story\n",
    "            story = []\n",
    "            \n",
    "        if '\\t' in line : # so this line has a question and an answer\n",
    "            # we have question,answer, and the number of the line that has the answer\n",
    "            # we ignore the last\n",
    "            q,a,_ = line.split('\\t')\n",
    "            # now we tokenise question\n",
    "            q = tokenise(q)\n",
    "            # the answer is always just one word, no need to tokenise\n",
    "            # now we want to add the story up till now to the stories\n",
    "            # this question to the questions\n",
    "            # and this answer to the answer\n",
    "            # recall that in the beggining of the for loop we removed the line num\n",
    "            # so we need to reconstruct our story so that it includes line numbers\n",
    "            story_till_now = [[str(i)] + sent for i,sent in enumerate(story)]\n",
    "            data.append((story_till_now,q,a))\n",
    "            story.append([''])\n",
    "        \n",
    "        else: # so this is just a part of the story\n",
    "            story.append(tokenise(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gen_story_q_a(single_train)\n",
    "test_data = gen_story_q_a(single_test)\n",
    "\n",
    "# take a copy for debugging later\n",
    "test_data_words = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we want to get the vocabulary so that we can form a word2idx dict\n",
    "# we also want to get the max_seq_length for our data to provide padding\n",
    "all_data = train_data + test_data\n",
    "story_words = [w for story,_,_ in all_data for line in story for w in line]\n",
    "question_words = [w for _,q,_ in all_data for w in q]\n",
    "answer_words = [a for _,_,a in all_data]\n",
    "all_words = story_words + question_words + answer_words\n",
    "# lets add out pad token too\n",
    "# in case the test data has a token not in train, we have the none token\n",
    "# these are basically line numbers when a story in test is longer than the longest in train\n",
    "all_words = ['<PAD>'] + list(set(all_words))\n",
    "V = len(all_words)\n",
    "# now we create our word2idx\n",
    "word2idx = {w:i for i,w in enumerate(all_words)}\n",
    "\n",
    "# now again we need to get the max length for padding\n",
    "\n",
    "# note that for the stories we have pad in two dimensions\n",
    "\n",
    "# first we ask what is the length of the longest line\n",
    "max_line_length = max([len(line) for story,_,_ in all_data for line in story])\n",
    "# we also need to ask what is the max number of lines in a stroy\n",
    "max_story_length = max([len(story) for story,_,_ in all_data])\n",
    "# and we want to know max len of each question\n",
    "max_question_length = max([len(q) for _,q,_ in all_data])\n",
    "# again, the answer is just one word so no need to pad\n",
    "# now we are ready to vectorise our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(sample):\n",
    "    story,q,a = sample\n",
    "    # first change words to indicies\n",
    "    story = [[word2idx[w] for w in line] for line in story]\n",
    "    q = [word2idx[w] for w in q]\n",
    "    a = np.array(word2idx[a])\n",
    "    # next is padding\n",
    "    # the story is of shape NxT , N:sentences, T:words\n",
    "    # we need to pad both these dimensions\n",
    "    # as for the N dimesnion we add lines\n",
    "    story += [[0]*max_line_length]*(max_story_length-len(story))\n",
    "    # as for the T dimension, Tensorflow can handle this\n",
    "    story = pad_sequences(story,maxlen=max_line_length)\n",
    "    # question has only one dimension T\n",
    "    q = pad_sequences([q],maxlen=max_question_length)\n",
    "    # now we are done\n",
    "    return (story,q[0],a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets vectorise our data and stack them into matricies\n",
    "train_data = [vectorise(sample) for sample in train_data]\n",
    "test_data = [vectorise(sample) for sample in test_data]\n",
    "\n",
    "# now stack\n",
    "train_stories = np.stack([story for story,_,_ in train_data])\n",
    "train_questions = np.stack([q for _,q,_ in train_data])\n",
    "train_answers = np.stack([a for _,_,a in train_data])\n",
    "\n",
    "# again for test\n",
    "test_stories = np.stack([story for story,_,_ in test_data])\n",
    "test_questions = np.stack([q for _,q,_ in test_data])\n",
    "test_answers = np.stack([a for _,_,a in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are ready to make our model\n",
    "# we take in 2 inputs , story , questions\n",
    "# the answer is our target\n",
    "input_stories = Input(shape=(train_stories.shape[1:]))\n",
    "input_questions = Input(shape=(train_questions.shape[1:]))\n",
    "\n",
    "# next we create and embedding layer for input\n",
    "embedding_stories = Embedding(V,EMBEDDING_DIM)\n",
    "\n",
    "# next we pass in the input stories through embedding to get words vectors\n",
    "x_stories = embedding_stories(input_stories) # shape = N x Stories x lines x Embeddings\n",
    "# then for each line, sum all vectors\n",
    "x_stories = Lambda(lambda x:K.sum(x,axis=2))(x_stories)\n",
    "\n",
    "# next, we do the same for the question\n",
    "embedding_questions = Embedding(V,EMBEDDING_DIM)\n",
    "x_question = embedding_questions(input_questions)\n",
    "x_question = Lambda(lambda x:K.sum(x,axis=1))(x_question)\n",
    "\n",
    "# next we dot the story with the question\n",
    "story_question = dot([x_stories,x_question],axes=-1)\n",
    "# now softmax to get weights\n",
    "softmax = Activation('softmax')\n",
    "weights = softmax(story_question)\n",
    "\n",
    "# now again, dot the sentences with the weights\n",
    "story_weights = dot([x_stories,weights],axes=[1,-1])\n",
    "# pass through a final dense layer\n",
    "dense = Dense(V,activation='softmax')\n",
    "output = dense(story_weights)\n",
    "\n",
    "model = Model([input_stories,input_questions],output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we compile our model\n",
    "model.compile(\n",
    "  optimizer=RMSprop(lr=1e-2),\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 [==============================] - 4s 10ms/step - loss: 1.5122 - accuracy: 0.4235 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0094 - accuracy: 0.9973 - val_loss: 0.0044 - val_accuracy: 0.9980\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0063 - accuracy: 0.9986 - val_loss: 0.0036 - val_accuracy: 0.9980\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0044 - accuracy: 0.9993 - val_loss: 3.0171e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.0364e-06 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "r = model.fit(\n",
    "  [train_stories, train_questions],\n",
    "  train_answers,\n",
    "  epochs=5,\n",
    "  batch_size=32,\n",
    "  validation_data=([test_stories, test_questions], test_answers)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WOOOOOOOOOOOOOW\n",
    "# we dont even need to make a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we can do is print the weights\n",
    "# so we can see if the model looks for the correct sentence\n",
    "idx2word = {v:k for k,v in word2idx.items()}\n",
    "model_weights = Model([input_stories,input_questions],[output,weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 \t 0 mary went back to the kitchen\n",
      "0.0 \t 1 john went to the bathroom\n",
      "0.0 \t 2 \n",
      "1.0 \t 3 daniel travelled to the office\n",
      "0.0 \t 4 sandra journeyed to the bathroom\n",
      "where is daniel\n",
      "answer :  office  model answer :  office\n",
      "--------------------------\n",
      "Another story? [Y/n] : Y\n",
      "1.0 \t 0 sandra moved to the bedroom\n",
      "0.0 \t 1 mary went back to the kitchen\n",
      "where is sandra\n",
      "answer :  bedroom  model answer :  bedroom\n",
      "--------------------------\n",
      "Another story? [Y/n] : Y\n",
      "0.0 \t 0 john travelled to the hallway\n",
      "0.0 \t 1 john travelled to the bathroom\n",
      "0.0 \t 2 \n",
      "0.0 \t 3 sandra moved to the kitchen\n",
      "0.0 \t 4 john journeyed to the bedroom\n",
      "0.0 \t 5 \n",
      "0.03 \t 6 daniel journeyed to the hallway\n",
      "0.0 \t 7 john journeyed to the bathroom\n",
      "0.0 \t 8 \n",
      "0.96 \t 9 daniel moved to the bedroom\n",
      "0.0 \t 10 mary went to the bathroom\n",
      "0.0 \t 11 \n",
      "0.0 \t 12 mary went back to the bedroom\n",
      "0.01 \t 13 sandra went to the bathroom\n",
      "where is daniel\n",
      "answer :  bedroom  model answer :  bedroom\n",
      "--------------------------\n",
      "Another story? [Y/n] : Y\n",
      "0.04 \t 0 sandra travelled to the hallway\n",
      "0.96 \t 1 sandra went to the garden\n",
      "where is sandra\n",
      "answer :  garden  model answer :  garden\n",
      "--------------------------\n",
      "Another story? [Y/n] : Y\n",
      "0.0 \t 0 mary went to the kitchen\n",
      "1.0 \t 1 john went back to the kitchen\n",
      "where is john\n",
      "answer :  kitchen  model answer :  kitchen\n",
      "--------------------------\n",
      "Another story? [Y/n] : Y\n",
      "0.0 \t 0 john journeyed to the garden\n",
      "0.0 \t 1 sandra journeyed to the office\n",
      "0.0 \t 2 \n",
      "0.0 \t 3 mary journeyed to the bathroom\n",
      "0.0 \t 4 daniel went to the hallway\n",
      "0.0 \t 5 \n",
      "0.0 \t 6 mary went back to the kitchen\n",
      "0.0 \t 7 mary went back to the bedroom\n",
      "0.0 \t 8 \n",
      "0.0 \t 9 mary went back to the bathroom\n",
      "0.0 \t 10 john went back to the kitchen\n",
      "0.0 \t 11 \n",
      "0.0 \t 12 john moved to the hallway\n",
      "1.0 \t 13 daniel moved to the kitchen\n",
      "where is daniel\n",
      "answer :  kitchen  model answer :  kitchen\n",
      "--------------------------\n",
      "Another story? [Y/n] : Y\n",
      "0.0 \t 0 daniel travelled to the hallway\n",
      "1.0 \t 1 john moved to the office\n",
      "where is john\n",
      "answer :  office  model answer :  office\n",
      "--------------------------\n",
      "Another story? [Y/n] : n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    i = np.random.choice(len(test_data_words))\n",
    "    story_words,q_words,a_words = test_data_words[i]\n",
    "    \n",
    "    story = test_stories[i:i+1]\n",
    "    q = test_questions[i:i+1]\n",
    "    answer,weights = model_weights([story,q])\n",
    "    weights = weights[0]\n",
    "    for i in range(len(story_words)):\n",
    "        print(np.round(weights[i].numpy(),2),'\\t',' '.join(story_words[i]))\n",
    "    print(' '.join(q_words))\n",
    "    print('answer : ',a_words,' model answer : ',idx2word[np.argmax(answer)])\n",
    "    print('--------------------------')\n",
    "    resp = input('Another story? [Y/n] : ')\n",
    "    if resp.lower().startswith('n'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can clearly see that our model recognises the relevant sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for two supporting facts\n",
    "# the data preprocessing is the same, so we will cal lthe above functions\n",
    "\n",
    "two_train = [line.rstrip() for line in open('datasets\\\\bAbI\\\\tasks_1-20_v1-2\\\\en-10k\\\\qa2_two-supporting-facts_train.txt')]\n",
    "two_test = [line.rstrip() for line in open('datasets\\\\bAbI\\\\tasks_1-20_v1-2\\\\en-10k\\\\qa2_two-supporting-facts_test.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gen_story_q_a(two_train)\n",
    "test_data = gen_story_q_a(two_test)\n",
    "test_data_words = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = train_data + test_data\n",
    "story_words = [w for story,_,_ in all_data for line in story for w in line]\n",
    "question_words = [w for _,q,_ in all_data for w in q]\n",
    "answer_words = [a for _,_,a in all_data]\n",
    "\n",
    "all_words = story_words + question_words + answer_words\n",
    "all_words = ['<PAD>'] + list(set(all_words))\n",
    "V = len(all_words)\n",
    "word2idx = {w:i for i,w in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_line_length = max([len(line) for story,_,_ in all_data for line in story])\n",
    "max_story_length = max([len(story) for story,_,_ in all_data])\n",
    "max_question_length = max([len(q) for _,q,_ in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [vectorise(sample) for sample in train_data]\n",
    "test_data = [vectorise(sample) for sample in test_data]\n",
    "\n",
    "train_stories = np.stack([story for story,_,_ in train_data])\n",
    "train_questions = np.stack([q for _,q,_ in train_data])\n",
    "train_answers = np.stack([a for _,_,a in train_data])\n",
    "\n",
    "test_stories = np.stack([story for story,_,_ in test_data])\n",
    "test_questions = np.stack([q for _,q,_ in test_data])\n",
    "test_answers = np.stack([a for _,_,a in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for the model\n",
    "\n",
    "# we take in 2 inputs , story , questions\n",
    "# the answer is our target\n",
    "input_stories = Input(shape=(train_stories.shape[1:]))\n",
    "input_questions = Input(shape=(train_questions.shape[1:]))\n",
    "\n",
    "# next we create and embedding layer for input\n",
    "embedding_stories = Embedding(V,EMBEDDING_DIM)\n",
    "\n",
    "# next we pass in the input stories through embedding to get words vectors\n",
    "x_stories = embedding_stories(input_stories) # shape = N x Stories x lines x Embeddings\n",
    "# then for each line, sum all vectors\n",
    "x_stories = Lambda(lambda x:K.sum(x,axis=2))(x_stories)\n",
    "\n",
    "# next, we do the same for the question\n",
    "embedding_questions = Embedding(V,EMBEDDING_DIM)\n",
    "x_question = embedding_questions(input_questions)\n",
    "x_question = Lambda(lambda x:K.sum(x,axis=1))(x_question)\n",
    "\n",
    "story_question_1 = dot([x_stories,x_question],axes=-1)\n",
    "# now softmax to get weights\n",
    "softmax = Activation('softmax')\n",
    "weights_1 = softmax(story_question_1)\n",
    "\n",
    "# the difference begins here\n",
    "# now we introduce the first hop\n",
    "embedding_hop_1 = Embedding(V,EMBEDDING_DIM)\n",
    "x_stories_hop_1 = embedding_hop_1(input_stories)\n",
    "x_stories_hop_1 = Lambda(lambda x:K.sum(x,axis=2))(x_stories_hop_1)\n",
    "\n",
    "\n",
    "# now again, dot the new embedded stories with the weights\n",
    "story_weights_1 = dot([x_stories_hop_1,weights_1],axes=[1,-1])\n",
    "# pass through a dense layer (shared between hops)\n",
    "dense = Dense(EMBEDDING_DIM,activation='elu')\n",
    "output_hop_1 = dense(story_weights_1) # hop 2 treats this as the question\n",
    "# now for hop 2\n",
    "# hop 2 takes x_stories as story\n",
    "# and output_hop_1 as question\n",
    "# from there, everything is the same\n",
    "\n",
    "# so we dot the sentences and the question then take softmax to get the weigths\n",
    "story_question_2 = dot([x_stories,output_hop_1],axes=-1)\n",
    "# now softmax to get weights\n",
    "weights_2 = softmax(story_question_2)\n",
    "\n",
    "embedding_hop_2 = Embedding(V,EMBEDDING_DIM)\n",
    "x_stories_hop_2 = embedding_hop_2(input_stories)\n",
    "x_stories_hop_2 = Lambda(lambda x:K.sum(x,axis=2))(x_stories_hop_2)\n",
    "\n",
    "# now again, dot the new embedded stories with the weights\n",
    "story_weights_2 = dot([x_stories_hop_2,weights_2],axes=[1,-1])\n",
    "# pass through the dense layer\n",
    "output_hop_2 = dense(story_weights_2)\n",
    "\n",
    "# now create final dense layer\n",
    "final_dense = Dense(V,activation='softmax')\n",
    "output = final_dense(output_hop_2)\n",
    "model = Model([input_stories,input_questions],output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=RMSprop(lr=5e-3),\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "313/313 [==============================] - 5s 13ms/step - loss: 2.0114 - accuracy: 0.1868 - val_loss: 1.5078 - val_accuracy: 0.3790\n",
      "Epoch 2/30\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.2627 - accuracy: 0.4869 - val_loss: 0.8433 - val_accuracy: 0.6900\n",
      "Epoch 3/30\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6788 - accuracy: 0.7470 - val_loss: 0.6826 - val_accuracy: 0.7370\n",
      "Epoch 4/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.5140 - accuracy: 0.8108 - val_loss: 0.5331 - val_accuracy: 0.8080\n",
      "Epoch 5/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.4167 - accuracy: 0.8495 - val_loss: 0.4697 - val_accuracy: 0.8210\n",
      "Epoch 6/30\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.3466 - accuracy: 0.8805 - val_loss: 0.4146 - val_accuracy: 0.8580\n",
      "Epoch 7/30\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.3190 - accuracy: 0.8911 - val_loss: 0.4612 - val_accuracy: 0.8520\n",
      "Epoch 8/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.2817 - accuracy: 0.9073 - val_loss: 0.4287 - val_accuracy: 0.8510\n",
      "Epoch 9/30\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2514 - accuracy: 0.9235 - val_loss: 0.3234 - val_accuracy: 0.8790\n",
      "Epoch 10/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.2288 - accuracy: 0.9302 - val_loss: 0.3523 - val_accuracy: 0.8870\n",
      "Epoch 11/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.2024 - accuracy: 0.9369 - val_loss: 0.3535 - val_accuracy: 0.8900\n",
      "Epoch 12/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.2191 - accuracy: 0.9359 - val_loss: 0.3673 - val_accuracy: 0.8960\n",
      "Epoch 13/30\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1881 - accuracy: 0.9439 - val_loss: 0.3352 - val_accuracy: 0.9040\n",
      "Epoch 14/30\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1787 - accuracy: 0.9490 - val_loss: 0.3214 - val_accuracy: 0.8960\n",
      "Epoch 15/30\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1616 - accuracy: 0.9527 - val_loss: 0.3361 - val_accuracy: 0.9020\n",
      "Epoch 16/30\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1577 - accuracy: 0.9554 - val_loss: 0.3358 - val_accuracy: 0.9190\n",
      "Epoch 17/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1763 - accuracy: 0.9517 - val_loss: 0.3162 - val_accuracy: 0.9120\n",
      "Epoch 18/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1513 - accuracy: 0.9602 - val_loss: 0.3482 - val_accuracy: 0.9090\n",
      "Epoch 19/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1528 - accuracy: 0.9592 - val_loss: 0.4170 - val_accuracy: 0.8900\n",
      "Epoch 20/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1733 - accuracy: 0.9575 - val_loss: 0.4075 - val_accuracy: 0.8940\n",
      "Epoch 21/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1440 - accuracy: 0.9631 - val_loss: 0.4614 - val_accuracy: 0.8780\n",
      "Epoch 22/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1460 - accuracy: 0.9619 - val_loss: 0.4610 - val_accuracy: 0.8830\n",
      "Epoch 23/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1312 - accuracy: 0.9662 - val_loss: 0.4375 - val_accuracy: 0.8920\n",
      "Epoch 24/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1599 - accuracy: 0.9600 - val_loss: 0.3962 - val_accuracy: 0.9030\n",
      "Epoch 25/30\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.1438 - accuracy: 0.9665 - val_loss: 0.4684 - val_accuracy: 0.8940\n",
      "Epoch 26/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1513 - accuracy: 0.9598 - val_loss: 0.5032 - val_accuracy: 0.8870\n",
      "Epoch 27/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1314 - accuracy: 0.9661 - val_loss: 0.4330 - val_accuracy: 0.8950\n",
      "Epoch 28/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1384 - accuracy: 0.9684 - val_loss: 0.4878 - val_accuracy: 0.8920\n",
      "Epoch 29/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1241 - accuracy: 0.9696 - val_loss: 0.3790 - val_accuracy: 0.9010\n",
      "Epoch 30/30\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1260 - accuracy: 0.9698 - val_loss: 0.3965 - val_accuracy: 0.9140\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "r = model.fit(\n",
    "  [train_stories, train_questions],\n",
    "  train_answers,\n",
    "  epochs=30,\n",
    "  batch_size=32,\n",
    "  validation_data=([test_stories, test_questions], test_answers)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy\n",
    "plt.plot(r.history['accuracy'], label='acc')\n",
    "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again lets try to take a look at the weights\n",
    "idx2word = {v:k for k,v in word2idx.items()}\n",
    "model_weights = Model([input_stories,input_questions],[output,weights_1,weights_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 \t 0.0 \t 0 john went to the bathroom\n",
      "0.0 \t 0.0 \t 1 john journeyed to the kitchen\n",
      "0.0 \t 0.0 \t 2 mary travelled to the hallway\n",
      "0.0 \t 0.0 \t 3 john got the football there\n",
      "0.0 \t 0.0 \t 4 sandra journeyed to the bathroom\n",
      "0.0 \t 0.0 \t 5 sandra travelled to the bedroom\n",
      "0.0 \t 0.0 \t 6 john travelled to the bedroom\n",
      "0.0 \t 0.0 \t 7 sandra moved to the office\n",
      "0.0 \t 0.0 \t 8 \n",
      "0.0 \t 0.0 \t 9 john went back to the kitchen\n",
      "0.0 \t 0.03 \t 10 mary journeyed to the garden\n",
      "0.0 \t 0.0 \t 11 john journeyed to the bathroom\n",
      "0.0 \t 0.0 \t 12 john travelled to the hallway\n",
      "0.0 \t 0.0 \t 13 mary took the apple there\n",
      "0.0 \t 0.0 \t 14 mary dropped the apple\n",
      "0.0 \t 0.0 \t 15 \n",
      "0.0 \t 0.0 \t 16 mary took the apple there\n",
      "0.0 \t 0.0 \t 17 daniel moved to the hallway\n",
      "0.0 \t 0.97 \t 18 mary went back to the office\n",
      "0.0 \t 0.0 \t 19 john dropped the football\n",
      "0.0 \t 0.0 \t 20 \n",
      "1.0 \t 0.0 \t 21 mary dropped the apple\n",
      "0.0 \t 0.0 \t 22 sandra journeyed to the garden\n",
      "where is the apple\n",
      "answer :  office  model answer :  office\n",
      "--------------------------\n",
      "Another story? [Y/n] : Y\n",
      "0.0 \t 0.0 \t 0 john went to the bathroom\n",
      "0.0 \t 0.0 \t 1 john journeyed to the kitchen\n",
      "0.0 \t 0.0 \t 2 mary travelled to the hallway\n",
      "1.0 \t 0.0 \t 3 john got the football there\n",
      "0.0 \t 0.0 \t 4 sandra journeyed to the bathroom\n",
      "0.0 \t 0.0 \t 5 sandra travelled to the bedroom\n",
      "0.0 \t 1.0 \t 6 john travelled to the bedroom\n",
      "0.0 \t 0.0 \t 7 sandra moved to the office\n",
      "where is the football\n",
      "answer :  bedroom  model answer :  bedroom\n",
      "--------------------------\n",
      "Another story? [Y/n] : Y\n",
      "0.0 \t 0.0 \t 0 daniel went to the office\n",
      "0.03 \t 0.0 \t 1 mary took the apple there\n",
      "0.0 \t 1.0 \t 2 mary went back to the garden\n",
      "0.97 \t 0.0 \t 3 mary left the apple\n",
      "0.0 \t 0.0 \t 4 \n",
      "0.0 \t 0.0 \t 5 sandra moved to the bathroom\n",
      "0.0 \t 0.0 \t 6 john journeyed to the hallway\n",
      "where is the apple\n",
      "answer :  garden  model answer :  garden\n",
      "--------------------------\n",
      "Another story? [Y/n] : n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    i = np.random.choice(len(test_data_words))\n",
    "    story_words,q_words,a_words = test_data_words[i]\n",
    "    \n",
    "    story = test_stories[i:i+1]\n",
    "    q = test_questions[i:i+1]\n",
    "    answer,weights_1,weights_2 = model_weights([story,q])\n",
    "    weights_1 = weights_1[0]\n",
    "    weights_2 = weights_2[0]\n",
    "    for i in range(len(story_words)):\n",
    "        print(np.round(weights_1[i].numpy(),2),'\\t',np.round(weights_2[i].numpy(),2),'\\t',' '.join(story_words[i]))\n",
    "    print(' '.join(q_words))\n",
    "    print('answer : ',a_words,' model answer : ',idx2word[np.argmax(answer)])\n",
    "    print('--------------------------')\n",
    "    resp = input('Another story? [Y/n] : ')\n",
    "    if resp.lower().startswith('n'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we see we get decent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
