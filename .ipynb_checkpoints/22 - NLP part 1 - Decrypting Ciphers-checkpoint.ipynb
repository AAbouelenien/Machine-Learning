{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very intersting beginning for NLP , as mentioned in the title we will be decrypting ciphers , in the process we will be introduced to some important concepts in NLP\n",
    "\n",
    "we encode a message by substitutung each letter with a different letter , then we use the very same mapping to decrypt the message , now our goal is to decrypt the message without having the key , we will see how can probability play a role in this\n",
    "\n",
    "before we proceed lets take an example , lets say this is our map (key)\n",
    "\n",
    "<img src='extras/22.1.PNG'></img>\n",
    "\n",
    "now the Sender wants to encrypt the message : \"I LIKE CODING\"\n",
    "\n",
    "The substitutions are\n",
    "\n",
    "$I \\rightarrow Y , L \\rightarrow W , I \\rightarrow Y , K \\rightarrow R , E \\rightarrow N , C \\rightarrow J , O \\rightarrow G , D \\rightarrow S , I \\rightarrow Y , N \\rightarrow V , G \\rightarrow I $\n",
    "\n",
    "So encrpyted message : \"Y WYRN JGSYVI\"\n",
    "\n",
    "now the receiver uses the same key to decrypt message , he received message is \"Y WYRN JGYVYI\"\n",
    "\n",
    "the <strong>Reverse</strong> Substitution :\n",
    "\n",
    "$Y \\rightarrow I , W \\rightarrow L , Y \\rightarrow I , R \\rightarrow K , N \\rightarrow E , J \\rightarrow C , G \\rightarrow O , S \\rightarrow D , Y \\rightarrow I , V \\rightarrow N , I \\rightarrow G $\n",
    "\n",
    "the decrypted message again is \"I LIKE CODING\"\n",
    "\n",
    "again we want to decrypt the message without the key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first concept we discuss is Language Modelling\n",
    "\n",
    "Intuitively we want to build a model that assigns high probability to real words/sentences and low probability to unreal words/sentences , so for ex :\n",
    "\n",
    "model(\"I LIKE CODING\") $\\rightarrow$ big number (argumentative :) )\n",
    "\n",
    "model(\"Y WYRN JLOB\") $\\rightarrow$ small number\n",
    "\n",
    "The idea is that , if we decrypt the message correctly , our model should return a high probability , if the message is decrypted incorrectly , then we yeild a low probability\n",
    "\n",
    "The concepts we need now to build such model are N-grams and Markov Models\n",
    "\n",
    "<h3>N-gram</h3>\n",
    "\n",
    "N-gram refers to a sequence of N tokens \n",
    "\n",
    "tokens usually reger to individual words , but for this example they mean individual letters\n",
    "\n",
    "we usually work with small values of N  \n",
    "\n",
    "N=1 : Unigram : we consider individual letters by themselves\n",
    "\n",
    "N=2 : Bigram : we consider sequences of 2 letters\n",
    "\n",
    "N=3 : Trigram : we consider sequences of 3 letters\n",
    "\n",
    "<img src='extras/22.2.PNG' width='300'></img>\n",
    "\n",
    "for our example we will stick with bigrams and unigrams , to complete the picture we move on to Markov Models\n",
    "\n",
    "<h3>Markov Model</h3>\n",
    "\n",
    "The Markov Model is a probability model that states that the current stat $x_t$ depends only on the $x_{t-1}$ , not on any other previous state $x_{t-2}$ , $x_{t-3}$ , ...\n",
    "\n",
    "This is called MArkov assumption , probably false , but is still useful\n",
    "\n",
    "$$p(x_t|x_{t-1},x_{t-2},...) = p(x_t|x_{t-1})$$\n",
    "\n",
    "In our case , a state refers to a letter , so we want to know how often does one letter follow another letter (possibly the same letter) , the markov assumes that each letter depends only on the previous letter and not any other \n",
    "\n",
    "lets take an example \n",
    "\n",
    "consider the word \"CAT\" , we want to know :\n",
    "\n",
    "p(A|C) = how often does \"A\" follow \"C\" ?\n",
    "\n",
    "p(T|A) = how often \"T\" follows \"A\"\n",
    "\n",
    "given only the word (\"CAT\") , the probabilities are 100%\n",
    "\n",
    "Imagine we now have entire corpus of data (a set of text to learn from) , for ex all the text on Wikipidea\n",
    "\n",
    "Now we would be able to find bigram probabilities for every combination of letters AA,AB,AC, etc.\n",
    "\n",
    "In general :\n",
    "\n",
    "$$p(x_t|x_{t-1}) = \\frac{count\\{x_{t-1} \\rightarrow x_t\\}}{count\\{x_{t-1}\\}}$$\n",
    "\n",
    "That is the number of times the letter at position t follows the letter at position t-1 divide the number of times the letter at position t-1 appears in total , so all we need to do is <strong>count</strong>\n",
    "\n",
    "so for example :\n",
    "\n",
    "$$ p(A|C) = \\frac{\\# \\ of \\ times \\ \"CA\" \\  appears \\ in \\ dataset}{\\# \\ of \\ times \\ \"C\" \\ appears \\ in \\ dataset}$$\n",
    "\n",
    "Now how many Bigram probabilities do we have to calculate if we 26 letters in alphabet ?\n",
    "\n",
    "If we have V leeters in alhpabet , then we have $V^2$ bigrams , in english for example , we need to consider the following :\n",
    "\n",
    "<ul>\n",
    "<li>AA,AB,AC,...,AZ</li>\n",
    "<li>BA,BB,BC,...,BZ</li>\n",
    "<li>...</li>\n",
    "<li>ZA,ZB,ZC,...,ZZ</li>    \n",
    "</ul>\n",
    "\n",
    "so we have 26*26 = 676 different probabilities\n",
    "\n",
    "ok so now that we have this bigram model how can we use it ?\n",
    "\n",
    "Eventually we want to get the probability of a sentence , a sentence is composed of words , so it would help if we know how to get the probability of a word\n",
    "\n",
    "first we recall the rule of conditional probability :\n",
    "\n",
    "$$p(AB) = p(B|A)p(A)$$\n",
    "\n",
    "p(AB) : joint probability , the probability of sequnce AB\n",
    "\n",
    "p(B|A) : Conditional Bigram  , probability of B follows A\n",
    "\n",
    "p(A) : Marginal Unigram , probability of A appearing by itself\n",
    "\n",
    "what if we see a 3-letter word , we just have to use the chain rule of probability (that is use the previous rule in succession) :\n",
    "\n",
    "\n",
    "$ p(ABC) = p(C|AB)p(B|A)p(A)$\n",
    "\n",
    "\n",
    "$p(C|AB) = p(C|B)$ thanks to markov assumption !\n",
    "\n",
    "$p(ABC) = p(C|B)p(B|A)p(A)$\n",
    "\n",
    "If we want to extend this to any word of length T :\n",
    "\n",
    "$$p(x_1,x_2,...,x_T) = p(x_1)\\prod^T_{t=2} p(x_t|x_{t-1})$$\n",
    "\n",
    "A sentence is a sequence of multiple words , since words are seperate , we can consider the probability of each word sepearately then multiply all these together\n",
    "\n",
    "\n",
    "let $w_n$ be the nth word in a sentence of length N (n = 1...N)\n",
    "\n",
    "$T(n)$ = length of n'th word\n",
    "\n",
    "$x_t^{(n)}$ = letter in word n at position t\n",
    "\n",
    "$$p(w_1,w_2,...,w_N) = \\prod^N_{n=1} p(x_1^{(n)}) \\prod_{t=1}^{T(n)} p(x_T^{(n)} | x_{t-1}^{(n)})$$\n",
    "\n",
    "$$w_n = \\{x_1^{(n)},x_2^{(n)},...,x_{T(n)}^{(n)}\\}$$\n",
    "\n",
    "Note that our model is a simple one in which we consider only letters , a more complex model may also consider words alongside letters since words also follow a logical sequence , for our model a sentence \"CAT CAT CAT\" would be highly probable even if logically we cant have that sentence \n",
    "\n",
    "Next we consider an important modification Add-one Smoothing\n",
    "\n",
    "consider a rare bigram that never appears in the training corpus but does appear in the message , then the probability of the bigram is zero , this is problematic since our sentence model is a product of individual probabilities and whenever we multiply by 0 the whole thing becomes 0 , which doesnot give us a good indication of whether the sentence as a whole is probable or not \n",
    "\n",
    "so we use Add-One Smoothing , in this we give every possible bigram a small chance of occuring pretending it appeared at least once in training corpus\n",
    "\n",
    "Mathematically , we add one to the numerator and V to denominator so that probabilities sum up to 1\n",
    "\n",
    "$$p(x_t|x_{t-1}) = \\frac{count\\{x_{t-1} \\rightarrow x_t\\}+1}{count\\{x_{t-1}\\}+V}$$\n",
    "\n",
    "now lets check that probabilities do sum up to 1 after Add-One Smoothing\n",
    "\n",
    "first assume we are in state $x_{t-1}$ , what we want to show that the sum of probabilities of every possible next letter given $x_{t-1}$ = 1 \n",
    "\n",
    "$$\\sum^V_{v=1} p(v|x_{t-1}) = \\sum^V_{v=1} \\frac{count(x_{t-1} \\rightarrow v) + 1}{count(x_{t-1})+V}$$\n",
    "\n",
    "the denominator is independant of $v$ so we can get it out of summation\n",
    "\n",
    "$$\\frac{1}{count(x_t-1)+V} \\sum^{V}_{v=1} \\{ count(x_{t-1} \\rightarrow v) + 1\\}$$\n",
    "\n",
    "the term $\\sum\\limits^{V}_{v=1}  count(x_{t-1} \\rightarrow v) $ is basically how many times $x_{t-1}$ occured , so we can simplify this further to\n",
    "\n",
    "$$\\frac{1}{count(x_{t-1})+V} \\{ count(x_{t-1}  + V\\}$$\n",
    "\n",
    "$$=1$$\n",
    "\n",
    "Now how do we use this model for decryption :\n",
    "\n",
    "First we find individual bigram/unigram probabilities using a large text corpus\n",
    "\n",
    "Then we can calculate the probability of any sentence of words\n",
    "\n",
    "If our decryption is incorrect ex : \"G BGWQ LRSGNP\" then we get a low probability\n",
    "\n",
    "If our decryption is correct : \"I LIKE CODING\" then we get a high probability\n",
    "\n",
    "Then we can use this to search for translation/decryption that yeilds maximum likelihood (we wil see how later)\n",
    "\n",
    "One problem is that probabilities are small , asuume the probability of each transition = 0.1 , then a 100-character long sentence would have a probability of $10^{-100}$ , a very small number that can get rounded down to zero in computer , but if all get rounded to zero how can we find the maximum ?\n",
    "\n",
    "The solution to this is to take the log-likelihood instead of the raw likelihood\n",
    "\n",
    "$$log p(x_1,x_2,...,x_T) = logp(x_1) + \\sum^T_{t=2} logp(x_t|x_{t-1})$$\n",
    "\n",
    "This works since log is a monotonically increasing function , and of course the multiplication becomes a sumation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to consider how we will approach this problem , here is a psuedocode which might help\n",
    "\n",
    "```python\n",
    "decryption_maps = get_all_possible_decryption_maps()\n",
    "best_log_likelihodd = -inf\n",
    "best_message = None\n",
    "\n",
    "for map in decryption_maps:\n",
    "    message = decode(encrypted_message,map)\n",
    "    log_likelihood = caluclate_log_likelihhod(message)\n",
    "    if log_likelihood > best_log_likelihood:\n",
    "        best_log_likelihood = log_likelihood\n",
    "        best_message = message\n",
    "print(\"Final message:\",best_message)\n",
    "```\n",
    "\n",
    "so we get all possible decryption maps and use each of them to get the decoded message , the message which yeild the highest log-likelioohd is the best message , but\n",
    "the number of all possible maps are  $26! = 4\\times10^{26}$ which is infeasable so we need another method\n",
    "\n",
    "<h3>Genetic Algorithms</h3>\n",
    "\n",
    "skipping a lot of nonsense about the myth (aka evolution see <a href=\"https://www.youtube.com/playlist?list=PL56IcDjrf3YJr__TEOJ2UOv3jCzht1_yc\">Arabic</a> or <a href=\"https://www.youtube.com/playlist?list=PLPqH38Ki1fy3EB-8xmShVqpbQw99Do2B-\">English</a>) here is what we will do\n",
    "\n",
    "we imagine our decryption map as being a DNA string , for simplicity say our alphabet has only 4 letters A,T,C,G (which is the genetic code)  , a DNA sytring may be \"ATCG\" , then 'mutation' can occur to this DNA , these are substitution , so \"ATCG\" $\\rightarrow$ \"AACG\" , Insertions \"ATCG\" $\\rightarrow$ \"AATCG\" and Deletions \"ATCG\" $\\rightarrow$ \"TCG\" , Substitution may cause a letter to be repeated twice so not allowed , insertions and deletions are unallowed since the map needs to be of fixed length , so what we need to do is swapping\n",
    "\n",
    "so we begin by a pool of parents , each is some decryption map like {A:C,T:G,C:A,G:T} , the DNA string of this decryption map is \"CGAT\" , the offsprings (we create many) are just like the parent with some mutation (some letter swapped , so for ex A:G , T:C is changed to A:C , T:G the parent was \"CGAT\" the child becomes \"GCAT\")\n",
    "\n",
    "Then we use a fitness function , which is some function of the DNA (in our case log-likelihood) to guide us to the fittest DNA (best decryption map) , these become the new parents and so on \n",
    "\n",
    "note that when choosing the best DNA strings we include the parents too , bad mutations can happen so including the parent guarantees that at each iteration we get a log-likelihood at least as good as previous iteration \n",
    "\n",
    "why not just use gradient descent ?\n",
    "\n",
    "we cant , the gradient is the derivative of the fitness function wrt the parameters , the fitness function is the log-likelihood but the parameter is a map of coded letters to plaintext letters , obviously this is undifferentiable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try to write the code \n",
    "# our training corpus will be moby dick\n",
    "# https://www.gutenberg.org/ebooks/2701\n",
    "# download the palin-text and remove the head and tail\n",
    "# our test corpus is a paragraph from The Adventures of sherlock holmes\n",
    "# https://www.gutenberg.org/ebooks/1661\n",
    "# again you can choose any text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a message from Adventures of sherlock holmes\n",
    "alphabet = string.ascii_lowercase\n",
    "message = '''My cabby drove fast. I don't think I ever drove faster, but the others\n",
    "were there before us. The cab and the landau with their steaming horses\n",
    "were in front of the door when I arrived. I paid the man and hurried\n",
    "into the church. There was not a soul there save the two whom I had\n",
    "followed and a surpliced clergyman, who seemed to be expostulating with\n",
    "them. They were all three standing in a knot in front of the altar. I\n",
    "lounged up the side aisle like any other idler who has dropped into a\n",
    "church. Suddenly, to my surprise, the three at the altar faced round to\n",
    "me, and Godfrey Norton came running as hard as he could towards me.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for we keep only lower case leters in the message\n",
    "def process(text):\n",
    "    text = text.replace('\\n',' ')\n",
    "    return \"\".join([t.lower() for t in text if t.lower() in alphabet + \" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my cabby drove fast i dont think i ever drove faster but the others were there before us the cab and the landau with their steaming horses were in front of the door when i arrived i paid the man and hurried into the church there was not a soul there save the two whom i had followed and a surpliced clergyman who seemed to be expostulating with them they were all three standing in a knot in front of the altar i lounged up the side aisle like any other idler who has dropped into a church suddenly to my surprise the three at the altar faced round to me and godfrey norton came running as hard as he could towards me'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = process(message)\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next lets load in our train\n",
    "f =  open('datasets/moby_dick.txt',encoding='utf-8')\n",
    "corpus = f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next lets build up our unigram and bigram probabilities\n",
    "V = len(alphabet) # size of vocabulary\n",
    "unigram = np.ones(V) # count of each character + add one smoothing\n",
    "bigram = np.ones((V,V)) # bigram probabilities + add one smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build unigram\n",
    "for ch in corpus:\n",
    "    if ch in alphabet:\n",
    "        unigram[ord(ch)-ord('a')] +=1\n",
    "unigram = unigram/(unigram.sum()) # obtain probabilities\n",
    "unigram = np.log(unigram) # obtain log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build bigram\n",
    "bigram = np.ones((V,V)) # bigram probabilities + add one smoothing\n",
    "for i in range(len(corpus)-1):\n",
    "    ch1 = corpus[i]\n",
    "    ch2 = corpus[i+1]\n",
    "    if ch1 in alphabet and ch2 in alphabet:\n",
    "        bigram[ord(ch1)-ord('a'),ord(ch2)-ord('a')] +=1\n",
    "bigram = (bigram.T/bigram.sum(axis=1)).T # obtain probabilities\n",
    "bigram = np.log(bigram) # obtain log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need a function to get the log-likelihood of a sentence\n",
    "# this will later take a decoded message using the DNA string and return the the log-likelihood\n",
    "# so this is our fitness function\n",
    "def fit_word(word):\n",
    "    logP = unigram[ord(word[0])-ord('a')]\n",
    "    for i in range(1,len(word)-1):\n",
    "        ch1 = word[i]\n",
    "        ch2 = word[i+1]\n",
    "        logP += bigram[ord(ch1)-ord('a'),ord(ch2)-ord('a')]\n",
    "    return logP\n",
    "\n",
    "def fit_sentence(sentence):\n",
    "    logP = 0\n",
    "    sentence = sentence.split(' ')\n",
    "    for word in sentence:\n",
    "        logP += fit_word(word)\n",
    "    return logP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we want to define encode and decode functions\n",
    "\n",
    "def encode(message,map_):\n",
    "    encoded = \"\"\n",
    "    for ch in message:\n",
    "        if ch == ' ':\n",
    "            encoded += ' '\n",
    "        else:\n",
    "            encoded += map_[ch]\n",
    "            \n",
    "    return encoded\n",
    "\n",
    "def decode(message,map_):\n",
    "    rev_map_ = {v:k for k,v in map_.items()}\n",
    "    decoded = \"\"\n",
    "    for ch in message:\n",
    "        if ch == ' ':\n",
    "            decoded += ' '\n",
    "        else:\n",
    "            decoded += rev_map_[ch]\n",
    "            \n",
    "    return decoded\n",
    "\n",
    "# another function to change a DNA list to dict\n",
    "def DNA_to_dict(DNA):\n",
    "    alphabet = string.ascii_lowercase\n",
    "    dict_ = {}\n",
    "    for ch1,ch2 in zip(alphabet,DNA):\n",
    "        dict_[ch1] = ch2\n",
    "    return dict_\n",
    "\n",
    "# another function to get a random DNA list , lists will be easier tow work with than strings\n",
    "def get_random_DNA():\n",
    "    alphabet = string.ascii_lowercase\n",
    "    DNA = list(alphabet)\n",
    "    shuffle(DNA)\n",
    "    return DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_encoding = get_random_DNA()\n",
    "true_dict = DNA_to_dict(true_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encode(message,true_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want a function to get children form parent DNA\n",
    "def get_children(DNA,max_children):\n",
    "    children = []\n",
    "    for _ in range(max_children):\n",
    "        child = DNA.copy()\n",
    "        i1 = np.random.randint(0,len(child))\n",
    "        i2 = np.random.randint(0,len(child))\n",
    "        child[i1],child[i2] = child[i2],child[i1]\n",
    "        children.append(child)\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets begin by filling our pool with some parent\n",
    "pool = []\n",
    "max_parents = 20 # at each iteration we will get only fittest 5 parents\n",
    "max_children = 5 # each aprent will have 5 children\n",
    "for _ in range(max_parents):\n",
    "    pool.append(get_random_DNA())\n",
    "\n",
    "epochs = 1000\n",
    "# next lets loop\n",
    "costs = []\n",
    "for _ in range(epochs):\n",
    "    # first lets add all the children to the pool\n",
    "    children = []\n",
    "    for parent in pool:\n",
    "        children += get_children(parent,max_children)\n",
    "    pool += children\n",
    "    \n",
    "    # next for every DNA we get the likelihood\n",
    "    fitness = []\n",
    "    for DNA in pool:\n",
    "        decode_dict = DNA_to_dict(DNA)\n",
    "        decoded = decode(encoded,decode_dict)\n",
    "        score = fit_sentence(decoded)\n",
    "        fitness.append(score)\n",
    "    \n",
    "    # next we take the fittest of the population\n",
    "    idx = np.argpartition(np.array(fitness)*-1, max_parents)[:max_parents]\n",
    "    pool = [pool[i] for i in idx]\n",
    "    # append best score to costs\n",
    "    costs.append(fitness[idx[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD4CAYAAADo30HgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAbBUlEQVR4nO3de5Bc5Z3e8e+juekuoQtI1ggLR8LLxRjDLAYH2xsLg7LxRoKgKrbWBZuQqCCbcjbsZoOL8taStSvFJl4ccAVHgV0D3thssGWU1RIwlgNxisgMNhgJWfYIw2os2YyuSDPS3PqXP/od0T3q0VzOzPRMn+dT1TWn3/ec7vc09nn0vu+5KCIwMzMbMKPaDTAzs6nFwWBmZmUcDGZmVsbBYGZmZRwMZmZWpr7aDchqyZIlsWrVqmo3w8xsWnn55ZcPRsTSSnXTPhhWrVpFa2trtZthZjatSHprqDoPJZmZWRkHg5mZlXEwmJlZGQeDmZmVcTCYmVkZB4OZmZVxMJiZWZlpfx1DLXr5rSP8n591sGRuE0e7eujpK1S7SWY2Ba296Dw+uHLhuH+ug2EK+eKze/jAigVsevzlM+qkKjTIzKa0c+fPdDDUqiOdPbz05mEe3N52Rt1n1q7hrk9eWIVWmVleORiq4MCxkwix70gX/+2FN3j29V+dsc7NVzbzH276AA11ngYys8nlYJhk/2vnAe742g8r1v3Hmy9j7UXn0VAn5s1smOSWmZkVORgm0ZsHO8tC4fZrL+Ajf28xa86dx7nzm5jZUFfF1pmZFTkYJtixk7289PPDALz0ZvHvb1+1kj9dfyn1HiYysynIwTCBTvX288F7ny0rmzeznn/vUDCzKczBMIG+8vze08uf+cRqrr9kGefOa/KEsplNaQ6GCfSl534GwOc+dTG3X3tBlVtjZjYyDoYJ0NNX4OHvvwHAxy5c6lAws2nFwTDOOo53s+5LL3Cos4cPnb+Qr3z6imo3ycxsVDzYPc6+95O3OdTZwzmzG/jL3/11Zjc6e81sevFRa5y8daiTP35qF8//tAOA79z1cRbObqxyq8zMRi9Tj0HSRkm7JBUktQyq+6ykNkl7JN1QUn6lpNdS3QNS8fZwkpokPZHKd0halaVtk6m7r59/+tWXTocCwOI5DgUzm56yDiXtBG4CXigtlHQxcAtwCbAO+C+SBi7rfQjYBKxJr3Wp/HbgSESsBu4H7svYtknz/Z8d5I2OTgC+cOOl7Lr3BuTboZrZNJUpGCJid0TsqVC1HvhGRHRHxM+BNuAqScuB+RHxYkQE8BiwoWSbR9Pyk8BaTZOj61uHuk4v/86H38ucJo/Qmdn0NVGTzyuAfSXv21PZirQ8uLxsm4joA44Biyt9uKRNkloltXZ0dFRaZVJ98dliNv7bG95f5ZaYmWU37D9tJT0HLKtQdU9EPDXUZhXK4izlZ9vmzMKIzcBmgJaWlorrTJZv/bCdzp5+PnnxefzeP1hdzaaYmY2LYYMhIq4bw+e2AytL3jcD+1N5c4Xy0m3aJdUDC4DDY/juSXPwRDd/8D9eBeD3r1tT5daYmY2PiRpK2grcks40uoDiJPMPIuIAcFzS1Wn+4FbgqZJtbkvLNwPb0zzElNXy+eeIgD/+1MVc8p4F1W6Omdm4yDRLKulG4EFgKbBN0isRcUNE7JL018DrQB/wexHRnza7E/gqMAt4Or0AHgEel9RGsadwS5a2TabLmh0KZlY7MgVDRGwBtgxR9wXgCxXKW4FLK5SfAjZmaU+1fOj8c6rdBDOzcePzKseot79AQ534Z9deQN2MaXFWrZnZiDgYxuBfPNbK3rdP0NsfXLVqUbWbY2Y2rhwMY/Cd138FwMyGGXzswqVVbo2Z2fjy3VUzWHfJMj+Nzcxqjo9qGdx4RfPwK5mZTTMOhgzm+p5IZlaDHAwZzGqoG34lM7NpxsEwSqUXY89qdDCYWe1xMIxST3/h9LJ7DGZWixwMo9Td52Aws9rmYBilnpJgaGrwz2dmtcdHtlEq7TE01fvnM7Pa4yPbKJ3sKd4k9vxFs/1cZzOrSQ6GUTra1QPAn2444waxZmY1wcEwSoc7i8GwaHZjlVtiZjYxHAyj9P22gwAsmutgMLPa5GAYpVfbjwGwxMFgZjXKwTBKRzp7+K0Pvoemel/DYGa1ycEwSodOdLN0blO1m2FmNmEyBYOkjZJ2SSpIaikpXyzpe5JOSPryoG2ulPSapDZJDyid8ympSdITqXyHpFVZ2jbeIoLnf9pBZ08/S+c5GMysdmXtMewEbgJeGFR+Cvgc8IcVtnkI2ASsSa91qfx24EhErAbuB+7L2LZx1frWEW77ix8A8HE/tc3MalimYIiI3RGxp0J5Z0R8n2JAnCZpOTA/Il6M4m1KHwM2pOr1wKNp+UlgrabQFWS/PFbclf98y+Vc/J75VW6NmdnEmew5hhVAe8n79lQ2ULcPICL6gGPA4kofImmTpFZJrR0dHRPY3He9c6oXgGveV7FJZmY1Y9hHkEl6DlhWoeqeiHhqlN9XqQcQI6grL4zYDGwGaGlpqbjOeDnR3cfl9z7LjBnF5s2f1TCRX2dmVnXDBkNEXDeO39cOlD4ouRnYX1K3EmiXVA8sAA6P43ePyZsHO+krBBSK+eMb55lZrZvUo1xEHACOS7o6zR/cCgz0OrYCt6Xlm4HtUfq4tCoZPMsxhaY9zMwmRKan2Uu6EXgQWApsk/RKRNyQ6t4E5gONkjYA10fE68CdwFeBWcDT6QXwCPC4pDaKPYVbsrRtvJzq7T+9vP0PPl7FlpiZTY5MwRARW4AtQ9StGqK8FTjj1qQRcQrYmKU9E6Gz+91gWOLrF8wsBzxgPoyunneDYV5Tphw1M5sWHAxnUSgE2147AMBDv3OF5xfMLBccDGex9dX9/M9XiydNfWT1kiq3xsxscjgYzmLgorbfvup8Fvj6BTPLCQfDELr7+vnisz8F4O51v1bl1piZTR4HwxD+6v/9HcdOFnsMc2d60tnM8sPBMIRDnd2nl+tmeNLZzPLDwTCE/71ncm7OZ2Y21TgYhnC0qziMtHLRrCq3xMxscjkYhnCyt5+PXbiUv/3MR6vdFDOzSeVgGEJndx+/tmwe82b6NFUzyxcHQwX9haC7r8CshrpqN8XMbNI5GCo4me6oOrvRwWBm+eNgqKCrpw9wMJhZPjkYKuhKt9qe1egL28wsfxwMFRzu6gFg0RxPPJtZ/jgYKjh4vHjV89K5M6vcEjOzyedgqGDfkZMALJnXWOWWmJlNPgdDBdt/8iuWzG1k6Vw/ytPM8idTMEjaKGmXpIKklpLyT0p6WdJr6e8nSuquTOVtkh5QeiyapCZJT6TyHZJWZWlbFodO9HD5ynOor3Numln+ZD3y7QRuAl4YVH4Q+K2I+ABwG/B4Sd1DwCZgTXqtS+W3A0ciYjVwP3BfxraNWWdPH3ObfKqqmeVTpmCIiN0RsadC+Y8iYn96uwuYmXoEy4H5EfFiRATwGLAhrbceeDQtPwmsVZUestzZ3e9nMJhZbk3GWMk/AX4UEd3ACqC9pK49lZH+7gOIiD7gGLC40gdK2iSpVVJrR8f43x77RHcfc5ocDGaWT8Me/SQ9ByyrUHVPRDw1zLaXUBwSun6gqMJqMYK68sKIzcBmgJaWlorrjFVPX4GevgJzfXGbmeXUsEe/iLhuLB8sqRnYAtwaEXtTcTvQXLJaM7C/pG4l0C6pHlgAHB7Ld2fRcaJ4DcM5c3yqqpnl04QMJUlaCGwDPhsR/3egPCIOAMclXZ3mD24FBnodWylOVAPcDGxP8xCTavf+dwC4aPm8yf5qM7MpIevpqjdKageuAbZJeiZV/StgNfA5Sa+k17mp7k7gYaAN2As8ncofARZLagPuAu7O0rax+u5PfgXA+5fNr8bXm5lVXaaB9IjYQnG4aHD554HPD7FNK3BphfJTwMYs7ckqIvj6D/axYFYDcz35bGY55Su4Srxzqni77Rs/tGKYNc3MapeDocThzuJdVS9rXlDllpiZVY+DocShdEbSYt8jycxyzMFQYt+RLgCWL/Dtts0svxwMJfb88gQNdeKCJXOq3RQzs6pxMJR4+51TnDtvJg2+q6qZ5ZiPgCWOnuxl4Ww/ztPM8s3BUOJoVw/nzPatMMws3xwMJY529bLAPQYzyzkHQ4mjJ3tZOMvBYGb55mBICoXwUJKZGQ6G00709FEIPPlsZrnnYEiOdfUCsMBDSWaWcw6G5EhX8T5JCz2UZGY552BIjqYewzkeSjKznHMwJEdPFoPBcwxmlncOhuRYGkpaMMtDSWaWbw6G5EiXewxmZpD9mc8bJe2SVJDUUlJ+Vcmznl+VdGNJ3ZWSXpPUJukBSUrlTZKeSOU7JK3K0rbROnDsFEvmNvoGemaWe1mPgjuBm4AXKpS3RMTlwDrgv0oaeIjyQ8AmYE16rUvltwNHImI1cD9wX8a2jcr+oyd5z8JZk/mVZmZTUqZgiIjdEbGnQnlXRPSltzOBAJC0HJgfES9GRACPARvSeuuBR9Pyk8Dagd7EZNh3uIsVDgYzs4mbY5D0YUm7gNeAO1JQrADaS1ZrT2Wkv/sA0rrHgMUT1b5S3X39vHW4i9Xnzp2MrzMzm9Lqh1tB0nPAsgpV90TEU0NtFxE7gEskXQQ8KulpoFIPIAa+6ix1g9u0ieJwFOeff/5ZWj8yb7/TTX8hWLlodubPMjOb7oYNhoi4LssXRMRuSZ3ApRR7CM0l1c3A/rTcDqwE2tN8xALg8BCfuRnYDNDS0lIxPEbjWLqGYf5Mn5FkZjYhQ0mSLhiYbJb0XuD9wJsRcQA4LunqNH9wKzDQ69gK3JaWbwa2p3mICXf8VHE6ZP6sYXPSzKzmZToSptNQHwSWAtskvRIRNwDXAndL6gUKwL+MiINpszuBrwKzgKfTC+AR4HFJbRR7CrdkadtovHPKPQYzswGZgiEitgBbKpQ/Djw+xDatFIeVBpefAjZmac9YDfQY5s10j8HMzFdzAZ3dxWCY0+RgMDNzMFA8XRWgqd4/h5mZj4RAT18BgKb6uiq3xMys+hwMvBsMDXWTdqG1mdmU5WAAuvsKNNXPYBLvwGFmNmU5GCgGQ6PnF8zMAAcDMNBj8PyCmRk4GIDiHIPPSDIzK/LRkOLpqg4GM7MiHw0p9hg8x2BmVuSjIZ58NjMr5aMh0NXTx+xGTz6bmYGDAYDO7n7m+j5JZmaAgwGAzp4+Zjc6GMzMwMEAFHsMvrOqmVmRg4HiHMMczzGYmQEOBgqFoKvHPQYzswG5D4au3uKzGOY0ucdgZgYOBj+9zcxskEzBIGmjpF2SCpJaKtSfL+mEpD8sKbtS0muS2iQ9oHSva0lNkp5I5TskrcrStpE6HQw+K8nMDMjeY9gJ3AS8MET9/cDTg8oeAjYBa9JrXSq/HTgSEavTdvdlbNuIdHYPDCU5GMzMIGMwRMTuiNhTqU7SBuANYFdJ2XJgfkS8GBEBPAZsSNXrgUfT8pPAWk3Ck3M6ewZ6DJ5jMDODCZpjkDQH+HfAvYOqVgDtJe/bU9lA3T6AiOgDjgGLh/j8TZJaJbV2dHRkaqvnGMzMyg0bDJKek7Szwmv9WTa7F7g/Ik4M/rgK68YI6soLIzZHREtEtCxdunS4XTirzh6flWRmVmrYfyZHxHVj+NwPAzdL+jNgIVCQdAr4JtBcsl4zsD8ttwMrgXZJ9cAC4PAYvntUBnoMviWGmVnRhBwNI+KjA8uS/gQ4ERFfTu+PS7oa2AHcCjyYVt0K3Aa8CNwMbE/zEBPKQ0lmZuWynq56o6R24Bpgm6RnRrDZncDDQBuwl3fPWnoEWCypDbgLuDtL20bq9FlJnnw2MwMy9hgiYguwZZh1/mTQ+1bg0grrnQI2ZmnPWHT19NFUP4P6utxf62dmBvjKZzp7+jyMZGZWIvfB0NXdz6wGDyOZmQ3IfTB09xdoasj9z2Bmdlruj4i9fQUaPb9gZnZa7o+Ivf0FGhwMZman5f6I2NsfNNbn/mcwMzst90fEnr4CDXUTfq8+M7Npw8HgoSQzszK5PyL29hdo8lCSmdlpuT8ievLZzKxc7o+IxTmG3P8MZman5f6I2NsfDgYzsxK5PyL29Bd8uqqZWYncHxF9uqqZWbncB8PJnn4/vc3MrESug6G7r5+e/gJz/bxnM7PTch0MA09vm+vnMZiZnZbrYDhxqvi857kzG6rcEjOzqSPXwXCosxtwj8HMrFSmYJC0UdIuSQVJLSXlqySdlPRKen2lpO5KSa9JapP0gCSl8iZJT6TyHZJWZWnbSHz7R78AYOm8xon+KjOzaSNrj2EncBPwQoW6vRFxeXrdUVL+ELAJWJNe61L57cCRiFgN3A/cl7Ftw0qZxBXnnzPRX2VmNm1kCoaI2B0Re0a6vqTlwPyIeDEiAngM2JCq1wOPpuUngbUDvYmJ0lcosGhOIxP8NWZm08pEzjFcIOlHkp6X9NFUtgJoL1mnPZUN1O0DiIg+4BiwuNIHS9okqVVSa0dHx5gb2Ncf1M9wKJiZlRp21lXSc8CyClX3RMRTQ2x2ADg/Ig5JuhL4tqRLgEpH4Rj4qrPUlRdGbAY2A7S0tFRcZyR8nyQzszMNGwwRcd1oPzQiuoHutPyypL3AhRR7CM0lqzYD+9NyO7ASaJdUDywADo/2u0ejr1Cg3rfDMDMrMyH/XJa0VFJdWn4fxUnmNyLiAHBc0tVp/uBWYKDXsRW4LS3fDGxP8xATxkNJZmZnynq66o2S2oFrgG2SnklVHwN+LOlVihPJd0TEwL/+7wQeBtqAvcDTqfwRYLGkNuAu4O4sbRsJP6THzOxMma7siogtwJYK5d8EvjnENq3ApRXKTwEbs7RntPoK4aEkM7NBcv3P5d7+AvUzcv0TmJmdIddHxeJQknsMZmalch0MfT5d1czsDLk+KvYWgnoHg5lZmVwfFfv6CzT4dFUzszI5DwaflWRmNliug6G3UPBQkpnZILk+KhYKvvLZzGywXAdDfwQzfMttM7MyuQ6GQgEHg5nZILkOhv5C4CkGM7NyuT4sFiKo8xyDmVmZ3AeDH+tpZlYu18HQXwjqHAxmZmUcDB5KMjMrk+tgiPBZSWZmg+U6GIrXMVS7FWZmU0u+g8FDSWZmZ8h1MBQimOFgMDMrkykYJG2UtEtSQVLLoLrLJL2Y6l+TNDOVX5net0l6QOl8UUlNkp5I5TskrcrStpEoBB5KMjMbJGuPYSdwE/BCaaGkeuBrwB0RcQnwG0Bvqn4I2ASsSa91qfx24EhErAbuB+7L2LZh+XRVM7MzZQqGiNgdEXsqVF0P/DgiXk3rHYqIfknLgfkR8WJEBPAYsCFtsx54NC0/CazVBF59VigEgIeSzMwGmag5hguBkPSMpB9K+qNUvgJoL1mvPZUN1O0DiIg+4BiwuNKHS9okqVVSa0dHx5ga2B/FYHCPwcysXP1wK0h6DlhWoeqeiHjqLJ97LfDrQBfwXUkvA+9UWDcGvuosdeWFEZuBzQAtLS0V1xlOIdxjMDOrZNhgiIjrxvC57cDzEXEQQNLfAldQnHdoLlmvGdhfss1KoD3NUSwADo/hu0ekUCj+9QVuZmblJmoo6RngMkmz00H+48DrEXEAOC7p6jR/cCsw0OvYCtyWlm8Gtqd5iAlxeigp1yfsmpmdKevpqjdKageuAbZJegYgIo4Afw68BLwC/DAitqXN7gQeBtqAvcDTqfwRYLGkNuAu4O4sbRtO/8Dks3sMZmZlhh1KOpuI2AJsGaLuaxSHjgaXtwKXVig/BWzM0p7RGOiMOBjMzMrldiBloMfgW2KYmZXLbzD4rCQzs4pyGwwD09q+jsHMrFxug+HdyecqN8TMbIpxMDgZzMzK5DYYCr4lhplZRTkOhuLfGbn9BczMKsvtYbGzuw+Apvq6KrfEzGxqyW0w7PzFMQAuWj6/yi0xM5tachsMi+Y08smLz2PV4tnVboqZ2ZSS6ZYY09n1lyzj+ksq3U3czCzfcttjMDOzyhwMZmZWxsFgZmZlHAxmZlbGwWBmZmUcDGZmVsbBYGZmZRwMZmZWRgPPPp6uJHUAb41x8yXAwXFsznTgfc4H73M+ZNnn90bE0koV0z4YspDUGhEt1W7HZPI+54P3OR8map89lGRmZmUcDGZmVibvwbC52g2oAu9zPnif82FC9jnXcwxmZnamvPcYzMxsEAeDmZmVyW0wSFonaY+kNkl3V7s940HSSknfk7Rb0i5J/zqVL5L0HUk/S3/PKdnms+k32CPphuq1PhtJdZJ+JOlv0vua3mdJCyU9Kekn6b/3NTnY53+T/ne9U9LXJc2stX2W9BeS3pa0s6Rs1Pso6UpJr6W6ByRpVA2JiNy9gDpgL/A+oBF4Fbi42u0ah/1aDlyRlucBPwUuBv4MuDuV3w3cl5YvTvveBFyQfpO6au/HGPf9LuC/A3+T3tf0PgOPAv88LTcCC2t5n4EVwM+BWen9XwO/W2v7DHwMuALYWVI26n0EfgBcAwh4GviHo2lHXnsMVwFtEfFGRPQA3wDWV7lNmUXEgYj4YVo+Duym+H+o9RQPJKS/G9LyeuAbEdEdET8H2ij+NtOKpGbgHwEPlxTX7D5Lmk/xAPIIQET0RMRRanifk3pglqR6YDawnxrb54h4ATg8qHhU+yhpOTA/Il6MYko8VrLNiOQ1GFYA+0ret6eymiFpFfAhYAdwXkQcgGJ4AOem1Wrld/gS8EdAoaSslvf5fUAH8Jdp+OxhSXOo4X2OiF8A/wn4O+AAcCwinqWG97nEaPdxRVoeXD5ieQ2GSuNtNXPerqS5wDeB34+Id862aoWyafU7SPoU8HZEvDzSTSqUTat9pvgv5yuAhyLiQ0AnxSGGoUz7fU7j6uspDpm8B5gj6dNn26RC2bTa5xEYah8z73teg6EdWFnyvplit3Tak9RAMRT+KiK+lYp/lbqXpL9vp/Ja+B3+PvCPJb1JcUjwE5K+Rm3vczvQHhE70vsnKQZFLe/zdcDPI6IjInqBbwEfobb3ecBo97E9LQ8uH7G8BsNLwBpJF0hqBG4Btla5TZmlMw8eAXZHxJ+XVG0FbkvLtwFPlZTfIqlJ0gXAGoqTVtNGRHw2IpojYhXF/47bI+LT1PY+/xLYJ+n9qWgt8Do1vM8Uh5CuljQ7/e98LcU5tFre5wGj2sc03HRc0tXpt7q1ZJuRqfYsfBVn/3+T4lk7e4F7qt2ecdqnayl2GX8MvJJevwksBr4L/Cz9XVSyzT3pN9jDKM9cmGov4Dd496ykmt5n4HKgNf23/jZwTg72+V7gJ8BO4HGKZ+PU1D4DX6c4h9JL8V/+t49lH4GW9DvtBb5MusvFSF++JYaZmZXJ61CSmZkNwcFgZmZlHAxmZlbGwWBmZmUcDGZmVsbBYGZmZRwMZmZW5v8DUy7Uj0nkb10AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_DNA = pool[0]\n",
    "best_dict = DNA_to_dict(best_DNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original message:  my cabby drove fast i dont think i ever drove faster but the others were there before us the cab and the landau with their steaming horses were in front of the door when i arrived i paid the man and hurried into the church there was not a soul there save the two whom i had followed and a surpliced clergyman who seemed to be expostulating with them they were all three standing in a knot in front of the altar i lounged up the side aisle like any other idler who has dropped into a church suddenly to my surprise the three at the altar faced round to me and godfrey norton came running as hard as he could towards me\n",
      "-------------------------------------------------------------------------------------\n",
      "Decoded  message:  my cabby drove fast i dont think i ever drove faster but the others were there before us the cab and the landau with their steaming horses were in front of the door when i arrived i paid the man and hurried into the church there was not a soul there save the two whom i had followed and a surpliced clergyman who seemed to be expostulating with them they were all three standing in a knot in front of the altar i lounged up the side aisle like any other idler who has dropped into a church suddenly to my surprise the three at the altar faced round to me and godfrey norton came running as hard as he could towards me\n"
     ]
    }
   ],
   "source": [
    "print(\"Original message: \",message)\n",
    "print('-------------------------------------------------------------------------------------')\n",
    "print(\"Decoded  message: \",decode(encoded,best_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j  --->  q but we predicted:  j  ---> c\n",
      "z  --->  c but we predicted:  z  ---> q\n"
     ]
    }
   ],
   "source": [
    "# lets get wrong letters\n",
    "for (k,true_v),(_,best_v) in zip(true_dict.items(),best_dict.items()):\n",
    "    if true_v != best_v:\n",
    "        print(k,\" ---> \",true_v , 'but we predicted: ',k,' --->',best_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original message likelihood -942.66541918156 Best message likelihood  -942.66541918156\n"
     ]
    }
   ],
   "source": [
    "original_cost = fit_sentence(message)\n",
    "best_cost = fitness[idx[0]]\n",
    "print('Original message likelihood' , original_cost,\"Best message likelihood \",best_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original message contained neither z nor j , this means we actually got a perfect decoding :)\n",
    "# in some cases we might find that Best message likelihood is better than Original message likelihood\n",
    "# this means that within the context of moby dick the best message is more likely to occur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
