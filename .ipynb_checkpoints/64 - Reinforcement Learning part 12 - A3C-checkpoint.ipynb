{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8DpLxQ17S1S"
   },
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiSMddgM7Syb"
   },
   "source": [
    "In this next notebook of the seires we are going to study yet another famous deep reinforcement learning algorithm known as A3C \n",
    "\n",
    "---\n",
    "\n",
    "<h3>A3C</h3>\n",
    "\n",
    "A3C stands for Asynchronous Advantage Actor critic \n",
    "\n",
    "As we can see this term has three A's in it and one C hence the abbreviation A3C\n",
    "\n",
    "What's pretty nice is we've already done most of the groundwork for this algorithm\n",
    "\n",
    "In fact there's nothing new in terms of theory that we have to discuss \n",
    "\n",
    "In particular, we've covered everything we need to know already in the notebook on policy gradients \n",
    "\n",
    "As we may recall, the final form of the policy gradient algorithm we used was called the Actor Critic where the actor was a neural network that parameterise the policy and the critic was another neural network that parameterise the value function \n",
    "\n",
    "As we may recall the Advantage is just the term we use to measure the difference between the actual return of state s and the value at state s\n",
    "\n",
    "In other words it's how much better the action we took is relative to what we currently believe the value is\n",
    "\n",
    "So now we know where the terms Advantage an Actor Critic come from\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Gradient Review</h3>\n",
    "\n",
    "First let's review more closely how policy gradients work\n",
    "\n",
    "As we recall we're going to have to neural networks the policy network and the value network\n",
    "\n",
    "We can imagine that the policy network outputs $\\pi$ the policy and the value network outputs $V$ the value\n",
    "\n",
    "These networks have the weights $\\theta_p$ and $\\theta_v$ respectively\n",
    "\n",
    "$$\\pi(a \\vert s,\\theta_p) = NeuralNet(input : s, weights : \\theta_p)$$\n",
    "\n",
    "$$V(s,\\theta_p) = NeuralNet(input : s, weights : \\theta_v)$$\n",
    "\n",
    "The objective for the policy is derived by sort of working backwards from the actual policy gradient itself or in other words taking the integral of the gradient\n",
    "\n",
    "This is just the return\n",
    "\n",
    "$$L_p = - (G-V(s)) \\log \\pi(a \\vert s,\\theta_p)$$\n",
    "\n",
    "G : return\n",
    "\n",
    "Notice we've represented it as the loss here or in other words something to minimize rather than maximize since that's what we'll be using Tensorflow \n",
    "\n",
    "On the other hand the value network loss is simpler it's just the squared error between the return and the predicted value\n",
    "\n",
    "$$L_v = (G-V(s,\\theta_v))^2$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Pseudocode</h3>\n",
    "\n",
    "Then during training, all we need to do is loop through each step of the game sample in action and then update the weights of both neural networks as we can see in the pseudocode\n",
    "\n",
    "```python\n",
    "while not done:\n",
    "  a = pi.sample(s)\n",
    "  s_prime,r,done = env.step(a)\n",
    "  G = r + gamma * V(s_prime)\n",
    "  Lp = -(G-V(s)) * log(pi(s))\n",
    "  Lv = (G-V(s))^2\n",
    "  theta_p = theta_p - learning_rate * dLp / d theta_p\n",
    "  theta_v = theta_v - learning_rate * dLv / d theta_v\n",
    "```\n",
    "\n",
    "So the pseudocode is as follows\n",
    "\n",
    "While we are not done, we sample an action from the policy network, we get a \n",
    "\n",
    "Then we perform the action a in the environment and we get s' and r \n",
    "\n",
    "We set G equal to r + gamma * V(s')\n",
    "\n",
    "We calculate the policy loss and the value loss\n",
    "\n",
    "Finally we perform gradient descent to update $\\theta_p$ and $\\theta_v$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>N-Step return</h3>\n",
    "\n",
    "One minor difference between this and A3C is that we're going to take N steps at a time which means we can use the N step return rather than the TD return \n",
    "\n",
    "E.g 3 steps:\n",
    "\n",
    "$$V(s) = r + \\gamma r' + \\gamma^2 r'' + \\gamma^3 V(s''')$$\n",
    "\n",
    "So here we see another opportunity to combine the different things we've learned in this series previously\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Entropy</h3>\n",
    "\n",
    "Another minor difference is that we are going to regularise the policy loss by adding the entropy as a regularisation term\n",
    "\n",
    "If we recall the entropy of a distribution is the sum over all events of the probability of the event multiplied by the log probability of the event\n",
    "\n",
    "$$H = - \\sum^K_{k=1} \\pi_k \\log \\pi_k$$\n",
    "\n",
    "In our case the $k$'s just represent the different actions and $\\pi_k$ is just the probability of that action at the output of the policy network\n",
    "\n",
    "In the end we take the usual policy gradient loss and add the entropy multiplied by a regularization constant which controls the strength of the regularization term\n",
    "\n",
    "$$L^{'}_p = L_p + CH$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Entropy</h3>\n",
    "\n",
    "In practice what this does is it encourages exploration\n",
    "\n",
    "If we call entropy is somewhat like variance in that it measures the spread of a distribution\n",
    "\n",
    "We get the maximum entropy when each event has equal probability and we get zero entropy when all the probability goes to a single event or in other words the outcome is deterministic \n",
    "\n",
    "So using entropy as a regularization term allows us to fight between those two extremes \n",
    "\n",
    "One extreme being that all actions are equally probable and the other extreme being that only a single action can ever be chosen \n",
    "\n",
    "---\n",
    "\n",
    "<h3>Neural Networks</h3>\n",
    "\n",
    "For a given input state another big improvement with this algorithm is that will now be able to incorporate neural networks as our function approximator \n",
    "\n",
    "Specifically, we will be using convolutional neural networks just like we did for Deep Q-Learning to process the frames of the screen of an Atari game as input \n",
    "\n",
    "<img src=\"extras/64.0.PNG\" width=\"400\">\n",
    "\n",
    "One thing to note is that for vanilla policy gradients this approach didn't seem to work\n",
    "\n",
    "We'll see that with A3C we are now able to use neural networks and later in this section we'll explain why that's the case\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The 3 A's</h3>\n",
    "\n",
    "So what makes A3C interesting is that it's Asynchronous\n",
    "\n",
    "In other words what it does is take the algorithm we had before but make it so that it's an Asynchronous algorithm \n",
    "\n",
    "<img src=\"extras/64.1.PNG\" width=\"400\">\n",
    "\n",
    "In the rest of this section we're going to explain to exactly what that means\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Modern Computing</h3>\n",
    "\n",
    "First in modern computing what we often like to do is run things in parallel\n",
    "\n",
    "So for example suppose we had a for loop over one million files and we need to do some processing on \n",
    "\n",
    "Well in code this is very simple, we can just loop over all 1 million files and run our function on each file\n",
    "\n",
    "```python\n",
    "for f in file:\n",
    "  process_file(f)\n",
    "```\n",
    "\n",
    "But this process is slow\n",
    "\n",
    "If we have 2 million files the time it takes for this code to run just doubled\n",
    "\n",
    "So what can we do?\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Multiple Machines</h3>\n",
    "\n",
    "These days a popular method is to use multiple machines \n",
    "\n",
    "So we can have 1000 CPU cores each responsible for 1000 different files\n",
    "\n",
    "In this way we've reduced our computation time by a factor of 1000 minus any overhead for coordinating each working machine\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Modern Processors</h3>\n",
    "\n",
    "But we don't even have to go that far \n",
    "\n",
    "As you might already know, our computers, if they're reasonably modern, already has multiple cores which can run code in parallel\n",
    "\n",
    "So while we might only have one machine, we can still take advantage of the multiple cores that exist inside it\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Algorithm for A3C</h3>\n",
    "\n",
    "So what is the basic algorithm for A3C \n",
    "\n",
    "Conceptually we're going to have one set of master networks one for the policy and one for the value function\n",
    "\n",
    "We can think of that as a set of global networks\n",
    "\n",
    "Now every so often, this global network is going to send its weights to a set of worker machines each with their own copy of the policy network and the value network\n",
    "\n",
    "<img src=\"extras/64.2.PNG\" width=\"400\">\n",
    "\n",
    "Then each of these working machines is going to play a few episodes of the environment using its current network weights \n",
    "\n",
    "From its own experience, each worker can also calculate the policy gradient updates as well as the value function updates\n",
    "\n",
    "Now of course keep in mind that only this worker knows about its own updates\n",
    "\n",
    "---\n",
    "\n",
    "Finally the worker sends its gradients to the global networks so that the global network can update its parameters \n",
    "\n",
    "And every so often the Global Network gives its new updated parameters back to its worker machines \n",
    "\n",
    "So worker networks are always working with a relatively recent copy of the global network\n",
    "\n",
    "<img src=\"extras/64.3.PNG\" width=\"400\">\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Why is this good?</h3>\n",
    "\n",
    "So why is this good\n",
    "\n",
    "Well firstly, now the global master network has no work to do\n",
    "\n",
    "It's not going to be involved in actually playing the environment, only the workers do that\n",
    "\n",
    "So the workers hence their name are doing all the work \n",
    "\n",
    "They play the episode calculate the errors and find the update gradients \n",
    "\n",
    "The global master network is the beneficiary of these updates \n",
    "\n",
    "And of course since the global master network has multiple workers playing episodes it can gather a lot more experience in parallel than it could have by playing the environment by itself\n",
    "\n",
    "Long story short we save time\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Stability</h3>\n",
    "\n",
    "But there is another big advantage to doing things this way\n",
    "\n",
    "Remember that there's always some randomness in each game\n",
    "\n",
    "Each game isnt going to start the same way and each action sampled from a probabilistic policy will vary\n",
    "\n",
    "So every game that the workers play is going to be different\n",
    "\n",
    "If we recall sometimes in reinforcement learning the performance of our agent can drop off sharply due to a bad update\n",
    "\n",
    "By having multiple workers contribute to the global master network we can reduce the variance of our updates and have a more stable learning trajectory \n",
    "\n",
    "To state this another way, what we achieve by having multiple different workers played different episodes with the same parameters, we achieve more stability\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Should remind us of SGD</h3>\n",
    "\n",
    "This is like say instead of doing stochastic gradient descent where we look at one sample at a time we can do batch gradient descent where we look at several samples at a time\n",
    "\n",
    "<img src=\"extras/64.4.PNG\" width=\"400\">\n",
    "\n",
    "As we may recall this makes the loss per iteration much smoother\n",
    "\n",
    "---\n",
    "\n",
    "<h3>DQN</h3>\n",
    "\n",
    "This might also remind us of the Deep Q-Network notebook because in that case we also wanted to add different features that helped us stabilize learning \n",
    "\n",
    "In particular that involved techniques like freezing the target network and using an experience replay buffer \n",
    "\n",
    "Using an experienced replay buffer allowed us to look at multiple examples during each training step which is a little bit like batch gradient descent, and so intuitively we understand how this helps us stabilise learning \n",
    "\n",
    "In short stabilised learning is good and we can achieve that by looking at multiple samples at the same time\n",
    "\n",
    "It just so happens that with a A3C the way that we do this is by having multiple parallel copies of our agent playing the game\n",
    "\n",
    "This is what allows us to make use of neural networks as our function approximator whereas they don't work well with vanilla policy gradients without parallelisation\n",
    "\n",
    "In other words Deep Q-Networks and A3C both try to solve the same problem which is how can we make use of neural networks in classic reinforcement learning algorithms\n",
    "\n",
    "They just happen to do it in different ways\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Code Structure</h3>\n",
    "\n",
    "In the rest of this section, we're going to discuss how the code is going to be structured\n",
    "\n",
    "Remember that the actual algorithm we'll be using is nothing new\n",
    "\n",
    "It's just the same old Advantage Actor Critic that we saw before\n",
    "\n",
    "The difference now is that we want to have concurrent agents that asynchronously copy and update their parameters\n",
    "\n",
    "So the way the code is structured is this (according to instructor)\n",
    "\n",
    "We're going to have three files\n",
    "\n",
    "The first file is `main.py`\n",
    "\n",
    "This is sort of the master file which is going to be responsible for creating the master global policy network and a value network\n",
    "\n",
    "It's also going to coordinate the workers\n",
    "\n",
    "In other words it's going to create a handful of workers each with their own thread and then wait for them to complete \n",
    "\n",
    "The second file is `worker.py` \n",
    "\n",
    "As we can tell by the name this is going to contain all the classes and functions that make up the worker \n",
    "\n",
    "Each worker is responsible for creating its own version of the policy network and value network, copying the weights from the global networks, playing episodes of the game and calculating gradients to be sent back to the global network\n",
    "\n",
    "Finally the third file is `nets.py`\n",
    "\n",
    "This contains the definitions for the policy network and value network\n",
    "\n",
    "In the next few subsections we'll be looking at these pseudocode for each of these three files \n",
    "\n",
    "---\n",
    "\n",
    "<h3>main.py</h3>\n",
    "\n",
    "In the first file main.py, the basic structure is this \n",
    "\n",
    "First we're going to instantiate our global policy network and our global value network\n",
    "\n",
    "Then we're going to count how many CPUs are available and create that many threads with one worker object for each thread\n",
    "\n",
    "We're also going to create a global thread safe counter that's going to tell us when to quit since we're going to go up to a maximum number of steps\n",
    "\n",
    "---\n",
    "\n",
    "<h3>worker.py</h3>\n",
    "\n",
    "In the second file worker.py the basic structure is this \n",
    "\n",
    "We're going to have the run function which is going to be run by each thread \n",
    "\n",
    "The run function just runs a simple loop\n",
    "\n",
    "```\n",
    "def run():\n",
    "  in a loop:\n",
    "    copy params from global nets to local nets\n",
    "    run N steps of game (and store the data - s,a,r,s')\n",
    "    using gradients wrt local net, update the global net\n",
    "```\n",
    "conceptually its like:\n",
    "<ol>\n",
    "  <li>$$\\mathbf{g}_\\text{local} = \\frac{\\partial L(\\theta_\\text{local})}{\\partial \\theta_\\text{local}}$$</li>\n",
    "  <li>$$\\theta_\\text{global} = \\theta_\\text{global} - \\eta \\mathbf{g}_\\text{local}$$</li>\n",
    "</ol>\n",
    "\n",
    "But in reality, we'll use RMSprop\n",
    "\n",
    "First we copy the parameters from the global network to our local copy of the network\n",
    "\n",
    "Then we run N steps of the game \n",
    "\n",
    "At this stage we store all the data we need to pass into our neural network such as the states we encountered and the rewards\n",
    "\n",
    "Then we update the parameters of the global network using the gradients from the last N local steps\n",
    "\n",
    "To give an idea of how this might work, we can think of it in terms of the above equations\n",
    "\n",
    "The first step is to calculate the gradient which is the local cost with respect to the local weights\n",
    "\n",
    "The second step is to update the global weights using gradient descent with the local gradients\n",
    "\n",
    "Now of course in Tensorflow it's not going to look exactly like this because we don't actually explicitly write out the gradient descent update\n",
    "\n",
    "Instead we'll be using an adaptive training algorithm such as RMSprop and our actual goal will be to figure out what is the correct tensorflow function to apply this sort of mixed update where the gradient used to update the weights of one network actually comes from the loss of another network\n",
    "\n",
    "The worker.py file is probably going to be the most complicated of the three\n",
    "\n",
    "So we just have to wait and see what the exact details are\n",
    "\n",
    "Just keep in mind each of the above steps will actually turn out to be multiple functions\n",
    "\n",
    "---\n",
    "\n",
    "<h3>nets.py</h3>\n",
    "\n",
    "The last file we're going to discuss is nets.py\n",
    "\n",
    "This more or less just implements the policy network and the value network, although there are some important details to talk about before we dive into the code \n",
    "\n",
    "First these networks are going to be convolution neural networks which means they're going to consist of a series of convolutions followed by a series of fully connected layers\n",
    "\n",
    "In addition both networks are going to share a common body\n",
    "\n",
    "In other words only the last layer of each known network is going to be different and all the preceding layers will have shared weights\n",
    "\n",
    "<img src=\"extras/64.5.PNG\" width=\"400\">\n",
    "\n",
    "Conceptually we can think of this as a two headed dragon\n",
    "\n",
    "In the next section we'll look at this code in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUflK40_abhY"
   },
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7ch62etadjr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6R1kqXqZqD9"
   },
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVSjfGv1Zs_K"
   },
   "source": [
    "In this section, we are going to summarise everything we learned in this notebook\n",
    "\n",
    "---\n",
    "\n",
    "<h3>A3C - Section Summary</h3>\n",
    "\n",
    "A majority of this notebook is just code because we already learn all the theory we needed in the policy gradient section\n",
    "\n",
    "The only minor difference in theory is that we use the N step return instead of the TD return \n",
    "\n",
    "To recap, we learned how we could have a bunch of workers learning in parallel by using multi-threading\n",
    "\n",
    "In terms of theory this allows us to add stability to the training algorithm because we're essentially taking the average result from a bunch of different random episodes\n",
    "\n",
    "We hear that word a lot in reinforcement learning the concept of stability is very central\n",
    "\n",
    "This is mostly because stability is what is lacking\n",
    "\n",
    "That's why we can't simply plug in a neural network into a Q-Learning function approximator and expect it to work\n",
    "\n",
    "That's why we can't plug in a neural network into the vanilla policy gradient algorithm either \n",
    "\n",
    "And that's really the major theme of the recent few notebooks\n",
    "\n",
    "Since merely plugging in neural networks into existing algorithms causes instability, then what can we do to make things more stable \n",
    "\n",
    "With Deep Q-Learning that involves creating an experience replay buffer and having a target network that didn't change too often \n",
    "\n",
    "With A3C that involves having multiple workers playing different episodes in parallel \n",
    "\n",
    "At the same time, one big hurdle in reinforcement learning is hyperparameter search\n",
    "\n",
    "So even if our algorithm and our code is sound if we choose the wrong hyperparameters our agent simply won't perform well\n",
    "\n",
    "In other words lots of resources both in terms of compute and our own time and effort are needed in order to really determine whether or not an algorithm works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vn2vfm5pb5eq"
   },
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LRk8aCwb_6j"
   },
   "source": [
    "In this section we are going to summarise everything we've done in the recent few notebooks (last 5)\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Checkpoint Summary</h3>\n",
    "\n",
    "It's been a long journey to this point because not only have we had to study Deep Learning but we've had to study Reinforcement Learning as well\n",
    "\n",
    "These notebooks were about combining these two techniques and also a little bit about expanding our knowledge of reinforcement learning\n",
    "\n",
    "We tend to think of this as applying deep learning to reinforcement learning rather than the other way around\n",
    "\n",
    "Deep learning is more like a general technique that can be applied to supervised learning or unsupervised learning as well as reinforcement learning \n",
    "\n",
    "But reinforcement learning is an entirely different paradigm, where we have an intelligent agent trying to maximize its reward in some environment\n",
    "\n",
    "---\n",
    "\n",
    "So what did we learn in these notebooks\n",
    "\n",
    "Well we started these notebooks with a review of some basic techniques and both reinforcement learning and deep learning\n",
    "\n",
    "We looked at Markov decision processes and the three ways of learning how to solve and MDPs, Dynamic programming, Monte-Carlo methods and temporal difference learning\n",
    "\n",
    "Next we looked at the openAI Gym which provides us many environments for practicing reinforcement learning\n",
    "\n",
    "This is especially valuable because coding our own environments would be extremely time consuming and completely unrelated to reinforcement learning itself\n",
    "\n",
    "This allowed us to practice the reinforcement learning techniques we just reviewed and practice using tensorflow which is very helpful because of its automatic differentiation capabilities\n",
    "\n",
    "They let us build arbitrarily complex neural networks and the update equations are automatically generated from the structure of the cost function\n",
    "\n",
    "Also in this section we looked at a special type of neural network called the RBF network\n",
    "\n",
    "Unlike our usual deep neural networks these networks are shallow and use fixed feature representations\n",
    "\n",
    "That means features won't be learned using gradient descent\n",
    "\n",
    "---\n",
    "\n",
    "Next we looked at N step methods and TD-lambda \n",
    "\n",
    "We saw how these are both ways we can do something that is kind of in-between Monte-Carlo TD learning \n",
    "\n",
    "Whereas N step methods are discrete The lambda in TD lambda is continuous and between 0 and 1\n",
    "\n",
    "Now of course there's nothing that makes these better or worse than the existing reinforcement learning methods we learned about, they're this new hyperparameters to be chosen\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Gradient Methods</h3>\n",
    "\n",
    "Next we looked at policy gradient methods\n",
    "\n",
    "This gave us a totally different way of solving reinforcement learning problems \n",
    "\n",
    "Instead of making the policy just greedy or epsilon greedy with respect to Q, we parameterise the policy itself\n",
    "\n",
    "In this situation we parameterise both the policy and the state value function V(s) \n",
    "\n",
    "Parameterising the policy is interesting because it allows us to easily handle continuous action spaces as we saw with continuous mountain car\n",
    "\n",
    "The only change we had to make was instead of modeling the output as a discrete distribution we model the output as a Gaussian\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<h3>Deep Q-Learning</h3>\n",
    "\n",
    "Lastly we looked at Deep Q-LEarning \n",
    "\n",
    "Deep Q-Learning is just the combination of deep learning with reinforcement learning \n",
    "\n",
    "Initially deep learning looks like a pretty attractive option in reinforcement learning because we already have the theory behind the approximation methods and neural networks our function approximators\n",
    "\n",
    "So a naive approach would be to just plug in a neural network\n",
    "\n",
    "Of course this doesn't work, so we learned about new techniques to make it work\n",
    "\n",
    "In particular we looked at experience replay using a target network and combining previous data into the current state in order to model motion\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOBTYV9q5srA0bTnJ7D1U7o",
   "collapsed_sections": [
    "N8DpLxQ17S1S",
    "xUflK40_abhY",
    "K6R1kqXqZqD9",
    "vn2vfm5pb5eq"
   ],
   "name": "64 - Reinforcement Learning part 12 - A3C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
