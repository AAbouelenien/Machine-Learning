{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Method</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook will be moving on to a new topic known as Monte Carlo\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Monte Carlo Method</h3>\n",
    "\n",
    "Now, like the term dynamic programming, Monte Carlo is also not a technique that is specific to reinforcement learning\n",
    "\n",
    "We may have seen dynamic programming in other contexts, so, too, with Monte Carlo\n",
    "\n",
    "Monte Carlo is a general technique that can be used when we want to estimate some quantity with samples\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Typical Monte Carlo Example: Estimate $\\pi$</h3>\n",
    "\n",
    "The typical example people use to introduce Monte Carlo goes something like this, although the instructor is not a huge fan of this example\n",
    "\n",
    "Basically suppose we want to estimate $\\pi$\n",
    "\n",
    "Yes, we know that it's $3.14159...$, but let's pretend that we do not know this value\n",
    "\n",
    "How can we estimate $\\pi$?\n",
    "\n",
    "The Monte Carlo approach is this\n",
    "\n",
    "Suppose that we have a square and we're able to generate random samples of points uniformly inside this square \n",
    "\n",
    "For simplicity, let the side length of the square be 1\n",
    "\n",
    "Now let's draw a circle inside this square such that the square fills the circle\n",
    "\n",
    "Clearly the circle has diameter 1 or radius 1/2\n",
    "\n",
    "<img src='extras/56.1.PNG' width='300'></img>\n",
    "\n",
    "$\\large \\text{side length } = 1, \\text{radius of circle } = \\frac{1}{2}$\n",
    "\n",
    "Now let's consider the areas of the square and the circle\n",
    "\n",
    "$\\large A_\\text{square} = 1 \\times 1 = 1$\n",
    "\n",
    "$\\large A_\\text{circle} = \\pi r^2 = \\frac{\\pi}{4}$\n",
    "\n",
    "The key points are recognizes this\n",
    "\n",
    "The ratio of the area between the circle and the square is $\\frac{\\pi}{4}$\n",
    "\n",
    "This means that if we draw many samples uniformly inside the square, we will find that they fall inside the circle $\\frac{\\pi}{4}$\n",
    "\n",
    "That is to say, $\\frac{\\pi}{4}$ tells us the proportion of uniformally sampled points that will fall inside the circle\n",
    "\n",
    "Put another way, suppose we draw a very large number of points uniformly inside the square\n",
    "\n",
    "We should find that the number of points $C$ that fall inside the circle divided by the total number of points $N$ is approximately equal to $\\frac{\\pi}{4}$\n",
    "\n",
    "$\\large \\frac{\\text{# points in circle}}{\\text{# points in square}} = \\frac{C}{N} = \\frac{A_\\text{circle}}{A_\\text{square}} = \\frac{\\pi}{4}$\n",
    "\n",
    "Therefore, our estimate of $\\pi$, $\\hat \\pi$ will be \n",
    "\n",
    "$$\\large \\hat \\pi = 4 \\frac{C}{N}$$\n",
    "\n",
    "OK, so this is the typical example people use to introduce Monte Carlo, which gives us the sense\n",
    "that we can use sampling of random numbers to estimate some value\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Real-World Use of Monte Carlo</h3>\n",
    "\n",
    "So why does the isntructor not like this example?\n",
    "\n",
    "Well, this example doesn't really tell us how Monte Varlo is used in the real world\n",
    "\n",
    "In fact, the way Monte Carlo is used in the real world is much simpler, and it's actually a lot closer to how we will use it in these notebooks\n",
    "\n",
    "And obviously, this is because reinforcement learning is a real world problem\n",
    "\n",
    "So what is, in the instructor's opinion, a simpler introduction to Monte Carlo?\n",
    "\n",
    "The main application of Monte Carlo is in estimating expected values\n",
    "\n",
    "We may recall that we emphasised this in previous notebooks\n",
    "\n",
    "It played a large role in the MDP notebook, it played a large role in the dynamic programming notebook, and it's going to, again, play a larger role now\n",
    "\n",
    "So suppose we want the expected value of $X$ where $X$ is drawn from some distribution $p(X)$\n",
    "\n",
    "Now, normally, if we knew $p(X)$, we could just use the usual formula to calculate $E(X)$\n",
    "\n",
    "$$\\large E(X) = \\sum_x xp(x)$$\n",
    "\n",
    "This is an integral if $X$ is continuous and a sum if $X$ is discrete\n",
    "\n",
    "But suppose we do not know $p(X)$\n",
    "\n",
    "Here's a simple example, suppose that we're in charge of city planning and we want to know the expected speed of vehicles driving down some road \n",
    "\n",
    "In order to compute this expected speed, we must know the distribution of speeds $p(X)$\n",
    "\n",
    "Of course, there's no way for us to know this distribution\n",
    "\n",
    "WE can't call every car owner and ask them to tell us their speed distribution and then try to combine them with some overall distribution of speeds\n",
    "\n",
    "But what can we do instead?\n",
    "\n",
    "Well, we can simply measure the speeds of actual cars driving down the road\n",
    "\n",
    "If Iweadd them all together and divide by $N$, we will get the sample mean\n",
    "\n",
    "As you recall, the sample mean is an estimate of the true mean, which is the $E(X)$\n",
    "\n",
    "$$\\large E(X) \\approx \\frac{1}{N} \\sum^N_{i=1} x_i$$\n",
    "\n",
    "Believe it or not, this simple process is Monte Carlo :)\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Monte Carlo in Reinforcement Learning</h3>\n",
    "\n",
    "So how does this apply and reinforcement learning?\n",
    "\n",
    "Well, let's recall what we have done so far\n",
    "\n",
    "We've essentially solved the reinforcement learning problem\n",
    "\n",
    "Given some environment and state transition distribution, we know how to find the optimal policy for an agent acting in that environment\n",
    "\n",
    "So what's the problem?\n",
    "\n",
    "The problem is that our previous formulation had some pretty serious constraints\n",
    "\n",
    "Specifically, we don't actually know the state transition distribution, $p(s^\\prime \\vert s,a)$ is typically unknown.\n",
    "\n",
    "For example, imagine we're trying to build a computer program that drives an autonomous vehicle\n",
    "\n",
    "There's absolutely no way we can practically enumerate all the possible states that a vehicle could be in and then come up with the next state probabilities\n",
    "\n",
    "Furthermore, we'll recognize that our task in reinforcement learning is essentially to solve the Bellman equation to find the value function $V$ or $Q$, and according to the Bellman equation, this can be defined recursively in terms of an expectation\n",
    "\n",
    "So let's summarize what we have\n",
    "\n",
    "We have an expected value that we want to compute\n",
    "\n",
    "$$\\large V_\\pi(s) = \\sum_a \\color{green}{\\pi(a \\vert s)} \\sum_{s^\\prime,r} \\color{red}{p(s^\\prime,r \\vert s,a)}[r+\\gamma V_\\pi(s^\\prime)]$$\n",
    "\n",
    "recall : $\\color{green}{Agent}, \\color{red}{Enviroment}$\n",
    "\n",
    "The problem is we can't compute this expected value because we don't know the distribution $p$ that the expected value was taken over\n",
    "\n",
    "Now, technically, in this expected value, there are two distributions, there's $p$ which represents the environment dynamics and there's $\\pi$, which represents the agent's policy\n",
    "\n",
    "The policy is what we program in code, so this part we do know\n",
    "\n",
    "However, not knowing one part is enough to prevent us from computing this expected value\n",
    "\n",
    "And thus this is exactly the scenario we described previously for which Monte Carlo can be a possible solution\n",
    "\n",
    "We want an expected value, but we don't know the distribution, we only have samples\n",
    "\n",
    "Therefore, our solution will be to estimate this expected value with the sample mean\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Notebook Outline</h3>\n",
    "\n",
    "So the outline for this section will be as follows\n",
    "\n",
    "As we'll see, it's pretty simple and it mirrors\n",
    "what we did for dynamic programming\n",
    "\n",
    "Again, we'll start with the prediction task, that is finding the $V(s)$ given a policy\n",
    "\n",
    "This will allow us to establish the basics of the Monte Carlo method\n",
    "\n",
    "The second step will be to solve the control task\n",
    "\n",
    "As we'll see, using $V(s)$ is no longer a viable solution, so we'll need to involve the action value $Q$ \n",
    "\n",
    "We'll study two approaches for the control task, one that builds up from the basics but isn't that practical and one that is more practical\n",
    "\n",
    "This will involve applying what we learned in the bandit notebook specifically that explore exploit dilema\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Important Fact to Notice</h3>\n",
    "\n",
    "One clear distinction between this notebook and the previous notebooks is this \n",
    "\n",
    "Reinforcement learning is all about trying to learn from experience, this is even encoded in its name\n",
    "\n",
    "Reinforcement refers to the fact that we can provide the agent with rewards and the agent uses these rewards to modify and adapt its behavior\n",
    "\n",
    "Greater reward acts as positive reinforcement, encouraging similar behavior in the future\n",
    "\n",
    "Lesser rewards act as negative reinforcement, discouraging similar behavior in the future\n",
    "\n",
    "The curious thing about the dynamic programming section was that it did not involve gaining any experience\n",
    "\n",
    "We were able to solve the problem using only mathematics\n",
    "\n",
    "Of course, this relied on an unrealistic assumption that we knew the environment dynamics.\n",
    "\n",
    "Now, by building the environment ourselves, we can ensure that this was the case \n",
    "\n",
    "And noticed that this is actually a great benefit to us in this notebook\n",
    "\n",
    "Because we built the environment ourselves, we're able to use the same environment for every notebook that uses Gridworld\n",
    "\n",
    "By doing so, it allows us to compare each of the techniques we learn about in a very fair and consistent manner \n",
    "\n",
    "If we kept using different environments in each notebook, it wouldn't give us an intuitive sense for how these techniques compare to one another\n",
    "\n",
    "So using the same environment each time is an advantage in that sense\n",
    "\n",
    "Monte Carlo is the first notebook that will show us how to solve an MDP using experience only without needing to rely on the environment dynamics\n",
    "\n",
    "So as you go through this notebook, compare and contrast these methods with what we learned previously in terms of performance, effectiveness, implementation and assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll be discussing how we can use Monte Carlo for evaluating a policy \n",
    "\n",
    "That is given a policy $\\pi$, how do we find $V_\\pi$ or $Q_\\pi$ without making use of the environment dynamics \n",
    "\n",
    "---\n",
    "\n",
    "<h3>Monte Carlo Prediction</h3>\n",
    "\n",
    "To understand how to do this, let's start with the definition of the state value function $V_\\pi(s)$\n",
    "\n",
    "Note that we do not need to use the Bellman equation ( since again it depends on the envoroment dynamics which is now unavailable )\n",
    "\n",
    "In this case, it's more convenient to express $V_\\pi$ in terms of $G$\n",
    "\n",
    "$$\\large V_\\pi(s) = E[G_t \\vert S_t = s] \\approx \\frac{1}{N} \\sum^N_{i=1} G_{i,s} $$\n",
    "\n",
    "note : overloaded symbol $G_{i,s}$ represents the $i$'th sample return from the state $s$\n",
    "\n",
    "So what does this definition tell us?\n",
    "\n",
    "It tells us that if we want to find the value function, we can do so by taking the average of many returns sampled from the environment\n",
    "\n",
    "Note that because the expected value is conditioned on the state $s$, we will have a different estimate for each state\n",
    "\n",
    "Basically, all we need to do is play a bunch of episodes using our given policy and collect all the $G$s from those episodes\n",
    "\n",
    "When we're done, we averaged the $G$s so that we can estimate the expected value\n",
    "\n",
    "OK, so hopefully that's pretty simple\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Monte Carlo Prediction: Complication #1 </h3>\n",
    "\n",
    "Now, there are several complications to consider\n",
    "\n",
    "What if we want to know the value of a state not visited by our policy \n",
    "\n",
    "In this case, we would have no samples and hence no estimate\n",
    "\n",
    "One solution to this is to simply not compute any value for those states\n",
    "\n",
    "Since those states are never visited, their values are irrelevant\n",
    "\n",
    "Another solution to this is to manually put the agents into different starting states\n",
    "\n",
    "For example, in Gridworld, this would mean not starting in the same position on every episode\n",
    "\n",
    "Instead, we could choose starting positions at random to ensure that every state will have corresponding sample returns\n",
    "\n",
    "<img src='extras/56.2.PNG' width='300'></img>\n",
    "\n",
    "Note that this doesn't violate our policy because the return $G$ is calculated from future rewards, which are all received based on following the policy\n",
    "\n",
    "Another thing to consider is that if our policy is probabilistic with a non-zero probability of performing every action from every state, then this wouldn't be a problem\n",
    "\n",
    "Given enough time, we would collect a sufficient number of samples for each state\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Monte Carlo Prediction: Complication #2 </h3>\n",
    "\n",
    "The second complication to consider is this \n",
    "\n",
    "What if our policy is such that we encounter the same state more than once \n",
    "\n",
    "In this case, what is the return for the state?\n",
    "\n",
    "<img src='extras/56.3.PNG' width='250'></img>\n",
    "\n",
    "There are actually two solutions to this problem\n",
    "\n",
    "Solution number one is to consider the return only for the first time the state was visited\n",
    "\n",
    "This is called first visit Monte Carlo\n",
    "\n",
    "Solution number two is to consider the return for every time the state was visited\n",
    "\n",
    "This is called Every Visit Monte Carlo\n",
    "\n",
    "It turns out that we can prove theoretically that these will both converge to the true answer\n",
    "\n",
    "They have different convergence properties, but we would consider these details to be outside the scope of this notebook as they are not helpful for where we are going\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Monte Carlo Prediction: Complication #3 </h3>\n",
    "\n",
    "Yet another complication to consider is this \n",
    "\n",
    "This is somewhat related to the previous issue, which is that we may encounter the same state more than once\n",
    "\n",
    "So now let's consider the problem where our policy leads to an infinite cycle\n",
    "\n",
    "For example, suppose in one state the policy is to go left, but then in the state to the left, the policy is to go right\n",
    "\n",
    "Clearly, this will just lead to going left and right forever\n",
    "\n",
    "<img src='extras/56.4.PNG' width='250'></img>\n",
    "\n",
    "The greater issue here is that what if we have an episode that never ends \n",
    "\n",
    "In this case, Monte Carlo methods do not apply because by definition of the Monte Carlo method, we can only compute the value once we know the return, but we only know the return after the episode is terminated\n",
    "\n",
    "If the episode does not terminate, then the return cannot be computed and Monte Carlo methods cannot be employed\n",
    "\n",
    "Practically speaking, when it comes to our environment, we will declare our episode complete when it reaches a certain number of steps\n",
    "\n",
    "For example, we consider a $20$ steps or $100$ steps to be the end of an episode if we haven't yet reached the terminal state\n",
    "\n",
    "So even if there is an infinite cycle, the episode will still terminate\n",
    "\n",
    "Note that while this might seem like a hack, this is not really the case\n",
    "\n",
    "For example, in other environments like Cartpole and Mountain Car, which are part of OpenAI Gym,\n",
    "the episodes end after we reach 200 steps\n",
    "\n",
    "So this is a completely normal thing to do\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Pseudocode: Monte Carlo Prediciton</h3>\n",
    "\n",
    "OK, so now that you understand some of the hidden details of the Monte Carlo method, let's consider\n",
    "how this will look like in pseudocode \n",
    "\n",
    "```\n",
    "Given: π\n",
    "Initialise: V(s) = 0, returns(s) = [] for all s in state space\n",
    "\n",
    "Loop until convergance:\n",
    "    Play epsiode following π, obtain s(0),r(1),s(1),a(1),...,r(T),s(T)\n",
    "    G = 0\n",
    "    for t in {T-1,T-2,...,0}:\n",
    "        G = r(t+1) + γG\n",
    "        if s(t) not in {s(1),...,s(t-1)}:\n",
    "            returns(s).append(G)\n",
    "            V(s) = mean(returns(s))\n",
    "```\n",
    "\n",
    "To start will be given a policy $\\pi$ that we wish to evaluate\n",
    "\n",
    "To initialize our algorithm,wWe'll start by initializing our value function to zeros and we'll create a dictionary to store all of the returns we've collected for each state\n",
    "\n",
    "The key for this dictionary will be the state and the value will be a list of returns that we've collected for that state\n",
    "\n",
    "Next we'll enter a loop that will continue for as many iterations we think we need to obtain an accurate estimate\n",
    "\n",
    "Of course, this will depend on how many samples we wish to collect\n",
    "\n",
    "We might consider evaluating the accuracy using confidence intervals \n",
    "\n",
    "Inside the loop, we start by playing an episode using the given policy\n",
    "\n",
    "This will generate a sequence of states and rewards\n",
    "\n",
    "Next, we initialize a variable $G=0$\n",
    "\n",
    "This will hold the return for each step of the following loop\n",
    "\n",
    "Note that because the return is always based on the sum of future rewards, it's more practical to loop through our episode backwards and compute the return recursively\n",
    "\n",
    "The return $G$ starts out at zero since the return for the terminal state is zero\n",
    "\n",
    "Next, we loop through each timestep of the episode, starting at $T-1$ \n",
    "\n",
    "note that we do not start at the final timestep $T$ because we know that the value for that state will always be zero\n",
    "\n",
    "So inside this loop, we begin by updating the return $G$ using the recursive formula we derived in an earlier notebook\n",
    "\n",
    "Next, we check whether or not the state at time $t$ occurred earlier in the episode\n",
    "\n",
    "This is when we want to do first visit Monte Carlo\n",
    "\n",
    "If we want to do every visit Monte Carlo, then this check is not necessary\n",
    "\n",
    "So only if this state does not appear earlier in the episode do we proceed \n",
    "\n",
    "Next, we simply append $G$ to our list of sample returns for the state $s(t)$\n",
    "\n",
    "And finally, we update $V(s(t))$ by taking the average of the returns we've collected so far\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "<h3>One Tricky Implementation Detail</h3>\n",
    "\n",
    "OK, so before we end this section, we want to make a small note about one tricky implementation detail\n",
    "\n",
    "According to the instructor, we should attempt coding first then we can return here (we foud no problem finishing the excercise)\n",
    "\n",
    "---\n",
    "\n",
    "Basically, the key difficulty in implementing Monte Carlo is keeping track of the time indices\n",
    "\n",
    "```python\n",
    "grid.set_state(randomly_chosen_state)\n",
    "s = grid.current_state()\n",
    "\n",
    "states = [s]\n",
    "rewards = []\n",
    "\n",
    "while not grid.game_over():\n",
    "    a = policy[s]\n",
    "    r = r.grid.move(a)\n",
    "    rewards.append(r)\n",
    "    s = grid.current_state()\n",
    "    states.append(s)\n",
    "```\n",
    "\n",
    "Let's begin with a naive implementation of Monte Carlo, where we keep track of every state and reward that we encounter\n",
    "\n",
    "We start by randomly choosing a state to begin our episode \n",
    "\n",
    "Next week, grab the current state and store it in our list of states\n",
    "\n",
    "We also initialize a list to store our rewards, \n",
    "\n",
    "Since we haven't received any rewards yet, it's just an empty list\n",
    "\n",
    "Next, we do a loop that exits when the game is over\n",
    "\n",
    "Note that we're ignoring the maximum time steps for simplicity, although in the code you will want to implement that as well\n",
    "\n",
    "Inside the loop we grab the action according to our policy and perform that move in the environment\n",
    "\n",
    "We then receive a reward which we append to uor lists of rewards\n",
    "\n",
    "At this point, we can call the current state function again to get the new state and append this to our list of states\n",
    "\n",
    "So what's the problem with this?\n",
    "\n",
    "Firstly, notice how the list of states and rewards do not have the same length\n",
    "\n",
    "When we start the episode, we have an initial state, but there's no such thing as an initial reward\n",
    "\n",
    "The bigger problem is this\n",
    "\n",
    "When we index the states list and the rewards list, the index will not refer to the same time step in both lists\n",
    "\n",
    "This will lead to lots of confusion unless we track things very carefully\n",
    "\n",
    "For example, if you want to have some time index $t$ the state list indexed by $t$ does not refer to the same timestep as the rewards list index by $t$\n",
    "\n",
    "Therefore, using $t$ would be very misleading because it doesn't refer to the same timestep in both cases\n",
    "\n",
    "---\n",
    "\n",
    "Here's another way to do this that will make things a bit simpler to think about\n",
    "\n",
    "It's basically a one character change where we add a zero to the initial rewards list\n",
    "\n",
    "```python\n",
    "grid.set_state(randomly_chosen_state)\n",
    "s = grid.current_state()\n",
    "\n",
    "states = [s]\n",
    "rewards = [0]\n",
    "\n",
    "while not grid.game_over():\n",
    "    a = policy[s]\n",
    "    r = r.grid.move(a)\n",
    "    rewards.append(r)\n",
    "    s = grid.current_state()\n",
    "    states.append(s)\n",
    "```\n",
    "\n",
    "It's equivalent to saying we get a reward of zero at time zero when  we start the episode, which has no effect except to make the code easier to reason about\n",
    "\n",
    "Notice that now when we index the states list and the rewards list, the index will correspond to the same time step for both lists\n",
    "\n",
    "We'll see that in the code this value is never actually used\n",
    "\n",
    "In fact, if we want to go back to the pseudocode we saw previously, we can confirm that this is\n",
    "true even without looking at any code\n",
    "\n",
    "Furthermore, note that the state at the final timestep as of $s(T)$ is also never used\n",
    "\n",
    "So one kind of dangerous thing in terms of bugs is if we ignored the first reward and ignored the final state\n",
    "\n",
    "In that case, both the states list and the rewards list would have the same length, but they would also be off by one in terms of which index corresponds to which timestep\n",
    "\n",
    "So in the instructor's opinion, it's best to add these dummy values that will never be used, but make the time steps line up correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try implementing policy evaluation using Monte Carlo\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets implement our Gridworld enviroment\n",
    "# we will be using the same interface as described above\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,rows,cols,start_pos):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.pos = start_pos\n",
    "    \n",
    "    def current_state(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def set_state(self,state):\n",
    "        self.pos = state\n",
    "        \n",
    "    def set_actions_rewards(self,actions,rewards):\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        \n",
    "    def all_states(self,):\n",
    "        return [(i,j) for i in range(self.rows) for j in range(self.cols)]\n",
    "    \n",
    "    def is_terminal(self,state):\n",
    "        # we cant perform an action in a terminal state\n",
    "        # so we expect not to find it in the keys\n",
    "        return state not in self.actions.keys()\n",
    "    \n",
    "    def game_over(self):\n",
    "        return self.is_terminal(self.pos)\n",
    "    \n",
    "    def next_state(self,s,a):\n",
    "        # return next state should we take action a\n",
    "        # but do not take that action\n",
    "        i,j = s\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(s,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        return (i,j)\n",
    "    \n",
    "    def move(self,a):\n",
    "        # return next state should we take action a\n",
    "        # this time we actually make the move\n",
    "        # and thus return reward\n",
    "        i,j = self.pos\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(self.pos,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        self.pos = (i,j)\n",
    "        # if no reward return 0\n",
    "        return self.rewards.get(self.pos,0)\n",
    "    \n",
    "    # lets add methods to print values and actions\n",
    "    def print_values(self,V):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*9*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                v = V[i,j]\n",
    "                if v >= 0:\n",
    "                    # if 0 or +ve, add space so it aligns well when we have -\n",
    "                    print(\" %.2f|\" %v,end=\"\")\n",
    "                else:\n",
    "                    print(\"%.2f|\" %v,end=\"\")\n",
    "            print(\"\")\n",
    "    \n",
    "    def print_policy(self,P):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*6*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                print(\" %s |\" %P.get((i,j),' '),end=\"\")\n",
    "            print(\"\")\n",
    "                \n",
    "    def state_to_loc(self,state): # convert state tupe (0,1) to location 1\n",
    "        return state[0]*self.rows + state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standrad_grid():\n",
    "    g = GridWorld(3,4,(2,0))\n",
    "    rewards = {(0,3):1,(1,3):-1}\n",
    "    actions = {\n",
    "        (0,0): ('D','R'),\n",
    "        (0,1): ('L','R'),\n",
    "        (0,2): ('L','D','R'),\n",
    "        (1,0): ('U','D'),\n",
    "        (1,2): ('U','D','R'),\n",
    "        (2,0): ('U','R'),\n",
    "        (2,1): ('L','R'),\n",
    "        (2,2): ('L','R','U'),\n",
    "        (2,3): ('L','U')\n",
    "    }\n",
    "    g.set_actions_rewards(actions,rewards)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = standrad_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed policy (deterministic)\n",
    "policy = {\n",
    "    (2,0) : 'U',\n",
    "    (1,0) : 'U',\n",
    "    (0,0) : 'R',\n",
    "    (0,1) : 'R',\n",
    "    (0,2) : 'R',\n",
    "    (1,2) : 'R',\n",
    "    (2,1) : 'R',\n",
    "    (2,2) : 'R',\n",
    "    (2,3) : 'U'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | R |   |\n",
      "------------------\n",
      " U | R | R | U |\n"
     ]
    }
   ],
   "source": [
    "g.print_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_eval():\n",
    "    all_states = g.all_states()\n",
    "    state_space = len(all_states)\n",
    "    non_terminal_states = list(g.actions.keys())\n",
    "    \n",
    "    V = np.zeros(state_space).reshape((g.rows,g.cols))\n",
    "    returns = {s:[] for s in all_states if not g.is_terminal(s)}\n",
    "    gamma = 0.9\n",
    "    for episode in range(100): # play 100 episodes\n",
    "        # start in random state\n",
    "        start_idx = np.random.choice(len(non_terminal_states))\n",
    "        g.set_state(non_terminal_states[start_idx])\n",
    "        states_rewards = []\n",
    "        \n",
    "        for t in range(20) : # max 20 time steps for each episode\n",
    "            s = g.current_state()\n",
    "            a = policy[s]\n",
    "            r = g.move(a)\n",
    "            s_prime = g.current_state()\n",
    "            states_rewards.append((s,r))\n",
    "\n",
    "            \n",
    "            if g.is_terminal(s_prime):\n",
    "                break\n",
    "            \n",
    "        G = 0\n",
    "        # store seen (s,r) pairs\n",
    "        seen = set()\n",
    "        for s,r in reversed(states_rewards):\n",
    "            G = r + gamma*G\n",
    "            # first visit monte carlo\n",
    "            if s not in seen: # remove this for mulitstep monte carlo\n",
    "                seen.add((s,r))\n",
    "                returns[s].append(G)\n",
    "                V[s] = np.mean(returns[s])\n",
    "    return V\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = monte_carlo_policy_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n"
     ]
    }
   ],
   "source": [
    "g.print_values(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we just learn how to solve the predicition problem using Monte Carlo \n",
    "\n",
    "That is given a policy $\\pi$, we learned how to find $V_\\pi(s)$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Monte Carlo for Control</h3>\n",
    "\n",
    "As we know, the next question to consider is, how do we solve the control problem?\n",
    "\n",
    "That is, how do we find the best policy $\\pi^*$?\n",
    "\n",
    "---\n",
    "\n",
    "<h3>We already know how to solve this</h3>\n",
    "\n",
    "Let's take a moment to consider that we actually already have all the tools we need to do this\n",
    "\n",
    "In the previous notebook we learned about the concept of policy iteration\n",
    "\n",
    "This is the idea that if we want to find the best policy, all we need to do is start from a random\n",
    "policy\n",
    "\n",
    "Then we find that policy's value function\n",
    "\n",
    "After doing so, we can apply the policy improvement theorem, which allows us to find a better policy given an existing policy and its corresponding value\n",
    "\n",
    "So it seems like we're pretty much already there\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Rough Pseudocode</h3>\n",
    "\n",
    "So let's write out some rough pseudocode, so think about how this will work\n",
    "\n",
    "```\n",
    "Initialise: random policy π\n",
    "Loop:\n",
    "    # evaluation step\n",
    "    V(s) = MonteCarloEvaluate(π)\n",
    "    \n",
    "    # improvement step\n",
    "    for s in non-terminal states:\n",
    "```\n",
    "\n",
    "$\\qquad \\qquad \\quad \\pi(s) = \\arg \\max\\limits_a \\sum p(s^\\prime,r \\vert s,a)(r + \\gamma V(s^\\prime))$\n",
    "\n",
    "We'll start with a random policy\n",
    "\n",
    "We know how to find the value function for this policy, even if we do not know the environment transitions\n",
    "\n",
    "We know that we can just use experience from the environment and apply the Monte Carlo method\n",
    "\n",
    "The next step is the improvement step, where we essentially take the arg max of the right hand side of the Bellman equation\n",
    "\n",
    "This gives us the optimal action for each state\n",
    "\n",
    "But there's a problem here\n",
    "\n",
    "See, the right hand side involves an expected value\n",
    "\n",
    "We can't compute the expected value because it involves a summation over $p(s^\\prime,r \\vert s,a)$ and we've established that we do not know this\n",
    "\n",
    "So what can we do?\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Rough Pseudocode 2</h3>\n",
    "\n",
    "The answer is simple\n",
    "\n",
    "We recall that the right hand side of the expected value just happens to equal $Q$ \n",
    "\n",
    "$$\\large Q(s,a) = \\sum_{s^\\prime,r} p(s^\\prime,r \\vert s,a)(r+ \\gamma V(s^\\prime))$$\n",
    "\n",
    "If we know $Q$ than policy improvement is not a problem\n",
    "\n",
    "```\n",
    "Initialise: random policy π\n",
    "Loop:\n",
    "    # evaluation step\n",
    "    Q(s,a) = MonteCarloEvaluate(π)\n",
    "    \n",
    "    # improvement step\n",
    "    for s in non-terminal states:\n",
    "```\n",
    "\n",
    "$\\qquad \\qquad \\quad \\pi(s) = \\arg \\max\\limits_a Q(s,a)$\n",
    "\n",
    "So this should help us understand why for control problems in some of the following notebooks we $Q$ and not $V$ \n",
    "\n",
    "The one exception to this is dynamic programming were using $V$ makes sense because we can compute that expected value\n",
    "\n",
    "As an exercise, we might want to consider how we might modify the code for solving the prediction\n",
    "problem with $V$ to find $Q$ instead, that is, how can we solve the previous exercise for $Q$ instead of $V$?\n",
    "\n",
    "note : same conputation, this time for each tuple $(s,a)$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>How to improve our pseudocode</h3>\n",
    "\n",
    "Now, although the strategy we've come up with will work, it's still not ideal\n",
    "\n",
    "As you recall, there's one downside to policy iteration, which is that it's pretty slow\n",
    "\n",
    "This slowness gets even worse when we consider that because we need to use sampling gathering, enough experience can take a very long time\n",
    "\n",
    "Furthermore, because we now need to find $Q$ instead of we need even more samples than before \n",
    "\n",
    "As you recall $Q$ requires us to estimate $|S| \\times |A| $ values, whereas $V$ only requires $\\vert S \\vert$ values\n",
    "\n",
    "So that's a lot of values to estimate\n",
    "\n",
    "But we know there's a trick we can use which basically says forget about trying to accurately estimate the value, just combine policy improvement and the value update into a single step and they'll eventually converge to the optimal policy and the optimal value\n",
    "\n",
    "We call this method value iteration\n",
    "\n",
    "With Monte Carlo, we're going to do something similar \n",
    "\n",
    "Instead of playing many episodes to accurately\n",
    "estimate $Q$ We'll just play one episode \n",
    "\n",
    "After playing that one episode, we'll update $Q$ with the new returns we received and will run policy improvement on our newly updated $Q$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Problem</h3>\n",
    "\n",
    "So there's still one problem we have to consider\n",
    "\n",
    "We know that in order to run policy improvement, we must be able to search through $Q(s,a)$ over all actions for $a$ given state $s$\n",
    "\n",
    "This will tell us the best actions to perform given the state $s$\n",
    "\n",
    "$$\\large \\pi(s) = \\arg \\max_a Q(s,a)$$\n",
    "\n",
    "But this requires that we populate $Q(s,a)$ for all possible states and all possible actions\n",
    "\n",
    "Now, why is this a problem?\n",
    "\n",
    "Well, suppose that we've just started our algorithm\n",
    "\n",
    "We follow our policy and obtain samples according to our policy, but our policy only tells us which\n",
    "action to perform in each state\n",
    "\n",
    "Therefore, our $G$ samples will correspond only to the actions prescribed by our policy\n",
    "\n",
    "For other actions, we will not have any samples and therefore, taking in $\\arg \\max$ doesn't make any sense\n",
    "\n",
    "We can't take an $\\arg \\max$ over a list of values if we don't know all the values\n",
    "\n",
    "So what's the solution?\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Exploring Starts Method</h3>\n",
    "\n",
    "Imagine this, imagine that we start each episode from our randomly selected state and perform a randomly selected action\n",
    "\n",
    "If we like, picture grid world where we start from a random square each time and we randomly choose left, right, up or down as our initial action \n",
    "\n",
    "Our return for this state and this action, $Q(s_0,a_0)$ will just be the sum of rewards over that episode\n",
    "\n",
    "Now, if by this random selection we collect enough samples for all states and all actions, then our problem is solved\n",
    "\n",
    "And of course, since we get to choose these initial states and actions, it'll be pretty easy to make sure that this is the case\n",
    "\n",
    "We call this method the exploring starts method\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Pseudocode</h3>\n",
    "\n",
    "Let's look at some pseudocode so we can see how this will work in detail\n",
    "\n",
    "$\\text{Initialise}: \\\\ \\qquad \\pi = \\text{random policy} \\\\ \\qquad Q(s,a) = \\text{arbitary (0 for terminal states)} \\\\ \\qquad returns[s,a] = [] \\text{ (empty list for all non-terminal states)} \\\\ \\text{Loop:} \\\\ \\qquad \\text{Randomly select initial state-action } (s_0,a_0) \\\\ \\qquad \\text{Play an episode to get }(r_1,s_1,a_1,\\ldots,r_T,s_T) \\\\ \\qquad G = 0 \\\\ \\qquad \\text{for t in \\{T-1,T-2,...,0\\}:}  \\\\ \\qquad \\qquad G = r_{t+1} + \\gamma G \\\\ \\qquad \\qquad \\text{if } s_t,a_t \\text{ dont appear earlier in the episode:} \\\\ \\qquad \\qquad \\qquad \\text{returns}(s_t,a_t).append(G) \\\\ \\qquad \\qquad \\qquad Q(s_t,a_t) = mean(returns(s_t,a_t)) \\\\ \\qquad \\qquad \\qquad \\pi(s_t) = \\arg \\max\\limits_a Q(s_t,a)$\n",
    "\n",
    "so we begin by initializing some random policy $\\pi$ where we assign a random action for each state\n",
    "\n",
    "Note that this is a deterministic policy\n",
    "\n",
    "We'll then arbitrarily initialize a $Q$ table, note that this does not have to correspond to the policy $\\pi$\n",
    "\n",
    "We'll also create a data structure that will store the returns that we receive for each (state,action) pair\n",
    "\n",
    "These are the $G$ samples\n",
    "\n",
    "Initially, these will all be empty lists and every time we find a new $G$, we will appended to the list corresponding to the state action pair it goes with\n",
    "\n",
    "Next, we enter a loop that runs many times \n",
    "\n",
    "How many times we run this loop depends on the desired accuracy of our Monte Carlo estimate\n",
    "\n",
    "Inside the loop we'll choose at random and initial starting state $s_0$, and and initial action $a_0$ \n",
    "\n",
    "Then we'll play an episode starting from $(s_0,a_0)$ following our current policy $\\pi$\n",
    "\n",
    "This will give us a sequence of states, actions and rewards from our episode\n",
    "\n",
    "Next, we're essentially going to update $Q$ using Monte Carlo sampling and then run policy improvement\n",
    "\n",
    "As you recall, $G$ recursively depends on future $G$s, so it's easiest to live through that and so backwards\n",
    "\n",
    "So we'll start by initializing $G$ to zero\n",
    "\n",
    "Then we'll live through each step of the episode, starting at time step $T-1$ \n",
    "\n",
    "As we recall, there's no need to update the value of the terminal state because that is always $0$\n",
    "\n",
    "Inside the loop, we update $G$ using the usual recursive formula\n",
    "\n",
    "Next, we check whether or not the state-action pair $(s_t,a_t)$ appears anywhere earlier in our episode\n",
    "\n",
    "Only if this is not the case do we update $Q$\n",
    "\n",
    "As we recall, we call this first visit Monte Carlo\n",
    "\n",
    "So if it's OK to update, then we append $G$ to our list of returns for the given state-action pair\n",
    "\n",
    "Next, we update $Q$ by taking the sample mean of the returns we've collected for this state-action\n",
    "pair\n",
    "\n",
    "So this is our Monte Carlo estimate\n",
    "\n",
    "Finally, we perform policy improvement by setting $\\pi(s_t)$ to be the $\\arg \\max$ of $Q$ for the given state over all possible actions\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Making it even more efficient</h3>\n",
    "\n",
    "Now, there's one more thing to discuss in this section \n",
    "\n",
    "Note that the algorithm we've presented is not\n",
    "as efficient as it could be\n",
    "\n",
    "As we recall, we learned that calculating sample means can be inefficient, especially when we have lots of samples\n",
    "\n",
    "More samples means more things to add up\n",
    "\n",
    "And this will grow as we collect more and more samples, making each episode a slower and slower to get through\n",
    "\n",
    "But we've already learned how to improve this calculation\n",
    "\n",
    "So for the next exercise, which will be to implement the Monte Carlo exploring starts method, not only should we implement what we discussed, but also consider how to make it more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So here goes\n",
    "# We will attempt coding Monte Carlo for Control problem + Exploring starts\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets implement our Gridworld enviroment\n",
    "# we will be using the same interface as described above\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,rows,cols,start_pos):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.pos = start_pos\n",
    "        \n",
    "    def current_state(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def set_state(self,state):\n",
    "        self.pos = state\n",
    "        \n",
    "    def set_actions_rewards(self,actions,rewards):\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        \n",
    "    def all_states(self,):\n",
    "        return [(i,j) for i in range(self.rows) for j in range(self.cols)]\n",
    "    \n",
    "    def is_terminal(self,state):\n",
    "        # we cant perform an action in a terminal state\n",
    "        # so we expect not to find it in the keys\n",
    "        return state not in self.actions.keys()\n",
    "    \n",
    "    def game_over(self):\n",
    "        return self.is_terminal(self.pos)\n",
    "    \n",
    "    def next_state(self,s,a):\n",
    "        # return next state should we take action a\n",
    "        # but do not take that action\n",
    "        i,j = s\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(s,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        return (i,j)\n",
    "    \n",
    "    def move(self,a):\n",
    "        # return next state should we take action a\n",
    "        # this time we actually make the move\n",
    "        # and thus return reward\n",
    "        i,j = self.pos\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(self.pos,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        self.pos = (i,j)\n",
    "        # if no reward return 0\n",
    "        return self.rewards.get(self.pos,0)\n",
    "    \n",
    "    # lets add methods to print values and actions\n",
    "    def print_values(self,V):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*9*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                v = V[i,j]\n",
    "                if v >= 0:\n",
    "                    # if 0 or +ve, add space so it aligns well when we have -\n",
    "                    print(\" %.2f|\" %v,end=\"\")\n",
    "                else:\n",
    "                    print(\"%.2f|\" %v,end=\"\")\n",
    "            print(\"\")\n",
    "    \n",
    "    def print_policy(self,P):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*6*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                print(\" %s |\" %P.get((i,j),' '),end=\"\")\n",
    "            print(\"\")\n",
    "                \n",
    "    def state_to_loc(self,state): # convert state tupe (0,1) to location 1\n",
    "        return state[0]*self.rows + state[1]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standrad_grid():\n",
    "    g = GridWorld(3,4,(2,0))\n",
    "    rewards = {(0,3):1,(1,3):-1}\n",
    "    actions = {\n",
    "        (0,0): ('D','R'),\n",
    "        (0,1): ('L','R'),\n",
    "        (0,2): ('L','D','R'),\n",
    "        (1,0): ('U','D'),\n",
    "        (1,2): ('U','D','R'),\n",
    "        (2,0): ('U','R'),\n",
    "        (2,1): ('L','R'),\n",
    "        (2,2): ('L','R','U'),\n",
    "        (2,3): ('L','U')\n",
    "    }\n",
    "    g.set_actions_rewards(actions,rewards)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = standrad_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_control_exploring_starts():\n",
    "    all_states = g.all_states()\n",
    "    state_space = len(all_states)\n",
    "    non_terminal_states = list(g.actions.keys())\n",
    "    all_actions = ['U','R','D','L']\n",
    "    action_space = len(all_actions)\n",
    "    gamma = 0.9\n",
    "    # we will store deltas for Q\n",
    "    deltas = []\n",
    "    \n",
    "    \n",
    "    # lets follow the psuedocode\n",
    "    pi = {s:all_actions[np.random.choice(action_space)] for s in all_states if not g.is_terminal(s)}\n",
    "    num_samples = {}\n",
    "    Q = np.zeros((g.rows,g.cols,action_space))\n",
    "    Q_old = Q.copy()\n",
    "    episodes = 10000\n",
    "    \n",
    "    for episode in range(episodes): # play multiple episodes\n",
    "        \n",
    "        states_actions_rewards = []\n",
    "        \n",
    "        if (episode+1)%100 == 0:\n",
    "            print('episode: ',episode+1,'/',episodes,' done')\n",
    "        # start in a random position\n",
    "        s0 = non_terminal_states[np.random.choice(len(non_terminal_states))]\n",
    "        g.set_state(s0)\n",
    "        # and take a random action\n",
    "        a0 = all_actions[np.random.choice(action_space)]\n",
    "        r = g.move(a0)\n",
    "\n",
    "        states_actions_rewards.append((s0,a0,r))\n",
    "        \n",
    "        # now we can continue playing the game with our policy\n",
    "        for t in range(19): # for 20 actions, we already took 1, 19 remain\n",
    "            # since we already made a move\n",
    "            # check first whether or not we landed in a terminal state\n",
    "            if g.game_over():\n",
    "                break\n",
    "            s = g.current_state()\n",
    "            a = pi[s]\n",
    "            r = g.move(a)\n",
    "            \n",
    "            states_actions_rewards.append((s,a,r))\n",
    "            \n",
    "        G = 0\n",
    "        # store seen (s,a) pairs\n",
    "        seen = set()\n",
    "        for s,a,r in reversed(states_actions_rewards):\n",
    "            G = r + gamma*G\n",
    "            # first visit monte carlo\n",
    "            if s not in (seen): # remove this for mulitstep monte carlo\n",
    "                seen.add((s,a))\n",
    "                # update mean using our more effecient rule\n",
    "                num_samples[(s,a)] = num_samples.get((s,a),0)+1\n",
    "                mean_s_a = Q[s[0],s[1],all_actions.index(a)]\n",
    "                Q[s[0],s[1],all_actions.index(a)] = mean_s_a + 1/num_samples[(s,a)]*(G-mean_s_a)\n",
    "                pi[s] = all_actions[np.argmax(Q[s])]\n",
    "        delta = np.max(np.abs(Q - Q_old))\n",
    "        deltas.append(delta)\n",
    "        Q_old = Q.copy()\n",
    "    \n",
    "    # plot deltas\n",
    "    plt.plot(deltas)\n",
    "    plt.show()\n",
    "    # return V,pi    \n",
    "    return np.max(Q,axis=-1),pi   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  100 / 10000  done\n",
      "episode:  200 / 10000  done\n",
      "episode:  300 / 10000  done\n",
      "episode:  400 / 10000  done\n",
      "episode:  500 / 10000  done\n",
      "episode:  600 / 10000  done\n",
      "episode:  700 / 10000  done\n",
      "episode:  800 / 10000  done\n",
      "episode:  900 / 10000  done\n",
      "episode:  1000 / 10000  done\n",
      "episode:  1100 / 10000  done\n",
      "episode:  1200 / 10000  done\n",
      "episode:  1300 / 10000  done\n",
      "episode:  1400 / 10000  done\n",
      "episode:  1500 / 10000  done\n",
      "episode:  1600 / 10000  done\n",
      "episode:  1700 / 10000  done\n",
      "episode:  1800 / 10000  done\n",
      "episode:  1900 / 10000  done\n",
      "episode:  2000 / 10000  done\n",
      "episode:  2100 / 10000  done\n",
      "episode:  2200 / 10000  done\n",
      "episode:  2300 / 10000  done\n",
      "episode:  2400 / 10000  done\n",
      "episode:  2500 / 10000  done\n",
      "episode:  2600 / 10000  done\n",
      "episode:  2700 / 10000  done\n",
      "episode:  2800 / 10000  done\n",
      "episode:  2900 / 10000  done\n",
      "episode:  3000 / 10000  done\n",
      "episode:  3100 / 10000  done\n",
      "episode:  3200 / 10000  done\n",
      "episode:  3300 / 10000  done\n",
      "episode:  3400 / 10000  done\n",
      "episode:  3500 / 10000  done\n",
      "episode:  3600 / 10000  done\n",
      "episode:  3700 / 10000  done\n",
      "episode:  3800 / 10000  done\n",
      "episode:  3900 / 10000  done\n",
      "episode:  4000 / 10000  done\n",
      "episode:  4100 / 10000  done\n",
      "episode:  4200 / 10000  done\n",
      "episode:  4300 / 10000  done\n",
      "episode:  4400 / 10000  done\n",
      "episode:  4500 / 10000  done\n",
      "episode:  4600 / 10000  done\n",
      "episode:  4700 / 10000  done\n",
      "episode:  4800 / 10000  done\n",
      "episode:  4900 / 10000  done\n",
      "episode:  5000 / 10000  done\n",
      "episode:  5100 / 10000  done\n",
      "episode:  5200 / 10000  done\n",
      "episode:  5300 / 10000  done\n",
      "episode:  5400 / 10000  done\n",
      "episode:  5500 / 10000  done\n",
      "episode:  5600 / 10000  done\n",
      "episode:  5700 / 10000  done\n",
      "episode:  5800 / 10000  done\n",
      "episode:  5900 / 10000  done\n",
      "episode:  6000 / 10000  done\n",
      "episode:  6100 / 10000  done\n",
      "episode:  6200 / 10000  done\n",
      "episode:  6300 / 10000  done\n",
      "episode:  6400 / 10000  done\n",
      "episode:  6500 / 10000  done\n",
      "episode:  6600 / 10000  done\n",
      "episode:  6700 / 10000  done\n",
      "episode:  6800 / 10000  done\n",
      "episode:  6900 / 10000  done\n",
      "episode:  7000 / 10000  done\n",
      "episode:  7100 / 10000  done\n",
      "episode:  7200 / 10000  done\n",
      "episode:  7300 / 10000  done\n",
      "episode:  7400 / 10000  done\n",
      "episode:  7500 / 10000  done\n",
      "episode:  7600 / 10000  done\n",
      "episode:  7700 / 10000  done\n",
      "episode:  7800 / 10000  done\n",
      "episode:  7900 / 10000  done\n",
      "episode:  8000 / 10000  done\n",
      "episode:  8100 / 10000  done\n",
      "episode:  8200 / 10000  done\n",
      "episode:  8300 / 10000  done\n",
      "episode:  8400 / 10000  done\n",
      "episode:  8500 / 10000  done\n",
      "episode:  8600 / 10000  done\n",
      "episode:  8700 / 10000  done\n",
      "episode:  8800 / 10000  done\n",
      "episode:  8900 / 10000  done\n",
      "episode:  9000 / 10000  done\n",
      "episode:  9100 / 10000  done\n",
      "episode:  9200 / 10000  done\n",
      "episode:  9300 / 10000  done\n",
      "episode:  9400 / 10000  done\n",
      "episode:  9500 / 10000  done\n",
      "episode:  9600 / 10000  done\n",
      "episode:  9700 / 10000  done\n",
      "episode:  9800 / 10000  done\n",
      "episode:  9900 / 10000  done\n",
      "episode:  10000 / 10000  done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "V,pi = monte_carlo_control_exploring_starts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | U |   |\n",
      "------------------\n",
      " U | R | U | L |\n"
     ]
    }
   ],
   "source": [
    "g.print_values(V)\n",
    "g.print_policy(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we pretty accurate estimate of the value function\n",
    "# also we get our policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So although we've just learned how to use Monte Carlo for solving both the prediction problem and the control problem, there is still one small detail to consider\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Monte Carlo for Control</h3>\n",
    "\n",
    "As a quiz question, we want to think about what is impractical about the Monte Carlo exploring stars method?\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Limitations of Exploring Starts</h3>\n",
    "\n",
    "The answer is that exploring starts can't always be done in the real world\n",
    "\n",
    "Imagine, for example, building a self-driving car\n",
    "\n",
    "It's simply not possible to put out car into all possible states that it could ever be in \n",
    "\n",
    "Or imagine something simpler, like a video game\n",
    "\n",
    "Unless we hack into the video game, we can't just have our characters start in any state that we want\n",
    "\n",
    "The question is then, is there a solution to the problem of exploration that does not require exploring starts?\n",
    "\n",
    "And remember, the reason why we needed this in the first place, it was because we needed to fill up samples for all state action pairs in $Q$\n",
    "\n",
    "If our policy never tells us to perform action $a$ in state $s$, then we will never have any samples for action $a$ in state $s$ \n",
    "\n",
    "Without any samples, we don't have any estimate and we cannot choose the optimal action\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Epsilon-Greedy Monte Carlo Control</h3>\n",
    "\n",
    "The answer goes back to the classic method of epsilon greedy\n",
    "\n",
    "Of course, we can employ other methods as well, but this is the traditional solution\n",
    "\n",
    "So what does the algorithm look like?\n",
    "\n",
    "\n",
    "$\\text{Initialise:} \\\\ \\qquad \\pi = \\text{ random policy with } \\pi(a \\vert s)>0 \\text{ for all a,s} \\\\ \\qquad Q(s,a) = \\text{ arbitary (0 for terminal states)} \\\\ \\qquad \\text{returns(s,a) = [] (0 for terminal states)} \\\\ Loop: \\\\ \\qquad \\text{Reset to initial state }s_0 \\\\ \\qquad \\text{Play an episode to get } (a_0,r_1,s_1,a_1,\\ldots,r_T,s_T) \\\\ \\qquad G=0 \\\\ \\qquad \\text{for t in \\{T-1,T-2,...,0\\}:} \\\\ \\qquad \\qquad G = r_{t+1} + \\gamma G \\\\ \\qquad \\qquad\\text{if } s_t,a_t \\text{ dont appear earlier in the episode: } \\\\ \\qquad \\qquad \\qquad \\text{returns}(s_t,a_t).append(G) \\\\ \\qquad \\qquad \\qquad Q(s_t,a_t) = mean(returns(s_t,a_t)) \\\\ \\qquad \\qquad \\qquad \\text{Update } \\pi$\n",
    "\n",
    "Well, the initialization is essentially the same\n",
    "\n",
    "The only requirement is that our initial policy gives a non-zero probability to performing each action in every state\n",
    "\n",
    "For example, a uniform policy or an Epsilon greedy policy would both work \n",
    "\n",
    "Next we into a loop for some number of episodes\n",
    "\n",
    "Inside the Loop, we play a single episode according to the current policy $\\pi$, and we generate a sequence of states actions and rewards\n",
    "\n",
    "Next, as usual, we initialize our return $G$ to zero\n",
    "\n",
    "Then, as before, we loop through the episode in reverse, starting at timeStep $T-1$  \n",
    "\n",
    "Inside the loop we update $G$ using the usual recursive formula\n",
    "\n",
    "Then we check whether or not the current state action pair $s_t,a_t$ appears earlier in the\n",
    "episode\n",
    "\n",
    "This is for first visit Monte Carlo\n",
    "\n",
    "If it does not appear, then we continue \n",
    "\n",
    "As before we appends our new $G$ sample to our list of returns for this state-action action pair\n",
    "\n",
    "Then, as before we update $!$ using the sample mean of the returns we've collected so far for this state action pair\n",
    "\n",
    "Finally, we come to the new part ($\\text{Update } \\pi$) where we find the best action, $a^*$ from $Q$ for the given state\n",
    "\n",
    "But instead of making our policy just to always do action $a^*$ from the state $s(t)$, we use an\n",
    "epsilon greedy policy\n",
    "\n",
    "That is to say, our policy is now probabilistic\n",
    "\n",
    "For the action $a^*$, we assign the probability one minus epsilon plus epsilon divided by the size of the action space\n",
    "\n",
    "For all other actions, we assign the probability epsilon divided by the size of the action space\n",
    "\n",
    "$$\\large a^* = \\arg \\max_a Q(s_t,a)$$\n",
    "\n",
    "$$\\large \\pi(a^*,s_t) = 1 - \\varepsilon + \\frac{\\varepsilon}{\\vert A \\vert}$$\n",
    "\n",
    "$$\\large \\pi(a \\vert s_t) = \\frac{\\varepsilon}{\\vert A \\vert} \\text{ for } a \\neq a^*$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>How is this epsilon-greedy</h3>\n",
    "\n",
    "Now, one question we may have is how does the previous probabilistic policy correspond to Epsilon greedy?\n",
    "\n",
    "We are encouraged to to think about this as an exercise by ourselves\n",
    "\n",
    "So here's a computer function that does Epsilon greedy, where with probability $\\varepsilon$, we select an action at random from the action space with uniform probability\n",
    "\n",
    "Otherwise we choose the optimal action\n",
    "\n",
    "```python\n",
    "def epsilon_greedy(Q,s,eps):\n",
    "    if random() < eps:\n",
    "        return random action\n",
    "    else:\n",
    "        return argmax(Q(s,:))\n",
    "```\n",
    "\n",
    "We want to prove to ourselves that by following this computer program, we equivalently have this mathematical expression for the policy\n",
    "\n",
    "$$\\large a^* = \\arg \\max_a Q(s_t,a)$$\n",
    "\n",
    "$$\\large \\pi(a^*,s_t) = 1 - \\varepsilon + \\frac{\\varepsilon}{\\vert A \\vert}$$\n",
    "\n",
    "$$\\large \\pi(a \\vert s_t) = \\frac{\\varepsilon}{\\vert A \\vert} \\text{ for } a \\neq a^*$$\n",
    "\n",
    "Help : we can see how the probability of choosing at random is $\\varepsilon$, and for a uniform distribution, the probability of choosing any action $a$ is $\\frac{1}{\\vert A \\vert}$, so the probability of choosing randomly AND choosing action $a$ is equal to $\\varepsilon \\times \\frac{1}{A} = \\frac{\\varepsilon}{\\vert A \\vert}$, as for $a^*$, we need to remember that there are two ways of choosing it, either choosing it determinisitically because it is the best value, the probability of that is $1-\\varepsilon$, meaning the probability that we dont choose randomly, OR we can choose an action randomly and end up choosing $a^*$ by chance, the probability of choosing an action randomly as we have shown is $\\frac{\\varepsilon}{\\vert A \\vert}$, so the total probability is $1-\\varepsilon+ \\frac{\\varepsilon}{A}$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Summary</h3>\n",
    "\n",
    "OK, so that's everything we need to know about Monte Carlo without exploring stars\n",
    "\n",
    "We've just learned how to remove the need for exploring starts, which would be impractical in the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So here goes\n",
    "# We will attempt coding Monte Carlo for Control problem + Exploring starts\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets implement our Gridworld enviroment\n",
    "# we will be using the same interface as described above\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,rows,cols,start_pos):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.pos = start_pos\n",
    "        self.start_pos = start_pos\n",
    "    \n",
    "    def current_state(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def set_state(self,state):\n",
    "        self.pos = state\n",
    "        \n",
    "    def set_actions_rewards(self,actions,rewards):\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        \n",
    "    def all_states(self,):\n",
    "        return [(i,j) for i in range(self.rows) for j in range(self.cols)]\n",
    "    \n",
    "    def is_terminal(self,state):\n",
    "        # we cant perform an action in a terminal state\n",
    "        # so we expect not to find it in the keys\n",
    "        return state not in self.actions.keys()\n",
    "    \n",
    "    def game_over(self):\n",
    "        return self.is_terminal(self.pos)\n",
    "    \n",
    "    def next_state(self,s,a):\n",
    "        # return next state should we take action a\n",
    "        # but do not take that action\n",
    "        i,j = s\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(s,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        return (i,j)\n",
    "    \n",
    "    def move(self,a):\n",
    "        # return next state should we take action a\n",
    "        # this time we actually make the move\n",
    "        # and thus return reward\n",
    "        i,j = self.pos\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(self.pos,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        self.pos = (i,j)\n",
    "        # if no reward return 0\n",
    "        return self.rewards.get(self.pos,0)\n",
    "    \n",
    "    # lets add methods to print values and actions\n",
    "    def print_values(self,V):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*9*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                v = V[i,j]\n",
    "                if v >= 0:\n",
    "                    # if 0 or +ve, add space so it aligns well when we have -\n",
    "                    print(\" %.2f|\" %v,end=\"\")\n",
    "                else:\n",
    "                    print(\"%.2f|\" %v,end=\"\")\n",
    "            print(\"\")\n",
    "    \n",
    "    def print_policy(self,P):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*6*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                print(\" %s |\" %P.get((i,j),' '),end=\"\")\n",
    "            print(\"\")\n",
    "                \n",
    "    def state_to_loc(self,state): # convert state tupe (0,1) to location 1\n",
    "        return state[0]*self.rows + state[1]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pos = self.start_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standrad_grid():\n",
    "    g = GridWorld(3,4,(2,0))\n",
    "    rewards = {(0,3):1,(1,3):-1}\n",
    "    actions = {\n",
    "        (0,0): ('D','R'),\n",
    "        (0,1): ('L','R'),\n",
    "        (0,2): ('L','D','R'),\n",
    "        (1,0): ('U','D'),\n",
    "        (1,2): ('U','D','R'),\n",
    "        (2,0): ('U','R'),\n",
    "        (2,1): ('L','R'),\n",
    "        (2,2): ('L','R','U'),\n",
    "        (2,3): ('L','U')\n",
    "    }\n",
    "    g.set_actions_rewards(actions,rewards)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = standrad_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(best_action,eps=0.1):\n",
    "    if np.random.random() < eps:\n",
    "        return np.random.choice(['U','R','D','L'])\n",
    "    else:\n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_control_epsilon_greedy():\n",
    "    all_states = g.all_states()\n",
    "    state_space = len(all_states)\n",
    "    non_terminal_states = list(g.actions.keys())\n",
    "    all_actions = ['U','R','D','L']\n",
    "    action_space = len(all_actions)\n",
    "    gamma = 0.9\n",
    "    # we will store deltas for Q\n",
    "    deltas = []\n",
    "    \n",
    "    \n",
    "    # lets follow the psuedocode\n",
    "    pi = {s:all_actions[np.random.choice(action_space)] for s in all_states if not g.is_terminal(s)}\n",
    "    # number of samples for (s,a), used for mean calculation\n",
    "    num_samples = {}\n",
    "    # also for debugging purposes, store how many times we visisted each states s\n",
    "    num_visited = {}\n",
    "    Q = np.zeros((g.rows,g.cols,action_space))\n",
    "    Q_old = Q.copy()\n",
    "    episodes = 5000\n",
    "    \n",
    "    for episode in range(episodes): # play multiple episodes\n",
    "        states_actions_rewards = []\n",
    "        if (episode+1)%100 == 0:\n",
    "            print('episode: ',episode+1,'/',episodes,' done')\n",
    "        # reset to start position\n",
    "        g.reset()\n",
    "\n",
    "        for T in range(20):\n",
    "            s = g.current_state()\n",
    "            a = epsilon_greedy(pi[s])\n",
    "            r = g.move(a)\n",
    "\n",
    "            states_actions_rewards.append((s,a,r))\n",
    "            \n",
    "            s_prime = g.current_state()\n",
    "            num_visited[s_prime] = num_visited.get(s_prime,0)+1\n",
    "\n",
    "\n",
    "            if g.game_over():\n",
    "                break\n",
    "\n",
    "        G = 0\n",
    "        # store which (s,a) pairs we have seen before\n",
    "        seen = set()\n",
    "        for s,a,r in reversed(states_actions_rewards):\n",
    "            G = r + gamma*G\n",
    "            if (s,a) not in seen:\n",
    "                seen.add((s,a))\n",
    "                # first visit monte carlo\n",
    "                # update mean using our more effecient rule\n",
    "                num_samples[(s,a)] = num_samples.get((s,a),0)+1\n",
    "                mean_s_a = Q[s[0],s[1],all_actions.index(a)]\n",
    "                Q[s[0],s[1],all_actions.index(a)] = mean_s_a + 1/num_samples[(s,a)]*(G-mean_s_a)\n",
    "                pi[s] = all_actions[np.argmax(Q[s])]\n",
    "        delta = np.max(np.abs(Q - Q_old))\n",
    "        deltas.append(delta)\n",
    "        Q_old = Q.copy()\n",
    "    \n",
    "    # plot deltas\n",
    "    plt.plot(deltas)\n",
    "    plt.show()\n",
    "    # turn into a num_visited into a dataframe so it prints well\n",
    "    visited_df = np.zeros((g.rows,g.cols))\n",
    "    for i in range(g.rows):\n",
    "        for j in range(g.cols):\n",
    "            visited_df[i,j] = num_visited.get((i,j),0)\n",
    "    visited_df = pd.DataFrame(visited_df)\n",
    "    print(visited_df)\n",
    "    # return V,pi    \n",
    "    return np.max(Q,axis=-1),pi   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  100 / 5000  done\n",
      "episode:  200 / 5000  done\n",
      "episode:  300 / 5000  done\n",
      "episode:  400 / 5000  done\n",
      "episode:  500 / 5000  done\n",
      "episode:  600 / 5000  done\n",
      "episode:  700 / 5000  done\n",
      "episode:  800 / 5000  done\n",
      "episode:  900 / 5000  done\n",
      "episode:  1000 / 5000  done\n",
      "episode:  1100 / 5000  done\n",
      "episode:  1200 / 5000  done\n",
      "episode:  1300 / 5000  done\n",
      "episode:  1400 / 5000  done\n",
      "episode:  1500 / 5000  done\n",
      "episode:  1600 / 5000  done\n",
      "episode:  1700 / 5000  done\n",
      "episode:  1800 / 5000  done\n",
      "episode:  1900 / 5000  done\n",
      "episode:  2000 / 5000  done\n",
      "episode:  2100 / 5000  done\n",
      "episode:  2200 / 5000  done\n",
      "episode:  2300 / 5000  done\n",
      "episode:  2400 / 5000  done\n",
      "episode:  2500 / 5000  done\n",
      "episode:  2600 / 5000  done\n",
      "episode:  2700 / 5000  done\n",
      "episode:  2800 / 5000  done\n",
      "episode:  2900 / 5000  done\n",
      "episode:  3000 / 5000  done\n",
      "episode:  3100 / 5000  done\n",
      "episode:  3200 / 5000  done\n",
      "episode:  3300 / 5000  done\n",
      "episode:  3400 / 5000  done\n",
      "episode:  3500 / 5000  done\n",
      "episode:  3600 / 5000  done\n",
      "episode:  3700 / 5000  done\n",
      "episode:  3800 / 5000  done\n",
      "episode:  3900 / 5000  done\n",
      "episode:  4000 / 5000  done\n",
      "episode:  4100 / 5000  done\n",
      "episode:  4200 / 5000  done\n",
      "episode:  4300 / 5000  done\n",
      "episode:  4400 / 5000  done\n",
      "episode:  4500 / 5000  done\n",
      "episode:  4600 / 5000  done\n",
      "episode:  4700 / 5000  done\n",
      "episode:  4800 / 5000  done\n",
      "episode:  4900 / 5000  done\n",
      "episode:  5000 / 5000  done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0       1       2       3\n",
      "0  5592.0  5529.0  5369.0  4988.0\n",
      "1  5555.0     0.0   157.0     1.0\n",
      "2   572.0   172.0    32.0     2.0\n"
     ]
    }
   ],
   "source": [
    "V,pi = monte_carlo_control_epsilon_greedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.79| 0.89| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.71| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.63| 0.66| 0.76| 0.73|\n",
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | U |   |\n",
      "------------------\n",
      " U | R | U | L |\n"
     ]
    }
   ],
   "source": [
    "g.print_values(V)\n",
    "g.print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we look at our policy, we see that it makes sense\n",
    "\n",
    "Our agent is capable of getting to the goal from any state on the grid\n",
    "\n",
    "However, we'll notice that the policy and the values are not the same as what we got before\n",
    "\n",
    "So how did this happen?\n",
    "\n",
    "Well, looking at the sample counts for each state explains why \n",
    "\n",
    "We can see that our agent prefers to go up from the start \n",
    "\n",
    "Because of this, we have lots of samples for Up the path\n",
    "\n",
    "So along the left edge and the top edge\n",
    "\n",
    "But in the bottom right area, we have very few samples\n",
    "\n",
    "This is because our policy dictates that we should not go there\n",
    "\n",
    "The only time we end up going this way is if we randomly choose to due to Epsilon greedy\n",
    "\n",
    "We can see that as we go further and further away from what the policy dictates, the number of samples we collect gets smaller and smaller\n",
    "\n",
    "Therefore, we cannot expect the value function to be accurate for those states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll be summarizing everything we learned in the notebook\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Monte Carlo notebook Summary</h3>\n",
    "\n",
    "This section was all about the Monte Carlo method\n",
    "\n",
    "This notebook represented an important step in our study of reinforcement learning \n",
    "\n",
    "In the previous notebooks, all of our work was theoretical\n",
    "\n",
    "We didn't create any actual agents that would play games in an environment\n",
    "\n",
    "Our agents didn't learn from experience\n",
    "\n",
    "This notebook was the transition to the practical world where we did program an agent to produce and learn from experience\n",
    "\n",
    "So this was an important step\n",
    "\n",
    "---\n",
    "\n",
    "The main idea behind this section is actually pretty simple\n",
    "\n",
    "It all goes back to bandit notebook where we talked about how to estimate expected value\n",
    "\n",
    "$$\\large V_\\pi(s) = E[G_t \\vert S_t = s] \\approx \\frac{1}{N} \\sum^N_{i=1} G_{i,s}$$\n",
    "\n",
    "Since we don't know the probability distribution that this expected value is being taken with respect to, we can't compute it directly\n",
    "\n",
    "However, we can collect samples by playing many episodes\n",
    "\n",
    "Put simply, we estimate the expected value with the sample mean\n",
    "\n",
    "---\n",
    "\n",
    "Along the way, we learned several important concepts\n",
    "\n",
    "Firstly, we learned how the concept of policy iteration is applied in the context of sample based learning methods\n",
    "\n",
    "Although the version of policy iteration that we studied before would be possible, it was also not practical\n",
    "\n",
    "We also learned why it's necessary to estimate $Q$ rather than $V$ when it comes to control\n",
    "\n",
    "Lastly, we rediscovered the need for exploration and how Epsilon Greedy can be applied to serve that need"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
