{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to discuss Sequence-to-Sequence models or Seq-Seq for short\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Sequence-to-Sequence</h3>\n",
    "\n",
    "As usual, we start our Math section with some motivation, what problem are we trying to solve here ?\n",
    "\n",
    "We alluded to this problem before, and now we are going to consider it in full\n",
    "\n",
    "Suppose we are working on a machine translation system, and we want to be able to translate English to Japanese or English to Spanish\n",
    "\n",
    "Well, we know that the number of characters/words in the English sequence is not going to be the same as the number of characters/words in the Japanese sequence\n",
    "\n",
    "<img src='extras/35.1.PNG' width='300'></img>\n",
    "\n",
    "But in our study of RNNs so far, we always get one hidden state per input, and that always translate to one output per input\n",
    "\n",
    "In other words, with the standard RNN, \n",
    "\n",
    "<img src='extras/35.2.PNG' width='100'></img>\n",
    "\n",
    "if we keep all the outputs at each point of time, our output length is always going to be the same as our input length\n",
    "\n",
    "---\n",
    "\n",
    "The solution to this problem, is the Seq-to-Seq architecture\n",
    "\n",
    "At a high level, its actually a very simple concept\n",
    "\n",
    "So the main architecture is a dual RNN system\n",
    "\n",
    "<img src='extras/35.3.PNG' width='700'></img>\n",
    "\n",
    "The first RNN which takes in the input, is called the encoder\n",
    "\n",
    "The second RNN which produces the translation, or really any answer, is called the decoder\n",
    "\n",
    "note : we will be talking about specific applications later on such as translation, answering questions, conversation agents and so on\n",
    "\n",
    "As we may recall this is not the first time we have seen an Encoder/Decoder architecture (actually it is due to lack of organisation in notebooks' order :) )\n",
    "\n",
    "We saw the same type of thing when we looked at Autoencoders,GANs and so forth (search for them in future notebooks)\n",
    "\n",
    "So this is a very common idea in deep learning\n",
    "\n",
    "We take some raw input, like text,audio, or an image, and we create a little vector representation out of it, in this case thats the RNN state\n",
    "\n",
    "Then in the second stage, the decoder, we produce new data from that compressed vector representation\n",
    "\n",
    "So this type of architecture, can be used for any type of request-response type of task\n",
    "\n",
    "The most basic of which, is an autoencoder where we just try to reproduce what we put in\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Encoder</h3>\n",
    "\n",
    "So if we zoom in a little, how does this work ?\n",
    "\n",
    "Lets just consider the Encoder for now\n",
    "\n",
    "<img src='extras/35.4.PNG' width='400'></img>\n",
    "\n",
    "This just works like the standard RNN, whether that be an LSTM or a GRU\n",
    "\n",
    "We pass in the input sequence, and we get back a series of $h$s\n",
    "\n",
    "Importantly, with Seq-to-Seq, we only want to keep the last state of the sequence \n",
    "\n",
    "For an LSTM that would be $h_T$ and $c_T$, but for a GRU that would just be $h_T$\n",
    "\n",
    "In general, when we show these diagrams, we are just going to show $h$ for simplicity sake\n",
    "\n",
    "So in Keras, that means we would pass in ```return_sequences=False```  (default anyway)\n",
    "\n",
    "\n",
    "So what that gives us is $h_T$, which is a vector of size $M$\n",
    "\n",
    "we can think of this as a <strong>Thought Vector</strong>\n",
    "\n",
    "This one vector, which by the way does not contain any time information in and of itself, is just a single vector that somehow represents the input sentence\n",
    "\n",
    "Thats why we call is an encoding, its a small compact representation of the original input\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Interface</h3>\n",
    "\n",
    "Another way to think of the encoder and decoder is this\n",
    "\n",
    "Imagine that the Encoder's job is to fold up the input sequence into a small informative vector\n",
    "\n",
    "Then the Decoder's job is to unravel or unfold that vector into a new sequence\n",
    "\n",
    "<img src='extras/35.5.PNG' width='500'></img>\n",
    "\n",
    "The idea of Folding & Unfolding effeciently captures what happens to our input data as we pass it between the Encoder and the Decoder\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Decoder</h3>\n",
    "\n",
    "Now lets consider what happens at the Decoder's side\n",
    "\n",
    "So this is the novel part of the Seq-to-Seq architecture\n",
    "\n",
    "At the Decoder we have an entirely new RNN unit, but with the same size as the Encoder RNN\n",
    "\n",
    "The reason we want to do this is, instead of passing in any old initial state into the decoder RNN, we are going to pass in our Thought Vector from eariler\n",
    "\n",
    "So we can think of this as $h_0^\\prime$ for the decoder RNN, but its really just equal to the old $h_T$ from the old RNN\n",
    "\n",
    "For the first input $x_1$, we pass in a special token to denote the start of a sentence \n",
    "\n",
    "Typically we pass in ```<SOS>```, so we to remember to add those to our training set as well\n",
    "\n",
    "Now from this information, $h_0$ and $x_1$, our RNN is going to calculate $h_1$ and from that we can calculate $y_1$\n",
    "\n",
    "<img src='extras/35.6.PNG' width='500'></img>\n",
    "\n",
    "$y_1$ of course will be a vector of probabilities, so from their we can take the argmax to pick the most likely word in our target language\n",
    "\n",
    "---\n",
    "\n",
    "From this we are going to do something really interesting\n",
    "\n",
    "We might wonder, how can we predict the second word in the sentence and the third word and so on\n",
    "\n",
    "Well consider the fact that we now have somewhat of a hole in our model\n",
    "\n",
    "Our RNN has to take two things, the previous state, which is now $h_1$, which we just calculated and $x_2$ some kind of input\n",
    "\n",
    "Well, what if we just pass in the argmax of $y_1$ as $x_2$, and then we calculate $y_2$ and pass that in as the input to $x_3$ and so on\n",
    "\n",
    "so $y_1$ becomes $x_2$, $y_2$ becomes $x_3$, $y_3$ becomes $x_4$ and so on, so we get to the maximum length seqeunce\n",
    "\n",
    "<img src='extras/35.7.PNG' width='500'></img>\n",
    "\n",
    "note : Dense Layer comes after the LSTM with output size = $V$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Where have we seen this ?</h3>\n",
    "\n",
    "So where have we seen this type of thing before ?\n",
    "\n",
    "Well what this gives us is a language generation model !\n",
    "\n",
    "<img src='extras/35.8.PNG' width='500'></img>\n",
    "\n",
    "Which is something we are very comfortable with because we have studied it number of times in the past\n",
    "\n",
    "Our usual example is poerty generation, so we learn some probabilistic model of a set of poems and from that we can generate new poems\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Seq2Seq Summary</h3>\n",
    "\n",
    "So thats the basic idea behind Seq2Seq\n",
    "\n",
    "It allows us to solve the problem of mapping an input sequence to an output sequence that has a different length\n",
    "\n",
    "Its very simple when we draw it out on paper\n",
    "\n",
    "<img src='extras/35.9.PNG' width='500'></img>\n",
    "\n",
    "Although we see that the code is non-trivial\n",
    "\n",
    "note :  for our implementation later, we are going to be doing machine translation\n",
    "\n",
    "We can grab the data from the <a href='http://www.manythings.org/anki/'>here</a>\n",
    "\n",
    "Note that there are alot of teanslation datasets on this site such as English-to-Spanish, English-to-French,English-to-Russian and so on\n",
    "\n",
    "Since $\\text{All the data is the same}$, we can pick anyone we want, the same code will work on any of these data files\n",
    "\n",
    "We will be using English-to-Arabic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to talk about some applications of the Seq2Seq architecture\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Seq2Seq Applications</h3>\n",
    "\n",
    "This is rather important since we often mention that $\\text{All data is the samme}$\n",
    "\n",
    "This is a very powerful perspective, because if allows us to generalise our skills to new problem domains and it allows us to save time because we dont need to relearn the same thing over and over again\n",
    "\n",
    "We can just as easily build an image classifier as we can a spam detector both using the CNN\n",
    "\n",
    "Being able to generalise these ideas is very helpful\n",
    "\n",
    "---\n",
    "\n",
    "<h3>What tasks \"look like\" machine translation ?</h3>\n",
    "\n",
    "So earlier we talked about machine translation\n",
    "\n",
    "What are other tasks for which our data looks exactly the same ?\n",
    "\n",
    "Another popular task is called question answering \n",
    "\n",
    "This is a task where, we are told some story, and then we are asked a question about the story, and to demonstrate that we comprehend the story we give the correct answer\n",
    "\n",
    "So its a test of reading comprehension\n",
    "\n",
    "As an example, we may be given the Wikipidea page on Albert Einstein and then we might be given a question like Q:\"What theory is Albert Einstein best known for?\", and the neural network might output A:\"Relativity\" \n",
    "\n",
    "The idea here is, we pass in the entire story and the question as a contiguous (concatenated) sequence of words, and then that gets converted into a thought vector and then that gets converted into an answer at the decoder\n",
    "\n",
    "So we can imagine what a dataset like this would look like, it would be just liek a machine translation dataset\n",
    "\n",
    "We have an input sequence of words, and a target sequence of words and we just train the neural network to remember these pairs\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Chatbots</h3>\n",
    "\n",
    "Another popular example is chatbots\n",
    "\n",
    "Again, this dataset takes the exact saem format\n",
    "\n",
    "We say something to the chatbot, and then it gives us an appropriate response back\n",
    "\n",
    "So both chatbots and question answering take on  this request-response format, same as neural machine translation\n",
    "\n",
    "we input a sequence of words and it gives us back a sequence of words\n",
    "\n",
    "But we may argue that the Seq2Seq model in not ideally suited for chatbots (intructor's comment, his explanation follows)\n",
    "\n",
    "Lets explain why\n",
    "\n",
    "When we are having a conversation with someone, often we are conversing over a particular idea over the span of multiple questions and statements, that what a regular conversation looks like\n",
    "\n",
    "A conversation with only binary questions and answers would be akward and would have no flow\n",
    "\n",
    "In other words, a decent chatbot, should have some ability to remember, not only what we said in the past, but also what is has said itself\n",
    "\n",
    "Thats what we do when we are having a conversation, and it makes sense that a true conversational chatbot should be able to do that too\n",
    "\n",
    "so, we may argue that Seq2seq architectures are good for chatbots, because we are not teaching the neural network to actually have a conversation\n",
    "\n",
    "We are just teaching it to memorise what are essentially, question and answer pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to look at decoding in detail\n",
    " \n",
    "---\n",
    "\n",
    "<h3>Decoding in Detail</h3>\n",
    "\n",
    "Remember that decoding is the second part in the Seq2Seq architecture\n",
    "\n",
    "<img src='extras/35.10.PNG'></img>\n",
    "\n",
    "What's difficult about this, that we may not have realised from just the basic high level description, is that there are some implementation details we need to pay attention to in order to actually get our Decoder to work\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Implementation Issues</h3>\n",
    "\n",
    "One major issue is this, earlier we looked at how to implement an RNN in Keras\n",
    "\n",
    "One big difference between how we are doing things now vs how we did them before, is that we are now working with constant size sequences\n",
    "\n",
    "This does not seem like a problem at first, but it becomes clearer when we look at a picture\n",
    "\n",
    "<img src='extras/35.11.PNG' width='700'></img>\n",
    "\n",
    "Remember that each RNN unit has two inputs and an output, if we consider the previous hidden state to be an input\n",
    "\n",
    "So we have our Encoder-Decoder, and our input sentence goes into the encoder RNN\n",
    "\n",
    "The outputs of the Encoder RNN actually get ignored, since all we want from that is the final hidden state\n",
    "\n",
    "For the Decoder RNN, we take the outputs and we compare these to the target sentence, which in the case of machine translation is just the input sentence translated into whatever language we want to learn\n",
    "\n",
    "So now here is the problem\n",
    "\n",
    "We know that the decoder RNN has two inputs, the previous hidden state and an input sequence\n",
    "\n",
    "But what goes at the bottom $?$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>What we've seen</h3>\n",
    "\n",
    "In the previous Math section, we claimed that what we would do is take the previously generated word and feed that into the input at the next time step\n",
    "\n",
    "<img src='extras/35.12.PNG' width='400'></img>\n",
    "\n",
    "The researchers have found that, for training, there is something that works even better <strong>Teacher Forcing</strong>\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Teacher Forcing</h3>\n",
    "\n",
    "Teacher forcing works like this, instead of feeding the previously generated input into the bottom of the decoder, we instead feed in the true previous word\n",
    "\n",
    "<img src='extras/35.13.PNG' width='400'></img>\n",
    "\n",
    "so even if our model did not get the previous word right, teacher forcing corrects it so the model can predict the next word based on the real translation\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Why ?</h3>\n",
    "\n",
    "This helps the model to train because, as we can imagine, it wouldd be difficult for the model to learn the entire sentence at once\n",
    "\n",
    "If we think about how <u>we</u> learn a language, it would be very similar to that\n",
    "\n",
    "suppose we are trying to form a sentence, and we are just saying one word at a time trying to put it all together\n",
    "\n",
    "If we get a word wrong, our teacher might correct us, and then we can use that information to finish the rest of the sentence\n",
    "\n",
    "This is helpful because, if we get a word wrong and nobody corrects us, then all our future predictions will now be based on that first incorrect word which would just throw everyting off\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Teacher forcing</h3>\n",
    "\n",
    "So schematically, we now want to pass in the true target sequence into the bottom of the decoder\n",
    "\n",
    "But remember it has to be offset by one so that at the target, we are always trying to predict the next word\n",
    "\n",
    "If they are aligned perfectly then the decoder would just learn to copy its input which is useless\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Problem</h3>\n",
    "\n",
    "But now we have a problem, and this goes back to what we were discussing at the beginning of this lecture\n",
    "\n",
    "We know that keras works with constant size sequences, so if our input is of length 100, then our output will also be of length 100\n",
    "\n",
    "The problem is what do we do when we want to make new predictions at test time ?\n",
    "\n",
    "During training, we are always going to pass in the true target into the bottom of the decoder, but for testing we obviously cant do that\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Test Mode</h3>\n",
    "\n",
    "For testing, we go back to the original architecture where we pass in the previous output into the next input\n",
    "\n",
    "<img src='extras/35.14.PNG' width='400'></img>\n",
    "\n",
    "The problem with this is that, if we look at this RNN unit, its input sequenc length is 1\n",
    "\n",
    "So for prediction, the input sequence length is always going to be 1\n",
    "\n",
    "This is because we will be doing a loop and generating each loop one at a time\n",
    "\n",
    "we must do it this way, because we cant pass in the full sequence at once, that does not make sense because we have not generated it yet\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Problem (Summarised)</h3>\n",
    "\n",
    "So to summarise this issue into just few sentences\n",
    "\n",
    "<ol>\n",
    "    <li>Keras must have constant-sized inputs</li>\n",
    "    <li>Decoder input length during training is $T_y$ if we are using teacher forcing</li>\n",
    "    <li>The decoder input size during prediction is of size 1</li>\n",
    "</ol>\n",
    "\n",
    "2 and 3 are in conflict with 1\n",
    "\n",
    "so how do we solve this problem ?\n",
    "\n",
    "---\n",
    "\n",
    "<h2>The Solution</h2>\n",
    "\n",
    "Well the answer is to simply create two different models!\n",
    "\n",
    "The first model we create will be for training purposes only\n",
    "\n",
    "The second model we create will be for sampling, and we will make use of the previously defined decoding layers that were already part of the trained model\n",
    "\n",
    "So for the second model, we can define a new set of inputs, and for these inputs, the inut length will be 1\n",
    "\n",
    "Here is a psuedocode\n",
    "\n",
    "```python\n",
    "emb = Embedding()\n",
    "lstm = LSTM()\n",
    "dense = Dense()\n",
    "\n",
    "input1 = Input(length=Ty)\n",
    "model1 = Model(input1,dense(lstm(emb(input1))))\n",
    "\n",
    "input2 = Input(length=1)\n",
    "model2 = Model(input2,dense(lstm(emb(input2))))\n",
    "\n",
    "h = encoder model output\n",
    "x = <SOS>\n",
    "\n",
    "for t in range(Ty):\n",
    "    x,h = model2.predict(x,h)\n",
    "\n",
    "```\n",
    "\n",
    "So we can see that both models , model 1 and model 2,  both use the same layers that were defined at the beginning\n",
    "\n",
    "The only difference between the two models is that they have a different input\n",
    "\n",
    "And so the code for generating a translation is basically just the for loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to take a little bit of a digression and revisit the idea of poerty generation\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Poetry Revisited</h3>\n",
    "\n",
    "This is partly a review and partly not a review because we have done poetry generation in the past but we have not done it in keras\n",
    "\n",
    "And so thats going to introduce some interesting details that we have to pay attention to\n",
    "\n",
    "Now obviously, we are not doing this for no reason\n",
    "\n",
    "This is going to teach us the skills we need to complete our Seq2Seq model\n",
    "\n",
    "Plus generating Art with deep learning is just cool!\n",
    "\n",
    "So we should be interested in doing it\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Language Modelling</h3>\n",
    "\n",
    "In the previous section, we looked at the decoder side of the Seq2Seq architecture in detail, becasue thats basically everything we need to know in order to build a poetry generation model in keras\n",
    "\n",
    "<img src='extras/35.15.PNG' width='400'><img>\n",
    "\n",
    "If we think about it at a high level, poetry generation is just an instance of language modelling\n",
    "\n",
    "If we recall, language modelling just means next word prediction\n",
    "\n",
    "So we want to calculate $p(w_t | w_{t-1},w_{t-2})$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Structure of Data</h3>\n",
    "\n",
    "The general model for this, is an RNN, with the target sequence being just the input sequence offset by one\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><strong>Input</strong></td>\n",
    "        <td> $<$SOS$>$ </td>\n",
    "        <td>The</td>\n",
    "        <td>quick</td>\n",
    "        <td>brown</td>\n",
    "        <td>fox</td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td><strong>Target</strong></td>\n",
    "        <td>The</td>\n",
    "        <td>quick</td>\n",
    "        <td>brown</td>\n",
    "        <td>fox</td>\n",
    "        <td> $<$EOS$>$ </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "notice how they are both the same length, so its okay to add the ```<SOS>``` and ```<EOS>``` tokens\n",
    "\n",
    "Building a model like this, teaches the RNN how to predict the next word given all the previous words in a sentence\n",
    "\n",
    "And so later on, we can generate new sentences by sampling from the probability  distributions at the RNN output\n",
    "\n",
    "---\n",
    "\n",
    "<h3>What should this remind us of ?</h3>\n",
    "\n",
    "This looks exactly like the Seq2Seq Decoder RNN with teacher forcing\n",
    "\n",
    "As we recall, the input sequence there was the true translation and the target sequence was the same translation offset by one\n",
    "\n",
    "The only difference between language modelling an Seq2Seq, is that with language modelling the initial hidden state is jsut an arbitary parameter or just 0\n",
    "\n",
    "With Seq2Seq, the initial hidden state in the decoder is the output of the encoder\n",
    "\n",
    "other than that, these two problems have the exact same architecture\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Conclusion</h3>\n",
    "\n",
    "So this is why we want to review poetry generation again\n",
    "\n",
    "Its because poetry generation, or more generally language modelling, requires precisely the same code as the decoder RNN of a Seq2Seq model\n",
    "\n",
    "So by doing poetry generation first, we learn all of the little Keras tricks we need in order to build the second half of Seq2Seq\n",
    "\n",
    "Then when we actually write Seq2Seq in code, it wont be so scary looking (hopefully :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note : discovered later was running on CPU\n",
    "# so training time should be shorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we implement only the decoder for the poetry generation task\n",
    "# then we implement the entire Seq2Seq model for neural machine translation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense,Input,LSTM,Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from pyarabic.araby import strip_tashkeel\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "EMBEDDING_DIM = 300 # we have only one embedding file, D = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 500\n",
    "LATENT_DIM = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>poet_name</th>\n",
       "      <th>poem_title</th>\n",
       "      <th>poem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>الإمارات</td>\n",
       "      <td>خلفان بن مصبح</td>\n",
       "      <td>بدت تختال في حُلل الجمالِ</td>\n",
       "      <td>بدت تختال في حُلل الجمالِ\\nوجادت بالزيارة والو...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>الإمارات</td>\n",
       "      <td>خلفان بن مصبح</td>\n",
       "      <td>يا طائر الشعر القرير</td>\n",
       "      <td>يا طائر الشعر القرير\\nيا وحي إلهام الصدور\\nأسع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>الإمارات</td>\n",
       "      <td>خلفان بن مصبح</td>\n",
       "      <td>بنت حجرات أرى من عجب</td>\n",
       "      <td>بنت حجرات أرى من عجب\\nأن أرى فيك جمال العرب\\nد...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>الإمارات</td>\n",
       "      <td>خلفان بن مصبح</td>\n",
       "      <td>هذا الربيع بنور الحسن وافانا</td>\n",
       "      <td>هذا الربيع بنور الحسن وافانا\\nوقد كسا الأرض با...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>الإمارات</td>\n",
       "      <td>خلفان بن مصبح</td>\n",
       "      <td>روحي فداك وإن مُنحتُ صدوداً</td>\n",
       "      <td>روحي فداك وإن مُنحتُ صدوداً\\nأخفاك ما بي أم أط...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  category      poet_name                    poem_title  \\\n",
       "0   0  الإمارات  خلفان بن مصبح     بدت تختال في حُلل الجمالِ   \n",
       "1   1  الإمارات  خلفان بن مصبح          يا طائر الشعر القرير   \n",
       "2   2  الإمارات  خلفان بن مصبح          بنت حجرات أرى من عجب   \n",
       "3   3  الإمارات  خلفان بن مصبح  هذا الربيع بنور الحسن وافانا   \n",
       "4   4  الإمارات  خلفان بن مصبح   روحي فداك وإن مُنحتُ صدوداً   \n",
       "\n",
       "                                           poem_text  \n",
       "0  بدت تختال في حُلل الجمالِ\\nوجادت بالزيارة والو...  \n",
       "1  يا طائر الشعر القرير\\nيا وحي إلهام الصدور\\nأسع...  \n",
       "2  بنت حجرات أرى من عجب\\nأن أرى فيك جمال العرب\\nد...  \n",
       "3  هذا الربيع بنور الحسن وافانا\\nوقد كسا الأرض با...  \n",
       "4  روحي فداك وإن مُنحتُ صدوداً\\nأخفاك ما بي أم أط...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first lets load in the data, as always we can use robert frost poems\n",
    "# but this time we will try using the Arabic Poetry dataset\n",
    "# to learn our facourite poet, Antarah Ibn Shaddad\n",
    "# https://www.kaggle.com/ahmedabelal/arabic-poetry\n",
    "poems = pd.read_csv('datasets/Arabic Poetry/Arabic_poetry_dataset.csv')\n",
    "poems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>poet_name</th>\n",
       "      <th>poem_title</th>\n",
       "      <th>poem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13551</th>\n",
       "      <td>13725</td>\n",
       "      <td>العصر الجاهلي</td>\n",
       "      <td>عنترة بن شداد</td>\n",
       "      <td>دعوني أوفي السيف في الحرب حقه</td>\n",
       "      <td>دَعوني أُوَفّي السَيفَ في الحَربِ حَقَّهُ\\nوَأ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13552</th>\n",
       "      <td>13726</td>\n",
       "      <td>العصر الجاهلي</td>\n",
       "      <td>عنترة بن شداد</td>\n",
       "      <td>لقينا يوم صهباء سريه</td>\n",
       "      <td>لَقينا يَومَ صَهباءٍ سَرِيَّه\\nحَناظِلَةً لَهُ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13553</th>\n",
       "      <td>13727</td>\n",
       "      <td>العصر الجاهلي</td>\n",
       "      <td>عنترة بن شداد</td>\n",
       "      <td>سلوا عنا جهينة كيف باتت</td>\n",
       "      <td>سَلوا عَنّا جُهَينَةَ كَيفَ باتَت\\nتَهيمُ مِنَ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13554</th>\n",
       "      <td>13728</td>\n",
       "      <td>العصر الجاهلي</td>\n",
       "      <td>عنترة بن شداد</td>\n",
       "      <td>قف بالديار وصح إلى بيداه</td>\n",
       "      <td>قِف بِالدِيارِ وَصِح إِلى بَيداه\\nفَعَسى الدِي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13555</th>\n",
       "      <td>13729</td>\n",
       "      <td>العصر الجاهلي</td>\n",
       "      <td>عنترة بن شداد</td>\n",
       "      <td>ذكرت صبابتي من بعد حين</td>\n",
       "      <td>ذَكَرتُ صَبابَتي مِن بَعدِ حينِ\\nفَعادَ لِيَ ا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       category      poet_name                     poem_title  \\\n",
       "13551  13725  العصر الجاهلي  عنترة بن شداد  دعوني أوفي السيف في الحرب حقه   \n",
       "13552  13726  العصر الجاهلي  عنترة بن شداد           لقينا يوم صهباء سريه   \n",
       "13553  13727  العصر الجاهلي  عنترة بن شداد        سلوا عنا جهينة كيف باتت   \n",
       "13554  13728  العصر الجاهلي  عنترة بن شداد       قف بالديار وصح إلى بيداه   \n",
       "13555  13729  العصر الجاهلي  عنترة بن شداد         ذكرت صبابتي من بعد حين   \n",
       "\n",
       "                                               poem_text  \n",
       "13551  دَعوني أُوَفّي السَيفَ في الحَربِ حَقَّهُ\\nوَأ...  \n",
       "13552  لَقينا يَومَ صَهباءٍ سَرِيَّه\\nحَناظِلَةً لَهُ...  \n",
       "13553  سَلوا عَنّا جُهَينَةَ كَيفَ باتَت\\nتَهيمُ مِنَ...  \n",
       "13554  قِف بِالدِيارِ وَصِح إِلى بَيداه\\nفَعَسى الدِي...  \n",
       "13555  ذَكَرتُ صَبابَتي مِن بَعدِ حينِ\\nفَعادَ لِيَ ا...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems = poems[poems['poet_name'] == 'عنترة بن شداد']\n",
    "poems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets take the poems and process them\n",
    "# we have to take a decision as to whether or not strip tashkeel\n",
    "# and whether or not to use preatrained word vectors\n",
    "# so we try twice\n",
    "# - tashkeel + no embeddings (our preference)\n",
    "# - tashkeel stripped + pretrained word embeddings (for more practice)\n",
    "# since word embeddings come without tashkeel \n",
    "# its more logical to use them when tahskeel is removed \n",
    "\n",
    "# note : already tried stripping tashkeel, results not as good\n",
    "# lets add another plan\n",
    "\n",
    "# - tashkeel + pretrained word embeddings\n",
    "\n",
    "poems = list(poems['poem_text'])\n",
    "poems = [poem.split('\\n') for poem in poems]\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for poem in poems:\n",
    "    for line in poem:\n",
    "        line = line.rstrip()\n",
    "        if line:\n",
    "            # add <sos> token to input sentences\n",
    "            X.append('<بداية> ' + line)\n",
    "            # add <eos> token to target sentences\n",
    "            Y.append(line + ' <نهاية>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we the usual stuff\n",
    "# tokenising + padding\n",
    "# we did these a lot before, this is the same code\n",
    "\n",
    "# next lets tokenise our sentences\n",
    "# we need to pass filters = '', otherwise special characters are removed\n",
    "# recall <SOS> and <EOS> have <>\n",
    "tokeniser = Tokenizer(num_words=MAX_VOCAB_SIZE,filters='')\n",
    "tokeniser.fit_on_texts(X+Y)\n",
    "X = tokeniser.texts_to_sequences(X)\n",
    "Y = tokeniser.texts_to_sequences(Y)\n",
    "word2idx = tokeniser.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we pad our sentences\n",
    "max_seq_length = max(len(sent) for sent in X)\n",
    "max_seq_length = min(max_seq_length,MAX_SEQUENCE_LENGTH)\n",
    "V = min(len(word2idx)+1,MAX_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "X = pad_sequences(X,maxlen=max_seq_length,padding='post')\n",
    "Y = pad_sequences(Y,maxlen=max_seq_length,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to make a custom loss function to filter out the padding\n",
    "# we dont need our prediction of padding to affect hte loss\n",
    "# since we are lazy, a shortcut is to one-hot-encode the targets\n",
    "# and use categorical crossentropy instead of sparse categorical crossentropy\n",
    "# again , since this is language modelling there is no notion of accuracy\n",
    "# given the word 'The', there are multiple words that can come next\n",
    "# so we dont bother making a custom accuracy metric\n",
    "\n",
    "Y_one_hot = np.zeros((len(Y),max_seq_length,V))\n",
    "\n",
    "for n in range(len(Y)):\n",
    "    for t in range(max_seq_length):\n",
    "        word = Y[n,t]\n",
    "        if word>0:\n",
    "            Y_one_hot[n,t,word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    V,\n",
    "    EMBEDDING_DIM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are ready to build our model\n",
    "# for poetry generation we only build the decoder\n",
    "\n",
    "# input to decoder during training\n",
    "input_train = Input(shape=(max_seq_length,))\n",
    "initial_h = Input(shape=(LATENT_DIM,))\n",
    "initial_c = Input(shape=(LATENT_DIM,))\n",
    "x_train = embedding_layer(input_train)\n",
    "\n",
    "# decoder lstm, used during both train and test\n",
    "# we set return_state = True\n",
    "# while we dont need them here, we need them when making predictions\n",
    "lstm = LSTM(LATENT_DIM,return_sequences=True,return_state=True)\n",
    "# we set the hidden and cell states because we want to have control over them\n",
    "# we dont want keras to set them to any old value internally\n",
    "# when we have our encoder, we will be passing h(T) and c(T) here\n",
    "x_train,_,_ = lstm(x_train,initial_state=[initial_h,initial_c])\n",
    "\n",
    "# then we have a dense layer\n",
    "dense = Dense(V,activation='softmax')\n",
    "output_train = dense(x_train)\n",
    "model = Model([input_train,initial_h,initial_c],output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a learning rate scheduler\n",
    "# le = 0.005 for first 300 epohcs\n",
    "# then 0.0005 for next 200 epochs\n",
    "\n",
    "steps_per_epoch = np.ceil(len(X)*(1-VALIDATION_SPLIT)/BATCH_SIZE).astype('int32')\n",
    "boundaries = [steps_per_epoch*300]\n",
    "values = [0.005, 0.005]\n",
    "learning_rate_fn = PiecewiseConstantDecay(boundaries, values)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 10s 306ms/step - loss: 5.2739 - val_loss: 4.7471\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 6s 256ms/step - loss: 4.5055 - val_loss: 4.6364\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 5s 249ms/step - loss: 4.1952 - val_loss: 4.7647\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 4.1661 - val_loss: 4.7652\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 4.0924 - val_loss: 4.7731\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 4.0366 - val_loss: 4.7839\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 3.9908 - val_loss: 4.7950\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 3.9373 - val_loss: 4.8030\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 3.9110 - val_loss: 4.8184\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 3.8690 - val_loss: 4.8113\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 3.8274 - val_loss: 4.8236\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 3.8065 - val_loss: 4.8238\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 3.7762 - val_loss: 4.8235\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 3.7354 - val_loss: 4.8315\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 3.6950 - val_loss: 4.8441\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 3.6672 - val_loss: 4.8428\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 3.6404 - val_loss: 4.8489\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 3.6215 - val_loss: 4.8550\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 3.5767 - val_loss: 4.8637\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 3.5319 - val_loss: 4.8752\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 3.5172 - val_loss: 4.8792\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 3.4760 - val_loss: 4.8822\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 3.4336 - val_loss: 4.8945\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 3.4162 - val_loss: 4.9058\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 3.3915 - val_loss: 4.9200\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 3.3490 - val_loss: 4.9241\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 3.3186 - val_loss: 4.9390\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 3.2720 - val_loss: 4.9401\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 3.2412 - val_loss: 4.9578\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 3.2125 - val_loss: 4.9596\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 3.1833 - val_loss: 4.9773\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 3.1572 - val_loss: 4.9892\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 3.1303 - val_loss: 5.0104\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 3.0983 - val_loss: 5.0227\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 3.0605 - val_loss: 5.0491\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 3.0426 - val_loss: 5.0586\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 3.0178 - val_loss: 5.0719\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.9812 - val_loss: 5.0895\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.9535 - val_loss: 5.1164\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.9415 - val_loss: 5.1218\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.8901 - val_loss: 5.1430\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 2.8867 - val_loss: 5.1611\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.8558 - val_loss: 5.1781\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.8205 - val_loss: 5.1926\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.7938 - val_loss: 5.2042\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.7823 - val_loss: 5.2246\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.7317 - val_loss: 5.2624\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.7187 - val_loss: 5.2689\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.6959 - val_loss: 5.2831\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 2.6680 - val_loss: 5.3052\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.6470 - val_loss: 5.3140\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.6216 - val_loss: 5.3330\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 2.6114 - val_loss: 5.3459\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.5756 - val_loss: 5.3796\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.5585 - val_loss: 5.3910\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 2.5340 - val_loss: 5.4247\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 2.5185 - val_loss: 5.4368\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 5s 247ms/step - loss: 2.4999 - val_loss: 5.4490\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 2.4745 - val_loss: 5.4699\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 2.4558 - val_loss: 5.4957\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 2.4270 - val_loss: 5.5006\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 2.4030 - val_loss: 5.5195\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 2.4093 - val_loss: 5.5446\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 2.3730 - val_loss: 5.5558\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 2.3649 - val_loss: 5.5750\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 2.3467 - val_loss: 5.5971\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 2.3201 - val_loss: 5.6069\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 5s 244ms/step - loss: 2.3087 - val_loss: 5.6394\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 2.2798 - val_loss: 5.6404\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 2.2797 - val_loss: 5.6649\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 2.2436 - val_loss: 5.6913\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 2.2327 - val_loss: 5.6766\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 2.2200 - val_loss: 5.7136\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 5s 241ms/step - loss: 2.2003 - val_loss: 5.7314\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 2.1857 - val_loss: 5.7627\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 2.1655 - val_loss: 5.7691\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 2.1626 - val_loss: 5.7959\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 2.1305 - val_loss: 5.8097\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 2.1156 - val_loss: 5.8309\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 2.0861 - val_loss: 5.8472\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 5s 246ms/step - loss: 2.0787 - val_loss: 5.8452\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 5s 244ms/step - loss: 2.0605 - val_loss: 5.8738\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 2.0494 - val_loss: 5.9116\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 2.0282 - val_loss: 5.9048\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 2.0155 - val_loss: 5.9262\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 1.9983 - val_loss: 5.9509\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 1.9879 - val_loss: 5.9423\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 1.9702 - val_loss: 5.9908\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 1.9473 - val_loss: 5.9723\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 1.9363 - val_loss: 5.9971\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 1.9106 - val_loss: 6.0490\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.8964 - val_loss: 6.0540\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.8818 - val_loss: 6.0564\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.8753 - val_loss: 6.0908\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.8591 - val_loss: 6.0878\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.8365 - val_loss: 6.1396\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.8055 - val_loss: 6.1320\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.8176 - val_loss: 6.1993\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.7928 - val_loss: 6.1861\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.7841 - val_loss: 6.2081\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.7631 - val_loss: 6.2093\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.7576 - val_loss: 6.2476\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.7396 - val_loss: 6.2917\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.7331 - val_loss: 6.2477\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.7193 - val_loss: 6.2492\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.7016 - val_loss: 6.2968\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 5s 249ms/step - loss: 1.6929 - val_loss: 6.3152\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 1.6932 - val_loss: 6.3504\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 1.6737 - val_loss: 6.3415\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 5s 244ms/step - loss: 1.6737 - val_loss: 6.3414\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 1.6547 - val_loss: 6.3735\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 1.6325 - val_loss: 6.4135\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.6270 - val_loss: 6.4646\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.6236 - val_loss: 6.4291\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.6128 - val_loss: 6.4446\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.6081 - val_loss: 6.5137\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.5943 - val_loss: 6.5179\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.5730 - val_loss: 6.5128\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.5817 - val_loss: 6.5724\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.5551 - val_loss: 6.5522\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.5544 - val_loss: 6.5884\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.5404 - val_loss: 6.5699\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.5269 - val_loss: 6.5796\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.5146 - val_loss: 6.6359\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.5095 - val_loss: 6.6147\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.5059 - val_loss: 6.6955\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.5014 - val_loss: 6.6705\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 1.4924 - val_loss: 6.6865\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 1.4788 - val_loss: 6.6907\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.4788 - val_loss: 6.7365\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.4565 - val_loss: 6.7500\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.4561 - val_loss: 6.7515\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.4487 - val_loss: 6.7969\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.4370 - val_loss: 6.7567\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 1.4140 - val_loss: 6.8001\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 1.4155 - val_loss: 6.8797\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 1.4169 - val_loss: 6.7988\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.4090 - val_loss: 6.8491\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.4032 - val_loss: 6.8566\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.3965 - val_loss: 6.8788\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.3873 - val_loss: 6.8922\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.3724 - val_loss: 6.8870\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 1.3751 - val_loss: 6.9365\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.3688 - val_loss: 6.9868\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.3599 - val_loss: 6.9500\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.3491 - val_loss: 6.9811\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.3535 - val_loss: 7.0117\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.3342 - val_loss: 7.0330\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.3457 - val_loss: 7.0469\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.3403 - val_loss: 7.0649\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.3273 - val_loss: 7.0501\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.3179 - val_loss: 7.0979\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.3150 - val_loss: 7.0799\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.3061 - val_loss: 7.1235\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.3107 - val_loss: 7.1621\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.2978 - val_loss: 7.1177\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.2912 - val_loss: 7.2765\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.2869 - val_loss: 7.2440\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.2852 - val_loss: 7.2943\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 1.2734 - val_loss: 7.2903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2785 - val_loss: 7.3220\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2667 - val_loss: 7.3146\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.2734 - val_loss: 7.3119\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2659 - val_loss: 7.3543\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.2537 - val_loss: 7.3937\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 1.2500 - val_loss: 7.3557\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.2460 - val_loss: 7.3859\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2379 - val_loss: 7.4534\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2333 - val_loss: 7.4702\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2309 - val_loss: 7.4618\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2347 - val_loss: 7.5420\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2280 - val_loss: 7.5269\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2216 - val_loss: 7.5476\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2164 - val_loss: 7.5949\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2084 - val_loss: 7.5696\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2015 - val_loss: 7.6008\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.2043 - val_loss: 7.6008\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2006 - val_loss: 7.7250\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.2066 - val_loss: 7.6665\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.1947 - val_loss: 7.7202\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.1936 - val_loss: 7.6736\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.1854 - val_loss: 7.7573\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.1788 - val_loss: 7.7064\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.1779 - val_loss: 7.7633\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.1753 - val_loss: 7.7765\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.1769 - val_loss: 7.7833\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.1643 - val_loss: 7.8299\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.1659 - val_loss: 7.8808\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.1565 - val_loss: 7.8115\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.1680 - val_loss: 7.8884\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.1477 - val_loss: 7.8959\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.1573 - val_loss: 7.8784\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.1500 - val_loss: 7.9388\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.1495 - val_loss: 7.9878\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 1.1477 - val_loss: 8.0288\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.1436 - val_loss: 8.0497\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.1480 - val_loss: 8.0687\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.1359 - val_loss: 8.0211\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.1413 - val_loss: 8.0568\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.1304 - val_loss: 8.0652\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.1345 - val_loss: 8.0960\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.1336 - val_loss: 8.1660\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.1207 - val_loss: 8.1492\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.1215 - val_loss: 8.1758\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 1.1178 - val_loss: 8.2081\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 1.1124 - val_loss: 8.2323\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 1.1113 - val_loss: 8.2160\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.1034 - val_loss: 8.2636\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.1050 - val_loss: 8.3051\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.1055 - val_loss: 8.2649\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.1002 - val_loss: 8.3446\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.1058 - val_loss: 8.3320\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.0949 - val_loss: 8.3378\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 1.0993 - val_loss: 8.3771\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.1030 - val_loss: 8.4272\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.0915 - val_loss: 8.5357\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.0902 - val_loss: 8.5254\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.0892 - val_loss: 8.4978\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0874 - val_loss: 8.5014\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.0885 - val_loss: 8.5439\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 5s 247ms/step - loss: 1.0810 - val_loss: 8.6139\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 1.0820 - val_loss: 8.6072\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 1.0788 - val_loss: 8.5800\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 1.0791 - val_loss: 8.6104\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 1.0734 - val_loss: 8.6605\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.0692 - val_loss: 8.7016\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0712 - val_loss: 8.7048\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.0718 - val_loss: 8.7089\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.0660 - val_loss: 8.7545\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.0714 - val_loss: 8.7007\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0726 - val_loss: 8.7211\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.0639 - val_loss: 8.7976\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 1.0691 - val_loss: 8.7799\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0640 - val_loss: 8.8296\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.0611 - val_loss: 8.9240\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.0552 - val_loss: 8.9220\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 1.0555 - val_loss: 8.9817\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0557 - val_loss: 9.0146\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0457 - val_loss: 8.9433\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.0487 - val_loss: 8.9914\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.0444 - val_loss: 8.9784\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.0442 - val_loss: 9.0062\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 1.0474 - val_loss: 9.0276\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.0471 - val_loss: 8.9504\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.0521 - val_loss: 9.0645\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 1.0483 - val_loss: 9.0747\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 1.0456 - val_loss: 9.1810\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 1.0442 - val_loss: 9.1178\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.0379 - val_loss: 9.1398\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.0352 - val_loss: 9.1606\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 1.0327 - val_loss: 9.1448\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 1.0341 - val_loss: 9.1775\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 1.0278 - val_loss: 9.1942\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 1.0323 - val_loss: 9.2994\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0289 - val_loss: 9.3606\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.0255 - val_loss: 9.3538\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0263 - val_loss: 9.3503\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 1.0281 - val_loss: 9.3519\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.0273 - val_loss: 9.4579\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.0276 - val_loss: 9.3995\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.0191 - val_loss: 9.5368\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.0197 - val_loss: 9.4883\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0175 - val_loss: 9.4512\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.0154 - val_loss: 9.5075\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0179 - val_loss: 9.5616\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0164 - val_loss: 9.6416\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.0191 - val_loss: 9.6256\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.0101 - val_loss: 9.6158\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0132 - val_loss: 9.6876\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0129 - val_loss: 9.7154\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.0111 - val_loss: 9.7157\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 1.0087 - val_loss: 9.8083\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.0193 - val_loss: 9.8300\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 1.0158 - val_loss: 9.9001\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.0160 - val_loss: 9.7861\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0132 - val_loss: 9.8298\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.0094 - val_loss: 9.8214\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.0112 - val_loss: 9.8650\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.0057 - val_loss: 9.8647\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.0068 - val_loss: 9.8427\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9973 - val_loss: 9.8879\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9983 - val_loss: 9.8763\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9969 - val_loss: 9.9674\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9957 - val_loss: 9.9506\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9931 - val_loss: 10.0843\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 1.0026 - val_loss: 10.0119\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.9979 - val_loss: 10.0105\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9986 - val_loss: 9.9478\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9940 - val_loss: 10.0516\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9938 - val_loss: 10.1616\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9996 - val_loss: 10.1972\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9923 - val_loss: 10.2734\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9918 - val_loss: 10.2634\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9882 - val_loss: 10.1656\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9832 - val_loss: 10.2772\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9858 - val_loss: 10.3020\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9973 - val_loss: 10.3741\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9909 - val_loss: 10.4634\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9891 - val_loss: 10.4596\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9914 - val_loss: 10.4394\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9813 - val_loss: 10.3138\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 5s 249ms/step - loss: 0.9823 - val_loss: 10.4556\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9792 - val_loss: 10.3983\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9761 - val_loss: 10.5278\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9748 - val_loss: 10.4435\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 5s 241ms/step - loss: 0.9780 - val_loss: 10.6029\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9733 - val_loss: 10.5939\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9769 - val_loss: 10.6171\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9727 - val_loss: 10.6068\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9744 - val_loss: 10.6584\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9720 - val_loss: 10.5924\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9679 - val_loss: 10.6395\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9693 - val_loss: 10.7142\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9749 - val_loss: 10.6434\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9787 - val_loss: 10.6533\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9726 - val_loss: 10.7162\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9684 - val_loss: 10.6517\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9730 - val_loss: 10.7604\n",
      "Epoch 319/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 5s 233ms/step - loss: 0.9757 - val_loss: 10.7297\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9710 - val_loss: 10.8346\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9694 - val_loss: 10.6994\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9696 - val_loss: 10.8467\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9686 - val_loss: 10.7928\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9687 - val_loss: 10.8560\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9676 - val_loss: 10.9195\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9623 - val_loss: 10.9561\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9647 - val_loss: 10.9792\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9601 - val_loss: 10.9700\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9603 - val_loss: 10.9547\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9611 - val_loss: 10.9832\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9623 - val_loss: 10.9737\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9601 - val_loss: 11.0120\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9614 - val_loss: 11.1036\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9633 - val_loss: 11.0993\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9580 - val_loss: 11.0350\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9573 - val_loss: 11.1029\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9582 - val_loss: 11.1232\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9527 - val_loss: 11.1210\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9536 - val_loss: 11.2098\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9523 - val_loss: 11.2300\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9534 - val_loss: 11.2441\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9622 - val_loss: 11.3703\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9592 - val_loss: 11.2854\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9578 - val_loss: 11.3156\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9564 - val_loss: 11.2634\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9592 - val_loss: 11.3006\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9552 - val_loss: 11.4155\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9531 - val_loss: 11.3214\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.9507 - val_loss: 11.4865\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9437 - val_loss: 11.3581\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9486 - val_loss: 11.4165\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9439 - val_loss: 11.4549\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9506 - val_loss: 11.5920\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 5s 246ms/step - loss: 0.9554 - val_loss: 11.5199\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9479 - val_loss: 11.6693\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9514 - val_loss: 11.5168\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 5s 242ms/step - loss: 0.9552 - val_loss: 11.5438\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 0.9471 - val_loss: 11.5678\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9470 - val_loss: 11.5254\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9486 - val_loss: 11.6900\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.9456 - val_loss: 11.6621\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9469 - val_loss: 11.7205\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9410 - val_loss: 11.6512\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 0.9389 - val_loss: 11.7547\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9405 - val_loss: 11.7843\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9447 - val_loss: 11.7977\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 0.9500 - val_loss: 11.9225\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9567 - val_loss: 11.9958\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9524 - val_loss: 11.7971\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9519 - val_loss: 11.8265\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 0.9445 - val_loss: 11.6978\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9409 - val_loss: 11.7867\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9463 - val_loss: 11.7518\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 0.9372 - val_loss: 11.8241\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9383 - val_loss: 11.8603\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9357 - val_loss: 11.8615\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9307 - val_loss: 11.9445\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9382 - val_loss: 12.0272\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9432 - val_loss: 11.9683\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 0.9412 - val_loss: 11.9643\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9368 - val_loss: 11.9302\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9350 - val_loss: 11.9380\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9416 - val_loss: 12.0287\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 0.9381 - val_loss: 12.0827\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9393 - val_loss: 12.0782\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 5s 241ms/step - loss: 0.9369 - val_loss: 12.1057\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9298 - val_loss: 12.1203\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9355 - val_loss: 12.1456\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9380 - val_loss: 12.1729\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 0.9322 - val_loss: 12.2010\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9334 - val_loss: 12.2401\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9331 - val_loss: 12.1676\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9329 - val_loss: 12.2064\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 0.9281 - val_loss: 12.1696\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9299 - val_loss: 12.0987\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9303 - val_loss: 12.2947\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9276 - val_loss: 12.1493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 398/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9257 - val_loss: 12.3310\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9261 - val_loss: 12.2384\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 5s 242ms/step - loss: 0.9288 - val_loss: 12.3386\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 0.9320 - val_loss: 12.2686\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9400 - val_loss: 12.4995\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9388 - val_loss: 12.4245\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9440 - val_loss: 12.4507\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 0.9318 - val_loss: 12.5335\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9345 - val_loss: 12.3693\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 0.9220 - val_loss: 12.3814\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9243 - val_loss: 12.5205\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9221 - val_loss: 12.5541\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9223 - val_loss: 12.5082\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9253 - val_loss: 12.5372\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 0.9203 - val_loss: 12.6560\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9207 - val_loss: 12.5279\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9202 - val_loss: 12.5473\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 0.9245 - val_loss: 12.6659\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9226 - val_loss: 12.5223\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9331 - val_loss: 12.6069\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 0.9275 - val_loss: 12.7575\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 0.9298 - val_loss: 12.7608\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 5s 251ms/step - loss: 0.9273 - val_loss: 12.7475\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 5s 246ms/step - loss: 0.9317 - val_loss: 12.7609\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9205 - val_loss: 12.7672\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 0.9255 - val_loss: 12.8944\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 0.9199 - val_loss: 12.7792\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 5s 241ms/step - loss: 0.9219 - val_loss: 12.8411\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 5s 241ms/step - loss: 0.9195 - val_loss: 12.9136\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 0.9145 - val_loss: 12.7420\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 5s 250ms/step - loss: 0.9119 - val_loss: 12.8331\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 5s 251ms/step - loss: 0.9148 - val_loss: 12.7635\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9209 - val_loss: 12.8612\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9158 - val_loss: 12.8848\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9093 - val_loss: 12.9501\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 5s 244ms/step - loss: 0.9092 - val_loss: 12.9232\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 5s 243ms/step - loss: 0.9116 - val_loss: 13.0030\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 0.9113 - val_loss: 13.0161\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9169 - val_loss: 12.9855\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9202 - val_loss: 12.9420\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9118 - val_loss: 12.9814\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9112 - val_loss: 13.1221\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9136 - val_loss: 13.1406\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9166 - val_loss: 13.1655\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9130 - val_loss: 13.1834\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9146 - val_loss: 13.1173\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9114 - val_loss: 13.1403\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9171 - val_loss: 13.0433\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9073 - val_loss: 13.1211\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9160 - val_loss: 13.2006\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9133 - val_loss: 13.1212\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9176 - val_loss: 13.1389\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9135 - val_loss: 13.1973\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9126 - val_loss: 13.1470\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9089 - val_loss: 13.1996\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9135 - val_loss: 13.0896\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9118 - val_loss: 13.1459\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9126 - val_loss: 13.2100\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9067 - val_loss: 13.3802\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9061 - val_loss: 13.3369\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9079 - val_loss: 13.4832\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9170 - val_loss: 13.3519\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9247 - val_loss: 13.3912\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9298 - val_loss: 13.4072\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9216 - val_loss: 13.4129\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9196 - val_loss: 13.3504\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9219 - val_loss: 13.4423\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9262 - val_loss: 13.7156\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9234 - val_loss: 13.4818\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9326 - val_loss: 13.6510\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9279 - val_loss: 13.4566\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9167 - val_loss: 13.4354\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9138 - val_loss: 13.3649\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9125 - val_loss: 13.4147\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9089 - val_loss: 13.5337\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9092 - val_loss: 13.4119\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9078 - val_loss: 13.5388\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9054 - val_loss: 13.5850\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9054 - val_loss: 13.5846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 477/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9054 - val_loss: 13.5294\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.8986 - val_loss: 13.5645\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9031 - val_loss: 13.6054\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9083 - val_loss: 13.6862\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9083 - val_loss: 13.6273\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9009 - val_loss: 13.6264\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9026 - val_loss: 13.5865\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 0.9045 - val_loss: 13.6294\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9050 - val_loss: 13.6670\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.8980 - val_loss: 13.7601\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9044 - val_loss: 13.5784\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9019 - val_loss: 13.6760\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9054 - val_loss: 13.7805\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.8983 - val_loss: 13.7078\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9072 - val_loss: 13.6716\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9064 - val_loss: 13.7374\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9001 - val_loss: 13.7609\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 5s 244ms/step - loss: 0.8988 - val_loss: 13.6551\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.8971 - val_loss: 13.9004\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9128 - val_loss: 13.7811\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 0.9066 - val_loss: 13.8877\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9093 - val_loss: 13.8589\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9059 - val_loss: 13.8726\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 5s 243ms/step - loss: 0.9021 - val_loss: 13.9836\n"
     ]
    }
   ],
   "source": [
    "h_0 = np.zeros((len(X),LATENT_DIM))\n",
    "c_0 = np.zeros((len(X),LATENT_DIM))\n",
    "r = model.fit(\n",
    "    [X,h_0,c_0],Y_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split = VALIDATION_SPLIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(r.history['loss'],label = 'loss')\n",
    "plt.plot(r.history['val_loss'],label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for the test model\n",
    "input_test = Input(shape=(1))\n",
    "x_test = embedding_layer(input_test)\n",
    "h_t = Input(shape=(LATENT_DIM,))\n",
    "c_t = Input(shape=(LATENT_DIM,))\n",
    "# now we need the hidden & cell states\n",
    "x_test,h,c = lstm(x_test,initial_state=[h_t,c_t])\n",
    "output_test = dense(x_test)\n",
    "\n",
    "model_test = Model([input_test,h_t,c_t],[output_test,h,c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse word2idx\n",
    "idx2word = {v:k for k,v in word2idx.items()}\n",
    "\n",
    "# generate a line of poetry\n",
    "\n",
    "def gen_line():\n",
    "    x = word2idx['<بداية>']\n",
    "    x = np.array(x).reshape(1,1)\n",
    "    h_t = np.zeros((1,LATENT_DIM))\n",
    "    c_t = np.zeros((1,LATENT_DIM))\n",
    "    \n",
    "    line = []\n",
    "    \n",
    "    for i in range(max_seq_length):\n",
    "        x,h_t,c_t = model_test.predict([x,h_t,c_t])\n",
    "        # get probabilities\n",
    "        x = np.squeeze(x)\n",
    "        # when the model is not trained enough, sometimes x[0] is too high\n",
    "        # but we know this does not really correspond to any word (padding)\n",
    "        # so we set it to 0 then normalise before sampling\n",
    "        x[0] = 0\n",
    "        # renomralise x\n",
    "        x/=x.sum()\n",
    "        # now sample a word\n",
    "        x = np.random.choice(len(x),p=x)\n",
    "        # get word\n",
    "        word = idx2word[x]\n",
    "        # make current word x\n",
    "        x = np.array(x).reshape(1,1)\n",
    "        if word == '<نهاية>':\n",
    "            break\n",
    "        else:\n",
    "            line.append(word)\n",
    "    return ' '.join(line)\n",
    "\n",
    "\n",
    "# generate poem of 4 lines\n",
    "def gen_poem():\n",
    "     print('\\n'.join(gen_line() for i in range(4)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "وَاللَهِ ما يُمضي رَسولاً صادِق\n",
      "أَصفَيتُ وُدّاً مَن أَرادَ هَلاكي\n",
      "قَطَعتُ رِقابَهُم وَأَسَرتُ مِنهُم في\n",
      "عَلَيكِ مَقيلي مَفرِقَ يَجودُ\n",
      "------------------\n",
      "Generate Poem [Y/n]: Y\n",
      "-------------------\n",
      "فَبادِري وَاِنظُري طَعناً لا مُوَلَّهٌ حَيرانُ\n",
      "وَلَهُ في يَبرُدُ نَظرَةٌ ذِكرُهُنَّ لِخَلقٍ\n",
      "وِصالٌ وَلا يُلهيهِ مِن حَلِّهِ النَقا لِأَثري\n",
      "وَالنَصرُ مِن جُلَسائِهِ دونَ ضَمِّكَ وَالعِناقِ\n",
      "------------------\n",
      "Generate Poem [Y/n]: Y\n",
      "-------------------\n",
      "وَيَومَ البَذلِ نُعطي ما مَلَكن\n",
      "لَكِ مِنّي إِذا تَنَفَّستُ ذَميمٌ\n",
      "وَأَنا الأَسوَدُ وَالعَبدُ الَّذي\n",
      "يا أَبا اليَقظانِ أَغواكَ الطَمَع\n",
      "------------------\n",
      "Generate Poem [Y/n]: Y\n",
      "-------------------\n",
      "وَمَوكِبٍ خُضتُ أَعلاهُ وَأَسفَلَهُ\n",
      "زُرتَني تَطلُبُ مِنّي غَفلَةً\n",
      "تاجِراً يَشتَري النُفوسَ لَحمُه\n",
      "وَلَم يَطعَن في مِثلُهُ\n",
      "------------------\n",
      "Generate Poem [Y/n]: Y\n",
      "-------------------\n",
      "وَتَرَكتُها جَزَراً لِمَن ناواها\n",
      "ذَكَرتُ صَبابَتي مِن بَعدِ حينِ\n",
      "ماذا أُريدُ بِقَومٍ يَهدُرونَ دَمي\n",
      "حَمِدتُ تَجَلُّدي وَشكَرتُ صَبري\n",
      "------------------\n",
      "Generate Poem [Y/n]: Y\n",
      "-------------------\n",
      "عَرَكتُ نَوائِبَ الأَيّامِ حَتّى\n",
      "يا عَبلَ قَد دَنَتِ المَنِيَّةُ فَاِندُبي\n",
      "وَلا دِيارُهُمُ بِالأَهلِ وَالعارِضينَ\n",
      "كُرَبَ حُدودُ حَيرانُ تَنظُرُ نَدري النَوائِبِ\n",
      "------------------\n",
      "Generate Poem [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    gen_poem()\n",
    "    print('------------------')\n",
    "    more = input('Generate Poem [Y/n]: ')\n",
    "    if more.lower()=='n':\n",
    "        break\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try again without normalising\n",
    "def gen_line_no_norm():\n",
    "    x = word2idx['<بداية>']\n",
    "    x = np.array(x).reshape(1,1)\n",
    "    h_t = np.zeros((1,LATENT_DIM))\n",
    "    c_t = np.zeros((1,LATENT_DIM))    \n",
    "    line = []\n",
    "    for i in range(max_seq_length):\n",
    "        x,h_t,c_t = model_test.predict([x,h_t,c_t])\n",
    "        x = np.squeeze(x)\n",
    "        x = np.random.choice(len(x),p=x)\n",
    "        word = idx2word[x]\n",
    "        x = np.array(x).reshape(1,1)\n",
    "        if word == '<نهاية>':\n",
    "            break\n",
    "        else:\n",
    "            line.append(word)\n",
    "    return ' '.join(line)\n",
    "\n",
    "\n",
    "# generate poem of 4 lines\n",
    "def gen_poem_no_norm():\n",
    "     print('\\n'.join(gen_line_no_norm() for i in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "فَلا يَغشى مَعالِمَهُ ظَلامُ\n",
      "إِذا لَم يَثِب لِلأَمرِ إِلّا بِقائِدِ\n",
      "وَلا أُصغي لِقَهقَهَةِ القَناني\n",
      "وَلَولا سِناني وَالحُسامُ وَهِمَّتي\n",
      "------------------\n",
      "Generate Poem [Y/n]: Y\n",
      "-------------------\n",
      "أَزَجُّ نَقِيُّ الخَدِّ أَبلَجُ أَدعَجُ\n",
      "وَيُنعِمُ بِالجِمالِ وَبِالنِياقِ\n",
      "فَسَيفي وَهَذا الرُمحُ عَمّي عَهدِ\n",
      "سَتَعلَمُ أَيُّنا يَبقى أَبوكِ نائِبَةً\n",
      "------------------\n",
      "Generate Poem [Y/n]: Y\n",
      "-------------------\n",
      "أُذَكِّرُ قَومي ظُلمَهُم لي وَبَغيَهُم\n",
      "وَعُدتُ مِن فِراقِ في مُقَيَّدُ المَلا مِسكٌ يَعيبوا\n",
      "كَأَنَّ عَلَيهِ حُلَّةَ أُرجُوانِ\n",
      "تُرى هَذِهِ ريحُ الكَريهَةِ بِلا غِبتِ كَجَهَنَّمٍ\n",
      "------------------\n",
      "Generate Poem [Y/n]: Y\n",
      "-------------------\n",
      "مِن كُلِّ فَنٍّ لاحَ في الوَغى قاطِعَ الحَدِّ\n",
      "وَأَجعَلُها مِنَ الدُنيا اِهتِمامي\n",
      "أَلهو بِما فيهِ مِن فُؤادي لِثامِه\n",
      "وَأُفني حَواضِرَها وَالبَوادي\n",
      "------------------\n",
      "Generate Poem [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    gen_poem_no_norm()\n",
    "    print('------------------')\n",
    "    more = input('Generate Poem [Y/n]: ')\n",
    "    if more.lower()=='n':\n",
    "        break\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for someone familiar with Antarah's poetry\n",
    "# most of these lines are from his actual poems\n",
    "# which means our model has learned Antarah (to some extent)\n",
    "# one of my favourites when describing the beauty of Abla (generated above)\n",
    "# أَزَجُّ نَقِيُّ الخَدِّ أَبلَجُ أَدعَجُ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now again for option 3 : tashkeel + pretrained word embeddings\n",
    "# all we need to do is change embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 % done\n",
      "10.0 % done\n",
      "15.0 % done\n",
      "20.0 % done\n",
      "25.0 % done\n",
      "30.0 % done\n",
      "35.0 % done\n",
      "40.0 % done\n",
      "45.0 % done\n",
      "50.0 % done\n",
      "55.0 % done\n",
      "60.0 % done\n",
      "65.0 % done\n",
      "70.0 % done\n",
      "75.0 % done\n",
      "80.0 % done\n",
      "85.0 % done\n",
      "90.0 % done\n",
      "95.0 % done\n",
      "100.0 % done\n"
     ]
    }
   ],
   "source": [
    "# next we want to create our embedding matrix\n",
    "# first we load in the pre trained word vectors\n",
    "# we can find pretrained word embeddigs for arabic here\n",
    "# https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "# we strip the words in word2idx from tashkeel\n",
    "# search for a key that matches the current word, then place the vector\n",
    "\n",
    "word2tashkeel = {}\n",
    "\n",
    "for word in word2idx.keys():\n",
    "    key = strip_tashkeel(word)\n",
    "    if key not in word2tashkeel:\n",
    "        word2tashkeel[key] = []\n",
    "    word2tashkeel[key].append(word)\n",
    "\n",
    "file = 'datasets/fasttext ara/cc.ar.300.vec'\n",
    "word2vec = {}\n",
    "\n",
    "for i,line in enumerate(open(file,encoding='utf8')):\n",
    "    line = line.split()\n",
    "    word = line[0]\n",
    "    if (i+1)%100000 == 0:\n",
    "        print((i+1)*100/2000000,'% done')\n",
    "    if word in word2tashkeel:\n",
    "        tashkeel_words = word2tashkeel[word]\n",
    "        vec = np.asarray(line[1:],dtype='float32')\n",
    "        for tashkeeld in tashkeel_words:            \n",
    "            word2vec[tashkeeld] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = min(len(word2idx)+1,MAX_VOCAB_SIZE)\n",
    "# now lets create our embedding matrix\n",
    "embedding = np.zeros((V,EMBEDDING_DIM))\n",
    "\n",
    "# # now we fill the matrix with the pretrained word embeddings\n",
    "# # if a word is not in our pretrained vectors , we leave it as zeros\n",
    "\n",
    "for word,idx in word2idx.items():\n",
    "    vec = word2vec.get(word,0)\n",
    "    if idx < V:\n",
    "        embedding[idx] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer\n",
    "embedding_layer = Embedding(\n",
    "    V,\n",
    "    EMBEDDING_DIM,\n",
    "    weights = [embedding],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = Input(shape=(max_seq_length,))\n",
    "initial_h = Input(shape=(LATENT_DIM,))\n",
    "initial_c = Input(shape=(LATENT_DIM,))\n",
    "x_train = embedding_layer(input_train)\n",
    "lstm = LSTM(LATENT_DIM,return_sequences=True,return_state=True)\n",
    "x_train,_,_ = lstm(x_train,initial_state=[initial_h,initial_c])\n",
    "\n",
    "# then we have a dense layer\n",
    "dense = Dense(V,activation='softmax')\n",
    "output_train = dense(x_train)\n",
    "model = Model([input_train,initial_h,initial_c],output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = np.ceil(len(X)*(1-VALIDATION_SPLIT)/BATCH_SIZE).astype('int32')\n",
    "boundaries = [steps_per_epoch*300]\n",
    "values = [0.005, 0.005]\n",
    "learning_rate_fn = PiecewiseConstantDecay(boundaries, values)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 10s 318ms/step - loss: 5.2535 - val_loss: 4.7137\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 4.4830 - val_loss: 4.7004\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 5s 241ms/step - loss: 4.2675 - val_loss: 4.8184\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 5s 241ms/step - loss: 4.2550 - val_loss: 4.8257\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 5s 244ms/step - loss: 4.2229 - val_loss: 4.8388\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 5s 243ms/step - loss: 4.2053 - val_loss: 4.8466\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 6s 254ms/step - loss: 4.1956 - val_loss: 4.8650\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 6s 284ms/step - loss: 4.1592 - val_loss: 4.8762\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 6s 274ms/step - loss: 4.1431 - val_loss: 4.8902\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 6s 270ms/step - loss: 4.1387 - val_loss: 4.9034\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 6s 256ms/step - loss: 4.1021 - val_loss: 4.8980\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 5s 243ms/step - loss: 4.0992 - val_loss: 4.9052\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 4.0770 - val_loss: 4.9159\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 5s 247ms/step - loss: 4.0415 - val_loss: 4.9143\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 6s 258ms/step - loss: 4.0190 - val_loss: 4.9074\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 3.9792 - val_loss: 4.9185\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 3.9469 - val_loss: 4.9335\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 3.9140 - val_loss: 4.9376\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 3.8666 - val_loss: 4.9400\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 3.8407 - val_loss: 4.9530\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 3.7844 - val_loss: 4.9682\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 3.7685 - val_loss: 4.9747\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 3.7299 - val_loss: 4.9811\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 3.6965 - val_loss: 4.9908\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 3.6599 - val_loss: 4.9993\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 3.6333 - val_loss: 5.0129\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 3.5801 - val_loss: 5.0285\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 3.5619 - val_loss: 5.0355\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 3.5268 - val_loss: 5.0477\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 3.4863 - val_loss: 5.0610\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 3.4647 - val_loss: 5.0771\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 3.4464 - val_loss: 5.0931\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 5s 241ms/step - loss: 3.3946 - val_loss: 5.1083\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 3.3744 - val_loss: 5.1226\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 5s 246ms/step - loss: 3.3300 - val_loss: 5.1421\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 5s 244ms/step - loss: 3.3196 - val_loss: 5.1619\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 3.2821 - val_loss: 5.1804\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 3.2550 - val_loss: 5.1998\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 3.2184 - val_loss: 5.2214\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 3.2068 - val_loss: 5.2401\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 3.1738 - val_loss: 5.2613\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 3.1411 - val_loss: 5.2830\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 3.1074 - val_loss: 5.3021\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 3.1043 - val_loss: 5.3266\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 3.0524 - val_loss: 5.3452\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 3.0280 - val_loss: 5.3714\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 3.0060 - val_loss: 5.3953\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.9859 - val_loss: 5.4182\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 2.9515 - val_loss: 5.4391\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 2.9317 - val_loss: 5.4633\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.9071 - val_loss: 5.4893\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.8709 - val_loss: 5.5190\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 2.8494 - val_loss: 5.5477\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.8389 - val_loss: 5.5767\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.8135 - val_loss: 5.6041\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.7743 - val_loss: 5.6295\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.7360 - val_loss: 5.6434\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 2.7049 - val_loss: 5.6441\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.6770 - val_loss: 5.6633\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.6400 - val_loss: 5.6617\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.6061 - val_loss: 5.6821\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.5800 - val_loss: 5.7029\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.5558 - val_loss: 5.7259\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 2.5381 - val_loss: 5.7446\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 2.5131 - val_loss: 5.7590\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 2.4921 - val_loss: 5.7916\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.4604 - val_loss: 5.7975\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.4452 - val_loss: 5.8238\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 2.4040 - val_loss: 5.8518\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.4032 - val_loss: 5.8773\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.3870 - val_loss: 5.9014\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 2.3603 - val_loss: 5.9229\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.3333 - val_loss: 5.9536\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.3274 - val_loss: 5.9763\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.3005 - val_loss: 6.0123\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.2803 - val_loss: 6.0347\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 2.2657 - val_loss: 6.0606\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.2459 - val_loss: 6.0505\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.2303 - val_loss: 6.0736\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.2138 - val_loss: 6.0876\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 2.2020 - val_loss: 6.0891\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.1488 - val_loss: 6.1197\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.1621 - val_loss: 6.1465\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.1286 - val_loss: 6.1706\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.1187 - val_loss: 6.1780\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.0975 - val_loss: 6.2204\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.0898 - val_loss: 6.2334\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.0698 - val_loss: 6.2419\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 2.0451 - val_loss: 6.2712\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.0274 - val_loss: 6.3111\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 2.0177 - val_loss: 6.3157\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 2.0050 - val_loss: 6.3590\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.9912 - val_loss: 6.4026\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.9631 - val_loss: 6.4201\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.9657 - val_loss: 6.4188\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.9289 - val_loss: 6.4925\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.9212 - val_loss: 6.4797\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.9086 - val_loss: 6.4988\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 1.8884 - val_loss: 6.5694\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.8775 - val_loss: 6.5520\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.8690 - val_loss: 6.5671\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.8398 - val_loss: 6.5894\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.8174 - val_loss: 6.6585\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.8274 - val_loss: 6.6317\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.8173 - val_loss: 6.6655\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.8015 - val_loss: 6.7000\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.7827 - val_loss: 6.7013\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.7801 - val_loss: 6.7788\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.7631 - val_loss: 6.7533\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.7397 - val_loss: 6.8135\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.7495 - val_loss: 6.7940\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.7336 - val_loss: 6.8789\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.7004 - val_loss: 6.8732\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 1.7133 - val_loss: 6.8602\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.7027 - val_loss: 6.8267\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.6872 - val_loss: 6.9451\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.6699 - val_loss: 6.9624\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.6615 - val_loss: 6.9846\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.6612 - val_loss: 7.0287\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.6447 - val_loss: 7.0537\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.6491 - val_loss: 7.0611\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.6310 - val_loss: 7.0602\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.6340 - val_loss: 7.0295\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.6109 - val_loss: 7.1479\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.6012 - val_loss: 7.1890\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.5919 - val_loss: 7.2278\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.5793 - val_loss: 7.2236\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.5665 - val_loss: 7.1970\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.5687 - val_loss: 7.2101\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.5579 - val_loss: 7.2574\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.5506 - val_loss: 7.2407\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.5516 - val_loss: 7.2655\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.5244 - val_loss: 7.3591\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.5288 - val_loss: 7.3661\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.5124 - val_loss: 7.3596\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.5052 - val_loss: 7.3860\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.5091 - val_loss: 7.4098\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.4997 - val_loss: 7.4388\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.4854 - val_loss: 7.4789\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.4775 - val_loss: 7.5569\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.4822 - val_loss: 7.4959\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.4659 - val_loss: 7.3822\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.4644 - val_loss: 7.5842\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.4601 - val_loss: 7.5483\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.4471 - val_loss: 7.5809\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.4396 - val_loss: 7.5348\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.4393 - val_loss: 7.5986\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.4321 - val_loss: 7.6630\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.4237 - val_loss: 7.5809\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.4254 - val_loss: 7.7074\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.4115 - val_loss: 7.6215\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.4134 - val_loss: 7.6596\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.4147 - val_loss: 7.6519\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.4028 - val_loss: 7.8186\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.3884 - val_loss: 7.9062\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.3855 - val_loss: 7.9532\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.3750 - val_loss: 7.9084\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.3743 - val_loss: 7.7759\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.3641 - val_loss: 7.9091\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 5s 251ms/step - loss: 1.3628 - val_loss: 7.8813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 1.3575 - val_loss: 7.9263\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 5s 242ms/step - loss: 1.3563 - val_loss: 7.9712\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.3504 - val_loss: 8.0011\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 1.3342 - val_loss: 8.0789\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.3403 - val_loss: 7.9488\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.3324 - val_loss: 8.0413\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.3306 - val_loss: 8.0963\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.3128 - val_loss: 8.1120\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.3103 - val_loss: 8.1734\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.3105 - val_loss: 8.0248\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.3096 - val_loss: 8.2500\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.3114 - val_loss: 8.2317\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2927 - val_loss: 8.2130\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 1.3069 - val_loss: 8.2671\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 1.2901 - val_loss: 8.3201\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 1.2728 - val_loss: 8.3275\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.2844 - val_loss: 8.3499\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2735 - val_loss: 8.4084\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.2658 - val_loss: 8.3944\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2709 - val_loss: 8.4232\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2726 - val_loss: 8.5001\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2703 - val_loss: 8.5246\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2544 - val_loss: 8.5964\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2586 - val_loss: 8.4913\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.2638 - val_loss: 8.5520 - l\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.2423 - val_loss: 8.5601\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2515 - val_loss: 8.6207\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2374 - val_loss: 8.6487\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.2391 - val_loss: 8.7066\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2394 - val_loss: 8.6698\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2360 - val_loss: 8.6926\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.2331 - val_loss: 8.7420\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.2256 - val_loss: 8.8726\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2238 - val_loss: 8.7656\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2119 - val_loss: 8.8494\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.2050 - val_loss: 8.8255\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2076 - val_loss: 8.8346\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.1975 - val_loss: 8.8364\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.1994 - val_loss: 8.9464\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.2005 - val_loss: 9.0095\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.2021 - val_loss: 8.9926\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.1915 - val_loss: 9.0232\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.1970 - val_loss: 9.0120\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.1928 - val_loss: 9.1103\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.1863 - val_loss: 9.2078\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.1851 - val_loss: 9.2520\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.1828 - val_loss: 9.1317\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.1710 - val_loss: 9.3404\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.1712 - val_loss: 9.3186\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.1695 - val_loss: 9.2207\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.1646 - val_loss: 9.2596\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.1648 - val_loss: 9.3918\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.1657 - val_loss: 9.3543\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.1597 - val_loss: 9.4672\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.1556 - val_loss: 9.4624\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.1492 - val_loss: 9.4347\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.1516 - val_loss: 9.4141\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.1440 - val_loss: 9.5181\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 1.1426 - val_loss: 9.5260\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.1413 - val_loss: 9.4561\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.1437 - val_loss: 9.6002\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.1391 - val_loss: 9.4473\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.1395 - val_loss: 9.7113\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.1325 - val_loss: 9.5615\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.1379 - val_loss: 9.6541\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.1357 - val_loss: 9.6234\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 1.1246 - val_loss: 9.7579\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 1.1246 - val_loss: 9.8155\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 1.1246 - val_loss: 9.7043\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.1135 - val_loss: 9.9682\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.1168 - val_loss: 9.8183\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.1171 - val_loss: 9.7727\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.1180 - val_loss: 9.8890\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.1079 - val_loss: 9.8494\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.1136 - val_loss: 9.9106\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.1217 - val_loss: 10.0199\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.1090 - val_loss: 9.9276\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.1004 - val_loss: 9.9065\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.1083 - val_loss: 9.8520\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.1098 - val_loss: 9.9830\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.1046 - val_loss: 9.9424\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0978 - val_loss: 9.9839\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0970 - val_loss: 10.0428\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.0922 - val_loss: 10.1669\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 5s 216ms/step - loss: 1.0965 - val_loss: 10.1506\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 5s 216ms/step - loss: 1.0897 - val_loss: 10.2360\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0807 - val_loss: 10.2979\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0833 - val_loss: 10.3707\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 5s 216ms/step - loss: 1.0854 - val_loss: 10.3129\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0868 - val_loss: 10.3662\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0865 - val_loss: 10.3508\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0796 - val_loss: 10.4011\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0810 - val_loss: 10.3935\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0807 - val_loss: 10.4461\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0773 - val_loss: 10.3775\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0760 - val_loss: 10.5758\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.0696 - val_loss: 10.5596\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.0620 - val_loss: 10.5721\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0638 - val_loss: 10.5472\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0660 - val_loss: 10.6302\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0669 - val_loss: 10.7059\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0691 - val_loss: 10.7141\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0635 - val_loss: 10.6779\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0622 - val_loss: 10.5690\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0583 - val_loss: 10.6685\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0631 - val_loss: 10.7575\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0566 - val_loss: 10.8003\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0515 - val_loss: 10.8268\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0494 - val_loss: 10.7835\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0454 - val_loss: 10.7456\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.0585 - val_loss: 10.9084\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0504 - val_loss: 10.8370\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0468 - val_loss: 10.8785\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.0501 - val_loss: 10.8662\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0551 - val_loss: 11.0177\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0430 - val_loss: 10.9944\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0409 - val_loss: 10.9176\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0522 - val_loss: 11.1929\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0374 - val_loss: 10.9882\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0346 - val_loss: 11.0405\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.0477 - val_loss: 11.1209\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0364 - val_loss: 11.0947\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 5s 217ms/step - loss: 1.0512 - val_loss: 11.0168\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0280 - val_loss: 11.0423\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0282 - val_loss: 11.2292\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0375 - val_loss: 11.0857\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0272 - val_loss: 11.0907\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0329 - val_loss: 11.2729\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0234 - val_loss: 11.2993\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0229 - val_loss: 11.2808\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0273 - val_loss: 11.2699\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0251 - val_loss: 11.1619\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0244 - val_loss: 11.3599\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0205 - val_loss: 11.2297\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0180 - val_loss: 11.3204\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0199 - val_loss: 11.4753\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0188 - val_loss: 11.5187\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0210 - val_loss: 11.5849\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0135 - val_loss: 11.4678\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 1.0177 - val_loss: 11.6079\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0271 - val_loss: 11.5770\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0168 - val_loss: 11.4940\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0129 - val_loss: 11.5367\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0156 - val_loss: 11.4395\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0057 - val_loss: 11.5513\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.0048 - val_loss: 11.6142\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0059 - val_loss: 11.6923\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 5s 218ms/step - loss: 1.0132 - val_loss: 11.7076\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 1.0093 - val_loss: 11.4560\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 1.0069 - val_loss: 11.6264\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 1.0053 - val_loss: 11.7487\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 1.0123 - val_loss: 11.7273\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 1.0001 - val_loss: 11.6428\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0099 - val_loss: 11.7137\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0082 - val_loss: 11.7241\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0057 - val_loss: 11.8349\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0031 - val_loss: 11.9558\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0088 - val_loss: 11.8784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 319/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 1.0044 - val_loss: 11.8922\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.9979 - val_loss: 11.8694\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9886 - val_loss: 11.8486\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.9899 - val_loss: 11.8317\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.9948 - val_loss: 11.9576\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.9928 - val_loss: 11.9308\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9889 - val_loss: 12.0906\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 1.0037 - val_loss: 12.1172\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 0.9931 - val_loss: 12.0569\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9861 - val_loss: 12.0174\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.9878 - val_loss: 12.1481\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.9891 - val_loss: 12.2310\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9860 - val_loss: 12.0846\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9790 - val_loss: 12.2214\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.9912 - val_loss: 12.3200\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9825 - val_loss: 12.1677\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9882 - val_loss: 12.2851\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9944 - val_loss: 12.1971\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 0.9920 - val_loss: 12.3034\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.9822 - val_loss: 12.3123\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.9881 - val_loss: 12.2467\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 0.9892 - val_loss: 12.3493\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9871 - val_loss: 12.3270\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.9881 - val_loss: 12.4194\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9886 - val_loss: 12.4592\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9806 - val_loss: 12.4555\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9773 - val_loss: 12.4543\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9850 - val_loss: 12.4814\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9846 - val_loss: 12.5164\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9762 - val_loss: 12.5567\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9838 - val_loss: 12.5874\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9841 - val_loss: 12.5573\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9803 - val_loss: 12.5343\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9808 - val_loss: 12.5723\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 0.9781 - val_loss: 12.5790\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 0.9741 - val_loss: 12.6675\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.9817 - val_loss: 12.5706\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9780 - val_loss: 12.7814\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9745 - val_loss: 12.7022\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9693 - val_loss: 12.6987\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9788 - val_loss: 12.6669\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 0.9757 - val_loss: 12.7912\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 6s 261ms/step - loss: 0.9716 - val_loss: 12.7886\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 5s 242ms/step - loss: 0.9762 - val_loss: 12.7308\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9690 - val_loss: 12.8056\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9735 - val_loss: 12.9072\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.9738 - val_loss: 12.8934\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9731 - val_loss: 12.9628\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 6s 266ms/step - loss: 0.9699 - val_loss: 12.9668\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 6s 274ms/step - loss: 0.9708 - val_loss: 12.9314\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 5s 239ms/step - loss: 0.9718 - val_loss: 12.9604\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9636 - val_loss: 12.9378\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 0.9617 - val_loss: 12.9547\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9702 - val_loss: 13.0118\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.9696 - val_loss: 13.0119\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9724 - val_loss: 13.0714\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.9686 - val_loss: 13.1747\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9767 - val_loss: 13.1356\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9703 - val_loss: 13.1447\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9661 - val_loss: 13.2760\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9616 - val_loss: 13.1978\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9691 - val_loss: 13.2891\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 0.9627 - val_loss: 13.2035\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9730 - val_loss: 13.1904\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9678 - val_loss: 13.2683\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9623 - val_loss: 13.3488\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9569 - val_loss: 13.4185\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9560 - val_loss: 13.3696\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 5s 235ms/step - loss: 0.9546 - val_loss: 13.3824\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9620 - val_loss: 13.4002\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9607 - val_loss: 13.3637\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9552 - val_loss: 13.3490\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9626 - val_loss: 13.4490\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9735 - val_loss: 13.5680\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9682 - val_loss: 13.4892\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9682 - val_loss: 13.5386\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9645 - val_loss: 13.5610\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9613 - val_loss: 13.4769\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9561 - val_loss: 13.5972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 398/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9615 - val_loss: 13.5687\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9635 - val_loss: 13.6740\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9547 - val_loss: 13.6529\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9550 - val_loss: 13.6790\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9520 - val_loss: 13.7135\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9506 - val_loss: 13.6950\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9500 - val_loss: 13.6196\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9529 - val_loss: 13.6700\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9508 - val_loss: 13.6570\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9522 - val_loss: 13.6583\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9544 - val_loss: 13.6772\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 0.9572 - val_loss: 13.6782\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9511 - val_loss: 13.7547\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9496 - val_loss: 13.9038\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9613 - val_loss: 13.7636\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9520 - val_loss: 13.8290\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9611 - val_loss: 13.8449\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9578 - val_loss: 13.8143\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 0.9554 - val_loss: 13.8069\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9536 - val_loss: 13.8904\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9565 - val_loss: 13.9273\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9486 - val_loss: 13.9876\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9439 - val_loss: 13.9558\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9492 - val_loss: 13.9378\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9413 - val_loss: 14.0110\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9499 - val_loss: 14.0368\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9418 - val_loss: 13.9974\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9539 - val_loss: 14.0767\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9576 - val_loss: 14.0847\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9526 - val_loss: 14.1039\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9492 - val_loss: 14.0377\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9511 - val_loss: 14.1883\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9634 - val_loss: 14.1166\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 0.9614 - val_loss: 14.2180\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.9559 - val_loss: 14.1610\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 0.9573 - val_loss: 14.1859\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 0.9556 - val_loss: 14.1750\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 6s 259ms/step - loss: 0.9482 - val_loss: 14.2445\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 5s 246ms/step - loss: 0.9435 - val_loss: 14.1965\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 6s 253ms/step - loss: 0.9477 - val_loss: 14.2144\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 0.9513 - val_loss: 14.2852\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 5s 251ms/step - loss: 0.9498 - val_loss: 14.1903\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 5s 244ms/step - loss: 0.9443 - val_loss: 14.2930\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 6s 262ms/step - loss: 0.9447 - val_loss: 14.3649\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 6s 252ms/step - loss: 0.9455 - val_loss: 14.2930\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 5s 243ms/step - loss: 0.9427 - val_loss: 14.3071\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 6s 261ms/step - loss: 0.9401 - val_loss: 14.3756\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 6s 271ms/step - loss: 0.9392 - val_loss: 14.4117\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 5s 249ms/step - loss: 0.9417 - val_loss: 14.3840\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 5s 243ms/step - loss: 0.9352 - val_loss: 14.5034\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.9354 - val_loss: 14.3426\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9377 - val_loss: 14.4810\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 0.9427 - val_loss: 14.5215\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9372 - val_loss: 14.4936\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9436 - val_loss: 14.5267\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9436 - val_loss: 14.5432\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9449 - val_loss: 14.5910\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9419 - val_loss: 14.4908\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9379 - val_loss: 14.5173\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9463 - val_loss: 14.5361\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 0.9440 - val_loss: 14.5714\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 0.9393 - val_loss: 14.6210\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.9381 - val_loss: 14.7632\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9389 - val_loss: 14.6386\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9413 - val_loss: 14.7922\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9325 - val_loss: 14.7238\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 0.9432 - val_loss: 14.6871\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9311 - val_loss: 14.7601\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9336 - val_loss: 14.8573\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.9419 - val_loss: 14.6798\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9347 - val_loss: 14.6447\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 0.9414 - val_loss: 14.7294\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.9355 - val_loss: 14.7679\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.9301 - val_loss: 14.7308\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9384 - val_loss: 14.8534\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9345 - val_loss: 14.8153\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9439 - val_loss: 14.9496\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.9364 - val_loss: 14.9751\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9395 - val_loss: 15.0096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 477/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 0.9405 - val_loss: 14.9576\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9324 - val_loss: 14.9252\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9367 - val_loss: 15.0805\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9360 - val_loss: 14.9294\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9325 - val_loss: 14.9503\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 0.9325 - val_loss: 15.0087\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9302 - val_loss: 15.1043\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 0.9382 - val_loss: 15.0429\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9331 - val_loss: 15.0048\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 0.9358 - val_loss: 15.0859\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9371 - val_loss: 15.2128\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 5s 232ms/step - loss: 0.9373 - val_loss: 15.1344\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9289 - val_loss: 15.1846\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.9275 - val_loss: 15.1907\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 5s 229ms/step - loss: 0.9271 - val_loss: 15.2419\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.9275 - val_loss: 15.1974\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 5s 230ms/step - loss: 0.9324 - val_loss: 15.3290\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.9357 - val_loss: 15.2297\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 5s 236ms/step - loss: 0.9246 - val_loss: 15.1781\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 5s 248ms/step - loss: 0.9283 - val_loss: 15.2727\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 0.9278 - val_loss: 15.3625\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 0.9276 - val_loss: 15.3764\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 0.9226 - val_loss: 15.3191\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 5s 237ms/step - loss: 0.9295 - val_loss: 15.3953\n"
     ]
    }
   ],
   "source": [
    "h_0 = np.zeros((len(X),LATENT_DIM))\n",
    "c_0 = np.zeros((len(X),LATENT_DIM))\n",
    "r = model.fit(\n",
    "    [X,h_0,c_0],Y_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split = VALIDATION_SPLIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(r.history['loss'],label = 'loss')\n",
    "plt.plot(r.history['val_loss'],label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = Input(shape=(1))\n",
    "x_test = embedding_layer(input_test)\n",
    "h_t = Input(shape=(LATENT_DIM,))\n",
    "c_t = Input(shape=(LATENT_DIM,))\n",
    "x_test,h,c = lstm(x_test,initial_state=[h_t,c_t])\n",
    "output_test = dense(x_test)\n",
    "\n",
    "model_test = Model([input_test,h_t,c_t],[output_test,h,c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "وَلَيسَ لِخَلقٍ مِن مُداراتِها بُدُّ\n",
      "فَقَصَّرَ عَن لَحاقي في المَعالي\n",
      "بِما يَجزي بِهِ الخَيلَ العِتاقا\n",
      "وَيَحظى بِالغِنى وَالمالِ دوني\n",
      "------------------\n",
      "Generate Poem [Y/n]: Y\n",
      "-------------------\n",
      "إِن تَكُن تَشكو لِأَوجاعِ الهَوى\n",
      "بِأَطرافِ المُثَقَّفَةِ العَوالي\n",
      "يَعدونَ بِالمُستَلئِمينَ عَوابِس\n",
      "وَلَو أَرسَلتُ رُمحي مَع جَبانٍ\n",
      "------------------\n",
      "Generate Poem [Y/n]: Y\n",
      "-------------------\n",
      "وَقَد غَنّى عَلى الأَغصانِ طَيرٌ\n",
      "أَغَنُّ مَليحُ الدَلِّ أَحوَرُ أَكحَلٌ\n",
      "ضَجّوا فَصُحتُ عَلَيهِمُ أَلفَ ضَربَه مُخبِر وَأَبي القَسطَلِ فَاِندُبي\n",
      "وَقَد عايَنتَ مِن خَبري الفِعالا\n",
      "------------------\n",
      "Generate Poem [Y/n]: Y\n",
      "-------------------\n",
      "تَراهُ بِتَفريجِ الأُمورِ وَلَفِّه\n",
      "قَصيرَةٌ عَنكَ فَالأَيّامُ تَنقَلِبُ\n",
      "وَخَلَّينا نِسائَهُمُ حَيارى حينِ\n",
      "سِوى كَبِدٍ حَرّى تَذوبُ فَأَسقَمُ\n",
      "------------------\n",
      "Generate Poem [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    gen_poem()\n",
    "    print('------------------')\n",
    "    more = input('Generate Poem [Y/n]: ')\n",
    "    if more.lower()=='n':\n",
    "        break\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أَلا مَن لِأَمرٍ حازِمٍ قَد بَدا لِيا\n",
      "فَدَعَوني مِن بَعدِ أَنتَ تَعُدُّ نَفسَكَ سَيِّد مَوهِن\n",
      "إِلّا المُدَرَّعَ بَينَ عَرينَةِ الأَشبالِ\n",
      "يا أَيُّها المَلِكُ الَّذي راحاتُهُ\n",
      "------------------\n",
      "Generate Poem [Y/n]: Y\n",
      "-------------------\n",
      "وَخَبِّر عَن عُبَيلَةَ العِدا أَمُت مُنقَطِعِ مُضيئَةٌ\n",
      "عَرَكتُ نَوائِبَ الأَيّامِ حَتّى\n",
      "تَهُزُّ سُمرَ القَنا حِقداً عَلَيَّ بِمُهَنَّدٍ\n",
      "لِأَنّي فارِسٌ مِن نَسلِ حامِ\n",
      "------------------\n",
      "Generate Poem [Y/n]: Y\n",
      "-------------------\n",
      "وَخُذي وَسَيفي طَلَلٌ بِالرَقمَتَينِ الغَربُ اِستَمتُ وَالخَفَرِ\n",
      "هَزَمتُ تَميماً ثُمَّ جَندَلتُ كَبشَهُم\n",
      "إِلّا عَلى مَوكِبٍ كَاللَيلِ مُحتَبِكِ صَبر\n",
      "وَذَكَّرَني المَنازِلَ وَالمَغاني\n",
      "------------------\n",
      "Generate Poem [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    gen_poem_no_norm()\n",
    "    print('------------------')\n",
    "    more = input('Generate Poem [Y/n]: ')\n",
    "    if more.lower()=='n':\n",
    "        break\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very good results\n",
    "# very consistent though the loss is greater\n",
    "# also maybe with more tunining we can reach a better loos\n",
    "# very happy to get the first shatr of my favourite line\n",
    "# أَغَنُّ مَليحُ الدَلِّ أَحوَرُ أَكحَلٌ       أَزَجُّ نَقِيُّ الخَدِّ أَبلَجُ أَدعَجُ\n",
    "# we got the second shatr from the previous model\n",
    "# note : tried these again with sparse categorical cross entropy\n",
    "# so we took the padding into consideration for the loss\n",
    "# got very good results (probably even better than this, more correct lines)\n",
    "# but did not get my favourite line so keeping this :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for the main course\n",
    "# neural machine translation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense,Input,LSTM,Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  \n",
    "EPOCHS = 100  \n",
    "LATENT_DIM_ENCODER = LATENT_DIM_DECODER = 256 # these need to be the same\n",
    "NUM_SAMPLES = 15000  \n",
    "MAX_SEQ_LENGTH = 100\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM_ENCODER = 300 # these can differ\n",
    "EMBEDDING_DIM_DECODER = 300 # for arabic we only have a D = 300 file\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we load in the data\n",
    "# the input to encoder will be the sentences in english\n",
    "# the input & targets of decoder are the arabic sentences with the <sos> & <eos> tags\n",
    "\n",
    "input_encoder = [] # english sentences\n",
    "input_decoder = [] # <sos> + arabic sentences\n",
    "targets_decoder = [] # arabic sentences + <eos>\n",
    "\n",
    "for i,line in enumerate(open('datasets/Tab-delimited Bilingual Sentence Pairs/ara.txt',encoding='utf8')):\n",
    "    if i == NUM_SAMPLES:\n",
    "        break\n",
    "    line = line.rstrip()\n",
    "    if not line:\n",
    "        continue\n",
    "    eng,ara,other = line.split('\\t')\n",
    "    input_encoder.append(eng)\n",
    "    input_decoder.append('<sos> '+ara)\n",
    "    targets_decoder.append(ara+' <eos>')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we tokenise sentences\n",
    "# first we tokenise english sentneces\n",
    "\n",
    "# no <sos> or <eos> tags in english\n",
    "# also in arabic sentences, no sos or eos words ( IMPORTANT : already checkd)\n",
    "# so we can filter the data, our tags will be sos and eos\n",
    "# we need to do this because we will use pretrained word embeddings \n",
    "# at both our encoder and decoder\n",
    "# if a word is Hi. , it wont receive the word vector for Hi\n",
    "# of course we can do some extra work, but still they will be different words\n",
    "# so better to remove special characters\n",
    "\n",
    "tokeniser_encoder = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokeniser_encoder.fit_on_texts(input_encoder)\n",
    "input_encoder = tokeniser_encoder.texts_to_sequences(input_encoder)\n",
    "word2idx_eng = tokeniser_encoder.word_index\n",
    "\n",
    "# again for arabic sentences\n",
    "# this is the decoder part, same code as above\n",
    "\n",
    "tokeniser_decoder = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokeniser_decoder.fit_on_texts(input_decoder+targets_decoder)\n",
    "input_decoder = tokeniser_decoder.texts_to_sequences(input_decoder)\n",
    "targets_decoder = tokeniser_decoder.texts_to_sequences(targets_decoder)\n",
    "word2idx_ara = tokeniser_decoder.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next is padding\n",
    "\n",
    "# pad english sentences\n",
    "max_seq_length_encoder = max(len(sentence) for sentence in input_encoder)\n",
    "max_seq_length_encoder = min(max_seq_length_encoder,MAX_SEQ_LENGTH)\n",
    "\n",
    "input_encoder = pad_sequences(input_encoder,maxlen=max_seq_length_encoder,padding='post')\n",
    "\n",
    "# pad arabic sentences\n",
    "max_seq_length_decoder = max(len(sentence) for sentence in input_decoder)\n",
    "max_seq_length_decoder = min(max_seq_length_decoder,MAX_SEQ_LENGTH)\n",
    "\n",
    "input_decoder = pad_sequences(input_decoder,maxlen=max_seq_length_decoder,padding='post')\n",
    "targets_decoder = pad_sequences(targets_decoder,maxlen=max_seq_length_decoder,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next is load in pretrained word embeddings for both english and arabic\n",
    "# and form our embeddings layer for encoder & decoder\n",
    "\n",
    "# first english/encoder embedding layer\n",
    "# we use GloVe\n",
    "\n",
    "# load in the pre trained word vectors\n",
    "file = 'datasets/glove/glove.6B.'+str(EMBEDDING_DIM_ENCODER)+'d.txt'\n",
    "word2vec_eng = {}\n",
    "for line in open(file,encoding='utf8'):\n",
    "    line = line.split()\n",
    "    word = line[0]\n",
    "    if word in word2idx_eng:\n",
    "        vec = np.asarray(line[1:],dtype='float32')\n",
    "        word2vec_eng[word] = vec\n",
    "\n",
    "V_eng = min(len(word2idx_eng)+1,MAX_VOCAB_SIZE)\n",
    "\n",
    "# now lets create our embedding matrix\n",
    "embedding_eng = np.zeros((V_eng,EMBEDDING_DIM_ENCODER))\n",
    "\n",
    "# now we fill the matrix with the pretrained word embeddings\n",
    "# if a word is not in our pretrained vectors , we leave it as zeros\n",
    "\n",
    "for word,idx in word2idx_eng.items():\n",
    "    vec = word2vec_eng.get(word,0)\n",
    "    if idx < V_eng:\n",
    "        embedding_eng[idx] = vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next arabic/decoder embedding layer\n",
    "# we use fast-text vectors from previous example\n",
    "# takes some time, 2M words\n",
    "\n",
    "# load in pretrained word vectors\n",
    "word2vec_ara = {}\n",
    "for line in open('datasets/fasttext ara/cc.ar.300.vec',encoding='utf8'):\n",
    "    line = line.split()\n",
    "    word = line[0]\n",
    "    if word in word2idx_ara:\n",
    "        vec = np.asarray(line[1:],dtype='float32')\n",
    "        word2vec_ara[word] = vec\n",
    "\n",
    "V_ara = min(len(word2idx_ara)+1,MAX_VOCAB_SIZE)\n",
    "# now lets create our embedding matrix\n",
    "embedding_ara = np.zeros((V_ara,EMBEDDING_DIM_DECODER))\n",
    "\n",
    "# now we fill the matrix with the pretrained word embeddings\n",
    "# if a word is not in our pretrained vectors , we leave it as zeros\n",
    "\n",
    "for word,idx in word2idx_ara.items():\n",
    "    vec = word2vec_ara.get(word,0)\n",
    "    if idx < V_ara:\n",
    "        embedding_ara[idx] = vec\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create encoder embedding layer\n",
    "embedding_encoder = Embedding(\n",
    "    V_eng,\n",
    "    EMBEDDING_DIM_ENCODER,\n",
    "    weights = [embedding_eng],\n",
    ")\n",
    "\n",
    "# create decoder embedding layers\n",
    "embedding_decoder = Embedding(\n",
    "    V_ara,\n",
    "    EMBEDDING_DIM_DECODER,\n",
    "    weights = [embedding_ara],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are ready to build our model\n",
    "\n",
    "# first the encoder\n",
    "\n",
    "input_encoder_tf = Input(shape=(max_seq_length_encoder)) # NxT\n",
    "x_encoder = embedding_encoder(input_encoder_tf) # NxTxD\n",
    "# we need only hT\n",
    "lstm_encoder = LSTM(LATENT_DIM_ENCODER,return_state = True,return_sequences=False)\n",
    "# we dont need the output of encoder\n",
    "_,hT_encoder,cT_encoder = lstm_encoder(x_encoder) # NxLATENT_DIM , LATENT_DIM , LATENT_DIM\n",
    "# this finished our encoder\n",
    "# now for the decoder, same as before , with encoder output being decoder inputs\n",
    "\n",
    "input_decoder_tf = Input(shape=(max_seq_length_decoder)) # NxT\n",
    "x_decoder = embedding_decoder(input_decoder_tf) # NxTxD\n",
    "# we will need hs and cs while testing/translating\n",
    "lstm_decoder = LSTM(LATENT_DIM_DECODER,return_sequences=True,return_state=True)\n",
    "x_decoder,_,_ = lstm_decoder(x_decoder,initial_state=[hT_encoder,cT_encoder])\n",
    "# then we have a dense layer\n",
    "dense = Dense(V_ara,activation='softmax')\n",
    "output_decoder = dense(x_decoder)\n",
    "model = Model([input_encoder_tf,input_decoder_tf],output_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "# VERY IMPORTANT\n",
    "# when the data was shuffled, I noticed a huge imporve in val metrics\n",
    "# apparently what tensorflow does,\n",
    "# is that that when model.fit splits the data into train and val, \n",
    "# it takes the validation data FROM THE LAST SAMPLES BEFORE SHUFFLING (check the documentation)\n",
    "# unfortunately, our data file has sentences sorted in order of length\n",
    "# so first sentences are short, later sentences are longer\n",
    "# so our validation dataset had longer sentences thatn our trian which caused the problem\n",
    "input_encoder,input_decoder,targets_decoder = shuffle(input_encoder,input_decoder,targets_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model and train it\n",
    "# the instructor chose not to filter out padding this time\n",
    "# when trying to take the one-hot approach, we got bad results\n",
    "# seems its up to the data to decide our approach\n",
    "model.compile(\n",
    "  optimizer='rmsprop',\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "146/146 [==============================] - 196s 1s/step - loss: 2.3164 - accuracy: 0.8289 - val_loss: 0.9675 - val_accuracy: 0.8864\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 183s 1s/step - loss: 0.9477 - accuracy: 0.8843 - val_loss: 0.9374 - val_accuracy: 0.8887\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 179s 1s/step - loss: 0.9060 - accuracy: 0.8856 - val_loss: 0.9099 - val_accuracy: 0.8912\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 178s 1s/step - loss: 0.8463 - accuracy: 0.8912 - val_loss: 0.8935 - val_accuracy: 0.8928\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 176s 1s/step - loss: 0.8227 - accuracy: 0.8921 - val_loss: 0.8743 - val_accuracy: 0.8944\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 173s 1s/step - loss: 0.7892 - accuracy: 0.8937 - val_loss: 0.8572 - val_accuracy: 0.8961\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 173s 1s/step - loss: 0.7533 - accuracy: 0.8967 - val_loss: 0.8403 - val_accuracy: 0.8984\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 200s 1s/step - loss: 0.7168 - accuracy: 0.8992 - val_loss: 0.8342 - val_accuracy: 0.8992\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 196s 1s/step - loss: 0.6857 - accuracy: 0.9020 - val_loss: 0.8235 - val_accuracy: 0.9002\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 203s 1s/step - loss: 0.6541 - accuracy: 0.9049 - val_loss: 0.8182 - val_accuracy: 0.9018\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 237s 2s/step - loss: 0.6283 - accuracy: 0.9077 - val_loss: 0.8100 - val_accuracy: 0.9025\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 236s 2s/step - loss: 0.5999 - accuracy: 0.9105 - val_loss: 0.8026 - val_accuracy: 0.9034\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 225s 2s/step - loss: 0.5713 - accuracy: 0.9135 - val_loss: 0.7983 - val_accuracy: 0.9044\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 176s 1s/step - loss: 0.5494 - accuracy: 0.9166 - val_loss: 0.7982 - val_accuracy: 0.9050\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.5271 - accuracy: 0.9189 - val_loss: 0.7930 - val_accuracy: 0.9055\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.5038 - accuracy: 0.9222 - val_loss: 0.7915 - val_accuracy: 0.9056\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.4872 - accuracy: 0.9244 - val_loss: 0.7923 - val_accuracy: 0.9058\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.4649 - accuracy: 0.9272 - val_loss: 0.7914 - val_accuracy: 0.9061\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.4419 - accuracy: 0.9309 - val_loss: 0.7905 - val_accuracy: 0.9059\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.4260 - accuracy: 0.9339 - val_loss: 0.7964 - val_accuracy: 0.9064\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.4076 - accuracy: 0.9364 - val_loss: 0.7936 - val_accuracy: 0.9065\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.3943 - accuracy: 0.9380 - val_loss: 0.7948 - val_accuracy: 0.9064\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.3773 - accuracy: 0.9413 - val_loss: 0.7989 - val_accuracy: 0.9059\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.3692 - accuracy: 0.9423 - val_loss: 0.8003 - val_accuracy: 0.9060\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.3493 - accuracy: 0.9462 - val_loss: 0.7995 - val_accuracy: 0.9065\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.3321 - accuracy: 0.9489 - val_loss: 0.8057 - val_accuracy: 0.9066\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.3168 - accuracy: 0.9509 - val_loss: 0.8063 - val_accuracy: 0.9068\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.3105 - accuracy: 0.9528 - val_loss: 0.8130 - val_accuracy: 0.9056\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.2980 - accuracy: 0.9545 - val_loss: 0.8163 - val_accuracy: 0.9061\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 177s 1s/step - loss: 0.2907 - accuracy: 0.9561 - val_loss: 0.8181 - val_accuracy: 0.9058\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.2818 - accuracy: 0.9573 - val_loss: 0.8193 - val_accuracy: 0.9057\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.2711 - accuracy: 0.9590 - val_loss: 0.8256 - val_accuracy: 0.9054\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.2636 - accuracy: 0.9599 - val_loss: 0.8268 - val_accuracy: 0.9053\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.2526 - accuracy: 0.9618 - val_loss: 0.8240 - val_accuracy: 0.9059\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.2465 - accuracy: 0.9627 - val_loss: 0.8255 - val_accuracy: 0.9057\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 173s 1s/step - loss: 0.2382 - accuracy: 0.9640 - val_loss: 0.8310 - val_accuracy: 0.9058\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.2319 - accuracy: 0.9648 - val_loss: 0.8310 - val_accuracy: 0.9054\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.2251 - accuracy: 0.9659 - val_loss: 0.8323 - val_accuracy: 0.9051\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.2202 - accuracy: 0.9666 - val_loss: 0.8364 - val_accuracy: 0.9048\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.2098 - accuracy: 0.9680 - val_loss: 0.8390 - val_accuracy: 0.9044\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.2069 - accuracy: 0.9686 - val_loss: 0.8362 - val_accuracy: 0.9052\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 173s 1s/step - loss: 0.2000 - accuracy: 0.9696 - val_loss: 0.8363 - val_accuracy: 0.9054\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 173s 1s/step - loss: 0.1970 - accuracy: 0.9696 - val_loss: 0.8383 - val_accuracy: 0.9047\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.1901 - accuracy: 0.9709 - val_loss: 0.8395 - val_accuracy: 0.9049\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 176s 1s/step - loss: 0.1858 - accuracy: 0.9713 - val_loss: 0.8412 - val_accuracy: 0.9045\n",
      "Epoch 46/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1813 - accuracy: 0.9722 - val_loss: 0.8409 - val_accuracy: 0.9051\n",
      "Epoch 47/100\n",
      "146/146 [==============================] - 173s 1s/step - loss: 0.1796 - accuracy: 0.9725 - val_loss: 0.8428 - val_accuracy: 0.9046\n",
      "Epoch 48/100\n",
      "146/146 [==============================] - 173s 1s/step - loss: 0.1730 - accuracy: 0.9731 - val_loss: 0.8452 - val_accuracy: 0.9045\n",
      "Epoch 49/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1680 - accuracy: 0.9739 - val_loss: 0.8423 - val_accuracy: 0.9047\n",
      "Epoch 50/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.1647 - accuracy: 0.9746 - val_loss: 0.8457 - val_accuracy: 0.9044\n",
      "Epoch 51/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1579 - accuracy: 0.9756 - val_loss: 0.8465 - val_accuracy: 0.9046\n",
      "Epoch 52/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1556 - accuracy: 0.9761 - val_loss: 0.8502 - val_accuracy: 0.9042\n",
      "Epoch 53/100\n",
      "146/146 [==============================] - 173s 1s/step - loss: 0.1535 - accuracy: 0.9761 - val_loss: 0.8502 - val_accuracy: 0.9042\n",
      "Epoch 54/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1499 - accuracy: 0.9769 - val_loss: 0.8505 - val_accuracy: 0.9037\n",
      "Epoch 55/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1478 - accuracy: 0.9775 - val_loss: 0.8515 - val_accuracy: 0.9040\n",
      "Epoch 56/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1428 - accuracy: 0.9781 - val_loss: 0.8537 - val_accuracy: 0.9039\n",
      "Epoch 57/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1415 - accuracy: 0.9784 - val_loss: 0.8530 - val_accuracy: 0.9042\n",
      "Epoch 58/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1391 - accuracy: 0.9791 - val_loss: 0.8531 - val_accuracy: 0.9044\n",
      "Epoch 59/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.1357 - accuracy: 0.9790 - val_loss: 0.8547 - val_accuracy: 0.9037\n",
      "Epoch 60/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.1332 - accuracy: 0.9798 - val_loss: 0.8559 - val_accuracy: 0.9039\n",
      "Epoch 61/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1303 - accuracy: 0.9802 - val_loss: 0.8570 - val_accuracy: 0.9036\n",
      "Epoch 62/100\n",
      "146/146 [==============================] - 173s 1s/step - loss: 0.1293 - accuracy: 0.9806 - val_loss: 0.8579 - val_accuracy: 0.9039\n",
      "Epoch 63/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1268 - accuracy: 0.9811 - val_loss: 0.8600 - val_accuracy: 0.9037\n",
      "Epoch 64/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1229 - accuracy: 0.9815 - val_loss: 0.8600 - val_accuracy: 0.9035\n",
      "Epoch 65/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1240 - accuracy: 0.9818 - val_loss: 0.8603 - val_accuracy: 0.9039\n",
      "Epoch 66/100\n",
      "146/146 [==============================] - 177s 1s/step - loss: 0.1209 - accuracy: 0.9821 - val_loss: 0.8625 - val_accuracy: 0.9036\n",
      "Epoch 67/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1182 - accuracy: 0.9826 - val_loss: 0.8626 - val_accuracy: 0.9032\n",
      "Epoch 68/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1166 - accuracy: 0.9830 - val_loss: 0.8626 - val_accuracy: 0.9034\n",
      "Epoch 69/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1130 - accuracy: 0.9836 - val_loss: 0.8644 - val_accuracy: 0.9038\n",
      "Epoch 70/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1110 - accuracy: 0.9841 - val_loss: 0.8657 - val_accuracy: 0.9038\n",
      "Epoch 71/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1097 - accuracy: 0.9841 - val_loss: 0.8665 - val_accuracy: 0.9035\n",
      "Epoch 72/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.1083 - accuracy: 0.9847 - val_loss: 0.8693 - val_accuracy: 0.9028\n",
      "Epoch 73/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1069 - accuracy: 0.9851 - val_loss: 0.8680 - val_accuracy: 0.9033\n",
      "Epoch 74/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.1056 - accuracy: 0.9851 - val_loss: 0.8716 - val_accuracy: 0.9032\n",
      "Epoch 75/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.1049 - accuracy: 0.9856 - val_loss: 0.8713 - val_accuracy: 0.9032\n",
      "Epoch 76/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.1026 - accuracy: 0.9859 - val_loss: 0.8705 - val_accuracy: 0.9033\n",
      "Epoch 77/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.0990 - accuracy: 0.9865 - val_loss: 0.8717 - val_accuracy: 0.9034\n",
      "Epoch 78/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.0998 - accuracy: 0.9864 - val_loss: 0.8728 - val_accuracy: 0.9029\n",
      "Epoch 79/100\n",
      "146/146 [==============================] - 174s 1s/step - loss: 0.0989 - accuracy: 0.9865 - val_loss: 0.8728 - val_accuracy: 0.9029\n",
      "Epoch 80/100\n",
      "146/146 [==============================] - 178s 1s/step - loss: 0.0959 - accuracy: 0.9874 - val_loss: 0.8748 - val_accuracy: 0.9033\n",
      "Epoch 81/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.0959 - accuracy: 0.9874 - val_loss: 0.8729 - val_accuracy: 0.9032\n",
      "Epoch 82/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.0944 - accuracy: 0.9875 - val_loss: 0.8751 - val_accuracy: 0.9033\n",
      "Epoch 83/100\n",
      "146/146 [==============================] - 200s 1s/step - loss: 0.0928 - accuracy: 0.9880 - val_loss: 0.8769 - val_accuracy: 0.9030\n",
      "Epoch 84/100\n",
      "146/146 [==============================] - 177s 1s/step - loss: 0.0917 - accuracy: 0.9880 - val_loss: 0.8749 - val_accuracy: 0.9034\n",
      "Epoch 85/100\n",
      "146/146 [==============================] - 182s 1s/step - loss: 0.0903 - accuracy: 0.9886 - val_loss: 0.8792 - val_accuracy: 0.9025\n",
      "Epoch 86/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.0876 - accuracy: 0.9890 - val_loss: 0.8776 - val_accuracy: 0.9032\n",
      "Epoch 87/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.0887 - accuracy: 0.9888 - val_loss: 0.8782 - val_accuracy: 0.9032\n",
      "Epoch 88/100\n",
      "146/146 [==============================] - 180s 1s/step - loss: 0.0867 - accuracy: 0.9893 - val_loss: 0.8796 - val_accuracy: 0.9031\n",
      "Epoch 89/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.0852 - accuracy: 0.9894 - val_loss: 0.8813 - val_accuracy: 0.9031\n",
      "Epoch 90/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.0832 - accuracy: 0.9900 - val_loss: 0.8807 - val_accuracy: 0.9027\n",
      "Epoch 91/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.0821 - accuracy: 0.9899 - val_loss: 0.8843 - val_accuracy: 0.9023\n",
      "Epoch 92/100\n",
      "146/146 [==============================] - 177s 1s/step - loss: 0.0822 - accuracy: 0.9900 - val_loss: 0.8854 - val_accuracy: 0.9024\n",
      "Epoch 93/100\n",
      "146/146 [==============================] - 177s 1s/step - loss: 0.0803 - accuracy: 0.9903 - val_loss: 0.8834 - val_accuracy: 0.9031\n",
      "Epoch 94/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.0788 - accuracy: 0.9908 - val_loss: 0.8859 - val_accuracy: 0.9026\n",
      "Epoch 95/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.0799 - accuracy: 0.9904 - val_loss: 0.8857 - val_accuracy: 0.9033\n",
      "Epoch 96/100\n",
      "146/146 [==============================] - 176s 1s/step - loss: 0.0777 - accuracy: 0.9909 - val_loss: 0.8868 - val_accuracy: 0.9025\n",
      "Epoch 97/100\n",
      "146/146 [==============================] - 176s 1s/step - loss: 0.0759 - accuracy: 0.9914 - val_loss: 0.8868 - val_accuracy: 0.9028\n",
      "Epoch 98/100\n",
      "146/146 [==============================] - 179s 1s/step - loss: 0.0745 - accuracy: 0.9914 - val_loss: 0.8896 - val_accuracy: 0.9026\n",
      "Epoch 99/100\n",
      "146/146 [==============================] - 175s 1s/step - loss: 0.0742 - accuracy: 0.9915 - val_loss: 0.8881 - val_accuracy: 0.9025\n",
      "Epoch 100/100\n",
      "146/146 [==============================] - 176s 1s/step - loss: 0.0732 - accuracy: 0.9914 - val_loss: 0.8886 - val_accuracy: 0.9031\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(\n",
    "    [input_encoder,input_decoder],targets_decoder,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split = VALIDATION_SPLIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXxU9b3/8ddn9kx2sgJhF9kFJQJuqLVXRavUSiviQq2Wqq1aW63a/tra1nvbW3q73WKttW51RbSWVivXWhWtGwHZNzEsCVv2fU++vz++EwiQlUw4mcnn+XjMg8ycM+d8vhDe853v+Z5zxBiDUkqpyOdyugCllFLhoYGulFJRQgNdKaWihAa6UkpFCQ10pZSKEh6ndpyammpGjhzp1O6VUioirV69usgYk9beMscCfeTIkeTk5Di1e6WUikgisrujZV0OuYjIoyJSICIbu1jvdBFpFpF5x1OkUkqp3unOGPrjwMWdrSAibuC/gRVhqEkppdRx6DLQjTErgZIuVrsNeBEoCEdRSimleq7XY+giMhS4AvgMcHoX6y4CFgEMHz68t7tWSkWgxsZG8vPzqaurc7qUfi0QCJCVlYXX6+32e8JxUPTXwD3GmGYR6XRFY8zDwMMA2dnZehEZpQag/Px84uPjGTlyJF1lxkBljKG4uJj8/HxGjRrV7feFI9CzgedC/zCpwCUi0mSMeTkM21ZKRZm6ujoN8y6ICCkpKRQWFvbofb0OdGPMoY8PEXkc+LuGuVKqMxrmXTuev6PuTFt8FngfGCci+SJyo4jcLCI3H0eNvbbtQCW/WLGN4qp6J3avlFL9Vpc9dGPM1d3dmDHmy72qphtyC6v43Zs7uPSUwaTE+ft6d0qpKBQXF0dVVZXTZYRdxF3LJei3n0E1DU0OV6KUUv1LxAV6nN8NQFV9s8OVKKUinTGGu+++m8mTJzNlyhSef/55APbv38/s2bOZNm0akydP5p133qG5uZkvf/nLh9b91a9+5XD1x3LsWi7HK+gL9dDrtYeuVKT70d82sXlfRVi3OXFIAj+8bFK31n3ppZdYu3Yt69ato6ioiNNPP53Zs2fzzDPPcNFFF/G9732P5uZmampqWLt2LXv37mXjRnsVlLKysrDWHQ4R10OPDQV6dYP20JVSvfPuu+9y9dVX43a7ycjI4Nxzz2XVqlWcfvrpPPbYY9x///1s2LCB+Ph4Ro8eTW5uLrfddhuvvfYaCQkJTpd/jMjroYeGXHQMXanI192edF8xpv3zG2fPns3KlSt55ZVXuO6667j77ru5/vrrWbduHStWrGDJkiUsXbqURx999ARX3LnI7aHrGLpSqpdmz57N888/T3NzM4WFhaxcuZIZM2awe/du0tPT+epXv8qNN97ImjVrKCoqoqWlhSuvvJKf/OQnrFmzxunyjxFxPfSA14WI9tCVUr13xRVX8P777zN16lREhJ///OdkZmbyxBNPsHjxYrxeL3FxcTz55JPs3buXG264gZaWFgB++tOfOlz9sSIu0EWEWJ9He+hKqePWOgddRFi8eDGLFy8+YvnChQtZuHDhMe/rj73ytiJuyAUg1u+mWme5KKXUESIz0H0eqnXIRSmljhCRgR70u6nRaYtKKXWEyAx0n0eHXJRS6igRGeixPu2hK6XU0SIy0IN+HUNXSqmjRWSgx+mQi1JKHSMiAz3od1Oj89CVUidAXFxch8t27drF5MmTT2A1nYvIQG+dttjRdRiUUmogirgzRcH20FsM1De1EPC6nS5HKXW8/nEvHNgQ3m1mToE5P+tw8T333MOIESO49dZbAbj//vsREVauXElpaSmNjY088MADzJ07t0e7raur45ZbbiEnJwePx8Mvf/lLzj//fDZt2sQNN9xAQ0MDLS0tvPjiiwwZMoQvfelL5Ofn09zczPe//32uuuqqXjUbIjTQD1+gq0kDXSnVI/Pnz+eb3/zmoUBfunQpr732GnfeeScJCQkUFRUxa9YsLr/88h7dqHnJkiUAbNiwga1bt3LhhReyfft2HnroIe644w6uueYaGhoaaG5u5tVXX2XIkCG88sorAJSXl4elbREZ6EFf6yV0m0lxuBalVC900pPuK6eeeioFBQXs27ePwsJCkpOTGTx4MHfeeScrV67E5XKxd+9eDh48SGZmZre3++6773LbbbcBMH78eEaMGMH27ds544wz+M///E/y8/P5whe+wNixY5kyZQp33XUX99xzD5/73Oc455xzwtK2yBxD97fe5EJnuiilem7evHksW7aM559/nvnz5/P0009TWFjI6tWrWbt2LRkZGdTV1fVomx0d01uwYAHLly8nJiaGiy66iH/961+cfPLJrF69milTpnDffffx4x//OBzN6jrQReRRESkQkY0dLL9GRNaHHu+JyNSwVNaJQ4GuUxeVUsdh/vz5PPfccyxbtox58+ZRXl5Oeno6Xq+XN998k927d/d4m7Nnz+bpp58GYPv27ezZs4dx48aRm5vL6NGjuf3227n88stZv349+/btIxgMcu2113LXXXeF7SqO3RlyeRz4HfBkB8t3AucaY0pFZA7wMDAzLNV1IDY05KKX0FVKHY9JkyZRWVnJ0KFDGTx4MNdccw2XXXYZ2dnZTJs2jfHjx/d4m7feeis333wzU6ZMwePx8Pjjj+P3+3n++ed56qmn8Hq9ZGZm8oMf/IBVq1Zx991343K58Hq9/P73vw9Lu6Q7U/9EZCTwd2NMpxMuRSQZ2GiMGdrVNrOzs01OTk43yzzS5n0VXPLbd3jo2tO4ePLg49qGUsoZW7ZsYcKECU6XERHa+7sSkdXGmOz21g/3GPqNwD/CvM1jxPq1h66UUkcL2ywXETkfG+hnd7LOImARwPDhw497X8HQtEW9DZ1S6kTYsGED11133RGv+f1+PvzwQ4cqal9YAl1ETgEeAeYYY4o7Ws8Y8zB2jJ3s7OzjPs3zUA9dr7ioVEQyxvRojrfTpkyZwtq1a0/oPo/nTPheD7mIyHDgJeA6Y8z23m6vO2K8bnujaJ3lolTECQQCFBcX66U7OmGMobi4mEAg0KP3ddlDF5FngfOAVBHJB34IeEM7fQj4AZACPBj6xG3qaMA+XFpvFF2lY+hKRZysrCzy8/MpLCx0upR+LRAIkJWV1aP3dBnoxpiru1h+E3BTj/YaBkGfW8fQlYpAXq+XUaNGOV1GVIrIM0XBnlykY+hKKXVYxAZ60OfWMXSllGojYgO99ZroSimlrMgNdL/eKFoppdqK2EAP+j1U6ZCLUkodErGBHuvT+4oqpVRbERvoQR1DV0qpI0RsoLeOoevZZkopZUVsoAd9HppbDPVNLU6XopRS/ULEBnpsm/uKKqWUiuRA19vQKaXUESI/0PXAqFJKAREc6EG9r6hSSh0hYgO9tYeuV1xUSikrYgNde+hKKXWkiA30WL2vqFJKHSFyA/3QQVHtoSulFER0oIfmoeu0RaWUAiI40AMee6NonYeulFJWxAa6yyUEvW4dclFKqZCIDXSw10TXg6JKKWVFdKDH+tw6bVEppUK6DHQReVRECkRkYwfLRUR+KyI7RGS9iJwW/jLbF6s9dKWUOqQ7PfTHgYs7WT4HGBt6LAJ+3/uyuifW59EeulJKhXQZ6MaYlUBJJ6vMBZ401gdAkogMDleBnQn63XpxLqWUCgnHGPpQIK/N8/zQa8cQkUUikiMiOYWFhb3ese2ha6ArpRSEJ9ClndfavS+cMeZhY0y2MSY7LS2t1zsO+tx6gwullAoJR6DnA8PaPM8C9oVhu12K9WsPXSmlWoUj0JcD14dmu8wCyo0x+8Ow3S619tD1RtFKKQWerlYQkWeB84BUEckHfgh4AYwxDwGvApcAO4Aa4Ia+KhaA5kbY/FeYfCWxfg9NLYaG5hb8Hnef7lYppfq7LgPdGHN1F8sN8PWwVdSVtc/A324HEWJ9dsp7TX2zBrpSasCLvDNFp10DQ6fDK3eRTDkAVTqOrpRSERjobg/MfRAaqpix+b8AdKaLUkoRiYEOkD4ezruXwXtXcInrAyrrGp2uSCmlHBeZgQ5w5h00pJ/Cj72P8/6GrU5Xo5RSjovcQHd78F35BxJdtUxb/T3q9BIASqkBLnIDHSBjInuyv8s5rGHbXxc7XY1SSjkqsgMdGD3nm7znPp2Jm34B+9c5XY5SSjkm4gNdXC72nLOYYhNP3XNfhroKp0tSSilHRHygA1w6azL3mW/gLd8Nj14EpbucLkkppU64qAj0+ICXrNMu5sbme2gp3wt//AzsetfpspRS6oSKikAHWHjmCN5umswj4x+BmEHw5FxYv9TpspRSA01zE5TshNy3Yc+HULQDakrso/IglOXZn/tAl9dyiRQnpcdz+dQh/HLNAa647RXSXr0RXloEjbUwfaHT5Sml+pumeijcBgc3QdkeqK+AhiowLRCXCfGZ4I2xAVy2G6oKQAQkdN2ohqrQe6qh9YqvLU1Qsdf+2Zmz74TP3h/2JkVNoAN86z9O5pX1+/nNewU8cM0L8Py19kJeTXUw82tOl6eUOpoxNiSPVl9pA7TqINSWQkwyxGXYPwu3wt7VcGADuLwQHGQfYEO6qc6+v7YM6sqgphiqi6C6EJobwBMAj9+u0zZ4vUHwx9ufqwttsLeKH2z3D2BClxrxxdvg98WChAY7RCAxCwaNgeQR0NRg919bAgi4veD2QebksP41toqqQB+REsv8GcN47qM8vnrOaEbMfwaWfQX+8R37S3HuPe3/8iiljtQc6mm6PBBItKFVud/2Zg9ugsYaG6Yutw1Ib4wNxJYm22utK7czzuorbHA21R/edlMtVB6w26uvgrh0G5aBBDskUbEPGiq7rjEhdKfL6iJobrN9lwf8CRCTBIEkCKZC2gSITQG3367bVG/DO2OyfQwaZcO2bfurC207E4aCNxCev9c+Jk7dHCI7O9vk5OSEfbsFFXXMXvwmF03K5DfzT7XXT19+O6x7Bk6ZD5f/1n46KxWNGmpsb9C0HB4GaNUSCqmqAvtnXVkoeMvt+xprbPiW59khiLa9V3Ed2WPtDl+cDU1/fOj/XKgz5fHb4Yz4wfaDorrQBnxdBcRn2ABNGGJ7v3FptldeU2J76zXFkDLWXnE1LnQbS2Ps0KqIDWxX1BwabJeIrDbGZLe3LKp66ADpCQFuOGsUD739KYtmj2bSkET4/IMwaDS8+YD9Zb3qqcNf0ZRyWnPjkb3Z1nHZhir7Z03x4R5tQ/Xh97U029BtabI93Yp8+020J9z+wz1wbxB8QRg8FSZdAckjbYi3hn5cJmRMgoyJtufb0mRrb26wgdpYY4M/Jsn2kF0n6B4FIrZuFX09dIDymkbO+8WbjM2I5/lFs5DWYZYNy+DlWyBxGFzzAqSM6ZP9qyhTVQDl+aEer7Eh1xqkzQ02ZOtDB8gq99shg+pCOyTh8dvhCDj8vrpyqC62QV1XDo3Vne4esAEZn2l7va2/z+I6POzhiz3cs41Ns8tEOOIe7i6PHXaITbOPmOTDtamIMaB66ACJQS93XzSe7/5lA8vX7WPutNBY25R59oDFcwvsXPWrnoJR5zhbrDox6qvsmHBZHpTvseO+KSfZR3MjFGyGgi02kBtr7aNyHxzYCNUF3d+P2xc6gJZuw7up/vBwgLjsI5Bkfw8HTz3cmw0k2J6yP94+98fZ8PYGbfD64/ru70ZFjajsoQM0txg+v+TfFFTW8ca3zyPO3+azq2QnPHMVlOTCrFtgwuV2TC7Kx976rZYWO1xQuss+akshabgdJkscZsPQGBu81QV2+KHqoO3d1lfa4YrqQrusutj2hF1uG5515baH3Z1eMIQO8AXtIzgIMqfYYYbkUaEhhFAwu9z2IJrLa3vH/rjQgbhkPfCu+lRnPfSoDXSANXtK+cKD7/G1c0dz35wJRy6sLYPlt8HWV+w0pNh0mDrfzg/V8fXwKdlphytaNdUfPhhXthv2fQz71trhiuPli4Ngiu0VB1Nt2LaOLwcS7QyKuDRIyIKkYfZDorEWinfYh8sN6RPto/VAm1L91IANdIDvLFvHXz7eyz/umM1J6e18ba0thR1vwJblsOVvNhzOusP23H2xfV5fRKkugh3/tGPErQfwAkm2Jz1olD3horbUjg3vW2P/Xkt3drw9l9f2foeeBpmn2O0kj7TDEGV77Deoin2hlcUGb1x6aPZD+ok/+KZUPzCgA72oqp7zf/EW04Yl8eRXZhw+QNqegi3wxo9h26v2q/ewmTD6PDj5YntkP9q1NNtAPjStLTS7oboAPvkn5H1weOqay2uHGeoqDp9o0ZY31h6fGPMZSBt/+MQLty80PzjRXqLB4ztx7VMqCvQ60EXkYuA3gBt4xBjzs6OWJwJPAcOxB1p/YYx5rLNtnqhAB3j03Z38+O+befi66Vw4KbPrN+Stgk1/gdy3oGCTfW3YTDj9Jpg4NzrmsRtjz7b75P/sB1nhNij5tONTljMmw/hL7SP1ZPuBJ2LHtcv2HO6JxyTboE4YqmGtVB/oVaCLiBvYDvwHkA+sAq42xmxus853gURjzD0ikgZsAzKNMQ0dbfdEBnpjcwuX/vYdahubef3Ocwl4e/AVvfIgbHgBch61gReTDJPnwbQFMOTUyDgAVvypDe/GWntadNke2PxXOydfXHaoI228nfGRMARiUw9Pawsk2mGVQILTrVBK0ftpizOAHcaY3NDGngPmApvbrGOAeLHjGXFACdBvbvLpdbu4/7JJLHjkQx5emcvtF4zt/pvjM+DMb8CsW2Hn2/Dxn2HNk7DqjzYIh58BWdmQPgncob9OTwykjev7sd3GutCFhXbZoZLaUttjbj0Nu3I/bH0VirYd+T6X1w6FnP89GDfHDoEopSJedwJ9KJDX5nk+MPOodX4HLAf2AfHAVcYce56wiCwCFgEMHz78eOo9bmeelMolUzJ58K0dXDk9i6FJPTyhwuWCMefbR20ZbHoJtq+wj7VPH7t+IAlGnm0f8YPtLIxAog3cxhrbU/b47YFXX7w9080btGPM+avsdnf8M9SDHmU/PNzew2cUFu+wYd7ZVd3EDSPPguyvwOhz7QFETyB0VmBkXJtCKdV93Rly+SJwkTHmptDz64AZxpjb2qwzDzgL+BYwBngdmGqM6XAu2okccmm1t6yWC/7nLS4Yn8GSa04Lz0aNsXOniz7BflHB9pR3vQO5K+1JLMfD7YdRs23wluy0Mz6aGw/P7EjMsrNDhpwGqWPtuHVMkv1AaD0xxuM7fPU4pVRU6O2QSz4wrM3zLGxPvK0bgJ8Z++mwQ0R2AuOBj46j3j4zNCmGW849iV/9czvX7CjizJNSe79RkVAPetSRr0+db8O+dcZIbYnt2Xv8tifuCdheettrdjSGLpCUPtGGedtpk60fvN0Zs/cF9doWSg1A3Qn0VcBYERkF7AXmAwuOWmcPcAHwjohkAOOA3HAWGi5fO3c0L6zO4/6/beLV28/B4+7Ds0NF7Bh8fEZ4tqWUUp3oMs2MMU3AN4AVwBZgqTFmk4jcLCI3h1b7CXCmiGwA3gDuMcYU9VXRvRHwuvn+5yay/WAVf/5gt9PlKKVU2HTr4lzGmFeBV4967aE2P+8DLgxvaX3nwokZnDM2lV++vp3Lpg4hNS4K5pUrpQa8AXk1KhHhh5dNorahmcWvbev6DUopFQEGZKADnJQexw1njWTp6jzW5ZU5XY5SSvXagA10gNsvGEtKrJ8fLt9ES4sz17RRSqlwGdCBHh/wcu+c8azNK+PFNfldv0EppfqxAR3oAF84dSinDk/iv1/bRkVdo9PlKKXUcRvwge5yCT+6fBLF1fX8+vVPnC5HKaWO24APdIBTspJYMGM4j7+3kzV7enjXdKWU6ic00EPunTOezIQA31m2nvqmdm7YoJRS/ZwGekh8wMt/fWEKOwqq+N83djhdjlJK9ZgGehvnjUtn3vQsfv/2p2zcW+50OUop1SMa6Ef5/qUTSYn1cdcL62hoOuaS7kop1W9poB8lMejlp1+YwtYDlfzmje1Ol6OUUt2mgd6OCyZk8KXsLH7/1qd8rLNelFIRQgO9A9//3EQGJ8bw7aXrqG3QWS9Kqf5PA70D8QEvP593CrlF1fx8xVany1FKqS5poHfirJNS+fKZI3ns37tYub3Q6XKUUqpTGuhduHfOeE7OiOPbL6yjuKre6XKUUqpDGuhdCHjd/Gb+qZTXNnLPi+sxRi+zq5TqnzTQu2HC4ATuvXg8/9xSwNMf7nG6HKWUapcGejd9+cyRnHtyGj/5+2a2H6x0uhyllDqGBno3uVzC4i+eQnzAw23PfExdo05lVEr1LxroPZAeH+CXX5rGtoOVPPDKZqfLUUqpI3Qr0EXkYhHZJiI7ROTeDtY5T0TWisgmEXk7vGX2H7NPTuNrs0fz1Ad7eG3jfqfLUUqpQ7oMdBFxA0uAOcBE4GoRmXjUOknAg8DlxphJwBf7oNZ+49sXjmNqViLfWbae3cXVTpejlFJA93roM4AdxphcY0wD8Bww96h1FgAvGWP2ABhjCsJbZv/i87j43YLTcLmERU+uprq+yemSlFKqW4E+FMhr8zw/9FpbJwPJIvKWiKwWkevb25CILBKRHBHJKSyM7DMvhw0K8r9Xn8onBZV8Z5nOT1dKOa87gS7tvHZ0enmA6cClwEXA90Xk5GPeZMzDxphsY0x2Wlpaj4vtb84Zm8Y9F4/nlQ37eejtXKfLUUoNcJ5urJMPDGvzPAvY1846RcaYaqBaRFYCU4Gov6D4otmj2bivgp+v2Mq4zDg+Mz7D6ZKUUgNUd3roq4CxIjJKRHzAfGD5Uev8FThHRDwiEgRmAlvCW2r/JCL8/MpTmDQkgdufXasnHSmlHNNloBtjmoBvACuwIb3UGLNJRG4WkZtD62wBXgPWAx8BjxhjNvZd2f1LjM/NH6/PJuhzc+MTq/QiXkopR4hTB/Oys7NNTk6OI/vuK2vzyrjqD+8zNSuJP980A7/H7XRJSqkoIyKrjTHZ7S3TM0XDaNqwJBZ/cSof7SrhW0vX0dKiM1+UUidOdw6Kqh64fOoQDpTX8l+vbiU11sf9l09CpL2JQkopFV4a6H1g0ewxFFTU88i7O0lPCPD1809yuiSl1ACggd5HvnvJBIqq6lm8YhvJQR8LZg53uiSlVJTTQO8jLpfw83lTKa9t5HsvbyAu4OHyqUOcLkspFcX0oGgf8nlcPHjNdE4fOYhvPb+Wf2096HRJSqkopoHex2J8bv60MJsJgxO45ak1fJhb7HRJSqkopYF+AsQHvDzxlRkMGxTkpidy2LyvwumSlFJRSAP9BBkU6+PJr8wgLuDh+kc/0uuoK6XCTgP9BBqSFMOfb5xBc0sL1/3pIwoq6pwuSSkVRTTQT7CT0uN57IYZFFXVc80jH+p1X5RSYaOB7oBpw5J4ZGE2e0pquO5PH1Fe0+h0SUqpKKCB7pAzx6Tyh+ums6OgioWPfURlnYa6Uqp3NNAddN64dH634FQ27i1n4aMfUaGhrpTqBQ10h104KZPfLTiNDXvLufaRDymraXC6JKVUhNJA7wcunpzJQ9dOZ+v+Shb8UQ+UKqWOjwZ6P3HBhAweWZjNp4VVzHvofZ2nrpTqMQ30fmT2yWk889WZlNU0cMWD77FmT6nTJSmlIogGej8zfcQgXrr1LOIDHq5++ANe27jf6ZKUUhFCA70fGpUay0u3nMnEIQnc8vQaHnknF6fu/aqUihwa6P1USpyfZ786i4snZfLAK1u4f/kmmvUepUqpTmig92MBr5slC05j0ezRPPH+bm56YpXOVVdKdahbgS4iF4vINhHZISL3drLe6SLSLCLzwlfiwOZyCd+9ZAIPfH4y73xSxBVL/k1uYZXTZSml+qEuA11E3MASYA4wEbhaRCZ2sN5/AyvCXaSCa2eN4KmbZlJa08jcJf/mza0FTpeklOpnutNDnwHsMMbkGmMagOeAue2sdxvwIqBJ00dmjU7hr18/i6zkIDc8voqf/H0z9U3NTpellOonuhPoQ4G8Ns/zQ68dIiJDgSuAhzrbkIgsEpEcEckpLCzsaa0KGDYoyF9uPZOFZ4zgT+/u5PNL3mNHQaXTZSml+oHuBLq089rR0y1+DdxjjOm0u2iMedgYk22MyU5LS+tujeooAa+bH82dzJ8WZnOwoo7L/vffvJCT1/UblVJRrTuBng8Ma/M8C9h31DrZwHMisguYBzwoIp8PS4WqQxdMyOC1O85h2rAk7l62nm89v5bq+iany1JKOaQ7gb4KGCsio0TEB8wHlrddwRgzyhgz0hgzElgG3GqMeTns1apjpCcEeOqmmdz52ZN5ee1eLvvdu3oTaqUGqC4D3RjTBHwDO3tlC7DUGLNJRG4WkZv7ukDVNbdLuOOzY3n6pllU1TXx+Qf/zZPv79KzS5UaYMSp//TZ2dkmJyfHkX1Hs+Kqer79wjre2lbIhRMz+OkXppAS53e6LKVUmIjIamNMdnvL9EzRKJMS5+fRhafz/y6dwFvbCrno1yt5ffNBp8tSSp0AGuhRyOUSbjpnNMtvO4u0+ABffTKHu15YR0m13g1JqWimgR7Fxmcm8Nevn8XXzx/DXz7ey2f+5y2e+mC3XuRLqSilgR7lfB4Xd180nldvP4dxGfH8v5c3MnfJu6zaVeJ0aUqpMNNAHyDGZcbz3KJZ/PbqUymqbOCLD73Pbc9+zN6yWqdLU0qFicfpAtSJIyJcPnUIn52QzkNv5/KHtz/l/zYd4JqZI/jauaPJSAg4XaJSqhd02uIAll9aw69e/4SX1+7F7RKuyh7GotmjGTYo6HRpSqkOdDZtUQNdsae4hgff2sGLa/JpMXDplMEsmj2ayUMTnS5NKXUUDXTVLfvLa3n03Z08+1EeVfVNzD45ja+fN4YZowYh0t412pRSJ5oGuuqR8tpGnvpgN4++u5Pi6gayRyTztXPHcMH4dFwuDXalnKSBro5LbUMzS3PyeHhlLnvLahmZEuQrZ49i3vQsgj49nq6UEzTQVa80Nbfw2qYD/PGdnazLKyMxxsuCmcNZeMZIMhN1ZoxSJ5IGugoLYwyrd5fyp3d3smLTAVwiXDJlMF85exTThiU5XZ5SA0Jnga7fm1W3iQjZIz4HSlAAAAyeSURBVAeRPXIQeSU1PPbvXSzNyWP5un2cOjyJ688YwZzJgwl43U6XqtSApD101SuVdY0sW53PE+/tYldxDYkxXq44dShXzxjOuMx4p8tTKurokIvqcy0thg9yi3l2VR4rNh6gobmF6SOSuXrGcD53ivbalQoXDXR1QpVUN/DSmnye+XAPuUXVxAc8zJ02hKuyhzN5aILOaVeqFzTQlSOMMXyQW8LSnDxe3bCf+qYWxmfGc+VpWcw9dQjp8TpDRqme0kBXjiuvbWT5un0sW53Purwy3C7h7JNSuWhSJp+dmK7hrlQ3aaCrfmVHQSUvrtnLK+v3s6ekBhGYmpXE7LGpnHNyGtOGJeF165WdlWqPBrrql4wxbDtYyeubDvLG1gLW55fRYiDO7+Gsk1I4b1w6556cxpCkGKdLVarf0EBXEaG8ppH3c4t4e3sRb28rYF95HQCjUmOZNTqFM8akcMboFNLi/Q5XqpRzeh3oInIx8BvADTxijPnZUcuvAe4JPa0CbjHGrOtsmxroqjPGGD4pqGLl9kLe/7SYj3aWUFnfBMC4jHjOGJPCrNGDmDEqhUGxPoerVerE6VWgi4gb2A78B5APrAKuNsZsbrPOmcAWY0ypiMwB7jfGzOxsuxroqieamlvYuK+C9z8t5r1Pi1i1q4S6xhYAxqbHMXP0IGaOSmHm6EF6gFVFtd4G+hnYgL4o9Pw+AGPMTztYPxnYaIwZ2tl2NdBVbzQ0tbBhbxkf5Jbw4c4SVu8qobqhGbBDNKePTGbGqBSyRyQzIiWoc99V1OjttVyGAnltnucDnfW+bwT+0UEhi4BFAMOHD+/GrpVqn8/jYvqIQUwfMYivn2978Jv2VfDhzmI+2lnKik0HWZqTD0BKrI/TRiRz2vBkThuexClZScT49MxVFX26E+jtdW3a7daLyPnYQD+7veXGmIeBh8H20LtZo1Jd8rhdTB2WxNRhSSyabS9F8ElBFat3l4YeJby++aBd1yVMyUrkzDEpnDUmldNGJOulCVRU6E6g5wPD2jzPAvYdvZKInAI8AswxxhSHpzyljo/LJYzLjGdcZjwLZtpvg8VV9Xy8p4zVe0r5MLeYh97OZcmbn+JzuzglK5HTRw1i+vBkThmWqOPwKiJ1Zwzdgz0oegGwF3tQdIExZlObdYYD/wKuN8a8150d6xi6clplXSMf7Syxj10lbMgvp6nF/n8YnBjglKxEJg1JZOLgBCYNTSAzIaBj8cpxvRpDN8Y0icg3gBXYaYuPGmM2icjNoeUPAT8AUoAHQ7/wTR3tUKn+Ij7g5YIJGVwwIQOAmoYmNu6tYH1+Gevzy9m4t5z/23yQ1j5PQsDDuMx4xmbEMyYtjtGpsYxJi2Nocgxuvdeq6gf0xCKlOlFV38TW/RVs3l/BtgOVbD9YyfaDVZTXNh5aJ+B1MSYtjpPS4xiREsvIlCAjUmI5KS2OxKDXwepVNNI7Fil1nOL8nkN3aWpljKGkuoGdRdV8WljFJwer+KSgipxdpSxft4+2faTUOD9j0mIZnRbHmLRYRqXGMiIlSFZyUA/EqrDTQFeqh0SElDg/KXH+I4IeoL6pmbySWnYVVZNbVMWOgio+LazmtY37Ka1pPGLdtHg/IwYFD/XqhyTFkJ7gJz0+wNDkGOL8+t9T9Yz+xigVRn6Pm5PS7fALZByxrLS6gdyiKvJKaskrqSGvtIbdxTX8e0cRL66pO2ZbqXH+w0Ef7yct3k9mYoCs5CDDkmNIjfPj0rF71YYGulInSHKsj+mxg5g+4thltQ3NHKioo6CijoLKevJLbS9/Z3E1a/PKKKisO3Spg1Y+j4us5BiGDwoyNCmGjIQAGQl+0hMCDE4MMDghhoQYj87MGUA00JXqB2J8bkal2jH29hhjqKpv4kB5HfmlteSX1pBXanv6e0pqWJdXdsyQDtgDtimxflLjfKTE+Q/19O2fgTY/+3VMPwpooCsVAUSE+ICX+ICXsRnx7a5T39RMYWU9ByvqOFBez/7yWg5W1FFc1UBRdQMHyuvYsLec4qp6WtqZ3JYY4yU93n9oHD8t3k9anJ+UOB+pcX4GxfpIjPGSFPQS59eef3+kga5UlPB73GQl2xk0nWluMRRX1VNYVU9BZT2FFfUUVNqhnoOhIZ+PdpZQWFlPQ3NLu9vwe1xkJgbISLDBnxprvwEkBDzE+NzE+DzE+z2kxPkYFOsjJdav1885ATTQlRpg3C4hPSFAekKASZ2sZ4yhsr6Josp6iqsbKK1uoKy2kbKaBgor6zlQUc+B8lo276uguKqeirqmTvfr97hIDvpIjvWRGgr65KCPhBgvCQEPCQEviUGvXSfoJSnoIyno1dsR9oAGulKqXSJCQsBLQsDL6LSu129oaqGqvonaxmZqG5opr22kpLqBkmr7gVBWYz8MSqobKK5uYE9JDSVVDYduXNKReL+HhBgviaFHXMBDrM9NrN9zaAgoKeg7tDwh4CUhxmOHqPyeATUTSANdKRUWPo+LQZ6e3z2qpcVQ1dBEeU0j5bWNlNY0UBoK/9Jq+7yi1i4rr20kv7SW6vomquubKK9tPHT9nfaI2JPDbMh7SYzxkBTjIznWBn98wEOc34Z/64dDQoyXWL/90Ijze/BE0DcEDXSllKNcrsPfBIZ1vfoRWoeFyqobqahrPBT8lXVN9nldE5V19rWK2kYqapvILaqidI99Xt/U/jGCtuL9HhKDNvCDPjdBn4dg6BtCnN9DrN8dOmBtPxiCXjdBv13PfljYR4zX3ecHkjXQlVIRq+2w0PFoaGqhMhT8rd8AKmobqWlooqq+mcq6RspqDn87qGlooqymgb1lzVTXN1EV+qbQyZeEQzwuOXS84NpZI7jpnNHHVXOn+wj7FpVSKkL4PK5Dl3E4XsYYahubqQx9G6hpaA49mqiss6Hfuqyi1n5wpMUf//46o4GulFK9ICKhYRgPGQnO3hglckb7lVJKdUoDXSmlooQGulJKRQkNdKWUihIa6EopFSU00JVSKkpooCulVJTQQFdKqSghxnTjnNW+2LFIIbD7ON+eChSFsZxIMRDbPRDbDAOz3QOxzdDzdo8wxrR7/UvHAr03RCTHGJPtdB0n2kBs90BsMwzMdg/ENkN4261DLkopFSU00JVSKkpEaqA/7HQBDhmI7R6IbYaB2e6B2GYIY7sjcgxdKaXUsSK1h66UUuooGuhKKRUlIi7QReRiEdkmIjtE5F6n6+kLIjJMRN4UkS0isklE7gi9PkhEXheRT0J/Jjtda7iJiFtEPhaRv4eeD4Q2J4nIMhHZGvo3P2OAtPvO0O/3RhF5VkQC0dZuEXlURApEZGOb1zpso4jcF8q2bSJyUU/3F1GBLiJuYAkwB5gIXC0iE52tqk80Ad82xkwAZgFfD7XzXuANY8xY4I3Q82hzB7ClzfOB0ObfAK8ZY8YDU7Htj+p2i8hQ4HYg2xgzGXAD84m+dj8OXHzUa+22MfR/fD4wKfSeB0OZ120RFejADGCHMSbXGNMAPAfMdbimsDPG7DfGrAn9XIn9Dz4U29YnQqs9AXzemQr7hohkAZcCj7R5OdrbnADMBv4EYIxpMMaUEeXtDvEAMSLiAYLAPqKs3caYlUDJUS931Ma5wHPGmHpjzE5gBzbzui3SAn0okNfmeX7otaglIiOBU4EPgQxjzH6woQ+kO1dZn/g18B2gpc1r0d7m0UAh8FhoqOkREYklytttjNkL/ALYA+wHyo0x/0eUtzukozb2Ot8iLdClndeidt6liMQBLwLfNMZUOF1PXxKRzwEFxpjVTtdygnmA04DfG2NOBaqJ/GGGLoXGjecCo4AhQKyIXOtsVY7rdb5FWqDnA8PaPM/Cfk2LOiLixYb508aYl0IvHxSRwaHlg4ECp+rrA2cBl4vILuxQ2mdE5Cmiu81gf6fzjTEfhp4vwwZ8tLf7s8BOY0yhMaYReAk4k+hvN3Tcxl7nW6QF+ipgrIiMEhEf9gDCcodrCjsREeyY6hZjzC/bLFoOLAz9vBD464mura8YY+4zxmQZY0Zi/13/ZYy5lihuM4Ax5gCQJyLjQi9dAGwmytuNHWqZJSLB0O/7BdhjRdHebui4jcuB+SLiF5FRwFjgox5t2RgTUQ/gEmA78CnwPafr6aM2no39qrUeWBt6XAKkYI+KfxL6c5DTtfZR+88D/h76OerbDEwDckL/3i8DyQOk3T8CtgIbgT8D/mhrN/As9hhBI7YHfmNnbQS+F8q2bcCcnu5PT/1XSqkoEWlDLkoppTqgga6UUlFCA10ppaKEBrpSSkUJDXSllIoSGuhKKRUlNNCVUipK/H8beRC6rtLs8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loses\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# accuracies\n",
    "plt.plot(r.history['accuracy'], label='acc')\n",
    "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets make our test model\n",
    "\n",
    "# first we make the encoder\n",
    "encoder_model = Model(input_encoder_tf,[hT_encoder,cT_encoder])\n",
    "\n",
    "# next we make the test decoder\n",
    "input_decoder_test = Input(shape=(1))\n",
    "hT_from_encoder = Input(shape=(LATENT_DIM_ENCODER,))\n",
    "cT_from_encoder = Input(shape=(LATENT_DIM_ENCODER,))\n",
    "\n",
    "x_decoder_test = embedding_decoder(input_decoder_test)\n",
    "# now we need the hidden & cell states\n",
    "x_decoder_test,h_t_decoder_test,c_t_decoder_test = lstm_decoder(x_decoder_test,initial_state=[hT_from_encoder,cT_from_encoder])\n",
    "output_decoder_test = dense(x_decoder_test)\n",
    "\n",
    "translator = Model([input_decoder_test,hT_from_encoder,cT_from_encoder],[output_decoder_test,h_t_decoder_test,c_t_decoder_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a function that translates sentences\n",
    "# input is a normal sentence\n",
    "idx2word_ara = {v:k for k,v in word2idx_ara.items()}\n",
    "def translate(eng_sentence):\n",
    "    # tokensie and pad sentence\n",
    "    eng_sentence = tokeniser_encoder.texts_to_sequences([eng_sentence])\n",
    "    eng_sentence = pad_sequences(eng_sentence,maxlen=max_seq_length_encoder,padding='post')\n",
    "    # now pass sentence through encoder\n",
    "    h_t_translate,c_t_translate = encoder_model(eng_sentence)\n",
    "    \n",
    "    ara_sentence = []\n",
    "    word = word2idx_ara['sos']\n",
    "    word = np.array(word).reshape(1,1)\n",
    "\n",
    "    # next we predict word by word\n",
    "    for t in range(max_seq_length_decoder):\n",
    "        probs,h_t_translate,c_t_translate = translator([word,h_t_translate,c_t_translate])\n",
    "        # get probabilities\n",
    "        probs = np.squeeze(probs)\n",
    "        word_idx = np.argmax(probs)\n",
    "        # get word\n",
    "        word = idx2word_ara[word_idx]\n",
    "        if word == 'eos':\n",
    "            break\n",
    "        else:\n",
    "            ara_sentence.append(word)\n",
    "            # make current word x\n",
    "            word = np.array(word_idx).reshape(1,1)\n",
    "    return ' '.join(ara_sentence)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many horses does Tom own?\n",
      "كم حصانا لدى توم؟\n",
      "translation :  كم حصانا لدى توم؟\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Welcome back. We missed you.\n",
      "أهلاً بعودتك. لقد اشتقنا إليك!\n",
      "translation :  أهلاً بعودتك لقد اشتقنا إليك\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Don't forget to turn off the light.\n",
      "لا تنس إطفاء الأضواء.\n",
      "translation :  لا تنس إطفاء الأضواء\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Tom opened the door and asked Mary if she was busy.\n",
      "فتح توم الباب وسأل فيما إذا كانت ماريا مشغولة.\n",
      "translation :  فتح توم الباب وسأل فيما إذا كانت ماريا مشغولة\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "I like sports.\n",
      "أحب الرياضة.\n",
      "translation :  أحب ممارسة الألعاب الرياضية\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "My daughter is getting married in June.\n",
      "ستتزوج ابنتي في يونيو\n",
      "translation :  لقد حضر الاجتماع لي دعنا وصل\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Let me see it.\n",
      "دعني أراها.\n",
      "translation :  دعني أراها\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Let's forget that.\n",
      "لننسى الأمر.\n",
      "translation :  لننسى الأمر\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "That woman is much older than I am.\n",
      "تلك السيدة أكبر مني بكثير.\n",
      "translation :  تلك السيدة أكبر مني بكثير\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "When did you begin studying English?\n",
      "متى بدأتَ دراسة اللغة الانجليزية؟\n",
      "translation :  متى أتيت إلى بوسطن يوم من أعرف أحب أن يكون توم في الخارج\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Can't you stay a little longer?\n",
      "ألا يمكنك البقاء أطول قليلاً؟\n",
      "translation :  ألا يمكنك البقاء أطول قليلاً؟\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "The author of this article is a famous critic.\n",
      "محرر هذا المقال ناقد مشهور.\n",
      "translation :  محرر هذا المقال ناقد مشهور\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "They've come to make trouble.\n",
      "هُم جاءو ليصنعوا مشكلة.\n",
      "translation :  هُم هُم ليصنعوا مشكلة\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "When did you first meet him?\n",
      "متى قابلته اول مره؟\n",
      "translation :  متى قابلته اول مره؟\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "He is very handsome.\n",
      "إنه وسيم.\n",
      "translation :  إنه وسيم\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "I need to rest a moment.\n",
      "علي أن أرتاح لدقيقة.\n",
      "translation :  أحتاج إلى أن يسبق لي بأنه رأى توم كان يسبق لي\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "She pretended that she was sick.\n",
      "تظاهرت بأنها مريضة.\n",
      "translation :  بأنها مريضة\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Tom saw something strange.\n",
      "توم رأى شيئا غريبا.\n",
      "translation :  توم رأى شيئا غريبا\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "This word is derived from Latin.\n",
      "هذه الكلمة مشتقة من اللاتينية.\n",
      "translation :  هذه الكلمة مشتقة من اللاتينية\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "English is an international language.\n",
      "الإنجليزية لغة عالمية.\n",
      "translation :  الإنجليزية لغة عالمية\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "It's not important.\n",
      "ليس هذا مهماً.\n",
      "translation :  ليس هذا مهماً\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Is this your first visit to Japan?\n",
      "هل هذه أول زيارة لك إلى اليابان؟\n",
      "translation :  هل هذه أول زيارة لك إلى اليابان؟\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "It can't last long.\n",
      "لا يمكن أن يدوم طويلا.\n",
      "translation :  لا يمكن أن يدوم طويلا\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "I don't know what to do anymore.\n",
      "لا أعرف ماذا سأفعل بعد الآن.\n",
      "translation :  لا أعرف ماذا سأفعل بعد الآن\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "The important thing is you're here.\n",
      "كل ما يهم هو أنك هنا.\n",
      "translation :  كل ما يهم هو أنك هنا\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Thanks to the bad weather, the game was canceled.\n",
      "أُلغيت المباراة بسبب تغير الجو.\n",
      "translation :  كرة القدم هي اليابان\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Can you order one for me?\n",
      "هل يمكنك طلب واحد لي؟\n",
      "translation :  هل يمكنك طلب واحد لي؟\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "I want to live in the country.\n",
      "أريد العيش في قرية.\n",
      "translation :  أريد أن آتي أيضاً هو في ستعود؟\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "I agreed with her.\n",
      "وافقتها الرأي.\n",
      "translation :  وافقتها الرأي\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "I've always loved you.\n",
      "لطالما أحببتك.\n",
      "translation :  لطالما أحببتك\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Thank you for coming.\n",
      "شكرًا على قدومك.\n",
      "translation :  شكراً لكَ على فيه\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "You cannot be serious.\n",
      "لا يمكن أن تكون جاداً.\n",
      "translation :  لا يمكن أن تكون جاداً\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "I have never seen a red refrigerator.\n",
      "لم أرَ براداً أحمر من قبل.\n",
      "translation :  لم يسبق لي أن رأيت ثلاجةً حمراء\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "The old man died from hunger.\n",
      "مات الرجل العجوز من شدة الجوع.\n",
      "translation :  مات الرجل العجوز من شدة الجوع\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "My mother died during my absence.\n",
      "ماتت أمي في غيابي.\n",
      "translation :  ماتت أمي في غيابي\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "You did an excellent job.\n",
      "لقد قمت بعمل رائع.\n",
      "translation :  لقد قمت بعمل رائع\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "I'm so fat.\n",
      "أنا بدين جداً.\n",
      "translation :  أنا سمين جدا\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "If you're not with us then you're against us.\n",
      "إذا لم تكن معنا فأنت ضدنا.\n",
      "translation :  إذا لم تكن في صفنا فسنعدك من أعدائنا\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "I don't pretend to understand women.\n",
      "لا أدعي أنني أفهم النساء.\n",
      "translation :  لا أفهم أنه لم يستطع\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Go on.\n",
      "داوم.\n",
      "translation :  استمر\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "I paid 1,500 yen for this dictionary.\n",
      "لقد دفعت 1,500 ين لهذا القاموس.\n",
      "translation :  هناك من المال على الذهاب\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Pardon me?\n",
      "المعذرة؟\n",
      "translation :  إنهم من الممكن أن تكون على أي شيء\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "She has gone through many difficulties.\n",
      "لقد مرَّت بالكثير من الصعوبات.\n",
      "translation :  هل أعرف الكثير من السياح\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Don't talk in a loud voice here.\n",
      "لا تتكلم بصوت عالٍ هنا.\n",
      "translation :  لا أدري بعد الأشياء\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "The lake was frozen.\n",
      "تجمدت البحيرة.\n",
      "translation :  كان الماء في المال\n",
      "Continue [Y/n] : Y\n",
      "-------------------------\n",
      "Tom is coming for you.\n",
      "توم آت لأجلك/لك\n",
      "translation :  توم آت لأجلك لك\n",
      "Continue [Y/n] : n\n"
     ]
    }
   ],
   "source": [
    "# lets try sentences from our dataset\n",
    "eng_ara = [line.rstrip().split('\\t')[:2] for line in open('datasets/Tab-delimited Bilingual Sentence Pairs/ara.txt',encoding='utf8')]\n",
    "\n",
    "while True:\n",
    "    idx = np.random.choice(len(eng_ara))\n",
    "    eng,ara = eng_ara[idx]\n",
    "    trans = translate(eng)\n",
    "    print(eng)\n",
    "    print(ara)\n",
    "    print('translation : ',trans)\n",
    "    resp = input('Continue [Y/n] : ')\n",
    "    if resp.lower().startswith('n'):\n",
    "        break\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we can we get excellent results\n",
    "# whats interesting is that our model gets some sentences right, but not all the sentence\n",
    "# this means that the model is learning something rather than just memorising\n",
    "# within our limited dataset (10K - sentences 1-37 words), its expexted for the model to overfit\n",
    "# so a larger dataset might result in better results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
