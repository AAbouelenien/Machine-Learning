{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be looking at how to use the Markov Decision Process to solve real problems\n",
    "\n",
    "To recap what we've done so far and to understand where we are going next, let's quickly summarize the previous notebook\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Recap So Far</h3>\n",
    "\n",
    "So in the previous section, we learned about the Markov Decision Process\n",
    "\n",
    "So why was this important?\n",
    "\n",
    "The MDP is important because it gives us a framework for describing the reinforcement learning problem\n",
    "\n",
    "Without this framework, it's not clear what approach we should take\n",
    "\n",
    "But using this framework, we can imagine it like the foundation of a house or the structure of a building\n",
    "\n",
    "Once we have that, we can start to build on top of it\n",
    "\n",
    "This is better than, say, taking a bunch of sticks off the ground and trying to combine them together randomly to build a house\n",
    "\n",
    "To build a house that will last for a long time, we need a strong foundation and that requires us to build from a framework\n",
    "\n",
    "The framework for reinforcement learning is the MDP\n",
    "\n",
    "In fact, what we will see is that the Bellman equation, which we've already learned, can be used directly to solve our first important problem\n",
    "\n",
    "That is to say, all we need to do is implement the Bellman equation in Python code and we've already started to do practical work and reinforcement learning\n",
    "\n",
    "---\n",
    "\n",
    "OK, so let's start by reviewing the problem that we are trying to solve\n",
    "\n",
    "This follows directly from what we learned about MDPs\n",
    "\n",
    "So to start, we have two entities interacting\n",
    "\n",
    "These two entities are the Agent and the environment\n",
    "\n",
    "<img src='extras/55.1.PNG' width='500'></img>\n",
    "\n",
    "The agent is the thing that we are trying to program\n",
    "\n",
    "The environment is some game or some subset of the world that we want the agent to achieve some goal inside\n",
    "\n",
    "So how do the agent and the environment interact?\n",
    "\n",
    "This is entirely described by the states, actions and rewards\n",
    "\n",
    "At every time step, the agent gets to read some data from the environment\n",
    "\n",
    "We can think of these like sensors on a robot, like a camera, a temperature sensor, GPS and so forth\n",
    "\n",
    "We call this the state at the current time $t$, $s_t$ \n",
    "\n",
    "At the same time, the agent also receives a reward from the environment We call this $r_t$\n",
    "\n",
    "And remember, there is no negative or positive connotation associated with the term reward\n",
    "\n",
    "The reward is not like a cookie we get for behaving well\n",
    "\n",
    "The reward is just a number\n",
    "\n",
    "So the state and the reward go from the environment to the agent \n",
    "\n",
    "On the other hands, the action goes from the agent to the environment\n",
    "\n",
    "This is what the agent does in the environment, which has the effect of possibly changing the state\n",
    "\n",
    "Pressing a button in a video game or turning a steering wheel left or right\n",
    "\n",
    "So this is our system\n",
    "\n",
    "We have two components, the agent and the environment\n",
    "\n",
    "They communicate with each other via signals called states, actions and rewards\n",
    "\n",
    "Now, what are we trying to do with this system?\n",
    "\n",
    "We are trying to program the agent to meet some objective\n",
    "\n",
    "Of course, the term objective is too generic, but also stating specific objectives like win a game of chess or when a game of tic-tac-toe are too specific\n",
    "\n",
    "We need a general technique that we can apply to all kinds of problems\n",
    "\n",
    "So what is this technique?\n",
    "\n",
    "Ultimately, we would like to program the agents such that it maximizes its sum of future rewards\n",
    "\n",
    "We call this the return\n",
    "\n",
    "We said that we would like to program our agents such that it maximises the expected return\n",
    "\n",
    "We say expected return and not just the return, since both the environment and the agents can contain randomness\n",
    "\n",
    "Expected return means the average return\n",
    "\n",
    "So it's good to distinguish the reward from the return\n",
    "\n",
    "We don't just want to maximise the reward because the reward is instantaneous\n",
    "\n",
    "Trying to maximize the reward would be too short sighted\n",
    "\n",
    "Instead, we would like to maximise all the rewards in our future\n",
    "\n",
    "And so how do we accomplish this?\n",
    "\n",
    "Well, that's what we will discuss in this notebook\n",
    "\n",
    "But clearly, to solve this problem, we need a way to describe the agents and the environment using math\n",
    "\n",
    "The language we need is probability\n",
    "\n",
    "Thus, both the agent and the environment are described using probability distributions\n",
    "\n",
    "The agent is described by the policy $\\pi(a \\vert s)$ \n",
    "\n",
    "note that this can also be a deterministic function, but when we generalize that, we get a distribution\n",
    "\n",
    "The environment is described by the state transition distribution $p(s^\\prime,r \\vert s,a)$\n",
    "\n",
    "So to recap what our job is, the state transitions should be considered fixed\n",
    "\n",
    "This is because we don't have any control over the environment\n",
    "\n",
    "However, we do have control over the policy $\\pi$\n",
    "\n",
    "Therefore, our ultimate goal in reinforcement learning will be to find the best pie to meet our objectives\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Notebook online</h3>\n",
    "\n",
    "OK, so what's the outline for this notebook\n",
    "\n",
    "We'll see that with this section and the following notebooks, we are always going to follow the same basic pattern \n",
    "\n",
    "In reinforcement learning, there are two tasks that we are concerned with\n",
    "\n",
    "Task number one is called the prediction problem\n",
    "\n",
    "This is when we are given a policy and we want to evaluate the value of that policy\n",
    "\n",
    "In other words, we want to answer the question, how good is the given policy ?\n",
    "\n",
    "Task number two is the control problem\n",
    "\n",
    "This is when we are given an environment and we want to find the optimal policy for this environment\n",
    "\n",
    "In other words, we want to answer the question, what is the best policy?\n",
    "\n",
    "OK, so hopefully that's pretty simple.\n",
    "\n",
    "Task number one, tell me how good a given policy is\n",
    "\n",
    "And task number two, tell me what the best policy is\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook in particular, we will look at the following methods \n",
    "\n",
    "In order to solve task number one, we will employ a method called iterative policy evaluation\n",
    "\n",
    "We'll see that this is nothing but applying the Bellman equation repeatedly.\n",
    "\n",
    "This will give us a function where we can pass in a policy and get out its corresponding value \n",
    "\n",
    "In order to solve tasks number two, we will study a general approach known as policy improvement\n",
    "\n",
    "Once we understand the general approach, we will apply that approach in a quite straightforward manner\n",
    "\n",
    "The first method we will look at is called policy iteration\n",
    "\n",
    "This will work, but it's not particularly efficient\n",
    "\n",
    "The second method improves upon policy iteration, and it's called value iteration\n",
    "\n",
    "---\n",
    "\n",
    "<h3>The Big Picture</h3>\n",
    "\n",
    "One important fact that we want to stress about this notebook is this \n",
    "\n",
    "We'll be applying the Bellman equation directly\n",
    "\n",
    "We'll notice that the Bellmon equation depends on the environment dynamics which are encapsulated by the probability distribution $p(s^\\prime,r \\vert s,a)$\n",
    "\n",
    "It also depends on the policy, which can also be expressed as a probability distribution $\\pi(a \\vert s)$\n",
    "\n",
    "$$\\large V_\\pi(s) = \\sum_a \\pi(a \\vert s) \\sum_{s^\\prime} \\sum_r p(s^\\prime,r \\vert s,a) \\{r + \\gamma V_\\pi (s^\\prime) \\}$$\n",
    "\n",
    "Now it's reasonable to assume that we can express $\\pi(a \\vert s)$ as a distribution in code\n",
    "\n",
    "But what about $p(s^\\prime,r \\vert s,a)$\n",
    "\n",
    "We ask ourselves the question, is it reasonable to assume that we know this?\n",
    "\n",
    "$\\large \\pi(a \\vert s)$ : We know this (because we programmed it)\n",
    "\n",
    "$\\large p(s^\\prime,r \\vert s,a)$ : Do we know this ?\n",
    "\n",
    "\n",
    "Imagine we are engineering a self-driving car or we're trying to teach a robot to walk\n",
    "\n",
    "The data that we read from your sensors will give us the state at any particular time\n",
    "\n",
    "But do we actually know these state transition distributions?\n",
    "\n",
    "This would imply that we know what the readings on our sensors will be in the next timestep \n",
    "\n",
    "Or if we can't predict them exactly, we at least know their distribution \n",
    "\n",
    "In this notebook, we will assume that this is the case\n",
    "\n",
    "But we should keep this idea in the back of our minds that in reality, this is not something we necessarily know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll be studying our first reinforcement learning algorithm\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Evaluation</h3>\n",
    "\n",
    "As mentioned in the introduction, in reinforcement learning will be focused on two main tasks\n",
    "\n",
    "Task number one is given a policy, tell me how good it is\n",
    "\n",
    "That is, what is the value function for this policy ?\n",
    "\n",
    "Task number two is given an environment, tell me what the best policy is?\n",
    "\n",
    "This section will focus on task number one\n",
    "\n",
    "In particular, our goal is to find $V(s)$ or $Q(s,a)$, given a policy $\\pi(a \\vert s)$\n",
    "\n",
    "$$\\large \\text{Input : } \\pi(a \\vert s)$$\n",
    " \n",
    "$$\\large \\text{Output : } V_\\pi(s) \\text{ or } Q_\\pi(s,a)$$\n",
    "\n",
    "\n",
    "note that we typically subscript $V$ and $Q$ to make it explicit that the values depend on which policy is being followed\n",
    "\n",
    "Of course, this must be the case, we expect a good policy to have higher values for $V$ and $Q$ because they yield higher rewards\n",
    "\n",
    "We expect a bad policy to have lower values of $V$ and $Q$ because they yield the lower rewards\n",
    "\n",
    "So clearly the reward we get will depend on which policy we are following\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Dynamic Programming (DP)</h3>\n",
    "\n",
    "So the general approach that we will use in this notebook is called Dynamic Programming, or just DP for short\n",
    "\n",
    "The specific DP algorithm we will discuss in this section is called Policy Evaluation\n",
    "\n",
    "Again, our goal is given some policy $\\pi$ find the value function\n",
    "\n",
    "We'll start with the state $V(s)$ although the same techniques can be applied to the action value $Q$ as well\n",
    "\n",
    "To begin, let's recall the Bellman equation\n",
    "\n",
    "$$\\large V_\\pi(s) = \\sum_a \\pi(a \\vert s) \\sum_{s^\\prime} \\sum_r p(s^\\prime,r \\vert s,a) \\{r + \\gamma V_\\pi(s^\\prime)\\}$$\n",
    "\n",
    "What we should find interesting about this equation is that there is no need for any special algorithm to solve this problem\n",
    "\n",
    "That is to say, we can solve this problem without this notebook\n",
    "\n",
    "If we go through each symbol in this equation, we can recognize that everything is known except for the $V$s\n",
    "\n",
    "$$\\large \\color{red}{V_\\pi(s)} = \\sum_a \\color{green}{\\pi(a \\vert s)} \\sum_{s^\\prime} \\sum_r \\color{green}{p(s^\\prime,r \\vert s,a)} \\{\\color{green}{r} + \\color{green}{\\gamma} \\color{red}{V_\\pi(s^\\prime)}\\}$$\n",
    "\n",
    "$\\color{green}{known} - \\color{red}{Unknown}$\n",
    "\n",
    "We know $\\pi$, so that's just a number\n",
    "\n",
    "We also know $p(s^\\prime,r \\vert s,a)$, or at least we are assuming that we do, so that's just a number\n",
    "\n",
    "The reward $r$ is just a number\n",
    "\n",
    "And the discount factor $\\gamma$ is also just a number\n",
    "\n",
    "We know that when we multiply one number by another number, it's just another number\n",
    "\n",
    "So the only unknowns in this equation are the $V$s\n",
    "\n",
    "Furthermore, this equation is linear in $V$, there are no $V^2$ terms or exponentials, it's just summing and addition\n",
    "\n",
    "Everything is just a number $\\times$ $V$ along with addition\n",
    "\n",
    "Therefore, this is a linear equation in $V$ \n",
    "\n",
    "Now there are multiple Vee's because we have multiple states\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Evaluation - Nothing Special</h3>\n",
    "\n",
    "Suppose that we have $N$ states, then we have $V(s_1),V(s_2),\\ldots, V(s_N)$ \n",
    "\n",
    "If we were to write down the Bellman equation for each state explicitly and plug in all the values for $\\pi$, $p$, $r$ and $\\gamma$, we would have N equations and $N$ unknowns\n",
    "\n",
    "$$\\large V_\\pi(s_1) = b_1 + c_{11} V_\\pi(s_1) + c_{12}V_\\pi(s_2) + \\ldots + V_{1N}V_\\pi(s_N)$$\n",
    "\n",
    "$$\\large V_\\pi(s_2) = b_2 + c_{21} V_\\pi(s_1) + c_{22}V_\\pi(s_2) + \\ldots + V_{2N}V_\\pi(s_N)$$\n",
    "\n",
    "$$\\ldots$$\n",
    "\n",
    "$$\\large V_\\pi(s_N) = b_N + c_{N1} V_\\pi(s_1) + c_{N2}V_\\pi(s_2) + \\ldots + V_{NN}V_\\pi(S_N)$$\n",
    "\n",
    "\n",
    "Therefore, this is just a linear system and we can solve it using regular linear algebra\n",
    "\n",
    "The problem is this is not scalable in several ways\n",
    "\n",
    "Firstly, it doesn't handle the case where $N$ is large and secondly, it doesn't handle the case where $p$ is unknown, although neither does dynamic programming\n",
    "\n",
    "However, dynamic programming does set us up for the subsequent notebooks where we do assume that $P$ is unknown\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Finding V(s) Iteratively</h3>\n",
    "\n",
    "So what is an alternative way for finding the values for $V(s)$? \n",
    "\n",
    "As a side note, when we say $V(s)$, we really mean the values of $V(s)$ for all the states, but to avoid having to write down $V(s_1),V(s_2)$ and so on each time, we will just say $V(s)$ for convenience\n",
    "\n",
    "In any case, let's now move on to an iterative solution for finding $V(s)$\n",
    "\n",
    "This is the dynamic programming approach\n",
    "\n",
    "$$\\large \\text{Initialise: } v_0(s) = 0  \\text{or random for all states (0 for terminal s)}$$\n",
    "\n",
    "$$\\large v_{k+1}(s) = \\sum_a \\pi(a \\vert s) \\sum_{s^\\prime} \\sum_r p(s^\\prime,r \\vert s,a) \\left[r + \\gamma v_k(s^\\prime)\\right]$$\n",
    "\n",
    "So this might be surprising, but this is nothing but simply applying the Bellman equation over and\n",
    "over again\n",
    "\n",
    "There is a subtle use of overloaded notation here \n",
    "\n",
    "In this case, when we use the $=$ sign, we have to imagine that this is computer code\n",
    "\n",
    "So we're not really saying that the two sides are equal, although they are, but what we really mean is assign the value of the expression on the right side to the variable on the left side\n",
    "\n",
    "Also, note that we start by initializing $V(s)$ randomly or to zero\n",
    "\n",
    "One exception is the terminal state, which always has value zero\n",
    "\n",
    "We'll also notice that we're now subscripting $V$ with an index $k$\n",
    "\n",
    "This index $k$ denotes the timestep of our algorithm\n",
    "\n",
    "So we start at $k = 0$ and every time we update $V(s)$, $k$ increases by one to the next timestep\n",
    "\n",
    "Now to be clear, this is not a timestep in the environment, but a timestep in our policy evaluation loop\n",
    "\n",
    "That is, these are timestep is in our code\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Why does this work?</h3>\n",
    "\n",
    "Now, we might wonder, why does this work, why does performing this update again and again until\n",
    "$k$ approaches infinity lead us to finding $V_\\pi(s)$?\n",
    "\n",
    "The details are beyond the scope of this notebook, but they are closely related to other iterative algorithms in linear algebra\n",
    "\n",
    "So we can  look those up if we're interested \n",
    "\n",
    "Intuitively, we should recognize that the true value that we are looking for $V(\\pi \\vert s)$ is a fixed point for this update rule (see <strong>Banach fixed point theorem</strong>)\n",
    "\n",
    "So why is that?\n",
    "\n",
    "Well, what is a fixed point?\n",
    "\n",
    "A fixed point means that when we apply this update rule, $V(s)$ no longer changes\n",
    "\n",
    "So why does $V_\\pi(s)$ not change if we apply this update rule?\n",
    "\n",
    "Well, because this is just Bellman's equation\n",
    "\n",
    "$$\\large V_\\pi(s) = \\sum_a \\pi(a \\vert s) \\sum_{s^\\prime}\\sum_{r} p(s^\\prime,r \\vert s,a) \\{r + \\gamma V_\\pi(s^\\prime)\\}$$\n",
    "\n",
    "Bellman's Equation states that these two sides are equal if we plug in $V_\\pi(s)$ and that's if we plug in the true values for$V_\\pi(s)$\n",
    "\n",
    "Help : Imagine we already know $V_\\pi$, if we ask what is $V_\\pi(0)$, we can just look that up, its just a number, well one other much harder way is to calculate it using Bellman's equation, if we use the true $V_\\pi$ on the right side we will get also $V_\\pi$ on the left side, therefore $V_\\pi$ is a fixed point, so Bellman's equation allows us to calculate $V_\\pi(s)$ in terms of $V_\\pi(s^\\prime), s^\\prime \\text{ represents all the states}$\n",
    "\n",
    "Therefore, when we have found $V_\\pi(s)$, the right side is already equal to the left side and performing this update results in no change\n",
    "\n",
    "Therefore, we call it a fixed point\n",
    "\n",
    "So as long as the solution to this system of equations exists, then it will be found by dynamic programming\n",
    "\n",
    "---\n",
    "\n",
    "<h3>When do we stop?</h3>\n",
    "\n",
    "Another important aspect of this algorithm is when to quit\n",
    "\n",
    "We know that we will approach the true answer as $k$ approaches infinity, but of course, we can't wait an infinite amount of time for the answer\n",
    "\n",
    "Therefore, we need to have some exit condition\n",
    "\n",
    "Typically, as is the case with models like these, we exit when our algorithm has converged\n",
    "\n",
    "We check for convergence by looking at how much $V(s)$ has changed from one iteration to the next\n",
    "\n",
    "Specifically, let's create a new variable called $\\Delta$\n",
    "\n",
    "$\\Delta$ is the maximum change in $V(s)$ over all states during an iteration\n",
    "\n",
    "$$\\large \\Delta = \\max_s \\vert v_{k+1}(s) - v_k(s) \\vert $$\n",
    "\n",
    "Thus, at each iteration of the loop, we check the value of $\\Delta$ against some threshold\n",
    "\n",
    "We get to pick this threshold depending on how accurate we would like our function to be\n",
    "\n",
    "Once $\\Delta$ falls below this threshold, we can consider our algorithm converged and then we can exit\n",
    "\n",
    "Note that typical values of Delta are decimal numbers, for example, $10^{-3},10^{-5},10^{-8}$ and so forth\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Psuedocode</h3>\n",
    "\n",
    "OK, so let's look at how policy evaluation would be implemented in practice\n",
    "\n",
    "Note that this is just pseudocode\n",
    "\n",
    "In real code there are a few details we have yet to discuss, but looking at the pseudocode first will help us understand the principles\n",
    "\n",
    "\n",
    "$\\text{Given: } \\pi(a \\vert s)$\n",
    "\n",
    "$\\text{Initialise: }V(s) = 0 \\text{ or random (except terminal s, where V(s)=0)}$\n",
    "\n",
    "$Loop:$\n",
    "\n",
    "$\\qquad \\Delta = 0$\n",
    "\n",
    "$\\qquad \\text{for s in all_non_terminal_states:}$\n",
    "\n",
    "$\\qquad \\text{v_old = V(s) \\# store the existing value}$\n",
    "\n",
    "$\\qquad V(s) = \\sum_a \\pi(a \\vert s) \\sum_{s^\\prime}\\sum_r p (s^\\prime,r \\vert s,a) [r + \\gamma V(s^\\prime)]$\n",
    "\n",
    "$\\qquad \\Delta = \\max(\\Delta, \\vert \\text{v_old - V(s)} \\vert )$\n",
    "\n",
    "$\\text{if }\\Delta < \\text{threshold:}$\n",
    "\n",
    "$\\qquad break$\n",
    "\n",
    "First, we accept as input a policy $\\pi$\n",
    "\n",
    "We start by initializing $V(s)$ randomly\n",
    "\n",
    "Alternatively, it can also be initialized to all zeros\n",
    "\n",
    "Note that the terminal state should always be zero, so this value should always be initialized to zero and there's no need to update this value since it is known\n",
    "\n",
    "Next, we enter a loop, let's suppose that this is an infinite loop and we only break out of the loop after our algorithm has converged \n",
    "\n",
    "Inside the loop, we initialize Delta to zero.\n",
    "\n",
    "As we recall, $\\Delta$ will store the maximum change in $V(s)$ over the current iteration\n",
    "\n",
    "Next, we enter a second inner loop that loops over all the states in our environment except for the terminal state\n",
    "\n",
    "As we recall, the value for the terminal state does not need to be updated since it's always zero\n",
    "\n",
    "Note that there's an implicit assumption here, which is that we can loop through all the states\n",
    "\n",
    "So this doesn't handle the case where the state space is too large to loop through, or when the state space is infinite, or when the state space is continuous\n",
    "\n",
    "For these cases, they'll be discussed in later notebooks\n",
    "\n",
    "Dynamic programming requires this assumption\n",
    "\n",
    "OK, so for each state, we'll store the current estimate for $V(s)$ in a variable called $\\text{V_old}$ \n",
    "\n",
    "This is just a temporary variable to store our old value\n",
    "\n",
    "Next, we compute the new value for $\\text{V(s)}$ using the right side of the Bellman equation \n",
    "\n",
    "At this point, we now have our new value for $V(s)$ and we can compare this to our old value $\\text{v_old}$\n",
    "\n",
    "Remember that we would like to store the maximum change in our $\\Delta$ variable\n",
    "\n",
    "Therefore, we can assign $\\Delta$ to be the max of current $\\Delta$ and the current absolute difference between the new $V(s)$ and $\\text{v_old}$\n",
    "\n",
    "Once the inner loop is complete, we can check $\\Delta$ for this iteration\n",
    "\n",
    "If $\\Delta$ is less than our threshold, we can consider our algorithm converged and we can break out of our infinite loop\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Implementing the Bellman Equation</h3>\n",
    "\n",
    "Now, just to be super clear, how do we implement the Bellman equation? \n",
    "\n",
    "Note that it has three summations\n",
    "\n",
    "Well, we should recognize that summations in math are equivalent to loops in code\n",
    "\n",
    "So in order to break down the Bellman equation into code, we need some loops \n",
    "\n",
    "Specifically, since there are three summations, we need three nested for loops\n",
    "\n",
    "One over the action space for $a$, one over the state space for $s^\\prime$ and one over the possible rewards for $r$\n",
    "\n",
    "$\\text{value = 0}$\n",
    "\n",
    "$\\quad \\text{for a in all_actions:}$\n",
    "\n",
    "$\\quad \\quad \\text{for } s^\\prime \\text{ in all_states:} $\n",
    "\n",
    "$\\quad \\quad \\quad \\text{for r in all_rewards: }$\n",
    "\n",
    "$\\quad \\quad \\quad \\quad value \\ += \\ \\pi(a \\vert s)p(s^\\prime,r \\vert s,a) \\left[ r + \\gamma V(s^\\prime) \\right]$\n",
    "\n",
    "Then we simply accumulate the result in some variable\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Implementing Bellman (Deterministic Reward)</h3>\n",
    "\n",
    "Now, note that the reward is typically deterministic\n",
    "\n",
    "This is the case for many practical environments and it's also the case for the environments in these following notebooks\n",
    "\n",
    "Therefore, there is no practical need for the loop over the reward $r$\n",
    "\n",
    "Instead, we can treat the reward as simply a deterministic function of the state $s^\\prime$ \n",
    "\n",
    "$\\text{value = 0}$\n",
    "\n",
    "$\\quad \\text{for a in all_actions:}$\n",
    "\n",
    "$\\quad \\quad \\text{for } s^\\prime \\text{ in all_states:} $\n",
    "\n",
    "$\\quad \\quad \\quad value \\ += \\ \\pi(a \\vert s)p(s^\\prime,r \\vert s,a) \\left[ r(s^\\prime) + \\gamma V(s^\\prime) \\right]$\n",
    "\n",
    "In this scenario, we would only need two loops and the reward can simply be a dictionary look up into a reward function that accepts a state as input\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Data Structures</h3>\n",
    "\n",
    "So now that we're getting closer and closer to our Python implementation, it's worth thinking about what kind of data structures we will need \n",
    "\n",
    "In our Bellmon update, it's clear that according to this update rule, we need at least two versions of $V$\n",
    "\n",
    "One which will hold $V$ at iteration $k$ and one which will hold the updated $V$ at iteration $k+1$\n",
    "\n",
    "So if we use a list, then you would need two lists of the same size if we use a dictionary, then we would need two dictionaries of the same size\n",
    "\n",
    "And so why don't we need more than two?\n",
    "\n",
    "Well, suppose that we have two of these lists or dictionaries or arrays or whatever we are using\n",
    "to hold $V$, let's call them $A$ and $B$\n",
    "\n",
    "Let's suppose that we've initialised $V(s)$ in a variable called $A$ we'll call that $V_0(s)$\n",
    "\n",
    "Then we would store the updates in the variable $B$\n",
    "\n",
    "So now $B$ holds the latest version of $V(s)$, we'll call that $V_1(s)$\n",
    "\n",
    "But now we don't need the old $A$ anymore so it can hold the next set of updates\n",
    "\n",
    "And now $A$ will hold the latest version of $V(s)$, we'll call that the $V_2(s)$\n",
    "\n",
    "So we only need two structures because we can just keep alternating which one holds the latest version of $V(s)$\n",
    "\n",
    "$\\text{Ex: } A = V_0(s), B = V_1(s), A = V_2(s), \\ldots$\n",
    "\n",
    "Now, in practice, what we do is a lot simpler, although technically it goes against this update rule\n",
    "\n",
    "In practice, we simply store everything in the same structure\n",
    "\n",
    "We don't bother to have two arrays\n",
    "\n",
    "Therefore, when we update $V(s)$ at iteration $k+1$, technically it should only depend on values of this four different states at iteration $k$\n",
    "\n",
    "However, since we store everything in the same structure, $V(s)$ at iteration $k+1$ might depend on other values of $V(s)$ on the same iteration $k+1$\n",
    "\n",
    "Although this might sound wrong it actually ends up being more efficient\n",
    "\n",
    "We're encouraged to test it out yourself in the code\n",
    "\n",
    "And by the way, we call this in-place updating\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Representing $V(s)$,$\\pi(a \\vert s)$ amd $p(s^\\prime,r \\vert s,a)$</h3>\n",
    "\n",
    "So, as mentioned, something that's worth thinking about early and often is how you will actually implement these things in code\n",
    "\n",
    "In math, it's easy to write $V(s)$ and $p(s,r^\\prime \\vert s,a)$\n",
    "\n",
    "But what does this actually mean in code?\n",
    "\n",
    "Let's take something simple like $s$ \n",
    "\n",
    "In our grid world environment, each state is a position on the grid\n",
    "\n",
    "We can think of it like a maze and we're trying to reach the maze exit\n",
    "\n",
    "So one way to represent each state would be a tuple containing the coordinates on the grid\n",
    "\n",
    "For example, the top left would be (0,0) to the right of that would be (0,1) and so forth\n",
    "\n",
    "If our states are tuples, then an appropriate data structure to store $V(s)$ would be a dictionary\n",
    "\n",
    "In this case, the key to the dictionary is the state represented by a tuple ```int```s\n",
    "\n",
    "The value of the dictionary is a number representing $V(s)$\n",
    "\n",
    "```python\n",
    "V = {(0,0):0.51, (0,1):0.72, ...}\n",
    "```\n",
    "\n",
    "<img src='extras/55.2.PNG' width='400'></img>\n",
    "\n",
    "However, there are other possible approaches\n",
    "\n",
    "For example, suppose we just numbered each of the states 0,1,2 and so forth\n",
    "\n",
    "Now our states are just integers and therefore we can use an array or a list instead of a dictionary which is more computationally intensive\n",
    "\n",
    "E.g. $V = [0.51,0.72,\\ldots]$\n",
    "\n",
    "---\n",
    "\n",
    "What about the state transitions $p$ \n",
    "\n",
    "In this case, we have even more options\n",
    "\n",
    "Basically, we know that for each four tuple $(s^\\prime,r,s,a)$, we're going to have some associated probability value\n",
    "\n",
    "If states actions and rewards can be represented by a finite set of integers, then we might store $p$ in a four dimensional array\n",
    "\n",
    "```\n",
    "p[s',r,s,a] = p(s',r|s,a)\n",
    "```\n",
    "But remember that rewards are usually deterministic, so perhaps these can be removed entirely\n",
    "\n",
    "Another option would be to use a dictionary\n",
    "\n",
    "The key to this dictionary might be a triple containing $s^\\prime,s,a$\n",
    "\n",
    "As long as the states and actions are hashable, this would be an appropriate dictionary key\n",
    "\n",
    "```\n",
    "Key:(s',s,a)\n",
    "Value: p(s'|s,a)\n",
    "```\n",
    "\n",
    "Yet another way to store this distribution would be to use a dictionary with only $s$ as a key \n",
    "\n",
    "Then the value of this dictionary might be another nested dictionary with the action $a$ as the key\n",
    "\n",
    "Then the value of this nested dictionary might be yet another dictionary storing $s^\\prime$\n",
    "\n",
    "Finally, the value of this dictionary might be a tuple containing the associated probability and the corresponding reward $r$\n",
    "\n",
    "```\n",
    "probs = {s:{a:{s':p(s'|s,a),...}}}\n",
    "```\n",
    "\n",
    "The point of this is when there are so many variables, there are more options to choose from for how we want to implement this data structure and code\n",
    "\n",
    "So keep this in mind as we go through the notebook and when we wrtie the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to discuss how to design our reinforcement learning program\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Designing our RL Program</h3>\n",
    "\n",
    "First let's recap what we would normally do in supervised learning \n",
    "\n",
    "In supervised learning, we are interested in implementing the algorithm we just learned about \n",
    "\n",
    "The main outline is always the same no matter what supervised learning algorithm we're implementing \n",
    "\n",
    "The main steps are as follows:\n",
    "\n",
    "<ol>\n",
    "    <li>Load in the data</li>\n",
    "    <li>Instantiate your model</li>\n",
    "    <li>Train our model</li>\n",
    "    <li>Evaluate our model</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "Our job as the implementer of the algorithm is to write up the fit and predict functions\n",
    "\n",
    "This is the case for all supervised learning algorithms\n",
    "\n",
    "In other words we might want to think of this as a fill in the blanks type of task\n",
    "\n",
    "Steps 1 to 4 are boilerplate meaning that no matter what algorithm we're implementing you still have to do these things\n",
    "\n",
    "```python\n",
    "class MyModel:\n",
    "    def fit(X,Y):\n",
    "        # YOUR JOB\n",
    "    def predict(X):\n",
    "        # YOUR JOB\n",
    "        \n",
    "# boilerplate\n",
    "Xtrain, Ytrain, Xtest, Ytest = get_data() # 1. Get data\n",
    "model = MyModel() # 2. Instantiate model\n",
    "model.fit(Xtrain,Ytrain) # 3. Train model\n",
    "model.score(Xtest,Ytest) # 4. Evaluate model     \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "In this course, since this is reinforcement learning that's not going to be what we'll do\n",
    "\n",
    "However that doesn't mean there isn't a pattern to be followed\n",
    "\n",
    "As we did in the bandit notebook, we're going to learn about several different algorithms but the beauty of these algorithms is that like supervised learning they all have the same interface\n",
    "\n",
    "So whatever our design, we're basically just going to be implementing the same thing multiple times\n",
    "\n",
    "The only difference should be the algorithm itself not the layout\n",
    "\n",
    "---\n",
    "\n",
    "With that said it is still possible to go over the basic steps that our script must perform\n",
    "\n",
    "First realize that there are two different problems we will try to solve in the following notebooks\n",
    "\n",
    "So we'll have two different kinds of scripts\n",
    "\n",
    "The first kind of problem is the prediction problem\n",
    "\n",
    "This is where, given the policy we would like to find the corresponding value function $V$ or $Q$\n",
    "\n",
    "The second kind of problem is the control problem\n",
    "\n",
    "This is where our agent will engage with its environment and do actual learning\n",
    "\n",
    "The goal of the second problem is to find the optimal policy and the optimal corresponding value function\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Prediction Problem</h3>\n",
    "\n",
    "The first problem, the prediction problem is the easier of the two\n",
    "\n",
    "Our job is essentially to do one single thing, find the value function \n",
    "\n",
    "The algorithms we discuss in this notebook are iterative, so the basic outline will be like this\n",
    "\n",
    "$\\text{Goal: Find V(s)}$\n",
    "\n",
    "```python\n",
    "given: policy\n",
    "V(s) = initial value\n",
    "for t in range(max_iterations):\n",
    "    states,actions,rewards = play_game(policy)\n",
    "    update V(s) given (states,actions,rewards) using the algorithm we learned\n",
    "print useful info (change in V(s) vs time, final V(s), policy)\n",
    "\n",
    "```\n",
    "\n",
    "First we initialize $V(s)$ \n",
    "\n",
    "Then in a loop, we play the game according to the given policy \n",
    "\n",
    "From this playing of the game, we collect data about the states, actions and rewards \n",
    "\n",
    "Then we use those states actions and rewards to update our value function $V(s)$ according to whatever algorithm we just learned about\n",
    "\n",
    "At the end we might want to plot or print some useful information such as the change in $V(s)$ per iteration, the final values of $V(s)$ and the actual policy itself so that we can ensure that the values we found make sense\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Control Problem</h3>\n",
    "\n",
    "The second type of problem the control problem is a bit more difficult because it requires us to update two things at the same time, the policy and the value function \n",
    "\n",
    "In this case we're not given a policy, but rather our goal is to update the policy according to whatever algorithm we learn\n",
    "\n",
    "```python\n",
    "initialise value function and policy\n",
    "for t in range(max_iterations):\n",
    "    states, actions, rewards = play_game(policy)\n",
    "    update value function and policy according to (states, actions, rewards) using the algorithm we learned\n",
    "print useful info (change in V(s) vs. time, final V(s), final policy)\n",
    "\n",
    "```\n",
    "\n",
    "Still the basic loop is the same\n",
    "\n",
    "First we initialize the value function that can be $V$ or $Q$ depending on the algorithm being discussed\n",
    "\n",
    "Then we enter a loop for a certain number of iterations \n",
    "\n",
    "Inside the loop, we play an episode of the game using the current policy \n",
    "\n",
    "From this we get a series of states, actions and rewards that the agent experienced\n",
    "\n",
    "Then we update the value function and the policy using whatever algorithm we just learn about \n",
    "\n",
    "At the end, we again might want a plot or print some useful information such as the change in the value function per iteration, the final values of the value function and the final policy so that we can verify that it makes sense\n",
    "\n",
    "Now we want to make it clear that this is just a very rough outline \n",
    "\n",
    "In actuality for some algorithms the policy is not explicitly represented in code and so it's not actually going to be represented by a python\n",
    "variable but again that's kind of part of the implementation, so it's partly our choice about how you want to do things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to start looking at the code for the Gridworld environment\n",
    "\n",
    "This is in preparation for implementing algorithms such as policy evaluation and policy iteration\n",
    "\n",
    "Obviously, in order to implement these algorithms, we need to have an environment to implement them on\n",
    "\n",
    "So our first step will be to actually implement Gridworld\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Gridworld in Code</h3>\n",
    "\n",
    "It's important to note that before we start, there are an infinite number of ways to do this, many of them are drastically different\n",
    "\n",
    "That's why it's so important for us to code by ourselves and implement our own design\n",
    "\n",
    "The instructor's design is merely one of many, and it's very possible that what we have in our head will be completely different from the instructor's\n",
    "\n",
    "What we end up designing is very dependent on our personality, our past life experiences and our general education background\n",
    "\n",
    "Probably someone who is a biologist will write code that's very different from someone who is a computer scientist\n",
    "\n",
    "And even still, one computer scientist will write code that's very different from another computer scientist\n",
    "\n",
    "So don't get stuck on trying to make our code like the instructor's code or trying to understand why the intructor's code isn't more like our code\n",
    "\n",
    "This is completely missing the point of these exercises\n",
    "\n",
    "Instead, it's better just to do everything completely using our own design from the beginning and to write everything in our own style\n",
    "\n",
    "What we present in the following lectures is just an example\n",
    "\n",
    "So let's begin\n",
    "\n",
    "---\n",
    "\n",
    "<h3>How will we use it?</h3>\n",
    "\n",
    "OK, so before even jumping into the code, we want to think about how we will use the code\n",
    "\n",
    "This is similar to test driven development\n",
    "\n",
    "The goal is to have some kind of object to represent our environment, a Gridworld object \n",
    "\n",
    "In the constructor for the environment, it might make sense to have a few arguments\n",
    "\n",
    "For example, the number of rows in the grid, the number of columns in the grid and the start position\n",
    "\n",
    "We might want to have a function that returns the current state\n",
    "\n",
    "This basically answers the question, in what position are we in at this current moment?\n",
    "\n",
    "As we've seen, the agent and the environment interact in a loop where the agent performs an action in the environment, arrives in the next state and received some reward\n",
    "\n",
    "Thus, we propose a function called ```move``` that takes in an action, does the action in the environment and then returns the reward associated with doing that action and arriving in the next state\n",
    "\n",
    "Obviously, if we want to know what state we're in after taking the action, we simply need to call the function ```current_state```, which we just described\n",
    "\n",
    "Of course, we can't loop forever, we must know when the game is over\n",
    "\n",
    "So we propose a function called ```game_over```, which returns true when we are in a terminal state and false otherwise\n",
    "\n",
    "All right, so we hope this seems pretty simple so far\n",
    "\n",
    "```python\n",
    "g = Gridworld(rows, cols, start_position)\n",
    "\n",
    "# returns current state\n",
    "g.current_state()\n",
    "\n",
    "# return reward\n",
    "reward = g.move(action)\n",
    "\n",
    "# returns true if in terminal state\n",
    "g.game_over()\n",
    "\n",
    "# use it like this\n",
    "\n",
    "g = Gridworld(rows,cols, start)\n",
    "while not g.game_over():\n",
    "    s = g.current_state()\n",
    "    a = policy(s)\n",
    "    r = g.move(a)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Specifying Rewards and Actions</h3>\n",
    "\n",
    "Of course, we have yet to discuss one crucial detail\n",
    "\n",
    "How do we specify what the rewards are?\n",
    "\n",
    "How do we specify states like walls where the agent cannot travel to?\n",
    "\n",
    "Now, again, this is very implementation specific, but here's what we will find in the repository (instructor's code on github LazyProgrammer)\n",
    "\n",
    "We have a function called ```set``` that accepts as input a dictionary containing rewards and also a dictionary of actions that result in movement in the grid world environment\n",
    "\n",
    "An example of the rewards dictionary would be as follows\n",
    "\n",
    "```python\n",
    "rewards = {(0,3):1, (1,3):-1}\n",
    "```\n",
    "\n",
    "<img src='extras/55.3.PNG' width='400'></img>\n",
    "\n",
    "A key $(0,3)$ with a corresponding value of $+1$, means that we will get $+1$ reward when we land in the state $(0,3)$\n",
    "\n",
    "<img src='extras/55.4.PNG' width='400'></img>\n",
    "\n",
    "A key $(1,3)$ with the corresponding value of $-1$ means that we will get $-1$ reward when we land in the state $(1,3)$\n",
    "\n",
    "<img src='extras/55.5.PNG' width='400'></img>\n",
    "\n",
    "A dictionary of actions can be specified as follows\n",
    "\n",
    "```python\n",
    "actions = {\n",
    "    (0,0) : ('D','R'),\n",
    "    (0,1) : ('L','R'),\n",
    "    ...\n",
    "}\n",
    "g.set(rewards,actions)\n",
    "```\n",
    "\n",
    "Let's focus on a few particular states so you get the idea\n",
    "\n",
    "<img src='extras/55.3.PNG' width='400'></img>\n",
    "\n",
    "Notice how the values for $0,0$ are ```D``` and ```R```, which stands for down and right\n",
    "\n",
    "That's because in the position $(0,0)$, only the actions down and right can result in us going to a different state\n",
    "\n",
    "Note that it's possible to use the action left and up, they simply don't result in going to a different state, it's the equivalent of walking into a wall\n",
    "\n",
    "Similarly, if we look at the values for $(0,1)$, we have only left and right\n",
    "\n",
    "It's not possible to go down because there is a wall in that position\n",
    "\n",
    "So if we enter the action down, we will simply be walking into a wall and we will remain in the\n",
    "same state\n",
    "\n",
    "Finally, notice that the position $(0,3)$ and $(1,3)$ do not appear in this dictionary at all\n",
    "\n",
    "Remember that these are terminal states, so no actions can be done from these states that would result in going to a different state\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Recap: Using the Gridworld class</h3>\n",
    "\n",
    "So here's an example of how we might use our Gridworld class\n",
    "\n",
    "```python\n",
    "g = Grid(3,4,(2,0))\n",
    "rewards = {(0,3):1,(1,3):-1}\n",
    "actions = {\n",
    "    (0,0): ('D','R'),\n",
    "    (0,1): ('L','R'),\n",
    "    ...\n",
    "}\n",
    "\n",
    "g.set(rewards,actions)\n",
    "\n",
    "# play the game\n",
    "while not g.game_over():\n",
    "    s = g.current_state()\n",
    "    a = policy(s)\n",
    "    r = g.move(a)\n",
    "```\n",
    "\n",
    "First, we instantiate an object of type Gridworld\n",
    "\n",
    "Then we call the set function passing in the rewards dictionary and the actions dictionary that we just described\n",
    "\n",
    "Then we can actually play the game with our agent using the ```move``` function described earlier\n",
    "\n",
    "---\n",
    "\n",
    "<h3>grid_world.py</h3>\n",
    "\n",
    "All right, so now that we've described how we will use the Gridworld class, let's have a look at the actual implementation\n",
    "\n",
    "```python\n",
    "class Grid: # Enviroment\n",
    "    def __init__(self,rows,cols,start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "```\n",
    "\n",
    "So here's the constructor \n",
    "\n",
    "As promised, it accepts as input three arguments\n",
    "\n",
    "The number of rows of the grid, the number of columns of the grid and the starting position\n",
    "\n",
    "All we do in the constructor is assign these to instance variables ```self.rows```, ```self.cols``` and ```self.i``` and ```self.j```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def set(self,rewards,actions):\n",
    "        # reward = dict of: (i,j):r (row,col): reward\n",
    "        # actions = dict of: (i,j): A ()row,col: action list\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "```\n",
    "\n",
    "Next, we have the set function that takes in a dictionary of rewards and actions\n",
    "\n",
    "Again, all we do is assign these two instance variables\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def set_state(self,s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "```\n",
    "\n",
    "Next, we have the set state function, which will be useful in later code examples\n",
    "\n",
    "All we do in this function is take as input a state $s$ and assign that to the instance variables ```self.i``` and ```self.j```\n",
    "\n",
    "Note that this is a bit like playing a video game with cheets \n",
    "\n",
    "Obviously in real life games, we can't simply set the state to whatever we want\n",
    "\n",
    "For example, if we're playing chess, we can't say we want to go to the state where we're one move away from a checkmate\n",
    "\n",
    "That's simply not allowable when we are playing chess\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def current_state(self):\n",
    "        return (self.i,self.j)\n",
    "```\n",
    "\n",
    "Next, we have the current state function, which returns the current state as a tuple containing the $i$ coordinate and the $j$ coordinate\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def is_terminal(self,s):\n",
    "        return s not in self.actions\n",
    "```\n",
    "\n",
    "Next, we have the ```is_terminal``` function, which accepts as input a state $s$ and tells us whether or not that state is a terminal state\n",
    "\n",
    "Roughly speaking, a terminal state is a state we can't move from, although there are subtleties there that we don't need to concern ourselves with\n",
    "\n",
    "Alternatively, we could have simply said terminal states explicitly, but we think it's reasonable to simply say that any state that does not appear in the actions dictionary is a terminal state\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def get_next_state(self,s,a):\n",
    "    i,j = s[0],s[1]\n",
    "    if a in self.actions[(i,j)]:\n",
    "        if a == 'U':\n",
    "            i -= 1\n",
    "        elif a == 'D':\n",
    "            i += 1\n",
    "        elif a == 'R':\n",
    "            j += 1\n",
    "        elif a == 'L':\n",
    "            j -= 1\n",
    "    return i,j\n",
    "```\n",
    "\n",
    "Next, we have the ```get_next_state``` function, which will be useful later on\n",
    "\n",
    "This accepts as input a state $s$ and an action $a$ and tells us what the next day we end up in will be\n",
    "\n",
    "This is, again, one of those cheat mode like features, and it only makes sense in this particular environment because this environment is assumed to be deterministic\n",
    "\n",
    "When we are in a state RsR and you perform an action $a$, we will always end up in the same next state\n",
    "\n",
    "So first we extract the $i$ and $j$ coordinates from the $s$ variable\n",
    "\n",
    "Next we check if the action $a$ is in the actions dictionary for the state $(i,j)$\n",
    "\n",
    "If it's not, then we can simply return the tuple $(i,j)$ since any action from the state won't change the state \n",
    "\n",
    "Otherwise, if the action is in the dictionary for this state, then we check what the action is\n",
    "\n",
    "It's important to remember the convention when we think about arrays and programming, rows count down and the columns count left to right\n",
    "\n",
    "So if the action is up, we subtract one from the row coordinate\n",
    "\n",
    "If the action is down, we add one to the row coordinate\n",
    "\n",
    "If the action is left, we subtract one from the column coordinate.\n",
    "\n",
    "And if the action is right, we add one to the column coordinate\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def move(self,action):\n",
    "        # check if legal move first\n",
    "        if action in self.actions[(self.i,self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -=1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -=1\n",
    "        return self.rewards.get((self.i,self.j),0)\n",
    "    \n",
    "```\n",
    "\n",
    "Next, we have the ```move``` function\n",
    "\n",
    "This is very similar to the get next state function, except that it actually performs the action in the environment rather than being simply a hypothetical calculation\n",
    "\n",
    "Notice how we use ```self.i``` and ```self.j```, the actual current position\n",
    "\n",
    "We do the same calculation as discussed previously to update the state, and then at the end, we return the reward associated with the resulting state\n",
    "\n",
    "This is the case even if we end up in the same state we started in\n",
    "\n",
    "Note that not all states have associated rewards\n",
    "\n",
    "So if a state doesn't exist in the rewards dictionary, we will assume that by default the reward is zero\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def undo_move(self,action):\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -=1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # should never happen\n",
    "        assert(self.currenst_state() in self.all_states())\n",
    "```\n",
    "\n",
    "Next, we have the ```undo_move``` function \n",
    "\n",
    "Again, we didn't discuss this before, since it's not critical to playing a game, but it may or may not be useful later on in the following notebooks\n",
    "\n",
    "As we can see, this simply reverses the calculation done inside the ```move``` function\n",
    "\n",
    "So all the updates for ```i``` and ```j``` are reversed.\n",
    "\n",
    "Note that in order to ensure that we don't end up in an illegal state, we have an assert at the end of this function\n",
    "\n",
    "We assert that the current state is in the set of all states\n",
    "\n",
    "We will look at the ```all_states``` function very shortly\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def game_over(self):\n",
    "    # returns true if game is over, else false\n",
    "    # true if in a state where no actions are possible\n",
    "    return (self.i,self.j) not in self.actions\n",
    "```\n",
    "\n",
    "Next, we have the game over function\n",
    "\n",
    "This tells us whether we are in a terminal state currently, which means the game is over\n",
    "\n",
    "This is slightly different from the ```is_terminal``` function, which checks whether the state we pass in as input is a terminal state\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def all_states(self):\n",
    "        # possibly buggy but simple way to get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(self.action.keys() | set(self.rewards.keys()))\n",
    "```\n",
    "\n",
    "Finally, we have the all states function \n",
    "\n",
    "Again, there are probably many ways to do this, but this was just a quick and dirty way to get all of the possible states, including the terminal states, as a single set \n",
    "\n",
    "To calculate this, we get the set of all states that appear in the actions dictionary and the set of all states that appear in the rewards dictionary\n",
    "\n",
    "We then return the union of these two sets\n",
    "\n",
    "The reason we want to do this is some states do not appear in actions dictionary such as terminal states\n",
    "\n",
    "Conversely, there are some states that don't appear in the rewards dictionary since there will be some states that don't yield any non-zero reward\n",
    "\n",
    "Obviously this calculation isn't fullproof, but it's probably good enough\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def standrad_grid():\n",
    "        g = Grid(3,4,(2,0))\n",
    "        rewards = {(0,3):1,(1,3):-1}\n",
    "        actions = {\n",
    "            (0,0): ('D','R'),\n",
    "            (0,1): ('L','R'),\n",
    "            (0,2): ('L','D','R'),\n",
    "            (1,0): ('U','D'),\n",
    "            (1,2): ('U','D','R'),\n",
    "            (2,0): ('U','R'),\n",
    "            (2,1): ('L','R'),\n",
    "            (2,2): ('L','R','U'),\n",
    "            (2,3): ('L','U')\n",
    "        }\n",
    "        g.set(rewards,actions)\n",
    "        return g\n",
    "```\n",
    "\n",
    "Finally, note that we have a helper function called ```standard_grid```, which does what we described earlier\n",
    "\n",
    "It creates a $3 \\times 4$ grid of the exact same type we have been discussing in the Math sections \n",
    "\n",
    "It instantiates a rewards dictionary and an action dictionary and then sets these on the grid and returns the Gridworld object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next section, we are going to look at the code for iterative policy evaluation\n",
    "\n",
    "We'll be using the deterministic version of GridWorld, which we just described\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Iterative Policy Evaluation in Code</h3>\n",
    "\n",
    "Again, before we start, we want to discuss at a high level what we are going to do before we dive into the implementation\n",
    "\n",
    "This is similar to a test driven development approach where we define what we want to implement before actually implementing it\n",
    "\n",
    "Another way to think about this is that this is as close as we will get to a fill in the blanks type of exercise\n",
    "\n",
    "In fact, we are defining where the blanks are\n",
    "\n",
    "So let's begin\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Visualise the value and policy</h3>\n",
    "\n",
    "First, we would like to have some functions to visualize the value and the policy\n",
    "\n",
    "We might have a function called the print values, which takes in a value table $V$ and a Gridworld object $G$ and prints the value of each state on top of a drawing of the environment\n",
    "\n",
    "We might have a function called print policy, which takes in a policy table $P$ and a Gridworld object $G$ and prints the action corresponding to each state on top of a drawing of the environment\n",
    "\n",
    "<img src='extras/55.6.PNG' width='700'></img>\n",
    "\n",
    "---\n",
    "\n",
    "<h3>State transition probabilities</h3>\n",
    "\n",
    "Next, because this is dynamic programming, we know that this algorithm involves using the state transition probabilities to calculate $V(s)$\n",
    "\n",
    "But our grid world environment does not have any such state transition probabilities\n",
    "\n",
    "So this is kind of an unusual step, but it's necessary in order to complete the exercise\n",
    "\n",
    "We are going to build the state transition probabilities from the environment (\"somehow\")\n",
    "\n",
    "We'll store the environment dynamics in a dictionary called transition probs\n",
    "\n",
    "In the most general case, we would have four arguments into our probability function, $s,a,s^\\prime,r$, however, to make things a little simpler we will assume that the reward is deterministic so that we only need three arguments into our probability function $s,a,s^\\prime$, ```probs[(s,a,s',r)] == p(s',r|s,a)```\n",
    "\n",
    "Therefore, the keys to the transition probs dictionary will be a triple containing the current state $s$, action $a$, and next state $s^\\prime$\n",
    "\n",
    "The value of the dictionary will obviously be the actual probability $p(s^\\prime \\vert s,a)$ ```probs[(s,a,s')] == p(s'|s,a)```\n",
    "\n",
    "How we actually populate this dictionary is an implementation detail, so we'll discuss that later\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy</h3>\n",
    "\n",
    "Next, because this is policy evaluation, we need a policy to evaluate \n",
    "\n",
    "We'll represent our policy as a dictionary with the key being the state and the value being the action for that state\n",
    "\n",
    "For example, Up, Down, Left or Right\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Run policy evaluation loop</h3>\n",
    "\n",
    "Next, we will initialise our value table and then run our policy evaluation loop \n",
    "\n",
    "At some point we'll also want to print the policy along with the value using the functions we defined earlier\n",
    "\n",
    "<ul>\n",
    "    <li>Initialise $V(s)=0$ for all $s$</li>\n",
    "    <li>Find $V(s)$ given the policy</li>\n",
    "    <li>Print the policy/value</li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Overall steps</h3>\n",
    "\n",
    "So overall, these are the steps\n",
    "\n",
    "<ul>\n",
    "    <li>Define helper functions to print the policy and the value</li>\n",
    "    <li>Create dictionaries to represent the state transition probabilities and the policy</li>\n",
    "    <li>Apply the iterative policy evaluation algorithm to find the value for the given policy</li>\n",
    "    <li>Call the print policy and print value functions to observe that the results make sense</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets implement our Gridworld enviroment\n",
    "# we will be using the same interface as described above\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,rows,cols,start_pos):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.pos = start_pos\n",
    "    \n",
    "    def current_state(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def set_state(self,state):\n",
    "        self.pos = state\n",
    "        \n",
    "    def set_actions_rewards(self,actions,rewards):\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        \n",
    "    def all_states(self,):\n",
    "        return [(i,j) for i in range(self.rows) for j in range(self.cols)]\n",
    "    \n",
    "    def is_terminal(self,state):\n",
    "        # we cant perform an action in a terminal state\n",
    "        # so we expect not to find it in the keys\n",
    "        return state not in self.actions.keys()\n",
    "    \n",
    "    def game_over(self):\n",
    "        return self.is_terminal(self.pos)\n",
    "    \n",
    "    def next_state(self,s,a):\n",
    "        # return next state should we take action a\n",
    "        # but do not take that action\n",
    "        i,j = s\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(s,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        return (i,j)\n",
    "    \n",
    "    def move(self,a):\n",
    "        # return next state should we take action a\n",
    "        # this time we actually make the move\n",
    "        # and thus return reward\n",
    "        i,j = self.pos\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(self.pos,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        self.pos = (i,j)\n",
    "        # if no reward return 0\n",
    "        return self.rewards.get(self.pos,0)\n",
    "    \n",
    "    # lets add methods to print values and actions\n",
    "    def print_values(self,V):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*9*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                v = V[i,j]\n",
    "                if v >= 0:\n",
    "                    # if 0 or +ve, add space so it aligns well when we have -\n",
    "                    print(\" %.2f|\" %v,end=\"\")\n",
    "                else:\n",
    "                    print(\"%.2f|\" %v,end=\"\")\n",
    "            print(\"\")\n",
    "    \n",
    "    def print_policy(self,P):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*6*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                print(\" %s |\" %P.get((i,j),' '),end=\"\")\n",
    "            print(\"\")\n",
    "                \n",
    "    def state_to_loc(self,state): # convert state tupe (0,1) to location 1\n",
    "        return state[0]*self.rows + state[1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standrad_grid():\n",
    "    g = GridWorld(3,4,(2,0))\n",
    "    rewards = {(0,3):1,(1,3):-1}\n",
    "    actions = {\n",
    "        (0,0): ('D','R'),\n",
    "        (0,1): ('L','R'),\n",
    "        (0,2): ('L','D','R'),\n",
    "        (1,0): ('U','D'),\n",
    "        (1,2): ('U','D','R'),\n",
    "        (2,0): ('U','R'),\n",
    "        (2,1): ('L','R'),\n",
    "        (2,2): ('L','R','U'),\n",
    "        (2,3): ('L','U')\n",
    "    }\n",
    "    g.set_actions_rewards(actions,rewards)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = standrad_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets build the transition probabilities\n",
    "def build_transition_probs(g):\n",
    "    # we want to calculate p(s'|s,a)\n",
    "    # if there is an action a to take us from s to s'\n",
    "    # then p(s' | s,a) is 1\n",
    "    # else it is 0\n",
    "    # recall that our enviroment is deterministic\n",
    "    # for each action, there is only one state to go to\n",
    "    all_states = g.all_states()\n",
    "    all_actions = ['U','R','D','L']\n",
    "    probs_env = {}\n",
    "    for s in all_states:\n",
    "        for a in all_actions:\n",
    "            s_prime = g.next_state(s,a)\n",
    "            probs_env[(s_prime,s,a)] = 1\n",
    "    \n",
    "    return probs_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = build_transition_probs(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed policy (deterministic)\n",
    "policy = {\n",
    "    (2,0) : 'U',\n",
    "    (1,0) : 'U',\n",
    "    (0,0) : 'R',\n",
    "    (0,1) : 'R',\n",
    "    (0,2) : 'R',\n",
    "    (1,2) : 'U',\n",
    "    (2,1) : 'R',\n",
    "    (2,2) : 'U',\n",
    "    (2,3) : 'L'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | U |   |\n",
      "------------------\n",
      " U | R | U | L |\n"
     ]
    }
   ],
   "source": [
    "g.print_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pi(a|s)\n",
    "def build_action_probs(policy):\n",
    "    all_states = g.all_states()\n",
    "    state_space = len(all_states)\n",
    "    pi = {}\n",
    "    for s in all_states:\n",
    "        if not g.is_terminal(s):\n",
    "            a = policy.get(s)\n",
    "            pi[a,s] = 1\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = build_action_probs(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we perform policy evaluation\n",
    "def policy_eval(g):\n",
    "    all_states = g.all_states()\n",
    "    V = np.zeros((g.rows,g.cols))\n",
    "    all_actions = ['U','R','D','L']\n",
    "    delta = 0\n",
    "    thresh = 1e-5\n",
    "    gamma = 0.9\n",
    "    \n",
    "    i = 1\n",
    "    while True:\n",
    "        # this is just to calculate delta\n",
    "        V_old = V.copy()\n",
    "        for s in all_states:\n",
    "            v_new = 0\n",
    "            for a in all_actions:\n",
    "                for s_prime in all_states:\n",
    "                    v_new += pi.get((a,s),0)*p.get((s_prime,s,a),0)*(g.rewards.get(s_prime,0) + gamma*V[s_prime])\n",
    "            V[s] = v_new\n",
    "        delta = np.max(np.abs(V_old-V))\n",
    "        print('iteration: ',i,' delta: ',delta)\n",
    "        print('Values: ')\n",
    "        g.print_values(V)\n",
    "        print('')\n",
    "        if delta < thresh:\n",
    "            break\n",
    "        i += 1\n",
    "        V_old = V.copy()\n",
    "    return V             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1  delta:  1.0\n",
      "Values: \n",
      "---------------------------\n",
      " 0.00| 0.00| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.81| 0.73|\n",
      "\n",
      "iteration:  2  delta:  0.9\n",
      "Values: \n",
      "---------------------------\n",
      " 0.00| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.73| 0.81| 0.73|\n",
      "\n",
      "iteration:  3  delta:  0.81\n",
      "Values: \n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "\n",
      "iteration:  4  delta:  0.0\n",
      "Values: \n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "V = policy_eval(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our algorithm converges quite fast\n",
    "# we can see that the values make sense \n",
    "# even if we dont check them by hand (feel free to do so)\n",
    "# of course the value right beside the winning state is just 1\n",
    "# since a reward of 1 is obtained at the winning state\n",
    "# one step away from that is 0.9, since gamma is 0.9\n",
    "# one step away from 0.9 is 0.81, since 0.9 x 0.9 = 0.81\n",
    "# and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to look at a slightly more complex version of grid world called Windi Gridworld\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Windy Gridworld in code</h3>\n",
    "\n",
    "Previously, our Gridworld was deterministic, meaning all of the transition probabilities were either zero or one \n",
    "\n",
    "In  Windy Gridworld, we extend this idea so that the transition probabilities can be anything\n",
    "\n",
    "Unfortunately, this code is quite a bit messier than the deterministic Gridworld code, since we have to specify all the probabilities\n",
    "\n",
    "Let's start again with a kind of test driven development approach where we discuss how we would like the code to work before looking at the actual implementation\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Representing transition probabilities</h3>\n",
    "\n",
    "So for windy Gridworld, the main new data structure we want is something to represent the transition probabilities\n",
    "\n",
    "Again, this will vary widely depending on our personal design choices, but the instructor's design was to represent the probabilities as a dictionary\n",
    "\n",
    "The dictionary is structured as follows\n",
    "\n",
    "Similar to our previous code, we've decided that only the next states may be probabilistic\n",
    "\n",
    "The rewards will be a deterministic function of the state\n",
    "\n",
    "Therefore, we only need to concern ourselves with the current state, action and the next state\n",
    "\n",
    "The key to the dictionary will be a tuple of state and action\n",
    "\n",
    "These represent $s$ and $a$. ```key:(s,a)``` \n",
    "\n",
    "The value in the dictionary will be another dictionary containing the next state probabilities \n",
    "\n",
    "Inside this nested dictionary, the key will be the possible next state and the value will be the probability of going there, ```value:{s':prob}```\n",
    "\n",
    "For example, a dictionary with the key ```(0,2)``` and value ```0.7``` means that we will go to state ```(0,2)``` with probability ```0.7```    \n",
    " \n",
    "\n",
    "If there's another key ```(1,3)``` with the value ```0.3```, that means we will go to state ```(1,3)``` with probability ```0.3```\n",
    "\n",
    "```python\n",
    "probs = {\n",
    "    ((2,0),'U') : {(1,0):1.0},\n",
    "    ((1,2),'U'):{(0,2):0.7,(1,3):0.3}\n",
    "}\n",
    "```\n",
    "\n",
    "Of course, these probabilities must sum to one in order to represent a valid distribution\n",
    "\n",
    "Note that we can also have only a single possible next state with probability one\n",
    "\n",
    "This just means that the transition is deterministic\n",
    "\n",
    "---\n",
    "\n",
    "In our version of Windy Gridworld, the list of probabilities is quite long, but we are encouraged to look at it carefully in case the instructor made any errors (instructor's request)\n",
    "\n",
    "Creating this dictionary is repetitive work, so errors are very easy to make\n",
    "\n",
    "We'll notice that most of the grid is still deterministic\n",
    "\n",
    "It's just represented in probabilistic form\n",
    "\n",
    "```python\n",
    "probs = {\n",
    "    ((2,0),'U'):{(1,0):1.0},\n",
    "    ((2,0),'D'):{(2,0):1.0},\n",
    "    ...\n",
    "    ((1,2),'U'): {(0,2):0.5, (1,3):0.5}\n",
    "}\n",
    "```\n",
    "\n",
    "The relevant part of this dictionary, which is what makes the windedness interesting, is the state right beside the losing state \n",
    "\n",
    "For this state, we obviously want to go up since that brings us closer to the winning state\n",
    "\n",
    "However, we've made the transition probabilistic so that even if our action is up, we still have\n",
    "some probability of going to the right\n",
    "\n",
    "This makes it so that going in this direction is less safe than simply going around along the left wall to get to the goal state\n",
    "\n",
    "If we recall, the path from the starting position to the goal state is the same whether we go up, up, right, right , right or  right, right, up, up, right \n",
    "\n",
    "---\n",
    "\n",
    "<h3>Instantiating Windy Gridworld</h3>\n",
    "\n",
    "To continue on with our test driven approach, the way we would like to use our dictionary of transition probabilities is like this\n",
    "\n",
    "It's very similar to before where we instantiate a windy Gridworld object with the number of rows and columns and the start position \n",
    "\n",
    "In the set function, instead of just the rewards dictionary and the actions dictionary, we also pass in the state transition probabilities\n",
    "\n",
    "```python\n",
    "g = WindyGrid(3,4,(2,0))\n",
    "rewards = {(0,3):1,(1,3):-1}\n",
    "actions = {...}\n",
    "probs = {...}\n",
    "g.set(rewards,actions,probs)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Other functions</h3>\n",
    "\n",
    "Other than this, our API should be the same as before\n",
    "\n",
    "We still want to have a move function which takes in an action and returns a reward\n",
    "\n",
    "We still want a current state function to return the current state\n",
    "\n",
    "We still want and is terminal function, a game over function and an all states function, which we've seen before, so you know how these might be used\n",
    "\n",
    "One function we will not have is the ```get_next_state``` function\n",
    "\n",
    "As we recall, this takes in a current state $s$, action $a$ and returns the next state that doing this action will land us in\n",
    "\n",
    "As a quiz question, think about why we will not have this function for Windy Gridworld ?\n",
    "\n",
    "```python\n",
    "g = WindyGrid(rows,cols,start_position)\n",
    "g.move(action)\n",
    "g.current_state()\n",
    "g.is_terminal(state)\n",
    "g.game_over()\n",
    "g.all_states()\n",
    "\n",
    "# why not g.next_state(state,action)?\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<h3>get_next_state(state,action)</h3>\n",
    "\n",
    "So the reason we won't have a function called ```get_next_state``` for windy Gridworld is because having a function like this doesn't make sense any longer\n",
    "\n",
    "The next state is probabilistic, given a state and an action\n",
    "\n",
    "Therefore, we can't possibly return a single next state\n",
    "\n",
    "Now that you understand how we will use Wendy Grid World, let's dive into the implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindyGridWorld:\n",
    "    def __init__(self,rows,cols,start_pos):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.pos = start_pos\n",
    "    \n",
    "    def current_state(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def set_state(self,state):\n",
    "        self.pos = state\n",
    "        \n",
    "    def set_actions_rewards_probs(self,actions,rewards,probs):\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        # we need probs to know where to go after taking action\n",
    "        self.probs = probs\n",
    "        \n",
    "    def all_states(self,):\n",
    "        return [(i,j) for i in range(self.rows) for j in range(self.cols)]\n",
    "    \n",
    "    def is_terminal(self,state):\n",
    "        # we cant perform an action in a terminal state\n",
    "        # so we expect not to find it in the keys\n",
    "        return state not in self.actions.keys()\n",
    "    \n",
    "    def game_over(self):\n",
    "        return self.is_terminal(self.pos)\n",
    "    \n",
    "    def move(self,a):\n",
    "        # this time we sample a next state\n",
    "        next_state_2_probs = self.probs(self.pos,a)\n",
    "        next_states = list(next_state_2_probs.keys())\n",
    "        probs = list(next_state_2_probs.values())\n",
    "        next_state = np.random.choice(next_states,p=probs)\n",
    "        self.pos = next_state\n",
    "        return self.rewards.get(self.pos,0)\n",
    "    \n",
    "    # lets add methods to print values and actions\n",
    "    def print_values(self,V):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*9*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                v = V[i,j]\n",
    "                if v >= 0:\n",
    "                    # if 0 or +ve, add space so it aligns well when we have -\n",
    "                    print(\" %.2f|\" %v,end=\"\")\n",
    "                else:\n",
    "                    print(\"%.2f|\" %v,end=\"\")\n",
    "            print(\"\")\n",
    "    \n",
    "    def print_policy(self,P):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*6*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                print(\" %s |\" %P.get((i,j),' '),end=\"\")\n",
    "            print(\"\")\n",
    "                \n",
    "    def state_to_loc(self,state): # convert state tupe (0,1) to location 1\n",
    "        return state[0]*self.rows + state[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windy_grid():\n",
    "    g = WindyGridWorld(3,4,(2,0))\n",
    "    rewards = {(0,3):1,(1,3):-1}\n",
    "    actions = {\n",
    "        (0,0): ('D','R'),\n",
    "        (0,1): ('L','R'),\n",
    "        (0,2): ('L','D','R'),\n",
    "        (1,0): ('U','D'),\n",
    "        (1,2): ('U','D','R'),\n",
    "        (2,0): ('U','R'),\n",
    "        (2,1): ('L','R'),\n",
    "        (2,2): ('L','R','U'),\n",
    "        (2,3): ('L','U')\n",
    "    }\n",
    "    probs = {\n",
    "        ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "        ((2, 0), 'D'): {(2, 0): 1.0},\n",
    "        ((2, 0), 'L'): {(2, 0): 1.0},\n",
    "        ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "        ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "        ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "        ((1, 0), 'L'): {(1, 0): 1.0},\n",
    "        ((1, 0), 'R'): {(1, 0): 1.0},\n",
    "        ((0, 0), 'U'): {(0, 0): 1.0},\n",
    "        ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "        ((0, 0), 'L'): {(0, 0): 1.0},\n",
    "        ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "        ((0, 1), 'U'): {(0, 1): 1.0},\n",
    "        ((0, 1), 'D'): {(0, 1): 1.0},\n",
    "        ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "        ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "        ((0, 2), 'U'): {(0, 2): 1.0},\n",
    "        ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "        ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "        ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "        ((2, 1), 'U'): {(2, 1): 1.0},\n",
    "        ((2, 1), 'D'): {(2, 1): 1.0},\n",
    "        ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "        ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "        ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "        ((2, 2), 'D'): {(2, 2): 1.0},\n",
    "        ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "        ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "        ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "        ((2, 3), 'D'): {(2, 3): 1.0},\n",
    "        ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "        ((2, 3), 'R'): {(2, 3): 1.0},\n",
    "        ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},\n",
    "        ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "        ((1, 2), 'L'): {(1, 2): 1.0},\n",
    "        ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "      }\n",
    "    g.set_actions_rewards_probs(actions,rewards,probs)\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = windy_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another change is that we will introduce a probabilistic policy\n",
    "# recall that starting at position (2,0)\n",
    "# we had two paths to the winning state, UURRR or RRUUR\n",
    "# so we give a prob 0f 0.5 to go R and 0.5 to go up\n",
    "# very similar to our transitions (probs)\n",
    "policy = {\n",
    "    (2,0) : {'U':0.5,'R':0.5},\n",
    "    (1,0) : {'U':1},\n",
    "    (0,0) : {'R':1},\n",
    "    (0,1) : {'R':1},\n",
    "    (0,2) : {'R':1},\n",
    "    (1,2) : {'U':1},\n",
    "    (2,1) : {'R':1},\n",
    "    (2,2) : {'U':1},\n",
    "    (2,3) : {'L':1}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      " {'R': 1} | {'R': 1} | {'R': 1} |   |\n",
      "------------------\n",
      " {'U': 1} |   | {'U': 1} |   |\n",
      "------------------\n",
      " {'U': 0.5, 'R': 0.5} | {'R': 1} | {'U': 1} | {'L': 1} |\n"
     ]
    }
   ],
   "source": [
    "g.print_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the policy is now pretty ugly\n",
    "# one more thing for my future self to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here lets follow the same steps\n",
    "# lets also calculate rewards r\n",
    "\n",
    "def build_transition_probs_and_rewards(g):\n",
    "    # probs_env : p(s'|s,a)\n",
    "    # r : rewards @ s'\n",
    "    probs = g.probs\n",
    "    probs_env = {}\n",
    "    r = {}\n",
    "    for (s,a),next_state_2_probs in probs.items():\n",
    "        for s_prime,prob in next_state_2_probs.items():\n",
    "            probs_env[s_prime,s,a] = prob\n",
    "            r[s_prime] = g.rewards.get(s_prime,0)\n",
    "    return probs_env,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,r = build_transition_probs_and_rewards(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_action_probs(policy):\n",
    "    # pi : pi(a|s)\n",
    "    pi = {}\n",
    "    for s,a2prob in policy.items():\n",
    "        for a,prob in a2prob.items():\n",
    "            pi[a,s] = prob\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = build_action_probs(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we perform policy evaluation\n",
    "def policy_eval(g):\n",
    "    all_states = g.all_states()\n",
    "    V = np.zeros((g.rows,g.cols))\n",
    "    all_actions = ['U','R','D','L']\n",
    "    delta = 0\n",
    "    thresh = 1e-5\n",
    "    gamma = 0.9\n",
    "    \n",
    "    i = 1\n",
    "    while True:\n",
    "        # this is just to calculate delta\n",
    "        V_old = V.copy()\n",
    "        for s in all_states:\n",
    "            v_new = 0\n",
    "            for a in all_actions:\n",
    "                for s_prime in all_states:\n",
    "                    v_new += pi.get((a,s),0)*p.get((s_prime,s,a),0)*(g.rewards.get(s_prime,0) + gamma*V[s_prime])\n",
    "            V[s] = v_new\n",
    "        delta = np.max(np.abs(V_old-V))\n",
    "        print('iteration: ',i,' delta: ',delta)\n",
    "        print('Values: ')\n",
    "        g.print_values(V)\n",
    "        print('')\n",
    "        if delta < thresh:\n",
    "            break\n",
    "        i += 1\n",
    "        V_old = V.copy()\n",
    "    return V             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1  delta:  1.0\n",
      "Values: \n",
      "---------------------------\n",
      " 0.00| 0.00| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00|-0.05| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00|-0.04|-0.04|\n",
      "\n",
      "iteration:  2  delta:  0.9\n",
      "Values: \n",
      "---------------------------\n",
      " 0.00| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00|-0.05| 0.00|\n",
      "---------------------------\n",
      " 0.00|-0.04|-0.04|-0.04|\n",
      "\n",
      "iteration:  3  delta:  0.81\n",
      "Values: \n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-0.05| 0.00|\n",
      "---------------------------\n",
      " 0.31|-0.04|-0.04|-0.04|\n",
      "\n",
      "iteration:  4  delta:  0.0\n",
      "Values: \n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-0.05| 0.00|\n",
      "---------------------------\n",
      " 0.31|-0.04|-0.04|-0.04|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "V = policy_eval(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we look at the value, we can see that it still converges quite fast\n",
    "# the important part is making sure these values make sense\n",
    "# In the previous script the values were symmetric for either paths to the goal\n",
    "# this time they are not symmetric\n",
    "# due to the fact that the state transition beside the losing state is now probabilistic\n",
    "# we'll recall that even if we select the action to go up\n",
    "# the enviroment dynamic dicdate that we still have a 50% chance of ending up in the losing state\n",
    "# feel free to do the calculation on paper to check\n",
    "# One thing that might seem strange is that, one step awawy from -0.05 is -0.04 but one step away from that -0.04\n",
    "# we might think why doesnt this value decrease in magnitude since we have a gamma of 0.9\n",
    "# in fact it does decrease but we dont see it since we are cutting of first 2 decimal places only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.81    ,  0.9     ,  1.      ,  0.      ],\n",
       "       [ 0.729   ,  0.      , -0.05    ,  0.      ],\n",
       "       [ 0.309825, -0.0405  , -0.045   , -0.0405  ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll be continuing our discussion of dynamic programming\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Dynamic Programming Continued</h3>\n",
    "\n",
    "Let's recall again the two tasks that we care about in reinforcement learning \n",
    "\n",
    "Task number one is given a policy, tell me the value of that policy\n",
    "\n",
    "Task number two is given an environment, tell me the best policy \n",
    "\n",
    "Note that we just solved task number one in the previous sections\n",
    "\n",
    "So in this section, we'll start on solving task number two\n",
    "\n",
    "The main principle we need for solving task number two is an idea called <strong>policy improvement</strong>\n",
    "\n",
    "This answers the question, given a policy, how do we find a better policy?\n",
    "\n",
    "We can imagine that if we've able to answer this question, then I've solved the problem\n",
    "\n",
    "This is because if we can find a better policy, given an existing policy, then we can just keep iterating on our policy \n",
    "\n",
    "Since each step leads to a better policy we will have a monotonically increasing improvement in policies\n",
    "\n",
    "So how does policy improvement work?\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Improvement</h3>\n",
    "\n",
    "Let's begin with what we know so far and then just make a very tiny change\n",
    "\n",
    "We'll start with a given policy $\\pi$ \n",
    "\n",
    "From the previous lectures, we now know how to find its value function $V_\\pi(s)$\n",
    "\n",
    "Now, suppose that we just consider one single state $s$ \n",
    "\n",
    "And suppose that when we are in this state $s$, instead of following the policy, we decide to do something else\n",
    "\n",
    "Suppose that we take some action $a$ which is not the same as the action we would have taken according to the policy $\\pi$\n",
    "\n",
    "Now how can we find the value of this action if we were to perform the action in this state and then follow our given policy thereafter?\n",
    "\n",
    "Of course, this is what the action value $Q$ tells us\n",
    "\n",
    "$Q_\\pi(s,a)$ tells us the value of doing action $a$ while in state $s$ and then following the policy $\\pi$ thereafter\n",
    "\n",
    "Intuitively, we can see that if $Q_\\pi(s,a) > V_\\pi(s)$, then our return for this episode  is expected to be better than if we had just followed $\\pi$ the whole time\n",
    "\n",
    "The subtle thing to notice is that what we are talking about here is just a single action out of an entire series of actions\n",
    "\n",
    "Imagine playing a game from start to end\n",
    "\n",
    "As we recall, we call this an episode\n",
    "\n",
    "Essentially, an episode is just a series of states, actions and rewards in a sequence\n",
    "\n",
    "So we play one episode and on just this one action, we decide to deviate from our existing policy\n",
    "\n",
    "The $Q$ function tells us that if the value for this action is greater than $V_\\pi$, then making just this one change will have improved our expected return\n",
    "\n",
    "---\n",
    "\n",
    "<h3>How do we find a better action?</h3>\n",
    "\n",
    "The next question to answer is this \n",
    "\n",
    "Suppose that we have some state $s$ and we are interested in finding some better action\n",
    "\n",
    "We know that we can look at $Q$ to tell us whether or not a new action will be better than the current policy\n",
    "\n",
    "So how can we find such an action?\n",
    "\n",
    "Better yet, how can we find the best action?\n",
    "\n",
    "Well, here's a simple solution\n",
    "\n",
    "Why not just look through the $Q$ table over all  possible actions from this given state $s$ and then pick the one that gives us the maximum $Q$?\n",
    "\n",
    "As we may recall, this is called the $\\arg \\max$\n",
    "\n",
    "The best action to perform in the state would be \n",
    "\n",
    "$$\\large a^* = \\arg \\max_a Q_\\pi(s,a)$$\n",
    "\n",
    "And again, this is just a change to one single action over an entire episode\n",
    "\n",
    "And then we follow the prescribed policy $\\pi$ thereafter\n",
    "\n",
    "Let's call the action we chose $a^*$\n",
    "\n",
    "So if $Q_\\pi(s,a^*) > V_\\pi(s)$ events than we have chosen the best action from the state $s$ that improves the expected return\n",
    "\n",
    "---\n",
    "\n",
    "<h3>A Subtly Different Question</h3>\n",
    "\n",
    "So we have just seen how deviating from our policy by performing just one action differently during an episode can improve our expected return\n",
    "\n",
    "The next question to consider is this\n",
    "\n",
    "We know that it's possible to encounter the same state twice or more during an episode \n",
    "\n",
    "so we can ask a similar but subtly different question\n",
    "\n",
    "What if we perform this other action not just once, but every time we visit that state?\n",
    "\n",
    "In this case, we have created a new policy because the action we prescribed to this specific state is now different than what it was before\n",
    "\n",
    "So let's call this new policy $\\pi^\\prime(s)$\n",
    "\n",
    "Our original given policy was $\\pi(s)$\n",
    "\n",
    "Now, this is a very subtle difference from what we were discussing before\n",
    "\n",
    "Previously, we considered the case where we changed the action only once and then followed the given policy thereafter\n",
    "\n",
    "This is different because now we are saying every time we see the state $s$, we are going to perform a different action given by $\\pi^\\prime$\n",
    "\n",
    "In this case, we do not follow the prescribed policy by thereafter\n",
    "\n",
    "---\n",
    "\n",
    "The next question to answer is this, is $\\pi^\\prime(s)$ a better policy than $\\pi(s)$?\n",
    "\n",
    "Now, this might seem obvious, but the reasoning is subtle and it's actually not obvious if we think about it\n",
    "\n",
    "Suppose that we choose one single state to change the action\n",
    "\n",
    "We look at $Q_\\pi(s,a)$ and we take the $\\arg \\max$ over all actions $a$\n",
    "\n",
    "Then we say, our new policy, $\\pi^\\prime$, will replace the existing action with this new action\n",
    "\n",
    "Suppose that all other state action mappings in the policy remain the same\n",
    "\n",
    "Previously we showed that if we only change the action once and follow the given policy thereafter, this leads to an improvement\n",
    "\n",
    "This just follows directly from the Bellman equation for $Q$ \n",
    "\n",
    "$$\\large Q_\\pi(s,a) = \\sum_{s^\\prime}\\sum_r p(s^\\prime,r \\vert s,a) \\{r + \\gamma V_\\pi(s^\\prime)\\}$$\n",
    "\n",
    "The right hand side contains $V_\\pi$, which means that we have to follow the existing policy $\\pi$ for this equation to hold\n",
    "\n",
    "But now we are asking a slightly different question\n",
    "\n",
    "Now the right hand side no longer applies because we are no longer following $\\pi$, but we've created a new policy $\\pi^\\prime$\n",
    "\n",
    "So hopefully we can see that, in fact, it's not obvious that the value for the new policy is better than that of the old policy\n",
    "\n",
    "We actually do not have any equation telling us that this should be the case\n",
    "\n",
    "Help : First we need to understand $Q$,  lets assume for simplicity that our enviroment is fully deterministic, we then express $Q$ as $Q(s,a) = r + \\gamma V_\\pi(s^\\prime)$, this means that the expected sum of future rewards when in state $s$ and doing action $a$ and arriving at state $s^\\prime$ is simply the reward we get for arriving at $s^\\prime$ + the sum of the future rewards when we are in $s^\\prime$, again since everything is deterministic no need to sum over $s^\\prime$ or $r$\n",
    "\n",
    "The idea is, for once we can ask, if we are in state $s$, which action $a^*$ gives us the highest value $Q_\\pi(s,a)$, obviously this action is the best action to perform now\n",
    "\n",
    "But if so, why dont we keep doing this forever?\n",
    "\n",
    "The idea is that, when calculating $Q_\\pi(s,a)$ to show that $a^*$ is the best action, we used $V_\\pi(s^\\prime)$, this means that we assumed that we will follow the poilicy $\\pi$ once we have reached state $s^\\prime$\n",
    "\n",
    "So our deduction that $a^*$ is the best action is based on the assumption that we will follow $\\pi$ thereafter, again this is mathematically manifested in the usage of $V_\\pi$, now once we reach $s^\\prime$ we have to abide by our assumption, so this result is only valid provided that we follow $\\pi$ thereafter \n",
    "\n",
    "Its as if we have promised th follow $\\pi$ thereafter to get $a^*$ in return\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Improvement Theorem</h3>\n",
    "\n",
    "Luckily, it turns out to be true, the expected return for following $\\pi^\\prime$ is better than the expected return for following $\\pi$\n",
    "\n",
    "And note that when we say better than, we're just using casual words for $\\ge$\n",
    "\n",
    "And this is the case when we choose $\\pi^\\prime$ as described previously\n",
    "\n",
    "Now, the proof of this is outside the scope of this notebook\n",
    "\n",
    "This is one of those rare situations where we don't think it adds anything to the intuition and in fact, we think it takes away from it\n",
    "\n",
    "But if we're curious, we are encouraged to think about it ourselves\n",
    "\n",
    "Help : We know that we depend on $V_\\pi$, intuitevly we can just follow $\\pi$ to get the rewards as expected, once we reach state $s$ again, we know that the future rewards from this point are higher if we chose $a^*$, so we can rechose $a^*$\n",
    "\n",
    "What this is called is the policy improvment theorem\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy</h3>\n",
    "\n",
    "So to state it more formally, the policy improvement theorem is this\n",
    "\n",
    "It says that, suppose we choose some different policy $\\pi^\\prime(s)$ for a given state $s$\n",
    "\n",
    "Remember that for now we are just considering one single state\n",
    "\n",
    "Suppose that \n",
    "\n",
    "$$\\large \\text{If } Q_\\pi(s,\\pi^\\prime(s)) \\ge V_\\pi(s)$$\n",
    "\n",
    "So we're following the same process we described before \n",
    "\n",
    "We are choosing some different action, $\\pi^\\prime(s)$ of events thereafter we continue to follow the policy $\\pi$\n",
    "\n",
    "Thats why both $Q$ and $V$ are subscripts by $\\pi$\n",
    "\n",
    "The policy improvement theorem says that if this is true, then it is also true that \n",
    "\n",
    "$$\\large \\text{Then } V_{\\pi^\\prime} \\ge V_\\pi(s) \\text{ for all } s \\in S$$\n",
    "\n",
    "\n",
    "And therefore $\\pi^\\prime$ is a better policy than $\\pi$\n",
    "\n",
    "So note the subtle distinction between these two statements\n",
    "\n",
    "We're saying that if this action was better to change once for the state $s$, then it's also better to change it to this action every time we visit the state $s$\n",
    "\n",
    "---\n",
    "\n",
    "Also note that if we have a strict inequality when we change the action once, then we'll have a strict inequality when we change the action permanently\n",
    "\n",
    "This means that if we find some action such that $Q_\\pi$ is greater then but not equal to $N_\\pi$, then updating our policy with this new action will lead to a new value function $V^\\prime_\\pi$ that is strictly greater than $V_\\pi$ \n",
    "\n",
    "$$\\large \\text{If} Q_\\pi(s,\\pi^\\prime(s)) \\boxed{>} V_\\pi(s)$$\n",
    "\n",
    "$$\\large \\text{Then } V_{\\pi^\\prime} \\boxed{>} V_\\pi(s) \\text{ for all } s \\in S$$\n",
    "\n",
    "note : the $\\text{ for all } s \\in S$ is for both lines\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Improvement</h3>\n",
    "\n",
    "The next question to consider is this, up until now, we've been considering what happens if we change the action for some individual state $s$\n",
    "\n",
    "But let's say we perform this process for every state $s$ in the states because this is the process of policy improvement\n",
    "\n",
    "We are given some policy $\\pi$ and we would like to improve it\n",
    "\n",
    "Let's also assume we've computed $Q_\\pi$ and $V_\\pi$ for all states and all actions\n",
    "\n",
    "We know how to do this because this is just policy evaluation\n",
    "\n",
    "So basically, we can think of policy improvement in pseudocode form\n",
    "\n",
    "$\\text{Let S = set of all states, except terminal state}$\n",
    "\n",
    "$\\text{for } s \\in S:$\n",
    "\n",
    "$\\qquad \\pi^\\prime(s) = \\arg \\max_a Q_\\pi(s,a)$\n",
    "\n",
    "$\\text{for } s \\in S:$\n",
    "\n",
    "$\\qquad \\pi^\\prime(s) = \\arg \\max_a \\sum\\limits_{s^\\prime}\\sum\\limits_r p(s^\\prime,r \\vert s,a) \\{r + \\gamma V_\\pi(s^\\prime)\\}$\n",
    "\n",
    "First, we have a loop that iterates over all states in the state space except for the terminal state\n",
    "\n",
    "Since we never take any actions from the terminal state, it is irrelevant \n",
    "\n",
    "Now for each of these actions, we take the $\\arg \\max$ over all actions of $Q_\\pi(s,a)$\n",
    "\n",
    "Now suppose that we like working with $V$ instead of $Q$ \n",
    "\n",
    "In this case we can replace $Q$ with the right hand side of the Bellman equation\n",
    "\n",
    "OK, so this is the process of policy improvement\n",
    "\n",
    "It basically means you improve the action for each state overall states\n",
    "\n",
    "And by performing this process, we have improved our policy not just over a single state but overall states\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Convergance</h3>\n",
    "\n",
    "Let's now return to this concept of convergence\n",
    "\n",
    "We can first recognise that by following the process of policy improvement, it's not possible for the policy to get worse\n",
    "\n",
    "Therefore, the policy will always improve monotonically\n",
    "\n",
    "Now, let's suppose that we reach a point where we perform the policy improvement process, and it turns out that the new policy is the same as the old policy, $\\pi(s) = \\pi^\\prime(s)$, then that must mean that $V_\\pi(s) = V_{\\pi^\\prime}(s)$\n",
    "\n",
    "We can write this an equation form simply by plugging in what we had on the previous subsections for the policy improvement assignment\n",
    "\n",
    "$$\\large V_\\pi(s) = \\max_a \\sum_{s^\\prime} \\sum_r p(s^\\prime,r \\vert s,a) \\{r + \\gamma V_\\pi(s^\\prime)\\}$$\n",
    "\n",
    "In this case, we are asking what happens when both sides are equal?\n",
    "\n",
    "Well, you may recognize this from the previous notebooks\n",
    "\n",
    "This is called the Bellman Optimality Equation\n",
    "\n",
    "note : $V_\\pi(s) \\text{ is also called } V^*(s)$\n",
    "\n",
    "So if we can reach this point where the Bellman optimality equation is satisfied, then we found the optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll get one step closer to applying the policy improvement principle in practice\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Iteration</h3>\n",
    "\n",
    "So far, we've learned how to improve a given policy\n",
    "\n",
    "Interestingly, the input to this process is a policy, and the outputs of this process is also a policy $\\pi^\\prime = \\text{PolicyImprovement}(\\pi)$\n",
    "\n",
    "The output policy happens to be better than the input policy\n",
    "\n",
    "Now, consider what would happen if we applied the process again\n",
    "\n",
    "Well, reason should tell us that the result would be an even better policy  $\\pi^{\\prime\\prime} = \\text{PolicyImprovement}(\\pi)$\n",
    "\n",
    "If we apply the process again an even better policy $\\pi^{\\prime\\prime\\prime} = \\text{PolicyImprovement}(\\pi)$\n",
    "\n",
    "So this is the concept of policy iteration\n",
    "\n",
    "---\n",
    "\n",
    "Now, we have to zoom in a little bit, because there's one more step that we've almost forgotten about\n",
    "\n",
    "The process of policy improvement we described in the previous section requires us to find the value function\n",
    "\n",
    "Therefore, we know that policy evaluation should be part of this process\n",
    "\n",
    "Specifically, the full process is this\n",
    "\n",
    "Suppose we're given some initial random policy $\\pi_0$\n",
    "\n",
    "The next step is to evaluate this policy\n",
    "\n",
    "So we find $V_{\\pi0}$\n",
    "\n",
    "As we recall, this is needed for the policy improvement step\n",
    "\n",
    "Then we learned in the last section that we can improve the current policy by following the process of policy improvement \n",
    "\n",
    "This will give us $\\pi_1$\n",
    "\n",
    "In order to improve $\\pi_1$, we must first find its value function\n",
    "\n",
    "So we do another round of policy evaluation and this gives us $V_{\\pi1}$\n",
    "\n",
    "Clearly we can now improve this policy to get $\\pi_2$ which is better than $\\pi_1$\n",
    "\n",
    "Then if we want to improve $\\pi_2$, we have to find its value\n",
    "\n",
    "So we evaluate $\\pi_2$ and we get $V_{\\pi_2}$\n",
    "\n",
    "And obviously we can repeat this pattern until our policy stops changing\n",
    "\n",
    "This is the process of policy iteration\n",
    "\n",
    "$\\text{Given: initial random policy } \\pi_0$\n",
    "\n",
    "$V_{\\pi0} = \\text{PolicyEvaluation}(\\pi_0)$\n",
    "\n",
    "$\\pi_1 = \\text{PolicyImprovement}(V_{\\pi0})$\n",
    "\n",
    "$V_{\\pi1} = \\text{PolicyEvaluation}(\\pi_1)$\n",
    "\n",
    "$\\pi_{2} = \\text{PolicyImprovement}(V_{\\pi_1})$\n",
    "\n",
    "<img src='extras/55.7.PNG' width='200'><img>\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Iteration Pseudocode</h3>\n",
    "\n",
    "So we hope we'll agree that given what we've learned so far, the process of policy iteration is pretty straightforward \n",
    "\n",
    "It's nothing but the repetition of two previous concepts we already learned about\n",
    "\n",
    "Nonetheless, it's still helpful to walk through some pseudocode before we move on to implementing this ourselves\n",
    "\n",
    "Now, at a high level, this is probably pretty basic\n",
    "\n",
    "We start by initializing some random policy\n",
    "\n",
    "Then we have an infinite loop \n",
    "\n",
    "Inside this loop we simply have two steps\n",
    "\n",
    "Step number one is to evaluate the current policy\n",
    "\n",
    "Step number two is to perform policy improvement\n",
    "\n",
    "At this point, we can check if our new policy is the same as our old policy\n",
    "\n",
    "And if that's the case, we can exit the infinite loop\n",
    "\n",
    "```\n",
    "Initialise: random policy\n",
    "Loop:\n",
    "    V = evaluate policy\n",
    "    policy = improve policy using V\n",
    "    if new policy = old policy: break\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Iteration Pseudocode in Detail</h3>\n",
    "\n",
    "Let's now zoom in so that we can consider this in more detail $\\color{green}{\\text{Policy Evaluation}},\\color{blue}{\\text{Policy Improvement}}$\n",
    "\n",
    "\n",
    "$\\text{Inititalise random policy } \\pi \\\\ \\text{Loop:} \\\\ \\qquad \\color{green}{\\boxed{ \\text{Initialise: V(s) = 0 or random (except terminal s, where V(s) = 0)} \\\\ \\qquad  \\text{Loop:} \\\\ \\qquad  \\qquad \\Delta = 0 \\\\ \\qquad  \\qquad \\text{for s in all_non_terminal_states:} \\\\ \\qquad  \\qquad \\qquad v_{\\text{old}} = V(s) \\text{# store the existing value} \\\\ \\qquad  \\qquad \\qquad V(s) = \\sum_a \\pi(a \\vert s) \\sum_{s^\\prime} \\sum_r p(s^\\prime,r \\vert s,a)[r + \\gamma V(s^\\prime)] \\\\ \\qquad  \\qquad \\qquad \\Delta = max(\\Delta, \\vert v_{\\text{old}} - V(s) \\vert) \\\\ \\qquad  \\qquad if \\Delta < \\text{threshold}: \\\\ \\qquad  \\qquad \\qquad \\text{break}}} \\\\ \\color{blue}{\\qquad \\boxed{ \\text{is_policy_stable = true} \\\\ \\qquad \\text{for s in all_non_terminal_states:}\\\\ \\qquad \\qquad a_{\\text{old}} = \\pi(s) \\\\ \\qquad \\qquad \\pi(s) = \\arg \\max\\limits_a \\sum\\limits_{s^\\prime} \\sum_r p(s^\\prime,r \\vert s,a)[r + \\gamma V(s^\\prime)] \\\\ \\qquad \\qquad if a_{\\text{old}} \\neq \\pi(s): \\text{is_policy_stable=false} \\\\ \\qquad \\text{if is_policy_stable}: \\\\ \\qquad \\qquad break}}$\n",
    "\n",
    "Again, we start by initializing some random policy\n",
    "\n",
    "Step number one is to evaluate this policy, so let's write this out explicitly\n",
    "\n",
    "This is essentially just a copy of what we learned previously \n",
    "\n",
    "As before we start by initializing $V(s)$ and then proceeding with an infinite loop\n",
    "\n",
    "Next, we create a variable $\\Delta$ which will store the maximum change on each iteration\n",
    "\n",
    "Then we loop through all the states in the state space except for the terminal state\n",
    "\n",
    "Inside this loop, we save the old value of $V(s)$\n",
    "\n",
    "Then we compute the new value of $V(s)$ using the Bellman equation\n",
    "\n",
    "Then we update $\\Delta$ using the absolute difference between the new $V(s)$ and the old $V(s)$\n",
    "\n",
    "Outside the loop, we check whether or not Delta is less than our minimum threshold\n",
    "\n",
    "If it is, then we can break out of the loop and policy evaluation is complete\n",
    "\n",
    "Step number two, proceeds like this\n",
    "\n",
    "First, we create a flag that tells us whether or not our policy has changed\n",
    "\n",
    "We'll call this ```is_policy_stable```\n",
    "\n",
    "The initial value for this is true, but if any of the subsequent steps lead to a change, we will set it to false\n",
    "\n",
    "Next, we loop through all the states in the state space except for the terminal state \n",
    "\n",
    "Inside this loop, we save a copy of the action given by our existing policy for the state $s$\n",
    "\n",
    "We'll call this ```a_old```\n",
    "\n",
    "Then we perform policy improvement for the state using the ```arg max``` equation We learned previously.\n",
    "\n",
    "Note that instead of using ```Q```, we use ```V```\n",
    "\n",
    "This is because, as we recall, ```V``` is easier to compute than ```Q``` \n",
    "\n",
    "For ```V```, if we have `S` states, then we only need to hold it `S` values\n",
    "\n",
    "But if we have `A` actions, then we need `S x A` values to hold `Q` \n",
    "\n",
    "So using `V` is a bit more memory efficient\n",
    "\n",
    "Next, we check whether our improved action is the same as ```a_old```\n",
    "\n",
    "If this is not the case, then we set our `is_policy_stable` flag to false\n",
    "\n",
    "When we finish looping through all the states, we check whether or not there `is_policy_stable` flag has been set to false\n",
    "\n",
    "If that was not the case and the flag is still true, then we know we can quit because the policy has not changed\n",
    "\n",
    "Since the policy hasn't changed, the value will also not change\n",
    "\n",
    "And of course, if the policy has changed, then training is not yet complete and we loop back around to perform both steps once again\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Subtle Point</h3>\n",
    "\n",
    "Now, there's one extra subtle point to consider\n",
    "\n",
    "Recall that as we discussed before, optimal values are unique, but optimal policies are not unique\n",
    "\n",
    "This is because if one value function is better than another value function, then by definition only one of them can possibly be optimal\n",
    "\n",
    "On the other hand, two or more different policies can lead to the same value function\n",
    "\n",
    "Therefore, neither is better than the other, they are both optimal\n",
    "\n",
    "But what would happen if we performed our algorithm and we just kept switching back and forth between two distinct optimal policies?\n",
    "\n",
    "In this case, the loop would never terminate\n",
    "\n",
    "Therefore, we can add one extra condition for whether or not we should quit\n",
    "\n",
    "Specifically, if the policy is stable, then we know it's OK to quit\n",
    "\n",
    "But also if the value is stable, then we also know it's OK to quit\n",
    "\n",
    "This allows us to recognize that it's possible for more than one policy to be optimal, but if we find that the value is no longer changing, then whatever policy we have found stable or not is an acceptable answer\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Small Improvement</h3>\n",
    "\n",
    "So there's one slight opportunity for improvement in the preceding pseudocode\n",
    "\n",
    "Specifically, consider what will happen if the policy only changes by a small amount from one iteration to the next\n",
    "\n",
    "Say only one or two actions have changed \n",
    "\n",
    "In this case the value for the new policy will not be that different from the value of the old policy\n",
    "\n",
    "So it seems wasteful to start the policy evaluation process from scratch on each round\n",
    "\n",
    "Instead, we would like our initialization point to be close to where we want to end up \n",
    "\n",
    "To that end, instead of initializing `V` to a bunch of zeros or random numbers each time, it's more efficient to save the current `V` and use that as our initial value for the next round of policy evaluation\n",
    "\n",
    "In this way, we can utilize the fact that if `V` doesn't change that much, then policy evaluation will be completed in fewer steps\n",
    "\n",
    "We swill test this out in code and observe this for ourselves for the next excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets write policy iteration code \n",
    "# we will be using the deterministic GridWorld enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets implement our Gridworld enviroment\n",
    "# we will be using the same interface as described above\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,rows,cols,start_pos):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.pos = start_pos\n",
    "    \n",
    "    def current_state(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def set_state(self,state):\n",
    "        self.pos = state\n",
    "        \n",
    "    def set_actions_rewards(self,actions,rewards):\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        \n",
    "    def all_states(self,):\n",
    "        return [(i,j) for i in range(self.rows) for j in range(self.cols)]\n",
    "    \n",
    "    def is_terminal(self,state):\n",
    "        # we cant perform an action in a terminal state\n",
    "        # so we expect not to find it in the keys\n",
    "        return state not in self.actions.keys()\n",
    "    \n",
    "    def game_over(self):\n",
    "        return self.is_terminal(self.pos)\n",
    "    \n",
    "    def next_state(self,s,a):\n",
    "        # return next state should we take action a\n",
    "        # but do not take that action\n",
    "        i,j = s\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(s,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        return (i,j)\n",
    "    \n",
    "    def move(self,a):\n",
    "        # return next state should we take action a\n",
    "        # this time we actually make the move\n",
    "        # and thus return reward\n",
    "        i,j = self.pos\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(self.pos,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        self.pos = (i,j)\n",
    "        # if no reward return 0\n",
    "        return self.rewards.get(self.pos,0)\n",
    "    \n",
    "    # lets add methods to print values and actions\n",
    "    def print_values(self,V):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*9*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                v = V[i,j]\n",
    "                if v >= 0:\n",
    "                    # if 0 or +ve, add space so it aligns well when we have -\n",
    "                    print(\" %.2f|\" %v,end=\"\")\n",
    "                else:\n",
    "                    print(\"%.2f|\" %v,end=\"\")\n",
    "            print(\"\")\n",
    "    \n",
    "    def print_policy(self,P):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*6*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                print(\" %s |\" %P.get((i,j),' '),end=\"\")\n",
    "            print(\"\")\n",
    "                \n",
    "    def state_to_loc(self,state): # convert state tupe (0,1) to location 1\n",
    "        return state[0]*self.rows + state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standrad_grid():\n",
    "    g = GridWorld(3,4,(2,0))\n",
    "    rewards = {(0,3):1,(1,3):-1}\n",
    "    actions = {\n",
    "        (0,0): ('D','R'),\n",
    "        (0,1): ('L','R'),\n",
    "        (0,2): ('L','D','R'),\n",
    "        (1,0): ('U','D'),\n",
    "        (1,2): ('U','D','R'),\n",
    "        (2,0): ('U','R'),\n",
    "        (2,1): ('L','R'),\n",
    "        (2,2): ('L','R','U'),\n",
    "        (2,3): ('L','U')\n",
    "    }\n",
    "    g.set_actions_rewards(actions,rewards)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = standrad_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets build the transition probabilities\n",
    "def build_transition_probs_rewards(g):\n",
    "    # we want to calculate p(s'|s,a)\n",
    "    # if there is an action a to take us from s to s'\n",
    "    # then p(s' | s,a) is 1\n",
    "    # else it is 0\n",
    "    # recall that our enviroment is deterministic\n",
    "    # for each action, there is only one state to go to\n",
    "    all_states = g.all_states()\n",
    "    all_actions = ['U','R','D','L']\n",
    "    probs_env = {}\n",
    "    r = {}\n",
    "    for s in all_states:\n",
    "        for a in all_actions:\n",
    "            s_prime = g.next_state(s,a)\n",
    "            probs_env[(s_prime,s,a)] = 1\n",
    "        r[s] = g.rewards.get(s,0)\n",
    "\n",
    "    \n",
    "    return probs_env,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,r = build_transition_probs_rewards(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pi(a|s)\n",
    "def build_action_probs(policy):\n",
    "    all_states = g.all_states()\n",
    "    state_space = len(all_states)\n",
    "    pi = {}\n",
    "    for s in all_states:\n",
    "        if not g.is_terminal(s):\n",
    "            a = policy.get(s)\n",
    "            pi[a,s] = 1\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets follow the algorithm\n",
    "\n",
    "# we initialise V[s] to zeros here\n",
    "# since once we are inside we will be using the old values\n",
    "# initialise V(s) = 0\n",
    "all_states = g.all_states()\n",
    "state_space = len(all_states)\n",
    "V = np.zeros(state_space).reshape((g.rows,g.cols))\n",
    "\n",
    "\n",
    "all_actions = ['D','U','L','R']\n",
    "action_space = len(all_actions)\n",
    "threshold = 1e-5\n",
    "gamma = 0.9\n",
    "\n",
    "# initialise random policy\n",
    "policy = {s:all_actions[np.random.choice(action_space)] for s in all_states if not g.is_terminal(s)}\n",
    "\n",
    "\n",
    "# loop\n",
    "while True:\n",
    "    # skip initialisation we already did that\n",
    "    # next time we get here we will be using the old value\n",
    "    \n",
    "    # policy evaluation\n",
    "    pi = build_action_probs(policy)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_old = V.copy()\n",
    "        for s in all_states:\n",
    "            # continue only for non-termimal states\n",
    "            if g.is_terminal(s):\n",
    "                continue\n",
    "            v_new = 0\n",
    "            for a in all_actions:\n",
    "                for s_prime in all_states:\n",
    "                    v_new += pi.get((a,s),0)*p.get((s_prime,s,a),0)*(r.get(s_prime,0)+gamma*V[s_prime])\n",
    "            V[s] = v_new \n",
    "        delta = np.max(np.abs(V-V_old))\n",
    "        if delta < threshold:\n",
    "            break\n",
    "            \n",
    "    # policy improvement\n",
    "    policy_old = policy.copy()\n",
    "    for s in all_states:\n",
    "        action_values = []\n",
    "        if g.is_terminal(s):\n",
    "            continue\n",
    "        for a in all_actions:\n",
    "            action_value = 0\n",
    "            for s_prime in all_states:\n",
    "                action_value += p.get((s_prime,s,a),0)*(r.get(s_prime,0)+gamma*V[s_prime])\n",
    "            action_values.append(action_value)\n",
    "        policy[s] = all_actions[np.argmax(action_values)]\n",
    "    if policy == policy_old:\n",
    "        break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({(0, 0): 'R',\n",
       "  (0, 1): 'R',\n",
       "  (0, 2): 'R',\n",
       "  (1, 0): 'U',\n",
       "  (1, 2): 'U',\n",
       "  (2, 0): 'U',\n",
       "  (2, 1): 'R',\n",
       "  (2, 2): 'U',\n",
       "  (2, 3): 'L'},\n",
       " array([[0.81  , 0.9   , 1.    , 0.    ],\n",
       "        [0.729 , 0.    , 0.9   , 0.    ],\n",
       "        [0.6561, 0.729 , 0.81  , 0.729 ]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n"
     ]
    }
   ],
   "source": [
    "g.print_values(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | U |   |\n",
      "------------------\n",
      " U | R | U | L |\n"
     ]
    }
   ],
   "source": [
    "g.print_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that our optimal policy chooses 'U' in the starting state\n",
    "# of course it would be as valid to go 'R'\n",
    "# since that would still lead to the same Optimal Value\n",
    "# this is an example of how the optimal policy is not unique, while the optimal value is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to look at policy iteration for Windy Gridworld\n",
    "\n",
    "We are going to use the same Windy Gridworld as we looked at previously with a state beside the losing state is windy\n",
    "\n",
    "<img src='extras/55.8.PNG' width='400'></img>\n",
    "\n",
    "This makes it so that even if we choose to go up, we may still end up being pushed to the right such that we land in a losing state\n",
    "\n",
    "Before we look at the code, it will be useful to think about what we expect our algorithm to do \n",
    "\n",
    "In our previous scripts taking either path to the winning state would be an optimal choice\n",
    "\n",
    "But now, because of the fact that going beside the losing state is more dangerous, we might expect that our algorithm tries to take us away from this state\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Policy Iteration (Windy) in Code</h3>\n",
    "\n",
    "One thing that's not immediately clear is this\n",
    "\n",
    "What should the action be if we are right beside the Losing state?\n",
    "\n",
    "Obviously, we might think the best choice is to simply not go there\n",
    "\n",
    "But remember, this is reinforcement learning and not our mind\n",
    "\n",
    "So we still have to pick an optimal action for every state\n",
    "\n",
    "What's not immediately clear is whether it's better to simply go up because the winning state is closer and to take the risk\n",
    "\n",
    "Or is it better to go down and away from the danger?\n",
    "\n",
    "<img src='extras/55.9.PNG' width='300'></img>\n",
    "\n",
    "---\n",
    "\n",
    "Here's something else that will make this problem a little more complicated\n",
    "\n",
    "What if we add an associated cost for taking a step anywhere in the environment?\n",
    "\n",
    "In other words, suppose that for every state we land in that is not one of the terminal states, we get a negative reward\n",
    "\n",
    "This should alter our agents behavior\n",
    "\n",
    "Suppose the reward for going to any non terminal state is just zero\n",
    "\n",
    "Then it's always good to go down when we are beside the losing state because this imposes no penalty and going down as a deterministic move\n",
    "\n",
    "<img src='extras/55.10.PNG' width='300'></img>\n",
    "\n",
    "If we choose down, we will always go down\n",
    "\n",
    "Only going up is probabilistic\n",
    "\n",
    "However, what happens when the reward becomes negative?\n",
    "\n",
    "For example, if we get $-10$ reward for each step we take, it's probably better to just jump into the losing state, which only gives us a $-1$ reward\n",
    "\n",
    "<img src='extras/55.11.PNG' width='300'></img>\n",
    "\n",
    "However, those are two extremes\n",
    "\n",
    "What would be interesting is to see how the policy changes for those in-between values\n",
    "\n",
    "What if we set the reward at each state to be $-0.1$?\n",
    "\n",
    "What about $-0.2$ or $0.5$?\n",
    "\n",
    "These will be investigated in the next code section\n",
    "\n",
    "---\n",
    "\n",
    "<h3>windy_grid_penalized</h3>\n",
    "\n",
    "Let's begin by extending our `windy_grid` helper function to `windy_grid_penalized`\n",
    "\n",
    "This takes in a parameter called step cost, which will be the reward for going to any state that is not the terminal state\n",
    "\n",
    "The terminal state rewards will remain as $+1$ and $-1$\n",
    "\n",
    "Of course, this is very easy to implement, using our existing API\n",
    "\n",
    "We simply need to extend our rewards dictionary\n",
    "\n",
    "```python\n",
    "def windy_grid_penalised(step_cost=-0.1):\n",
    "    g = WindyGrid(3,4,(2,0))\n",
    "    rewards = {\n",
    "        (0,0): step_cost,\n",
    "        (0,1): step_cost,\n",
    "        (0,2): step_cost,\n",
    "        ...,\n",
    "        (0,3): 1,\n",
    "        (1,3): -1    \n",
    "    }\n",
    "    actions = {...}\n",
    "    probs = {...}\n",
    "    g.set(rewards,actions,probs)\n",
    "    return g\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class WindyGridWorld:\n",
    "    def __init__(self,rows,cols,start_pos):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.pos = start_pos\n",
    "    \n",
    "    def current_state(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def set_state(self,state):\n",
    "        self.pos = state\n",
    "        \n",
    "    def set_actions_rewards_probs(self,actions,rewards,probs):\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        # we need probs to know where to go after taking action\n",
    "        self.probs = probs\n",
    "        \n",
    "    def all_states(self,):\n",
    "        return [(i,j) for i in range(self.rows) for j in range(self.cols)]\n",
    "    \n",
    "    def is_terminal(self,state):\n",
    "        # we cant perform an action in a terminal state\n",
    "        # so we expect not to find it in the keys\n",
    "        return state not in self.actions.keys()\n",
    "    \n",
    "    def game_over(self):\n",
    "        return self.is_terminal(self.pos)\n",
    "    \n",
    "    def move(self,a):\n",
    "        # this time we sample a next state\n",
    "        next_state_2_probs = self.probs(self.pos,a)\n",
    "        next_states = list(next_state_2_probs.keys())\n",
    "        probs = list(next_state_2_probs.values())\n",
    "        next_state = np.random.choice(next_states,p=probs)\n",
    "        self.pos = next_state\n",
    "        return self.rewards.get(self.pos,0)\n",
    "    \n",
    "    # lets add methods to print values and actions\n",
    "    def print_values(self,V):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*9*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                v = V[i,j]\n",
    "                if v >= 0:\n",
    "                    # if 0 or +ve, add space so it aligns well when we have -\n",
    "                    print(\" %.2f|\" %v,end=\"\")\n",
    "                else:\n",
    "                    print(\"%.2f|\" %v,end=\"\")\n",
    "            print(\"\")\n",
    "    \n",
    "    def print_policy(self,P):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*6*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                print(\" %s |\" %P.get((i,j),' '),end=\"\")\n",
    "            print(\"\")\n",
    "                \n",
    "    def state_to_loc(self,state): # convert state tupe (0,1) to location 1\n",
    "        return state[0]*self.rows + state[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windy_grid_penalised(step_cost=-0.1):\n",
    "    g = WindyGridWorld(3,4,(2,0))\n",
    "    rewards = {\n",
    "        (0,3):1,\n",
    "        (1,3):-1,\n",
    "        (0,0):step_cost,\n",
    "        (0,1):step_cost,\n",
    "        (0,2):step_cost,\n",
    "        (1,0):step_cost,\n",
    "        (1,1):step_cost,\n",
    "        (1,2):step_cost,\n",
    "        (2,0):step_cost,\n",
    "        (2,1):step_cost,\n",
    "        (2,2):step_cost,\n",
    "        (2,3):step_cost,\n",
    "    }\n",
    "    \n",
    "    actions = {\n",
    "        (0,0): ('D','R'),\n",
    "        (0,1): ('L','R'),\n",
    "        (0,2): ('L','D','R'),\n",
    "        (1,0): ('U','D'),\n",
    "        (1,2): ('U','D','R'),\n",
    "        (2,0): ('U','R'),\n",
    "        (2,1): ('L','R'),\n",
    "        (2,2): ('L','R','U'),\n",
    "        (2,3): ('L','U')\n",
    "    }\n",
    "    probs = {\n",
    "        ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "        ((2, 0), 'D'): {(2, 0): 1.0},\n",
    "        ((2, 0), 'L'): {(2, 0): 1.0},\n",
    "        ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "        ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "        ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "        ((1, 0), 'L'): {(1, 0): 1.0},\n",
    "        ((1, 0), 'R'): {(1, 0): 1.0},\n",
    "        ((0, 0), 'U'): {(0, 0): 1.0},\n",
    "        ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "        ((0, 0), 'L'): {(0, 0): 1.0},\n",
    "        ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "        ((0, 1), 'U'): {(0, 1): 1.0},\n",
    "        ((0, 1), 'D'): {(0, 1): 1.0},\n",
    "        ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "        ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "        ((0, 2), 'U'): {(0, 2): 1.0},\n",
    "        ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "        ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "        ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "        ((2, 1), 'U'): {(2, 1): 1.0},\n",
    "        ((2, 1), 'D'): {(2, 1): 1.0},\n",
    "        ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "        ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "        ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "        ((2, 2), 'D'): {(2, 2): 1.0},\n",
    "        ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "        ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "        ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "        ((2, 3), 'D'): {(2, 3): 1.0},\n",
    "        ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "        ((2, 3), 'R'): {(2, 3): 1.0},\n",
    "        ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},\n",
    "        ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "        ((1, 2), 'L'): {(1, 2): 1.0},\n",
    "        ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "      }\n",
    "    g.set_actions_rewards_probs(actions,rewards,probs)\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with different costs\n",
    "# a 0 cost is the normal GridWorld Enviroment\n",
    "step_costs = [0,-0.1,-0.2,-0.4,-0.5,-2]\n",
    "envs = [windy_grid_penalised(c) for c in step_costs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games():\n",
    "    def build_transition_probs_and_rewards(g):\n",
    "        # probs_env : p(s'|s,a)\n",
    "        # r : rewards @ s'\n",
    "        probs = g.probs\n",
    "        probs_env = {}\n",
    "        r = {}\n",
    "        for (s,a),next_state_2_probs in probs.items():\n",
    "            for s_prime,prob in next_state_2_probs.items():\n",
    "                probs_env[s_prime,s,a] = prob\n",
    "                r[s_prime] = g.rewards.get(s_prime,0)\n",
    "        return probs_env,r\n",
    "    \n",
    "    # build pi(a|s)\n",
    "    def build_action_probs(policy):\n",
    "        all_states = g.all_states()\n",
    "        state_space = len(all_states)\n",
    "        pi = {}\n",
    "        for s in all_states:\n",
    "            if not g.is_terminal(s):\n",
    "                a = policy.get(s)\n",
    "                pi[a,s] = 1\n",
    "        return pi\n",
    "\n",
    "\n",
    "    for g,c in zip(envs,step_costs):\n",
    "        print(\"step cost: \",c)\n",
    "        # from here lets follow the same steps\n",
    "        # lets also calculate rewards r\n",
    "\n",
    "\n",
    "        p,r = build_transition_probs_and_rewards(g)\n",
    "\n",
    "\n",
    "        # lets follow the algorithm\n",
    "\n",
    "        # we initialise V[s] to zeros here\n",
    "        # since once we are inside we will be using the old values\n",
    "        # initialise V(s) = 0\n",
    "        all_states = g.all_states()\n",
    "        state_space = len(all_states)\n",
    "        V = np.zeros(state_space).reshape((g.rows,g.cols))\n",
    "\n",
    "\n",
    "        all_actions = ['D','U','L','R']\n",
    "        action_space = len(all_actions)\n",
    "        threshold = 1e-5\n",
    "        gamma = 0.9\n",
    "\n",
    "        # initialise random policy\n",
    "        policy = {s:all_actions[np.random.choice(action_space)] for s in all_states if not g.is_terminal(s)}\n",
    "\n",
    "\n",
    "        # loop\n",
    "        while True:\n",
    "            # skip initialisation we already did that\n",
    "            # next time we get here we will be using the old value\n",
    "\n",
    "            # policy evaluation\n",
    "            pi = build_action_probs(policy)\n",
    "            while True:\n",
    "                delta = 0\n",
    "                V_old = V.copy()\n",
    "                for s in all_states:\n",
    "                    # continue only for non-termimal states\n",
    "                    if g.is_terminal(s):\n",
    "                        continue\n",
    "                    v_new = 0\n",
    "                    for a in all_actions:\n",
    "                        for s_prime in all_states:\n",
    "                            v_new += pi.get((a,s),0)*p.get((s_prime,s,a),0)*(r.get(s_prime,0)+gamma*V[s_prime])\n",
    "                    V[s] = v_new \n",
    "                delta = np.max(np.abs(V-V_old))\n",
    "                if delta < threshold:\n",
    "                    break\n",
    "\n",
    "            # policy improvement\n",
    "            policy_old = policy.copy()\n",
    "            for s in all_states:\n",
    "                action_values = []\n",
    "                if g.is_terminal(s):\n",
    "                    continue\n",
    "                for a in all_actions:\n",
    "                    action_value = 0\n",
    "                    for s_prime in all_states:\n",
    "                        action_value += p.get((s_prime,s,a),0)*(r.get(s_prime,0)+gamma*V[s_prime])\n",
    "                    action_values.append(action_value)\n",
    "                policy[s] = all_actions[np.argmax(action_values)]\n",
    "            if policy == policy_old:\n",
    "                break\n",
    "\n",
    "        g.print_values(V)\n",
    "        g.print_policy(policy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step cost:  0\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.48| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.59| 0.53| 0.48|\n",
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | D |   |\n",
      "------------------\n",
      " U | L | L | L |\n",
      "step cost:  -0.1\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00|-0.04| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.18| 0.06|-0.04|\n",
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | D |   |\n",
      "------------------\n",
      " U | L | L | L |\n",
      "step cost:  -0.2\n",
      "---------------------------\n",
      " 0.43| 0.70| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.19| 0.00|-0.15| 0.00|\n",
      "---------------------------\n",
      "-0.03|-0.23|-0.34|-0.50|\n",
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | U |   |\n",
      "------------------\n",
      " U | L | U | L |\n",
      "step cost:  -0.4\n",
      "---------------------------\n",
      " 0.05| 0.50| 1.00| 0.00|\n",
      "---------------------------\n",
      "-0.36| 0.00|-0.25| 0.00|\n",
      "---------------------------\n",
      "-0.72|-0.96|-0.62|-0.96|\n",
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | U |   |\n",
      "------------------\n",
      " U | R | U | L |\n",
      "step cost:  -0.5\n",
      "---------------------------\n",
      "-0.14| 0.40| 1.00| 0.00|\n",
      "---------------------------\n",
      "-0.63| 0.00|-0.30| 0.00|\n",
      "---------------------------\n",
      "-1.06|-1.19|-0.77|-1.00|\n",
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | U |   |\n",
      "------------------\n",
      " U | R | U | U |\n",
      "step cost:  -2\n",
      "---------------------------\n",
      "-2.99|-1.10| 1.00| 0.00|\n",
      "---------------------------\n",
      "-4.69| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      "-6.15|-4.61|-2.90|-1.00|\n",
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | R |   |\n",
      "------------------\n",
      " R | R | U | U |\n"
     ]
    }
   ],
   "source": [
    "play_games()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets look at the results\n",
    "\n",
    "`step_cost = 0` , so no penalty\n",
    "\n",
    "As promised, The policy is to go away from the state right beside the losing state\n",
    "\n",
    "We choose to go down because that action is deterministic and always leads to us going down\n",
    "\n",
    "But going up would be dangerous because there's a chance we would end up in the losing state \n",
    "\n",
    "By going  down we guarantee that we will reach the winning state and thus the value for this state is positive\n",
    "\n",
    "We might want to double check that the value goes down by a factor of $0.9$ for every step away from the goal state that we go\n",
    "\n",
    "It starts at $1$, then $0.9$, then $0.81$, then $0.73$, then $0.66$ then $0.59$ then $0.53$, then $0.48$\n",
    "\n",
    "---\n",
    "\n",
    "`step_cost = -0.1`\n",
    "\n",
    "\n",
    "As we can see, this death penalty is not enough to change our policy\n",
    "\n",
    "However, the value of being in the state beside the losing state goes down to a negative number\n",
    "\n",
    "---\n",
    "\n",
    "`step_cost = -0.2`\n",
    "\n",
    "This is where things get interesting\n",
    "\n",
    "Now, when we are beside the losing state, we choose to go up instead of down\n",
    "\n",
    "This is because going all the way around now costs more than the expected cost of simply going up\n",
    "\n",
    "Or put another way, they expected return of going up is higher than the expected return of going all the way around\n",
    "\n",
    "Pay attention to the threshold of this decision as well\n",
    "\n",
    "If we are below the state, beside the losing state, we still choose to go up.\n",
    "\n",
    "But if we are to the left of that state, which is under the wall, we choose to go around, even though the winning state is further away\n",
    "\n",
    "---\n",
    "\n",
    "`step_cost = -0.4`\n",
    "\n",
    "So our optimal policy has changed again\n",
    "\n",
    "Now, if we are in the state underneath the wall, we choose to take the direct path to the winning\n",
    "state rather than going all the way around\n",
    "\n",
    "The step cost is simply too high to justify taking a long way\n",
    "\n",
    "---\n",
    "\n",
    "`step_cost =-0.5`\n",
    "\n",
    "So this is also interesting\n",
    "\n",
    "If we look at the state below the losing state, notice that the policy is now to simply go directly into the losing state instead of going around and attempting to get to the winning state\n",
    "\n",
    "---\n",
    "\n",
    "`step_cost = -2` \n",
    "\n",
    "At this point, the step cost is so high that even when we are right beside the losing state, it's\n",
    "better to just go into the losing state instead of trying to go up to get to the winning state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll be continuing our discussion of how to solve task number two, which is given an environment, how do we find the best policy?\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Recap</h3>\n",
    "\n",
    "In the previous sections, we saw that it was possible by simply combining two key ideas\n",
    "\n",
    "The first idea was policy evaluation\n",
    "\n",
    "In order to improve a policy, we have to know how good that policy is\n",
    "\n",
    "This is done through the value function and the value function is found by performing policy evaluation\n",
    "\n",
    "The second key idea is policy improvements\n",
    "\n",
    "We determine the simple method that guarantees we can achieve a policy as good or better than a given policy just as long as we know its value function\n",
    "\n",
    "We observe that if we just perform these two operations repeatedly, we would improve the policy each time\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Drawback of Policy Iteration</h3>\n",
    "\n",
    "Now, there is one drawback to the method we just described \n",
    "\n",
    "To see this, consider that the policy improvement process is a loop\n",
    "\n",
    "```\n",
    "Loop until convergance: \n",
    "    # policy evaluation\n",
    "    Loop until convergance:\n",
    "        Find V(s)\n",
    "    # policy improvement\n",
    "    Loop over all non-terminal states:\n",
    "        Find (s)\n",
    "```\n",
    "\n",
    "We repeat this loop until convergance \n",
    "\n",
    "Inside this loop, we have two steps\n",
    "\n",
    "The first step is policy evaluation, and the second step is policy improvement\n",
    "\n",
    "The second step is relatively efficient as it only requires a single pass through every state\n",
    "\n",
    "On the other hand, the first step policy evaluation is not efficient\n",
    "\n",
    "In fact, this step requires another loop in which we must wait for conversions\n",
    "\n",
    "Therefore, we actually have two nested loops, both of which could potentially last for a very long time as we wait for them to converge\n",
    "\n",
    "So hopefully we can see why this process would be slow\n",
    "\n",
    "---\n",
    "\n",
    "<h3>2 Ideas for Improvement</h3>\n",
    "\n",
    "So here are two ways that we can consider speeding up the process of policy iteration\n",
    "\n",
    "First, note that the policy itself is not really needed\n",
    "\n",
    "Recall that the policy is simply the $\\arg \\max$ of $Q$\n",
    "\n",
    "$$\\large \\pi_{k+1}(s) = \\arg \\max_a \\sum_{s^\\prime}\\sum_r p(s^\\prime,r \\vert s,a) \\{r + \\gamma v_k(s^\\prime)\\}$$\n",
    "\n",
    "We can also state this as the policy is the $\\arg \\max$ of the right hand side of the Bellman equation\n",
    "\n",
    "In this case, we can use $V$\n",
    "\n",
    "However, note that on the subsequent step, when we want to evaluate the policy, we don't really care about the $\\arg \\max$ of this expression, we care about the actual value of this expression\n",
    "\n",
    "$\\large v_{k+1}(s) = \\max_a \\sum_{s^\\prime}\\sum_r p(s^\\prime,r \\vert s,a) \\{r + \\gamma v_k(s^\\prime)\\}$\n",
    "\n",
    "In other words, if we just want the value, we can take the max instead of the $\\arg max$ and forget about which action is being chosen altogether\n",
    "\n",
    "---\n",
    "\n",
    "Here's another key idea\n",
    "\n",
    "We know that the policy evaluation step is problematic\n",
    "\n",
    "This is because this inner loop can potentially last forever\n",
    "\n",
    "We know that, practically speaking, we can cut this short by stopping when we reach some threshold for the largest change in $V(s)$\n",
    "\n",
    "Previously, we learned about one way to improve this calculation in the context of policy iteration\n",
    "\n",
    "It was that instead of initializing $V(s)$ randomly or to all zeros, we could simply start at whatever values were currently stored in $V(s)$\n",
    "\n",
    "This speeds things up, since we know that $V(s)$ won't change that much from one policy to the next\n",
    "\n",
    "The result of this is that policy evaluation converges in fewer steps\n",
    "\n",
    "Here's one way we can extend this idea\n",
    "\n",
    "Instead of waiting for $V(s)$ to converge, what if we simply stop the evaluation process after a few steps?\n",
    "\n",
    "And what if we take this to the extreme?\n",
    "\n",
    "What if we perform just a single step of policy evaluation?\n",
    "\n",
    "We might have a hunch that this converges since both the policy and the value function are both moving towards some optimal point\n",
    "\n",
    "<img src='extras/55.12.PNG' width='400'></img>\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Value Iteration</h3>\n",
    "\n",
    "To come up with our next algorithm, we just need to combine these two ideas \n",
    "\n",
    "To recap, here's what they are\n",
    "\n",
    "Idea number one, instead of taking the $\\arg \\max$ and getting the action, just take the max and get the value directly\n",
    "\n",
    "Idea number two, instead of going through many steps of policy evaluation, just do a single step\n",
    "\n",
    "When we combine these two ideas together, we get an algorithm called Value iteration\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Value Iteration</h3>\n",
    "\n",
    "So value iteration works like this\n",
    "\n",
    "$\\text{Initialise V(s) randomly or to 0, except terminal state which must be 0 } \\\\ \\text{Loop}: \\\\ \\qquad \\Delta = 0 \\\\ \\qquad \\text{for s in all_non_terminal_states:} \\\\ \\qquad \\qquad v_{\\text{old}} = V(s) \\ \\ \\# \\text{ store the existing value} \\\\ \\qquad \\qquad V(s) = \\max\\limits_a \\sum\\limits_{s^\\prime}\\sum\\limits_r p(s^\\prime,r \\vert s,a)[r + \\gamma V(s^\\prime)] \\\\ \\qquad \\qquad \\Delta = max(\\Delta, \\vert v_{\\text{old}} - V(s)\\vert) \\\\ \\qquad \\text{if } \\Delta < \\text{threshold:} \\\\ \\qquad \\qquad \\text{break} \\\\ \\text{# We only need to find the policy once, after V* is found} \\\\\\text{for s in all_non_terminal_states:} \\\\ \\quad \\pi^*(s) = \\arg \\max\\limits_a \\sum\\limits_{s^\\prime} \\sum\\limits_r p(s^\\prime,r \\vert s,a)[r + \\gamma V^*(s^\\prime)]$  \n",
    "\n",
    "We can see that it's a pretty short algorithm\n",
    "\n",
    "So we start by initializing $V(s)$ randomly, except for the terminal state, which must have a value of zero\n",
    "\n",
    "Then we enter an infinite loop\n",
    "\n",
    "Inside this loop, we create a variable called $\\Delta$\n",
    "\n",
    "This will store the maximum change for this iteration of the loop\n",
    "\n",
    "We'll initialize this value to zero, and we'll update it for each state that we see\n",
    "\n",
    "Next, we loop through all the states in our state space except for the terminal state whose value should be fixed at zero\n",
    "\n",
    "Inside the second loop, we store the current value of the $V(s)$ inside a variable called a $v_{\\text{old}}$\n",
    "\n",
    "Next, we take the $\\arg \\max$ of this expression, which is the equivalent to taking the max over all actions of the $Q$ function for the given state\n",
    "\n",
    "This gives us the new value for $V(s)$\n",
    "\n",
    "Note that this is similar to what we do for policy improvement, except that instead of taking the $\\arg \\max$ we now just take the max directly and forget about the actual action we would have chosen\n",
    "\n",
    "Next, we update $\\Delta$ to be the max of the current $\\Delta$ and the current absolut change in $V(s)$\n",
    "\n",
    "Finally, we check whether or not Delta is less than our threshold for convergence\n",
    "\n",
    "If it is, then we break out of the loop\n",
    "\n",
    "Once we've completed this loop, we found the optimal value function $V^*$\n",
    "\n",
    "In order to find the optimal policy $\\pi^*$, we need to loop through each state one last time\n",
    "\n",
    "Inside this loop, we set $\\p^*$ to be the $\\arg \\max$ of the usual Bellman expression\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Value Iteration: Things to notice</h3>\n",
    "\n",
    "So here are some interesting facts to note about value iteration\n",
    "\n",
    "Note how in this pseudocode, we end up with only one infinite loop that stops when $V(s)$ converges\n",
    "\n",
    "This is better than before where we had two nested infinite loops \n",
    "\n",
    "As before, while this one loop could potentially go on forever, practically speaking, we quit when the maximum change in $V(s)$ drops below some threshold\n",
    "\n",
    "Furthermore, note that we don't have to deal with the issue where more than one policy can lead to\n",
    "the optimal value function\n",
    "\n",
    "As we recall, for policy iteration, it's possible for us to flip flop between two different optimal\n",
    "policies, since optimal policies are not unique \n",
    "\n",
    "Since we're now dealing with the value function which is unique, the knowing once you break out of the loop is easy\n",
    "\n",
    "Finally, note the parallels between this and policy evaluation\n",
    "\n",
    "Policy evaluation is nothing but treating the Bellman equation as an update rule\n",
    "\n",
    "We take the right hand side and we assign it to the left hand side\n",
    "\n",
    "Convergence is achieved when the right hand side is equal to the left hand side, which is a fixed point for the update rule\n",
    "\n",
    "Similarly, value iteration is nothing but treating the Bellman optimality equation as an update rule\n",
    "\n",
    "Again, we take the right hand side and assign it to the left hand side\n",
    "\n",
    "We've reached convergence when the right hand side is equal to the left hand side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try value evaluation for windy gridworld\n",
    "import numpy as np\n",
    "\n",
    "class WindyGridWorld:\n",
    "    def __init__(self,rows,cols,start_pos):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.pos = start_pos\n",
    "    \n",
    "    def current_state(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def set_state(self,state):\n",
    "        self.pos = state\n",
    "        \n",
    "    def set_actions_rewards_probs(self,actions,rewards,probs):\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        # we need probs to know where to go after taking action\n",
    "        self.probs = probs\n",
    "        \n",
    "    def all_states(self,):\n",
    "        return [(i,j) for i in range(self.rows) for j in range(self.cols)]\n",
    "    \n",
    "    def is_terminal(self,state):\n",
    "        # we cant perform an action in a terminal state\n",
    "        # so we expect not to find it in the keys\n",
    "        return state not in self.actions.keys()\n",
    "    \n",
    "    def game_over(self):\n",
    "        return self.is_terminal(self.pos)\n",
    "    \n",
    "    def move(self,a):\n",
    "        # this time we sample a next state\n",
    "        next_state_2_probs = self.probs(self.pos,a)\n",
    "        next_states = list(next_state_2_probs.keys())\n",
    "        probs = list(next_state_2_probs.values())\n",
    "        next_state = np.random.choice(next_states,p=probs)\n",
    "        self.pos = next_state\n",
    "        return self.rewards.get(self.pos,0)\n",
    "    \n",
    "    # lets add methods to print values and actions\n",
    "    def print_values(self,V):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*9*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                v = V[i,j]\n",
    "                if v >= 0:\n",
    "                    # if 0 or +ve, add space so it aligns well when we have -\n",
    "                    print(\" %.2f|\" %v,end=\"\")\n",
    "                else:\n",
    "                    print(\"%.2f|\" %v,end=\"\")\n",
    "            print(\"\")\n",
    "    \n",
    "    def print_policy(self,P):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*6*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                print(\" %s |\" %P.get((i,j),' '),end=\"\")\n",
    "            print(\"\")\n",
    "                \n",
    "    def state_to_loc(self,state): # convert state tupe (0,1) to location 1\n",
    "        return state[0]*self.rows + state[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windy_grid():\n",
    "    g = WindyGridWorld(3,4,(2,0))\n",
    "    rewards = {(0,3):1,(1,3):-1}\n",
    "    actions = {\n",
    "        (0,0): ('D','R'),\n",
    "        (0,1): ('L','R'),\n",
    "        (0,2): ('L','D','R'),\n",
    "        (1,0): ('U','D'),\n",
    "        (1,2): ('U','D','R'),\n",
    "        (2,0): ('U','R'),\n",
    "        (2,1): ('L','R'),\n",
    "        (2,2): ('L','R','U'),\n",
    "        (2,3): ('L','U')\n",
    "    }\n",
    "    probs = {\n",
    "        ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "        ((2, 0), 'D'): {(2, 0): 1.0},\n",
    "        ((2, 0), 'L'): {(2, 0): 1.0},\n",
    "        ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "        ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "        ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "        ((1, 0), 'L'): {(1, 0): 1.0},\n",
    "        ((1, 0), 'R'): {(1, 0): 1.0},\n",
    "        ((0, 0), 'U'): {(0, 0): 1.0},\n",
    "        ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "        ((0, 0), 'L'): {(0, 0): 1.0},\n",
    "        ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "        ((0, 1), 'U'): {(0, 1): 1.0},\n",
    "        ((0, 1), 'D'): {(0, 1): 1.0},\n",
    "        ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "        ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "        ((0, 2), 'U'): {(0, 2): 1.0},\n",
    "        ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "        ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "        ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "        ((2, 1), 'U'): {(2, 1): 1.0},\n",
    "        ((2, 1), 'D'): {(2, 1): 1.0},\n",
    "        ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "        ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "        ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "        ((2, 2), 'D'): {(2, 2): 1.0},\n",
    "        ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "        ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "        ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "        ((2, 3), 'D'): {(2, 3): 1.0},\n",
    "        ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "        ((2, 3), 'R'): {(2, 3): 1.0},\n",
    "        ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},\n",
    "        ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "        ((1, 2), 'L'): {(1, 2): 1.0},\n",
    "        ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "      }\n",
    "    g.set_actions_rewards_probs(actions,rewards,probs)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration():\n",
    "    def build_transition_probs_and_rewards(g):\n",
    "        # probs_env : p(s'|s,a)\n",
    "        # r : rewards @ s'\n",
    "        probs = g.probs\n",
    "        probs_env = {}\n",
    "        r = {}\n",
    "        for (s,a),next_state_2_probs in probs.items():\n",
    "            for s_prime,prob in next_state_2_probs.items():\n",
    "                probs_env[s_prime,s,a] = prob\n",
    "                r[s_prime] = g.rewards.get(s_prime,0)\n",
    "        return probs_env,r\n",
    "    \n",
    "    # build pi(a|s)\n",
    "    def build_action_probs(policy):\n",
    "        all_states = g.all_states()\n",
    "        state_space = len(all_states)\n",
    "        pi = {}\n",
    "        for s in all_states:\n",
    "            if not g.is_terminal(s):\n",
    "                a = policy.get(s)\n",
    "                pi[a,s] = 1\n",
    "        return pi\n",
    "\n",
    "    # from here lets follow the same steps\n",
    "    # lets also calculate rewards r\n",
    "\n",
    "\n",
    "    p,r = build_transition_probs_and_rewards(g)\n",
    "\n",
    "\n",
    "    # lets follow the algorithm\n",
    "\n",
    "    # we initialise V[s] to zeros here\n",
    "    # since once we are inside we will be using the old values\n",
    "    # initialise V(s) = 0\n",
    "    all_states = g.all_states()\n",
    "    state_space = len(all_states)\n",
    "    V = np.zeros(state_space).reshape((g.rows,g.cols))\n",
    "\n",
    "\n",
    "    all_actions = ['D','U','L','R']\n",
    "    action_space = len(all_actions)\n",
    "    threshold = 1e-5\n",
    "    gamma = 0.9\n",
    "\n",
    "    # initialise random policy\n",
    "    policy = {s:all_actions[np.random.choice(action_space)] for s in all_states if not g.is_terminal(s)}\n",
    "\n",
    "\n",
    "    # get V*\n",
    "    pi = build_action_probs(policy)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_old = V.copy()\n",
    "        for s in all_states:\n",
    "            # continue only for non-termimal states\n",
    "            if g.is_terminal(s):\n",
    "                continue\n",
    "            action_values = []\n",
    "            for a in all_actions:\n",
    "                v_action = 0\n",
    "                for s_prime in all_states:\n",
    "                    v_action += p.get((s_prime,s,a),0)*(r.get(s_prime,0)+gamma*V[s_prime])\n",
    "                action_values.append(v_action)\n",
    "            V[s] = np.max(action_values)\n",
    "        delta = np.max(np.abs(V-V_old))\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "    # calculate policy only once at the end\n",
    "    for s in all_states:\n",
    "        action_values = []\n",
    "        if g.is_terminal(s):\n",
    "            continue\n",
    "        for a in all_actions:\n",
    "            action_value = 0\n",
    "            for s_prime in all_states:\n",
    "                action_value += p.get((s_prime,s,a),0)*(r.get(s_prime,0)+gamma*V[s_prime])\n",
    "            action_values.append(action_value)\n",
    "        policy[s] = all_actions[np.argmax(action_values)]\n",
    "\n",
    "    g.print_values(V)\n",
    "    g.print_policy(policy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = windy_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.48| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.59| 0.53| 0.48|\n",
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | D |   |\n",
      "------------------\n",
      " U | L | L | L |\n"
     ]
    }
   ],
   "source": [
    "value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we get the sane answer as with policy iteration\n",
    "# However this is more effecient since we only need to do a single loop to find the optimal value\n",
    "# And thereafter another single loop to find the optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll summarize and review everything we learned in the notebook\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Dynamic Programming</h3>\n",
    "\n",
    "This notebook was all about how to solve two important problems in reinforcement learning\n",
    "\n",
    "The previous notebook showed us the framework for reinforcement learning problems, the MDP \n",
    "\n",
    "Using the MDP, we were able to build on this framework to solve the problems of prediction and control\n",
    "\n",
    "The prediction problem is a given a policy tell me the value of that policy\n",
    "\n",
    "The control problem is given an environment, tell me the best policy\n",
    "\n",
    "---\n",
    "\n",
    "So to solve the prediction problem, we use policy evaluation\n",
    "\n",
    "We learn that this is nothing but treating\n",
    "the Bellman equation like an update rule\n",
    "\n",
    "We noted that there are other solutions to this problem, such as using a basic linear solver\n",
    "\n",
    "However, this is limited because it doesn't scale and it doesn't give us something to build on for later notebooks\n",
    "\n",
    "In fact, we saw an example of that in this notebook\n",
    "\n",
    "Specifically, value iteration is essentially just like policy evaluation, except that instead of doing a summation, we take the max\n",
    "\n",
    "So even in this notebook, we've seen how the dynamic programming approach is more powerful than solving a linear system of equations \n",
    "\n",
    "To solve the control problem, we first learned about the principle of policy improvement\n",
    "\n",
    "It was here that we learned about the policy improvement theorem\n",
    "\n",
    "This theorem states that if changing an action  once improves the value for a given policy, then changing that action permanently for that state will lead to a better policy\n",
    "\n",
    "Using this theorem, we developed the process of policy iteration\n",
    "\n",
    "Policy iteration solves the control problem\n",
    "\n",
    "We can start with a completely random policy and after running the process for long enough, we will end up with the optimal policy\n",
    "\n",
    "One problem we discovered with policy iteration is that it can potentially be very slow\n",
    "\n",
    "It has one infinite loop nested inside another infinite loop\n",
    "\n",
    "We then considered a new approach called value iteration that just has a single loop\n",
    "\n",
    "Essentially, value iteration is like policy iteration, except that we combine the evaluation step and the improvement step into a single operation\n",
    "\n",
    "This converges much faster than policy iteration, and it doesn't require us to even store a policy at all\n",
    "\n",
    "The policy is only computed after the value iteration loop is complete\n",
    "\n",
    "---\n",
    "\n",
    "<h3>But wait !!!</h3>\n",
    "\n",
    "One interesting fact we may have noticed about this notebook is this \n",
    "\n",
    "Reinforcement learning is supposed to be all about learning from experience\n",
    "\n",
    "Imagine a robot playing a game or solving a maze \n",
    "\n",
    "By playing the game or solving the maze a large number of times, it can use the rewards it achieved to learn the optimal policy\n",
    "\n",
    "What was interesting about this notebook was that no actual games were played, so why did this happen?\n",
    "\n",
    "In fact, no games were played because we had full knowledge about the dynamics of the environment\n",
    "\n",
    "In other words, we knew as $p(s^\\prime,r \\vert s,a)$\n",
    "\n",
    "The lesson is that, when we know this probability distribution, we don't actually need to play the game\n",
    "\n",
    "The solution is just a matter of applying the Bellman equation or the Bellman optimality equation\n",
    "\n",
    "So is learning from experience unnecessary?\n",
    "\n",
    "The answer is no\n",
    "\n",
    "This is because in the real world, we don't know this probability distribution\n",
    "\n",
    "Imagine driving a car or playing a game of chess \n",
    "\n",
    "When we started the reinforcement learning series, we probably had in our minds this idea that we were going to teach and agents who play a game or solve some task \n",
    "\n",
    "We probably had in your mind that your program would learn from experience and not just use\n",
    "probabilities\n",
    "\n",
    "Now, of course, this is necessary in order to understand the next notebooks\n",
    "\n",
    "In the next notebook, we will consider the more realistic scenario where this distribution is not known\n",
    "\n",
    "In this case, we must learn from experience\n",
    "\n",
    "Finally, note that we have different names for these approaches \n",
    "\n",
    "When we know the transition probabilities, that is our model of the environment \n",
    "\n",
    "When we use this model to find the solution, we call this a model based approach\n",
    "\n",
    "When we learn from experience, we don't need a model of the environment, wo we call that a model free approach\n",
    "\n",
    "The few following notebooks, will focus on model free approaches \n",
    "\n",
    "In more advanced notebooks, we might go back to model based approaches or even combine these two approaches to get a hybrid approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
