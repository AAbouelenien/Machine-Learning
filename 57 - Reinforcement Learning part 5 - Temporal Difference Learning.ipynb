{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to move on to the next notebook of the series, which is all about Temporal Difference learning\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Temporal Difference (TD) Learning</h3>\n",
    "\n",
    "So it's always good to recap what we've done so far in order to have a better understanding of where we are going next\n",
    "\n",
    "We'll see that with each notebook of this series, we're going to peel back more and more layers, which allows us to handle more complex problems with more sophisticated techniques\n",
    "\n",
    "So in the last few notebooks, we've established the framework to be used for solving reinforcement learning problems\n",
    "\n",
    "This is the MDP\n",
    "\n",
    "<img src='extras/57.1.PNG'></img>\n",
    "\n",
    "We learned about how the agent uses rewards that it gets from the environment in order to learn how to act such that its future rewards will be maximized\n",
    "\n",
    "In the dynamic programming notebook, we learned about how to solve both prediction and control when the environment dynamics are known\n",
    "\n",
    "When this is the case, there is no need to actually play any episodes since a model of the environment allows us to find the optimal policy directly using Bellman's equations \n",
    "\n",
    "In the Monte Carlo notebook, we removed this restriction without any model of the environment, playing episodes and gaining experience is necessary\n",
    "\n",
    "We saw that the main principle behind the Monte Carlo method is pretty simple\n",
    "\n",
    "Just replace the expected value of the return with the sample mean of actual returns\n",
    "\n",
    "Now there were obviously some details we had to contend with, but the main idea itself was not very complex\n",
    "\n",
    "---\n",
    "\n",
    "<h3>New Ideas</h3>\n",
    "\n",
    "In this notebook, we'll progress to a few new ideas\n",
    "\n",
    "One question whose answer is not clear is, what if we have an environment such that episodes never end?\n",
    "\n",
    "We'll recall that from Monte Carlo to work, we require episodes to terminate so that the return can be computed\n",
    "\n",
    "We'll see how TD learning can tackle this problem\n",
    "\n",
    "<img src='extras/57.2.PNG' width='600'></img>\n",
    "\n",
    "Another way to view what we'll be doing in this notebook is this \n",
    "\n",
    "Recall that dynamic programming uses a technique called <strong>bootstrapping</strong> \n",
    "\n",
    "At each step of our algorithm, the value estimate is improved by iterating on the previous estimate\n",
    "\n",
    "Specifically, the new value at each state is estimated by using the current value at all the possible next states\n",
    "\n",
    "That is, the new $V(s)$ is improved by using all the current $V(s^\\prime)$\n",
    "\n",
    "On the other hand, Monte Carlo does not make use of this technique\n",
    "\n",
    "Instead, it learns purely from samples, taking the sample mean of all the returns it's seen so far \n",
    "\n",
    "In this notebook, we'll see that temporal difference learning borrows ideas from both of these techniques\n",
    "\n",
    "Temporal Difference Learning is a sample based method, so it uses samples from the environment to improve its estimate\n",
    "\n",
    "But at the same time, it's also a bootstrapping method\n",
    "\n",
    "We'll see how we make use of our estimates of $V(s^\\prime)$ in order to update $V(s)$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>TD Outline</h3>\n",
    "\n",
    "So as before, this notebook will proceed using the same basic outline\n",
    "\n",
    "First, we'll discuss how the TD principle can be applied to the prediction task\n",
    "\n",
    "Once we understand the basics of how it works, we can then apply it to the control task\n",
    "\n",
    "As with Monte Carlo, there are two variants of TR learning that we'll discuss in this series\n",
    "\n",
    "Now there are actually more that exist, but the two in this series are what we would consider to be the most fundamental\n",
    "\n",
    "The first one we're looking at is called SARSA, and the second one we'll look at is the famous Q-learning technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to look at how to apply TD learning to prediction\n",
    "\n",
    "\n",
    "As we recall, the problem is given a policy $\\pi$ we would like to find the $V_\\pi(s)$\n",
    "\n",
    "So the best place to begin is where we left off with using Monte Carlo methods\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Monte Carlo Recap</h3>\n",
    "\n",
    "Using the Monte Carlo method, we know that what we would like to estimate is the expected return from a given state $s$\n",
    "\n",
    "We do this by collecting lots of sample returns and then averaging those returns to get our estimate\n",
    "\n",
    "$$\\large V_N(s) = \\frac{1}{N} \\sum^N_{i=1} G_{i,s}$$\n",
    "\n",
    "Now, in the previous notebook, we remember that in the implementation we didn't use this exact formula to calculate the sample mean\n",
    "\n",
    "That is, we didn't just sum up all the samples and divide by the number of samples\n",
    "\n",
    "We'll recall that this is an inefficient computation because as we collect more and more samples, this takes longer and longer\n",
    "\n",
    "Instead, we learn that we could replace this computation with the one step computation based on the previous estimate\n",
    "\n",
    "$$\\large V_N(s) = \\color{red}{\\boxed{\\color{black}{\\frac{1}{N} \\sum^N_{i=1} G_{i,s}}}} = \\color{green}{\\boxed{\\color{black}{V_{N-1}(s) + \\frac{1}{N} (G_{N,s} - V_{N-1}(s))}}}$$\n",
    "\n",
    "$\\color{red}{\\text{Inefficient}},\\ \\color{green}{\\text{Efficient}}$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Constant Alpha / EWMA Monte Carlo</h3>\n",
    "\n",
    "One simple modification we can make to the Monte Carlo update is this \n",
    "\n",
    "Recall that if our targets are non stationary, then using a constant learning rate, $\\alpha$ results in an exponentially weighted moving\n",
    "average\n",
    "\n",
    "In fact, it would be quite easy to modify our previous Monte Carlo scripts to use a constant $\\alpha$ instead\n",
    "\n",
    "$$\\large V(s) \\leftarrow V(s) + \\alpha (G-V(s))$$\n",
    "\n",
    "So in this assignment, we've removed the $N$ subscripts for simplicity and we've replaced $\\frac{1}{N}$ with $\\alpha$, which denotes a constant\n",
    "\n",
    "$G$ represents the latest sample\n",
    "\n",
    "On the right hand side, $V(s)$ represents the existing estimate for $V(s)$ and on the left hand side, $V(s)$ represents our new estimate for $V(s)$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Bellman Equation Recap</h3>\n",
    "\n",
    "Let's consider another piece of the puzzle, the Bellman equation\n",
    "\n",
    "It states there the value function can be expressed in terms of an expected value of the next state value functions instead of the return\n",
    "\n",
    "$$\\large V_\\pi(s) = E_\\pi \\left[ \\color{green}{\\boxed{\\color{black}{R_{t+1} + \\gamma V_\\pi (S_{t+1})}}} \\vert S_t = s \\right]$$\n",
    "\n",
    "note : $\\color{green}{\\text{still an expected value}}$\n",
    "\n",
    "The Monte Carlo method did not make use of this equation, but temporal difference learning will\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Temporal Difference Learning</h3>\n",
    "\n",
    "So what is temporal difference learning?\n",
    "\n",
    "Temporal difference learning is simply to combine these two things together \n",
    "\n",
    "Instead of trying to compute the average of all the sample $G$s, let's compute the average of $R + \\gamma V(s^\\prime)$\n",
    "\n",
    "$\\large {{\\text{MC} \\\\ V(s) \\leftarrow V(s) + \\alpha(G-V(s)) \\qquad \\qquad \\qquad \\qquad } {\\text{DP} \\\\ V_\\pi(s) = E_\\pi \\left[ R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\vert S_t = s \\right] }} \\\\ {\\text{ }} \\\\ \\qquad  \\qquad \\qquad \\qquad \\large V(s) \\leftarrow  V(s) + \\alpha(r + \\gamma V(s^\\prime) - V(s))$\n",
    "\n",
    "The important thing to notice about this is that, unlike Monte Carlo, this does not require us to know $G$\n",
    "\n",
    "This means we do not have to wait until the episode is over to make an update\n",
    "\n",
    "This can be helpful in cases where episodes are very long or even infinite\n",
    "\n",
    "In effect, the agent can learn as it goes as long as it has one reward, it can perform this update \n",
    "\n",
    "Also recognize the use of bootstrapping \n",
    "\n",
    "Effectively, the target value, which is $r + \\gamma V(s^\\prime)$ depends on the value function estimate at the next state $s^\\prime$\n",
    "\n",
    "In other words, our estimate of $V(s)$ depends on another estimate\n",
    "\n",
    "The target is really composed of two parts, the part that we know for sure and the part we have to guess\n",
    "\n",
    "The reward $r$ is the part we know for sure\n",
    "\n",
    "The future rewards, we don't know yet so we have to guess\n",
    "\n",
    "And of course, our guess for that is $V(s^\\prime)$\n",
    "\n",
    "Now, luckily, despite the fact that we no longer use the full return, this has been proven to still converge to the correct answer\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Preudocode</h3>\n",
    "\n",
    "So here's the full pseudocode for temporal difference prediction\n",
    "\n",
    "```\n",
    "Given: π\n",
    "Initialise: V(s) = 0 for all s in state space\n",
    "\n",
    "Loop until convergance:\n",
    "    s = env.reset()\n",
    "    while s not terminal:\n",
    "        a = π(s) or a ~ π(a|s)\n",
    "        s',r = env.move(a)\n",
    "        V(s) = V(s) + α(r + γV(s')-V(s))\n",
    "        s = s'\n",
    "```\n",
    "\n",
    "One of the major changes going from Mont Carlo to temporal difference learning is that there is no need to play an entire episode to collect a list of states and rewards\n",
    "\n",
    "Instead, we can recognize that temporal difference learning only requires a single step before making an update\n",
    "\n",
    "Specifically, the value for state $s$ depends only on the results of the reward $r$ and the next state $s^\\prime$\n",
    "\n",
    "So to begin, we accept as input some policy $\\pi$ whose value function we want to find\n",
    "\n",
    "Next, we initialize $V(s)$ arbitrarily, except for the terminal states where we already know that $V(s) = 0$\n",
    "\n",
    "Next, we enter a loop that goes for some number of episodes\n",
    "\n",
    "Inside this loop, we reset our environment and begin a new episode at some state $s$\n",
    "\n",
    "Next, we enter another loop that goes on until the episode is complete \n",
    "\n",
    "Inside the second loop, we look at our policy to determine the next actions that perform given the state $s$ \n",
    "\n",
    "We perform this action which brings us to the next state $s^\\prime$ and yields the reward $r$ \n",
    "\n",
    "Next we update $V(s)$ using the temporal difference update we saw earlier\n",
    "\n",
    "Next, we update the $s$ variable to be $s^\\prime$ since that is now the current state\n",
    "\n",
    "OK, so pretty simple algorithm\n",
    "\n",
    "Again, note how there is no distinct function for playing an episode\n",
    "\n",
    "Both episode playing and value function updates are part of the same loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try implementing policy evaluation using RD learning\n",
    "# of course since we will be resetting our env\n",
    "# we need epsilon greedy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets implement our Gridworld enviroment\n",
    "# we will be using the same interface as described above\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,rows,cols,start_pos):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.pos = start_pos\n",
    "        self.start_pos = start_pos\n",
    "    \n",
    "    def current_state(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def set_state(self,state):\n",
    "        self.pos = state\n",
    "        \n",
    "    def set_actions_rewards(self,actions,rewards):\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        \n",
    "    def all_states(self,):\n",
    "        return [(i,j) for i in range(self.rows) for j in range(self.cols)]\n",
    "    \n",
    "    def is_terminal(self,state):\n",
    "        # we cant perform an action in a terminal state\n",
    "        # so we expect not to find it in the keys\n",
    "        return state not in self.actions.keys()\n",
    "    \n",
    "    def game_over(self):\n",
    "        return self.is_terminal(self.pos)\n",
    "    \n",
    "    def next_state(self,s,a):\n",
    "        # return next state should we take action a\n",
    "        # but do not take that action\n",
    "        i,j = s\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(s,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        return (i,j)\n",
    "    \n",
    "    def move(self,a):\n",
    "        # return next state should we take action a\n",
    "        # this time we actually make the move\n",
    "        # and thus return reward\n",
    "        i,j = self.pos\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(self.pos,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        self.pos = (i,j)\n",
    "        # if no reward return 0\n",
    "        return self.rewards.get(self.pos,0)\n",
    "    \n",
    "    # lets add methods to print values and actions\n",
    "    def print_values(self,V):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*9*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                v = V[i,j]\n",
    "                if v >= 0:\n",
    "                    # if 0 or +ve, add space so it aligns well when we have -\n",
    "                    print(\" %.2f|\" %v,end=\"\")\n",
    "                else:\n",
    "                    print(\"%.2f|\" %v,end=\"\")\n",
    "            print(\"\")\n",
    "    \n",
    "    def print_policy(self,P):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*6*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                print(\" %s |\" %P.get((i,j),' '),end=\"\")\n",
    "            print(\"\")\n",
    "                \n",
    "    def state_to_loc(self,state): # convert state tupe (0,1) to location 1\n",
    "        return state[0]*self.rows + state[1]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pos = self.start_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standrad_grid():\n",
    "    g = GridWorld(3,4,(2,0))\n",
    "    rewards = {(0,3):1,(1,3):-1}\n",
    "    actions = {\n",
    "        (0,0): ('D','R'),\n",
    "        (0,1): ('L','R'),\n",
    "        (0,2): ('L','D','R'),\n",
    "        (1,0): ('U','D'),\n",
    "        (1,2): ('U','D','R'),\n",
    "        (2,0): ('U','R'),\n",
    "        (2,1): ('L','R'),\n",
    "        (2,2): ('L','R','U'),\n",
    "        (2,3): ('L','U')\n",
    "    }\n",
    "    g.set_actions_rewards(actions,rewards)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = standrad_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed policy (deterministic)\n",
    "pi = {\n",
    "    (2,0) : 'U',\n",
    "    (1,0) : 'U',\n",
    "    (0,0) : 'R',\n",
    "    (0,1) : 'R',\n",
    "    (0,2) : 'R',\n",
    "    (1,2) : 'R',\n",
    "    (2,1) : 'R',\n",
    "    (2,2) : 'R',\n",
    "    (2,3) : 'U'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | U |   |\n",
      "------------------\n",
      " U | L | L | L |\n"
     ]
    }
   ],
   "source": [
    "g.print_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(best_action,eps=0.1):\n",
    "    if np.random.random() < eps:\n",
    "        return np.random.choice(['U','R','D','L'])\n",
    "    else:\n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_eval():\n",
    "    deltas = []\n",
    "    all_states = g.all_states()\n",
    "    V = np.zeros((g.rows,g.cols))\n",
    "    V_old = V.copy()\n",
    "    \n",
    "    all_actions = ['L','U','D','R']\n",
    "    \n",
    "    episodes = 10000\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        g.reset()\n",
    "        s = g.current_state()\n",
    "        while not g.game_over():\n",
    "            a = epsilon_greedy(pi[s])\n",
    "            r = g.move(a)\n",
    "            s_prime = g.current_state()\n",
    "            V[s] = V[s] + alpha*(r+gamma*V[s_prime]-V[s])\n",
    "            s = s_prime\n",
    "        delta = np.max(np.abs(V-V_old))\n",
    "        deltas.append(delta)\n",
    "        V_old = V.copy()\n",
    "    plt.plot(deltas)\n",
    "    plt.show()\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "V = TD_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So one interesting thing we'll notice is that the deltas do not decrease over time, as they did before\n",
    "# One reason this is the case is that our learning rate is now constant rather than decreasing with each sample as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.75| 0.84| 0.97| 0.00|\n",
      "---------------------------\n",
      " 0.68| 0.00|-0.97| 0.00|\n",
      "---------------------------\n",
      " 0.61|-0.77|-0.88|-0.99|\n"
     ]
    }
   ],
   "source": [
    "g.print_values(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another thing to keep in mind is the accuracy of our value function estimate because we're using epsilon greedy\n",
    "# There are some states here which will be visited only very rarely\n",
    "# This means that we have less opportunity to collect samples and hence those values will be less accurate\n",
    "# We'll recall that we encountered the same issue with Monte Carlo\n",
    "# however, because the value function for other states depends on the value function for those states this inaccuracy can propagate\n",
    "# One thing we might want to try in order to test how accurate this is, is to run dynamic programming, which essentially finds the exact solution\n",
    "# We can also do what we did before, where we counted how many times we were able to visit each state on the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the previous sections, we learned how to apply temporal difference learning \n",
    "\n",
    "As we come to expect, the next step is now to apply this principle to control\n",
    "\n",
    "So how can this be done?\n",
    "\n",
    "---\n",
    "\n",
    "<h3>SARSA - Temporal Difference Control</h3>\n",
    "\n",
    "The first method we're going to discuss is Sarsa\n",
    "\n",
    "How it works is essentially embedded in its name :o\n",
    "\n",
    "Clearly, the letters `SARSA = (s,a,r,s',a')` correspond to the states actions and rewards that we encounter while we play an episode\n",
    "\n",
    "<img src='extras/57.3.PNG' width='500'></img>\n",
    "\n",
    "Suppose that we are in state $s$\n",
    "\n",
    "Then we use our current policy to obtain the action we should perform, which we call $a$, then we perform that action\n",
    "\n",
    "This brings us to the next state $s^\\prime$\n",
    "\n",
    "We also obtain a corresponding reward $r$ at the same time\n",
    "\n",
    "Given the next state $s^\\prime,e$, we can use our policy again to determine the next action, $a^\\prime,e$\n",
    "\n",
    "So this gives us our SARSA tuple from which we can derive our value update\n",
    "\n",
    "---\n",
    "\n",
    "<h3>SARSA Update</h3>\n",
    "\n",
    "So as we recall from the Monte Carlo notebook, when we're doing prediction, it's ok to use $V$, but when it comes to control, we need to use $Q$\n",
    "\n",
    "This is because the optimal action will need to be derived from $Q$ and not $V$\n",
    "\n",
    "If we use $V$, then we would need to compute an expected value in order to know the optimal action which can't be found since we don't know the environment dynamics\n",
    "\n",
    "Otherwise, notice that this is essentially the $Q$ version of temporal difference prediction for the value function\n",
    "\n",
    "$$\\large Q(s,a) \\leftarrow Q(s,a) + \\alpha (r + \\gamma Q(s^\\prime,a^\\prime) - Q(s,a))$$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Control vs Prediction</h3>\n",
    "\n",
    "So what makes this different from prediction, how is this a control algorithm?\n",
    "\n",
    "Well, when it comes time to determine which actions to perform instead of following some given policy, we're always going to take the $\\arg \\max$ over $Q(s,a)$\n",
    "\n",
    "$$\\large a^* = \\arg \\max_a Q(s,a)$$\n",
    "\n",
    "That is to say, we'll use our most recent estimate of $Q$ to determine the best action to take \n",
    "\n",
    "As $Q$ approaches $Q^*$, both the value function and corresponding policy will approach their optimal values \n",
    "\n",
    "note : we call this \"generalised policy iteration\": Update Q (evaluation), then update policy (improvement), etc.\n",
    "\n",
    "One thing to remember, however, is that we don't always perform the action $a^*$\n",
    "\n",
    "This is because, as we've learned several times in this notebook, we need exploration\n",
    "\n",
    "Exploration is required because that's what allows us to have an accurate estimate for $Q$ \n",
    "\n",
    "By using Epsilon greedy, we can ensure that once in a while we choose a random action which helps us make $Q$ more accurate for all states and all actions\n",
    "\n",
    "But most of the time we perform the greedy action to obtain the best return\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<h3>SARSA Pseudocode</h3>\n",
    "\n",
    "So let's now look at the pseudocode for SARSA\n",
    "\n",
    "```\n",
    "Loop until convergance:\n",
    "    s = env.reset()\n",
    "    a = epsilon_greedy(Q(s,:))\n",
    "    while s not terminal:\n",
    "        s',r = env.move(a)\n",
    "        a' = epsilon_greedy(Q(s',:))\n",
    "        Q(s,a) = Q(s,a) + α(r + γQ(s',a')-Q(s,a))\n",
    "        s = s', a = a'\n",
    "```\n",
    "\n",
    "As we can see, it's quite short and it also contains one of the major features we saw with temporal difference prediction\n",
    "\n",
    "This was that playing the episode and updating the value now appear in the same loop\n",
    "\n",
    "This is unlike Monte Carlo, where we had a separate function to play the episode and updated the value only after the episode was complete\n",
    "\n",
    "So in SARSA, recognize that there's no need to initialize any random policy\n",
    "\n",
    "Instead, we just need to randomly initialize $Q$ because our policy is always derived from $Q$\n",
    "\n",
    "Note that the terminal states should still have their queue values equal to $0$\n",
    "\n",
    "Next, we enter a loop which will go for however many episodes we want to play \n",
    "\n",
    "Inside the loop, we reset our environment and obtain our initial estimate\n",
    "\n",
    "Next, we choose an action based on the current value stored in $Q$ and the current state $s$\n",
    "\n",
    "Next, we enter a loop that exits when the episode is complete\n",
    "\n",
    "Inside the loop, we perform the action $a$ and this yields a reward $r$ and the next state $s^\\prime$\n",
    "\n",
    "Once we have the next state $s^\\prime$, we can use this to find out what our next action should be which we'll call $a^\\prime$\n",
    "\n",
    "Notice that we now have our full SARSA tuple `(s,a,r,s',a')`\n",
    "\n",
    "Using this, we can now do our update for $Q$ Using the formula we learned before\n",
    "\n",
    "Finally we assign $s^\\prime$ to be $s$, and $a^\\prime$ to be $a$ for the next iteration of this loop\n",
    "\n",
    "OK, so thats SARSA \n",
    "\n",
    "Notice how using this algorithm, it doesn't matter how long our episode is, the agents is always learning\n",
    "\n",
    "Even if our episode is infinitely long, we could quit when you see that the value has converged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try implementing policy evaluation using RD learning\n",
    "# of course since we will be resetting our env\n",
    "# we need epsilon greedy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets implement our Gridworld enviroment\n",
    "# we will be using the same interface as described above\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,rows,cols,start_pos):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.pos = start_pos\n",
    "        self.start_pos = start_pos\n",
    "    \n",
    "    def current_state(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def set_state(self,state):\n",
    "        self.pos = state\n",
    "        \n",
    "    def set_actions_rewards(self,actions,rewards):\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        \n",
    "    def all_states(self,):\n",
    "        return [(i,j) for i in range(self.rows) for j in range(self.cols)]\n",
    "    \n",
    "    def is_terminal(self,state):\n",
    "        # we cant perform an action in a terminal state\n",
    "        # so we expect not to find it in the keys\n",
    "        return state not in self.actions.keys()\n",
    "    \n",
    "    def game_over(self):\n",
    "        return self.is_terminal(self.pos)\n",
    "    \n",
    "    def next_state(self,s,a):\n",
    "        # return next state should we take action a\n",
    "        # but do not take that action\n",
    "        i,j = s\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(s,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        return (i,j)\n",
    "    \n",
    "    def move(self,a):\n",
    "        # return next state should we take action a\n",
    "        # this time we actually make the move\n",
    "        # and thus return reward\n",
    "        i,j = self.pos\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(self.pos,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        self.pos = (i,j)\n",
    "        # if no reward return 0\n",
    "        return self.rewards.get(self.pos,0)\n",
    "    \n",
    "    # lets add methods to print values and actions\n",
    "    def print_values(self,V):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*9*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                v = V[i,j]\n",
    "                if v >= 0:\n",
    "                    # if 0 or +ve, add space so it aligns well when we have -\n",
    "                    print(\" %.2f|\" %v,end=\"\")\n",
    "                else:\n",
    "                    print(\"%.2f|\" %v,end=\"\")\n",
    "            print(\"\")\n",
    "    \n",
    "    def print_policy(self,P):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*6*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                print(\" %s |\" %P.get((i,j),' '),end=\"\")\n",
    "            print(\"\")\n",
    "                \n",
    "    def state_to_loc(self,state): # convert state tupe (0,1) to location 1\n",
    "        return state[0]*self.rows + state[1]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pos = self.start_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_grid():\n",
    "    g = GridWorld(3,4,(2,0))\n",
    "    rewards = {(0,3):1,(1,3):-1}\n",
    "    actions = {\n",
    "        (0,0): ('D','R'),\n",
    "        (0,1): ('L','R'),\n",
    "        (0,2): ('L','D','R'),\n",
    "        (1,0): ('U','D'),\n",
    "        (1,2): ('U','D','R'),\n",
    "        (2,0): ('U','R'),\n",
    "        (2,1): ('L','R'),\n",
    "        (2,2): ('L','R','U'),\n",
    "        (2,3): ('L','U')\n",
    "    }\n",
    "    g.set_actions_rewards(actions,rewards)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time we will be using the negative grid\n",
    "# so we can plot rewards agains episode\n",
    "# which is what we hope to maximise\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "    # in this game we want to try to minimize the number of moves\n",
    "    # so we will penalize every move\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "        (0, 0): step_cost,\n",
    "        (0, 1): step_cost,\n",
    "        (0, 2): step_cost,\n",
    "        (1, 0): step_cost,\n",
    "        (1, 2): step_cost,\n",
    "        (2, 0): step_cost,\n",
    "        (2, 1): step_cost,\n",
    "        (2, 2): step_cost,\n",
    "        (2, 3): step_cost,\n",
    "    })\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = negative_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(best_action,eps=0.1):\n",
    "    if np.random.random() < eps:\n",
    "        return np.random.choice(4)\n",
    "    else:\n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA():\n",
    "    deltas = []\n",
    "    all_states = g.all_states()\n",
    "    all_actions = ['L','U','D','R']\n",
    "    \n",
    "    Q = np.zeros((g.rows,g.cols,len(all_actions)))\n",
    "    episodes = 10000\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    \n",
    "    rewards =  np.zeros(episodes)\n",
    "    for episode in range(episodes):\n",
    "        g.reset()\n",
    "        s = g.current_state()\n",
    "        a = epsilon_greedy(np.argmax(Q[s],-1))\n",
    "        while not g.game_over():\n",
    "            r = g.move(all_actions[a])\n",
    "            rewards[episode] += r\n",
    "            s_prime = g.current_state()\n",
    "            a_prime = epsilon_greedy(np.argmax(Q[s_prime],axis=-1))\n",
    "            Q[s[0],s[1],a] = Q[s[0],s[1],a] + alpha*(r+gamma*Q[s_prime[0],s_prime[1],a_prime]-Q[s[0],s[1],a])\n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "    policy = np.argmax(Q,axis=-1)\n",
    "    policy = {s:all_actions[policy[s]] for s in all_states if not g.is_terminal(s)}\n",
    "    V = np.max(Q,axis=-1)\n",
    "    return V,policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "V,policy = SARSA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, so looking at this plot, we can see that a reward converges to the maximum quite fast\n",
    "# This is because this is a very simple environment \n",
    "# in more complex environments, we'll see that this normally takes more time\n",
    "# Notice that the reward fluctuates \n",
    "# because our policy sometimes makes us do random actions which reduce the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.60| 0.76| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.41| 0.00| 0.78| 0.00|\n",
      "---------------------------\n",
      " 0.26| 0.35| 0.56| 0.34|\n",
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | U |   |\n",
      "------------------\n",
      " U | R | U | L |\n"
     ]
    }
   ],
   "source": [
    "g.print_values(V)\n",
    "g.print_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy makes perfect sense\n",
    "# also seeks the shortest path since we used the negative grid\n",
    "# In order to make sense of the values\n",
    "# we can see that they generally decease the further away we get from the goal\n",
    "# again, should we want to check how accurate this is\n",
    "# we can compare it with the dynamic programming script\n",
    "# which gives the exact answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to look at an alternative temporal difference control algorithm called Q-Learning\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Q-Learning</h3>\n",
    "\n",
    "So why is this necessary?\n",
    "\n",
    "Well, if we've ever read about reinforcement learning on our own, we may have noticed that learning is quite popular\n",
    "\n",
    "On the other hand, SARSA is not\n",
    "\n",
    "There are many variations of learning, such as Deep Q-learning\n",
    "\n",
    "<img src='extras/57.4.PNG' width='500'></img>\n",
    "\n",
    "Maybe someone out there has implemented Deep SARSA, but it's certainly not as recognizable\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Epsilon-Greedy</h3>\n",
    "\n",
    "So what's our motivation for improving SARSA\n",
    "\n",
    "We'll recall early on where we discussed why Epsilon Greedy is not an optimal policy\n",
    "\n",
    "It helps us with exploration, but it also means that some percentage of the time we're just going to choose a suboptimal action randomly\n",
    "\n",
    "Q-Learning gives us one way of avoiding this\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Q-Learning vs. SARSA</h3>\n",
    "\n",
    "Essentially, Q-Learning is just one small change \n",
    "\n",
    "With SARSA, we'll recall that our target is \n",
    "\n",
    "$$\\large \\text{SARSA Target : } r + \\gamma Q(s^\\prime,a^\\prime)$$\n",
    "\n",
    "note : $Q(s^\\prime,a^\\prime)$ is the action we actually took\n",
    "\n",
    "With Q-Learning our target is \n",
    "\n",
    "$$\\large \\text{Q-Learning Target : } r + \\gamma \\max_{a^\\prime} Q(s^\\prime,a^\\prime)$$\n",
    "\n",
    "note : $Q(s^\\prime,a^\\prime)$ is the optimal action\n",
    "\n",
    "So what's the main difference?\n",
    "\n",
    "The main difference is that instead of using the actual next action in the target, we use the action we would have taken if we had chosen the current optimal action\n",
    "\n",
    "So in SARSA, when we're doing Epsilon greedy, sometimes we'll choose an action $a^\\prime$ that's not optimal\n",
    "\n",
    "This means that the target will correspond to that suboptimal action \n",
    "\n",
    "With Q-Learning we'll always use the maximum $Q$ meaning that we're learning the $Q$ function for the policy in which we always choose the best action\n",
    "\n",
    "---\n",
    "\n",
    "<h3>On-Policy vs Off-Policy</h3>\n",
    "\n",
    "So a new concept arises when we discuss the difference between SARSA and Q-learning \n",
    "\n",
    "Specifically, we can describe an algorithm as being either on-policy or off-policy\n",
    "\n",
    "SARSA is called an on-policy method because the $Q$ function we're learning is the $Q$ function for the policy that we're actually using\n",
    "\n",
    "Once we complete training, this will be the policy that we consider to be the best policy for the agent\n",
    "\n",
    "On the other hand, Q-Learning is an off-policy method\n",
    "\n",
    "This is because our actions are dictated by an Epsilon really policy\n",
    "\n",
    "However, the $Q$ function we are learning is for a purely greedy policy\n",
    "\n",
    "The greedy policy is what we would get if we always chose the optimal action which corresponds to taking the max over $Q$ \n",
    "\n",
    "We can differentiate between the two kinds of policies as follows\n",
    "\n",
    "The policy that we use to play the episode is called the <strong>behavior</strong> Policy\n",
    "\n",
    "The behavior policy dictates how we act in the environment\n",
    "\n",
    "It tells us which actions we should perform\n",
    "\n",
    "On the other hand, the policy that we are learning is called the <strong>target</strong> policy\n",
    "\n",
    "The target policy may not be the same as the one we are using to determine our actions during training\n",
    "\n",
    "But when we update $Q$, the $Q$ we want to find corresponds to this target policy\n",
    "\n",
    "Furthermore, the target policy is the policy we eventually want to end up with the one that represents an intelligent agents that knows how to maximize its rewards\n",
    "\n",
    "In fact, if weour further your studies in reinforcement learning, we may learn about methods in which our behavior policy can be completely random, that is uniform random, and we can still end up with an optimal target policy\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Q-Learning Pseudocode</h3>\n",
    "\n",
    "So to end this ssection, let's look at the pseudocode for Q-Learning \n",
    "\n",
    "```\n",
    "Initialise: Q(s,a) = random for all s in state space and a in action space\n",
    "\n",
    "Loop until convergance:\n",
    "    s = env.reset()\n",
    "    while s not terminal:\n",
    "        a = epsilon_greedy(Q(s,:))\n",
    "        s',r = env.move(a)\n",
    "        Q(s,a) = Q(s,a) + α(r + γ max_a' Q(s',a') - Q(s,a))\n",
    "        s = s'\n",
    "```\n",
    "\n",
    "As before, we're going to initialise $Q$ randomly except for terminal states where it's equal to zero\n",
    "\n",
    "Next, we enter a loop that plays for some number of episodes\n",
    "\n",
    "Inside this loop, we reset our environment and obtain the initial state, which we call $s$\n",
    "\n",
    "Next, we enter a loop that exits when the episode is complete\n",
    "\n",
    "Inside this loop, we use an epsilon greedy policy to get our action, which we call $a$\n",
    "\n",
    "So this follows our behavior policy\n",
    "\n",
    "Next, we take the action $A$ in the environment, which gives us back the reward $r$ and the next state $s^\\prime$\n",
    "\n",
    "Next we use `(s,a,r,s')` to update $Q$ using the formula we saw earlier\n",
    "\n",
    "Note one difference between this and SARSA where we no longer have to wait to get the next action $a^\\prime$ before updating $Q$\n",
    "\n",
    "Next, we assign $s^\\prime$ to be the current state $s$ for the next iteration of the loop\n",
    "\n",
    "OK, so that's it for Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try implementing policy evaluation using RD learning\n",
    "# of course since we will be resetting our env\n",
    "# we need epsilon greedy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets implement our Gridworld enviroment\n",
    "# we will be using the same interface as described above\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,rows,cols,start_pos):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.pos = start_pos\n",
    "        self.start_pos = start_pos\n",
    "    \n",
    "    def current_state(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def set_state(self,state):\n",
    "        self.pos = state\n",
    "        \n",
    "    def set_actions_rewards(self,actions,rewards):\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        \n",
    "    def all_states(self,):\n",
    "        return [(i,j) for i in range(self.rows) for j in range(self.cols)]\n",
    "    \n",
    "    def is_terminal(self,state):\n",
    "        # we cant perform an action in a terminal state\n",
    "        # so we expect not to find it in the keys\n",
    "        return state not in self.actions.keys()\n",
    "    \n",
    "    def game_over(self):\n",
    "        return self.is_terminal(self.pos)\n",
    "    \n",
    "    def next_state(self,s,a):\n",
    "        # return next state should we take action a\n",
    "        # but do not take that action\n",
    "        i,j = s\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(s,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        return (i,j)\n",
    "    \n",
    "    def move(self,a):\n",
    "        # return next state should we take action a\n",
    "        # this time we actually make the move\n",
    "        # and thus return reward\n",
    "        i,j = self.pos\n",
    "        # if we are allowed to do this action in our current position\n",
    "        if a in self.actions.get(self.pos,[]): \n",
    "            if a == 'R':\n",
    "                j += 1\n",
    "            if a == 'L':\n",
    "                j -= 1\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            if a == 'D':\n",
    "                i += 1\n",
    "        self.pos = (i,j)\n",
    "        # if no reward return 0\n",
    "        return self.rewards.get(self.pos,0)\n",
    "    \n",
    "    # lets add methods to print values and actions\n",
    "    def print_values(self,V):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*9*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                v = V[i,j]\n",
    "                if v >= 0:\n",
    "                    # if 0 or +ve, add space so it aligns well when we have -\n",
    "                    print(\" %.2f|\" %v,end=\"\")\n",
    "                else:\n",
    "                    print(\"%.2f|\" %v,end=\"\")\n",
    "            print(\"\")\n",
    "    \n",
    "    def print_policy(self,P):\n",
    "        for i in range(self.rows):\n",
    "            print('-'*6*self.rows)\n",
    "            for j in range(self.cols):\n",
    "                print(\" %s |\" %P.get((i,j),' '),end=\"\")\n",
    "            print(\"\")\n",
    "                \n",
    "    def state_to_loc(self,state): # convert state tupe (0,1) to location 1\n",
    "        return state[0]*self.rows + state[1]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pos = self.start_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_grid():\n",
    "    g = GridWorld(3,4,(2,0))\n",
    "    rewards = {(0,3):1,(1,3):-1}\n",
    "    actions = {\n",
    "        (0,0): ('D','R'),\n",
    "        (0,1): ('L','R'),\n",
    "        (0,2): ('L','D','R'),\n",
    "        (1,0): ('U','D'),\n",
    "        (1,2): ('U','D','R'),\n",
    "        (2,0): ('U','R'),\n",
    "        (2,1): ('L','R'),\n",
    "        (2,2): ('L','R','U'),\n",
    "        (2,3): ('L','U')\n",
    "    }\n",
    "    g.set_actions_rewards(actions,rewards)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_grid(step_cost=-0.1):\n",
    "    # in this game we want to try to minimize the number of moves\n",
    "    # so we will penalize every move\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "        (0, 0): step_cost,\n",
    "        (0, 1): step_cost,\n",
    "        (0, 2): step_cost,\n",
    "        (1, 0): step_cost,\n",
    "        (1, 2): step_cost,\n",
    "        (2, 0): step_cost,\n",
    "        (2, 1): step_cost,\n",
    "        (2, 2): step_cost,\n",
    "        (2, 3): step_cost,\n",
    "    })\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = negative_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(best_action,eps=0.1):\n",
    "    if np.random.random() < eps:\n",
    "        return np.random.choice(4)\n",
    "    else:\n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning():\n",
    "    deltas = []\n",
    "    all_states = g.all_states()\n",
    "    all_actions = ['L','U','D','R']\n",
    "    \n",
    "    Q = np.zeros((g.rows,g.cols,len(all_actions)))\n",
    "    episodes = 10000\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    \n",
    "    rewards =  np.zeros(episodes)\n",
    "    for episode in range(episodes):\n",
    "        g.reset()\n",
    "        s = g.current_state()\n",
    "        while not g.game_over():\n",
    "            a = epsilon_greedy(np.argmax(Q[s],-1))\n",
    "            r = g.move(all_actions[a])\n",
    "            rewards[episode] += r\n",
    "            s_prime = g.current_state()\n",
    "            Q[s[0],s[1],a] = Q[s[0],s[1],a] + alpha*(r+np.max(gamma*Q[s_prime[0],s_prime[1]],axis=-1)-Q[s[0],s[1],a])\n",
    "            s = s_prime\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "    policy = np.argmax(Q,axis=-1)\n",
    "    policy = {s:all_actions[policy[s]] for s in all_states if not g.is_terminal(s)}\n",
    "    V = np.max(Q,axis=-1)\n",
    "    return V,policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "V,policy = Q_Learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.14|\n",
      "------------------\n",
      " R | R | R |   |\n",
      "------------------\n",
      " U |   | U |   |\n",
      "------------------\n",
      " U | R | U | L |\n"
     ]
    }
   ],
   "source": [
    "g.print_values(V)\n",
    "g.print_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Math</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now that we've learned how to do both prediction and control using temporal difference learning, it's time to summarize this notebook\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Temporal Difference Learning notebook Summary</h3>\n",
    "\n",
    "This notebook was all about how to convert the Monte Carlo method into a one step update\n",
    "\n",
    "To do this, we reacquainted ourselves with the Bellman equation\n",
    "\n",
    "The definition of the value function has $G$ inside the expected value, \n",
    "\n",
    "$$\\large V_\\pi(s) = E \\left[ G_t \\vert S_t =s \\right] \\approx \\frac{1}{N} \\sum^N_{i=1}G_{i,s}$$\n",
    "\n",
    "but the Bellman equation replaces $G$ with $r+ \\gamma V(s^\\prime)$\n",
    "\n",
    "$$\\large V_\\pi(s) = E_\\pi \\left[ R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\vert S_t = s\\right]$$\n",
    "\n",
    "This makes it so that the value function depends on other values in the value function\n",
    "\n",
    "And of course, this is easier to sample\n",
    "\n",
    "In order to find $G$, we have to play an entire episode and sum up all the rewards\n",
    "\n",
    "But finding $r + \\gamma V(s^\\prime)$ only requires a single step into the next state $s^\\prime$\n",
    "\n",
    "---\n",
    "\n",
    "<h3>Bootstrapping</h3>\n",
    "\n",
    "We can also make note of how temporal difference learning brings us back to the concept of bootstrapping\n",
    "\n",
    "We first learned about bootstrapping in the context of dynamic programming, where we updated the value function using the existing value function\n",
    "\n",
    "As we kept iterating, our estimate improved\n",
    "\n",
    "The same thing happens with temporal difference learning\n",
    "\n",
    "We can imagine that when we first start the so called target, $r + \\gamma V(s^\\prime)$, is not accurate because we don't actually know $V(s^\\prime)$\n",
    "\n",
    "However, as we keep iterating, all the values become more accurate and this target too becomes more accurate\n",
    "\n",
    "---\n",
    "\n",
    "<h3>On-Policy vs Off-Policy</h3>\n",
    "\n",
    "We also learned about the important difference between on-policy and off-policy methods \n",
    "\n",
    "With on-policy methods like SARSA, our behavior policy and our target policy are the same\n",
    "\n",
    "This means that if our behavior policy is suboptimal, as in Epsilon greedy, then our target policy is also suboptimal \n",
    "\n",
    "With off-policy methods like Q-learning this need not be the case\n",
    "\n",
    "Q-Learning is one example of an off-policy method where a behavior policy does not need to match the target policy \n",
    "\n",
    "Because our target in the $Q$ update always uses the max, we're effectively asking to learn the value for the greedy action rather than the action we actually took"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
